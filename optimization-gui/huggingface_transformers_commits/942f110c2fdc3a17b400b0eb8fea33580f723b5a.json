{
    "author": "Wauplin",
    "message": "[style] Use 'x | y' syntax for processors as well (#43189)\n\n* Use 'x | y' syntax for type annotations\n\n* reapply style etc\n\n* fix type using themselves....\n\n* fix\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "942f110c2fdc3a17b400b0eb8fea33580f723b5a",
    "files": [
        {
            "sha": "048e997cfb7bfb6b85baf1b2d4a44f6588672546",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -50,16 +50,6 @@ extend-safe-fixes = [\n \"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n \"src/transformers/file_utils.py\" = [\"F401\"]\n \"src/transformers/utils/dummy_*.py\" = [\"F401\"]\n-# type validation does not work with `x | y` syntax, so we have to exclude all these files....\n-# TODO: really needs to be upstreamed in huggingface_hub\n-\"src/transformers/processing_utils.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/image_processing_utils_fast.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/video_processing_utils.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/image_utils.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/video_utils.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/models/**/image_processing_*.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/models/**/processing_*.py\" = [\"UP045\", \"UP007\"]\n-\"src/transformers/models/**/video_processing_*.py\" = [\"UP045\", \"UP007\"]\n \n [tool.ruff.lint.isort]\n lines-after-imports = 2"
        },
        {
            "sha": "6e80a8d76e69cea585ee54a8d0cdf72dedfdf148",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 53,
            "deletions": 53,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -73,17 +73,17 @@\n \n @lru_cache(maxsize=10)\n def validate_fast_preprocess_arguments(\n-    do_rescale: Optional[bool] = None,\n-    rescale_factor: Optional[float] = None,\n-    do_normalize: Optional[bool] = None,\n-    image_mean: Optional[Union[float, list[float]]] = None,\n-    image_std: Optional[Union[float, list[float]]] = None,\n-    do_center_crop: Optional[bool] = None,\n-    crop_size: Optional[SizeDict] = None,\n-    do_resize: Optional[bool] = None,\n-    size: Optional[SizeDict] = None,\n+    do_rescale: bool | None = None,\n+    rescale_factor: float | None = None,\n+    do_normalize: bool | None = None,\n+    image_mean: float | list[float] | None = None,\n+    image_std: float | list[float] | None = None,\n+    do_center_crop: bool | None = None,\n+    crop_size: SizeDict | None = None,\n+    do_resize: bool | None = None,\n+    size: SizeDict | None = None,\n     interpolation: Optional[\"F.InterpolationMode\"] = None,\n-    return_tensors: Optional[Union[str, TensorType]] = None,\n+    return_tensors: str | TensorType | None = None,\n     data_format: ChannelDimension = ChannelDimension.FIRST,\n ):\n     \"\"\"\n@@ -110,7 +110,7 @@ def validate_fast_preprocess_arguments(\n         raise ValueError(\"Only channel first data format is currently supported.\")\n \n \n-def safe_squeeze(tensor: \"torch.Tensor\", axis: Optional[int] = None) -> \"torch.Tensor\":\n+def safe_squeeze(tensor: \"torch.Tensor\", axis: int | None = None) -> \"torch.Tensor\":\n     \"\"\"\n     Squeezes a tensor, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -347,11 +347,11 @@ def pad(\n         self,\n         images: list[\"torch.Tensor\"],\n         pad_size: SizeDict = None,\n-        fill_value: Optional[int] = 0,\n-        padding_mode: Optional[str] = \"constant\",\n+        fill_value: int | None = 0,\n+        padding_mode: str | None = \"constant\",\n         return_mask: bool = False,\n-        disable_grouping: Optional[bool] = False,\n-        is_nested: Optional[bool] = False,\n+        disable_grouping: bool | None = False,\n+        is_nested: bool | None = False,\n         **kwargs,\n     ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n@@ -518,8 +518,8 @@ def rescale(\n     def normalize(\n         self,\n         image: \"torch.Tensor\",\n-        mean: Union[float, Iterable[float]],\n-        std: Union[float, Iterable[float]],\n+        mean: float | Iterable[float],\n+        std: float | Iterable[float],\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -541,11 +541,11 @@ def normalize(\n     @lru_cache(maxsize=10)\n     def _fuse_mean_std_and_rescale_factor(\n         self,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n         device: Optional[\"torch.device\"] = None,\n     ) -> tuple:\n         if do_rescale and do_normalize:\n@@ -561,8 +561,8 @@ def rescale_and_normalize(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, list[float]],\n-        image_std: Union[float, list[float]],\n+        image_mean: float | list[float],\n+        image_std: float | list[float],\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Rescale and normalize images.\n@@ -675,8 +675,8 @@ def _prepare_images_structure(\n     def _process_image(\n         self,\n         image: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: bool | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         device: Optional[\"torch.device\"] = None,\n     ) -> \"torch.Tensor\":\n         image_type = get_image_type(image)\n@@ -713,8 +713,8 @@ def _process_image(\n     def _prepare_image_like_inputs(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: bool | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         device: Optional[\"torch.device\"] = None,\n         expected_ndims: int = 3,\n     ) -> list[\"torch.Tensor\"]:\n@@ -756,13 +756,13 @@ def _prepare_image_like_inputs(\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n-        crop_size: Optional[SizeDict] = None,\n-        pad_size: Optional[SizeDict] = None,\n-        default_to_square: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        size: SizeDict | None = None,\n+        crop_size: SizeDict | None = None,\n+        pad_size: SizeDict | None = None,\n+        default_to_square: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -804,18 +804,18 @@ def _further_process_kwargs(\n \n     def _validate_preprocess_kwargs(\n         self,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, tuple[float]]] = None,\n-        image_std: Optional[Union[float, tuple[float]]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[SizeDict] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[SizeDict] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | tuple[float] | None = None,\n+        image_std: float | tuple[float] | None = None,\n+        do_resize: bool | None = None,\n+        size: SizeDict | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: SizeDict | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -873,7 +873,7 @@ def _preprocess_image_like_inputs(\n         *args,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[ImagesKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -898,12 +898,12 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        pad_size: Optional[SizeDict],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        pad_size: SizeDict | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "1328660c71a51fe942c5891d16662c1bff2ce0e5",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -91,7 +91,7 @@ class AnnotionFormat(ExplicitEnum):\n     COCO_PANOPTIC = AnnotationFormat.COCO_PANOPTIC.value\n \n \n-AnnotationType = dict[str, Union[int, str, list[dict]]]\n+AnnotationType = dict[str, int | str | list[dict]]\n \n \n def is_pil_image(img):\n@@ -944,12 +944,12 @@ class SizeDict:\n     Hashable dictionary to store image size information.\n     \"\"\"\n \n-    height: Optional[int] = None\n-    width: Optional[int] = None\n-    longest_edge: Optional[int] = None\n-    shortest_edge: Optional[int] = None\n-    max_height: Optional[int] = None\n-    max_width: Optional[int] = None\n+    height: int | None = None\n+    width: int | None = None\n+    longest_edge: int | None = None\n+    shortest_edge: int | None = None\n+    max_height: int | None = None\n+    max_width: int | None = None\n \n     def __getitem__(self, key):\n         if hasattr(self, key):"
        },
        {
            "sha": "0fbac07917264d4c0b0988becb13b2a9bf0a067f",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n import re\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -96,9 +95,9 @@ def _get_audio_token_length(self, audio_lengths: \"torch.Tensor\") -> \"torch.Tenso\n \n     def __call__(\n         self,\n-        text: Union[TextInput, list[TextInput]],\n-        audio: Optional[AudioInput] = None,\n-        output_labels: Optional[bool] = False,\n+        text: TextInput | list[TextInput],\n+        audio: AudioInput | None = None,\n+        output_labels: bool | None = False,\n         **kwargs: Unpack[AudioFlamingo3ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -203,8 +202,8 @@ def model_input_names(self) -> list[str]:\n \n     def apply_transcription_request(\n         self,\n-        audio: Union[str, list[str], AudioInput],\n-        prompt: Optional[Union[str, list[str]]] = None,\n+        audio: str | list[str] | AudioInput,\n+        prompt: str | list[str] | None = None,\n         **kwargs: Unpack[AudioFlamingo3ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -227,7 +226,7 @@ def apply_transcription_request(\n         \"\"\"\n \n         if isinstance(audio, str):\n-            audio_items: list[Union[str, np.ndarray]] = [audio]\n+            audio_items: list[str | np.ndarray] = [audio]\n         elif isinstance(audio, (list, tuple)) and audio and all(isinstance(el, str) for el in audio):\n             audio_items = list(audio)\n         else:"
        },
        {
            "sha": "381d9a75c69352a66d1c10d0b4d90748e44f6d75",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n # Build the list of all image processors\n from ...configuration_utils import PreTrainedConfig\n@@ -55,7 +55,7 @@\n if TYPE_CHECKING:\n     # This significantly improves completion suggestion performance when\n     # the transformers package is used with Microsoft's Pylance language server.\n-    IMAGE_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[Optional[str], Optional[str]]] = OrderedDict()\n+    IMAGE_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[str | None, str | None]] = OrderedDict()\n else:\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n@@ -259,12 +259,12 @@ def get_image_processor_class_from_name(class_name: str):\n \n \n def get_image_processor_config(\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    pretrained_model_name_or_path: str | os.PathLike,\n+    cache_dir: str | os.PathLike | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n     **kwargs,\n ):"
        },
        {
            "sha": "fe46e4de5a0eadc2a27580d5926b85cb718436f5",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n import importlib\n import os\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n # Build the list of all video processors\n from ...configuration_utils import PreTrainedConfig\n@@ -48,7 +48,7 @@\n if TYPE_CHECKING:\n     # This significantly improves completion suggestion performance when\n     # the transformers package is used with Microsoft's Pylance language server.\n-    VIDEO_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[Optional[str], Optional[str]]] = OrderedDict()\n+    VIDEO_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[str | None, str | None]] = OrderedDict()\n else:\n     VIDEO_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n@@ -116,12 +116,12 @@ def video_processor_class_from_name(class_name: str):\n \n \n def get_video_processor_config(\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    pretrained_model_name_or_path: str | os.PathLike,\n+    cache_dir: str | os.PathLike | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n     **kwargs,\n ):"
        },
        {
            "sha": "02ff82c92abc05fd32f68b0159e4247fed6e2c2b",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -118,8 +117,8 @@ def _prompt_split_image(self, num_patches):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[AyaVisionProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "e9b360c49279a4b4c07f31508973e5eea26dc6d8",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,6 @@\n \n import json\n import os\n-from typing import Optional\n \n import numpy as np\n \n@@ -167,7 +166,7 @@ def save_pretrained(\n \n         super().save_pretrained(save_directory, push_to_hub, **kwargs)\n \n-    def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n+    def _load_voice_preset(self, voice_preset: str | None = None, **kwargs):\n         voice_preset_paths = self.speaker_embeddings[voice_preset]\n \n         voice_preset_dict = {}\n@@ -203,7 +202,7 @@ def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n \n         return voice_preset_dict\n \n-    def _validate_voice_preset_dict(self, voice_preset: Optional[dict] = None):\n+    def _validate_voice_preset_dict(self, voice_preset: dict | None = None):\n         for key in [\"semantic_prompt\", \"coarse_prompt\", \"fine_prompt\"]:\n             if key not in voice_preset:\n                 raise ValueError(f\"Voice preset unrecognized, missing {key} as a key.\")"
        },
        {
            "sha": "df39bef555b5df59e533b7e7d6faf1f85082b12f",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 53,
            "deletions": 55,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Beit.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import INIT_SERVICE_KWARGS, BaseImageProcessor, BatchFeature, get_size_dict\n@@ -116,15 +114,15 @@ class BeitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        crop_size: dict[str, int] | None = None,\n+        rescale_factor: int | float = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -150,8 +148,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -192,18 +190,18 @@ def reduce_label(self, label: ImageInput) -> np.ndarray:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_reduce_labels: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_reduce_labels: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_reduce_labels:\n             image = self.reduce_label(image)\n@@ -225,18 +223,18 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -270,13 +268,13 @@ def _preprocess_image(\n     def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_reduce_labels: bool | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"Preprocesses a single segmentation map.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -317,21 +315,21 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -463,7 +461,7 @@ def preprocess(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "e1fe1fb19cbaecdf309970244116528190075762",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -73,7 +73,7 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[BeitImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -85,10 +85,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n+        segmentation_maps: ImageInput | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[BeitImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -130,10 +130,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if do_reduce_labels:\n@@ -165,7 +165,7 @@ def _preprocess(\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "6586d180c236d329e195dda6caf87e58ca8ec132",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for BiT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -91,15 +89,15 @@ class BitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -127,8 +125,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -175,20 +173,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "77d0696e9473fb08debfbb2ff5add80025e69ccf",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for BLIP.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -82,13 +80,13 @@ class BlipImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -112,8 +110,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -159,18 +157,18 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        do_convert_rgb: bool | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "a7e329e351a72a250a022f177493123eb70ac2a5",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Blip.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n@@ -48,8 +46,8 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[BlipProcessorKwargs],\n     ) -> BatchEncoding:\n         if images is None and text is None:"
        },
        {
            "sha": "e339854a6736ec1c5e8e23b31644e6533d4c27c8",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for BLIP-2.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -63,8 +61,8 @@ def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[Blip2ProcessorKwargs],\n     ) -> BatchEncoding:\n         if images is None and text is None:"
        },
        {
            "sha": "2b7a89b040d83d82a64ecc58762546578c17eb52",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Image processor class for BridgeTower.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -54,7 +54,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.vilt.image_processing_vilt.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -73,7 +73,7 @@ def make_pixel_mask(\n \n # Copied from transformers.models.vilt.image_processing_vilt.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -96,7 +96,7 @@ def get_resize_output_image_size(\n     shorter: int = 800,\n     longer: int = 1333,\n     size_divisor: int = 32,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     input_height, input_width = get_image_size(input_image, input_data_format)\n     min_size, max_size = shorter, longer\n@@ -178,16 +178,16 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -215,8 +215,8 @@ def resize(\n         size: dict[str, int],\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -261,8 +261,8 @@ def center_crop(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -294,9 +294,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -321,11 +321,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -375,21 +375,21 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "9524dbc26b2e553a32682c8933a71a0a0569b8aa",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Fast Image processor class for BridgeTower.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -182,7 +182,7 @@ def _pad_image(\n         self,\n         image: \"torch.Tensor\",\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -205,18 +205,18 @@ def _preprocess(\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n         size: SizeDict,\n-        size_divisor: Optional[int],\n+        size_divisor: int | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_pad: bool,\n         do_center_crop: bool,\n         crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "3406279ca0d818130d968ac115adb60faf8462cf",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Chameleon.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -83,15 +81,15 @@ class ChameleonImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PIL.Image.LANCZOS,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 0.0078,\n+        rescale_factor: int | float = 0.0078,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -119,8 +117,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -167,20 +165,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "c2773aa1b506a2e17700aae3e977a39631405702",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Chameleon.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -83,8 +81,8 @@ def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, ima\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[ChameleonProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "91694b5024eb3169fdb6949be61e00de5823204f",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Chinese-CLIP.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -95,15 +93,15 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -130,8 +128,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -168,20 +166,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "1dddcdc1f234445b9ebfa559de470ef41727a7a0",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for CLIP.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -94,15 +92,15 @@ class CLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -154,8 +152,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -201,20 +199,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> PIL.Image.Image:\n         \"\"\""
        },
        {
            "sha": "51edaec70e88d8255771f95af4c1056f4e71002c",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \n from functools import lru_cache\n-from typing import Optional, Union\n+from typing import Optional\n \n import numpy as np\n import torch\n@@ -135,7 +135,7 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n-        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        patch_size: tuple | int | dict | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n@@ -213,10 +213,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches:"
        },
        {
            "sha": "95f2872790dd11f7565d2c7cdd354a7b914a2a90",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -63,8 +62,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[Cohere2VisionProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "625baaa940c68e39cf51aaabae6d84bd59f53641",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 64,
            "deletions": 64,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import pathlib\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -81,9 +81,9 @@\n # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -112,7 +112,7 @@ def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -144,7 +144,7 @@ def get_image_size_for_max_height_width(\n \n \n # Copied from transformers.models.detr.image_processing_detr.safe_squeeze\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -182,7 +182,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -201,7 +201,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -258,7 +258,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by ConditionalDetr.\n@@ -353,9 +353,9 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for ConditionalDetr.\n@@ -674,8 +674,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -744,11 +744,11 @@ class ConditionalDetrImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n @requires(backends=(\"vision\",))\n@@ -811,18 +811,18 @@ class ConditionalDetrImageProcessor(BaseImageProcessor):\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n@@ -873,10 +873,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into ConditionalDetr model.\n@@ -907,8 +907,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -981,8 +981,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -1062,10 +1062,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1095,14 +1095,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -1179,24 +1179,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        do_convert_annotations: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1426,7 +1426,7 @@ def preprocess(\n \n     # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection with DeformableDetr->ConditionalDetr\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n+        self, outputs, threshold: float = 0.5, target_sizes: TensorType | list[tuple] = None, top_k: int = 100\n     ):\n         \"\"\"\n         Converts the raw output of [`ConditionalDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n@@ -1485,7 +1485,7 @@ def post_process_object_detection(\n         return results\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation with Detr->ConditionalDetr\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple[int, int]] | None = None):\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n@@ -1539,8 +1539,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n@@ -1623,8 +1623,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into image panoptic segmentation predictions. Only supports"
        },
        {
            "sha": "715ee8175305ae9cb32b003771ddc724f25441a2",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_conditional_detr.py file directly. One of our CI enforces this.\n #                \n import pathlib\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n from torch import nn\n@@ -88,7 +88,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by CONDITIONAL_DETR.\n@@ -199,9 +199,9 @@ def rgb_to_id(color):\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for CONDITIONAL_DETR.\n@@ -281,10 +281,10 @@ def prepare_annotation(\n         self,\n         image: torch.Tensor,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into CONDITIONAL_DETR model.\n@@ -470,7 +470,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):\n@@ -499,8 +499,8 @@ def pad(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]],\n-        masks_path: Optional[Union[str, pathlib.Path]],\n+        annotations: AnnotationType | list[AnnotationType] | None,\n+        masks_path: str | pathlib.Path | None,\n         return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -509,12 +509,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_convert_annotations: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        pad_size: Optional[SizeDict],\n-        format: Optional[Union[str, AnnotationFormat]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        pad_size: SizeDict | None,\n+        format: str | AnnotationFormat | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -671,7 +671,7 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple[int, int]] | None = None):\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n@@ -724,8 +724,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n@@ -807,8 +807,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into image panoptic segmentation predictions. Only supports"
        },
        {
            "sha": "6bc2fdb281045fc03f884b7425d0f9b1ef9d1b85",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 18,
            "deletions": 20,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for ConvNeXT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -102,14 +100,14 @@ class ConvNextImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        crop_pct: Optional[float] = None,\n+        size: dict[str, int] | None = None,\n+        crop_pct: float | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -133,8 +131,8 @@ def resize(\n         size: dict[str, int],\n         crop_pct: float,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -200,18 +198,18 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        crop_pct: Optional[float] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        crop_pct: float | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "ad05c912e10164e72f1fc7a2accfe4e28ffe7b0e",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for ConvNeXT.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -127,10 +127,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "f96dd26f1b5fe9ff371239b26a8876727f09c9e0",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \n import math\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -42,7 +42,7 @@ class CsmAudioKwargs(AudioKwargs, total=False):\n         audio input in the text sequence.\n     \"\"\"\n \n-    encoded_length_kwargs: Optional[dict[str, Any]]\n+    encoded_length_kwargs: dict[str, Any] | None\n \n \n class CsmProcessorKwargs(ProcessingKwargs, total=False):\n@@ -131,7 +131,7 @@ def _get_encoded_length(audio_length, kernel_sizes=None, strides=None, dilations\n     def save_audio(\n         self,\n         audio: AudioInput,\n-        saving_path: Union[str, Path, list[Union[str, Path]]],\n+        saving_path: str | Path | list[str | Path],\n         **kwargs: Unpack[CsmProcessorKwargs],\n     ):\n         # TODO: @eustlb, this should be in AudioProcessor\n@@ -165,10 +165,10 @@ def save_audio(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]],\n-        audio: Optional[AudioInput] = None,\n-        output_labels: Optional[bool] = False,\n-        depth_decoder_labels_ratio: Optional[float] = 1.0,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None,\n+        audio: AudioInput | None = None,\n+        output_labels: bool | None = False,\n+        depth_decoder_labels_ratio: float | None = 1.0,\n         **kwargs: Unpack[CsmProcessorKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "5b4cb4826179dd950b2dacd3d15dc2ea9cba7671",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n import torch.nn.functional as F\n@@ -83,7 +83,7 @@ def resize(\n     def pad_to_square(\n         self,\n         images: \"torch.Tensor\",\n-        background_color: Union[int, tuple[int, int, int]] = 0,\n+        background_color: int | tuple[int, int, int] = 0,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n@@ -140,10 +140,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> BatchFeature:"
        },
        {
            "sha": "b606ea2219e026d0f6cb86f31cb75f80af22ae83",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -104,7 +104,7 @@ def resize(\n     def pad_to_square(\n         self,\n         images: \"torch.Tensor\",\n-        background_color: Union[int, tuple[int, int, int]] = 0,\n+        background_color: int | tuple[int, int, int] = 0,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pads an image to a square based on the longest edge."
        },
        {
            "sha": "364ead0e8b292419e846fb0c10ae5f4233e65f3b",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 59,
            "deletions": 59,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import pathlib\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -97,21 +97,21 @@ class DeformableDetrImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -140,7 +140,7 @@ def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -172,7 +172,7 @@ def get_image_size_for_max_height_width(\n \n \n # Copied from transformers.models.detr.image_processing_detr.safe_squeeze\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -210,7 +210,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -229,7 +229,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -286,7 +286,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by DeformableDetr.\n@@ -381,9 +381,9 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for DeformableDetr.\n@@ -702,8 +702,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -816,18 +816,18 @@ class DeformableDetrImageProcessor(BaseImageProcessor):\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n@@ -878,10 +878,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into DeformableDetr model.\n@@ -912,8 +912,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -986,8 +986,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -1067,10 +1067,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1100,14 +1100,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -1184,24 +1184,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        do_convert_annotations: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1430,7 +1430,7 @@ def preprocess(\n         return encoded_inputs\n \n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n+        self, outputs, threshold: float = 0.5, target_sizes: TensorType | list[tuple] = None, top_k: int = 100\n     ):\n         \"\"\"\n         Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,"
        },
        {
            "sha": "58b31ec5def0719a092abd203595a0bb92fa803a",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_deformable_detr.py file directly. One of our CI enforces this.\n #                \n import pathlib\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n from torchvision.io import read_image\n@@ -79,7 +79,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by DEFORMABLE_DETR.\n@@ -190,9 +190,9 @@ def rgb_to_id(color):\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for DEFORMABLE_DETR.\n@@ -272,10 +272,10 @@ def prepare_annotation(\n         self,\n         image: torch.Tensor,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into DEFORMABLE_DETR model.\n@@ -461,7 +461,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):\n@@ -490,8 +490,8 @@ def pad(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]],\n-        masks_path: Optional[Union[str, pathlib.Path]],\n+        annotations: AnnotationType | list[AnnotationType] | None,\n+        masks_path: str | pathlib.Path | None,\n         return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -500,12 +500,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_convert_annotations: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        pad_size: Optional[SizeDict],\n-        format: Optional[Union[str, AnnotationFormat]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        pad_size: SizeDict | None,\n+        format: str | AnnotationFormat | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "fda2c9e485b072ac46e73baf27e632d3a1e3a770",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 18,
            "deletions": 20,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for DeiT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -83,15 +81,15 @@ class DeiTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PIL.Image.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        crop_size: dict[str, int] | None = None,\n+        rescale_factor: int | float = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -117,8 +115,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -164,19 +162,19 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "a454d68a42839a4b8b3660f5ef3065fe94fda48a",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for DepthPro.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -88,13 +88,13 @@ class DepthProImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -114,8 +114,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -174,9 +174,9 @@ def _validate_input_arguments(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, list[float]],\n-        image_std: Union[float, list[float]],\n-        data_format: Union[str, ChannelDimension],\n+        image_mean: float | list[float],\n+        image_std: float | list[float],\n+        data_format: str | ChannelDimension,\n     ):\n         if do_resize and None in (size, resample):\n             raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n@@ -191,17 +191,17 @@ def _validate_input_arguments(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -311,7 +311,7 @@ def preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthProDepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Post-processes the raw depth predictions from the model to generate"
        },
        {
            "sha": "3542368e4f152e89c5581b4d8819359de67f961c",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for DepthPro.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Optional\n \n import torch\n \n@@ -69,10 +69,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched scaling\n@@ -100,7 +100,7 @@ def _preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthProDepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Post-processes the raw depth predictions from the model to generate"
        },
        {
            "sha": "128619b4031c40474ccac0cd9e6ce21e8bf6b7b3",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 64,
            "deletions": 64,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import pathlib\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -99,19 +99,19 @@ class DetrImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n # From the original repo: https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/datasets/transforms.py#L76\n def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -144,9 +144,9 @@ def get_image_size_for_max_height_width(\n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -170,7 +170,7 @@ def get_resize_output_image_size(\n     return get_size_with_aspect_ratio(image_size, size, max_size)\n \n \n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -207,7 +207,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.vilt.image_processing_vilt.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -226,7 +226,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.vilt.image_processing_vilt.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -283,7 +283,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by DETR.\n@@ -376,9 +376,9 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for DETR.\n@@ -687,8 +687,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -800,18 +800,18 @@ class DetrImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n@@ -861,10 +861,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into DETR model.\n@@ -894,8 +894,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -967,8 +967,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -1045,10 +1045,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1077,14 +1077,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -1160,24 +1160,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        do_convert_annotations: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1407,7 +1407,7 @@ def preprocess(\n \n     # inspired by https://github.com/facebookresearch/detr/blob/master/models/detr.py#L258\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: TensorType | list[tuple] | None = None\n     ):\n         \"\"\"\n         Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -1459,7 +1459,7 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple[int, int]] | None = None):\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n@@ -1513,8 +1513,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n@@ -1597,8 +1597,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into image panoptic segmentation predictions. Only supports"
        },
        {
            "sha": "4ab2d1ed903c13d0670953179f90d5dbf52a5885",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Fast Image processor class for DETR.\"\"\"\n \n import pathlib\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n from torch import nn\n@@ -101,7 +101,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by DETR.\n@@ -212,9 +212,9 @@ def rgb_to_id(color):\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for DETR.\n@@ -294,10 +294,10 @@ def prepare_annotation(\n         self,\n         image: torch.Tensor,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into DETR model.\n@@ -483,7 +483,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):\n@@ -512,8 +512,8 @@ def pad(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]],\n-        masks_path: Optional[Union[str, pathlib.Path]],\n+        annotations: AnnotationType | list[AnnotationType] | None,\n+        masks_path: str | pathlib.Path | None,\n         return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -522,12 +522,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_convert_annotations: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        pad_size: Optional[SizeDict],\n-        format: Optional[Union[str, AnnotationFormat]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        pad_size: SizeDict | None,\n+        format: str | AnnotationFormat | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -627,7 +627,7 @@ def _preprocess(\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: TensorType | list[tuple] | None = None\n     ):\n         \"\"\"\n         Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -680,7 +680,7 @@ def post_process_object_detection(\n         return results\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple[int, int]] | None = None):\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n@@ -734,8 +734,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n@@ -818,8 +818,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into image panoptic segmentation predictions. Only supports"
        },
        {
            "sha": "56b4dbb36c2307c7542e3c2ab60fb9f633b93b0b",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n \n import math\n from pathlib import Path\n-from typing import Optional, Union\n \n from ...audio_utils import AudioInput, make_list_of_audio\n from ...feature_extraction_utils import BatchFeature\n@@ -94,9 +93,9 @@ def __init__(self, feature_extractor, tokenizer, audio_tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[str, list[str]],\n-        audio: Optional[AudioInput] = None,\n-        output_labels: Optional[bool] = False,\n+        text: str | list[str],\n+        audio: AudioInput | None = None,\n+        output_labels: bool | None = False,\n         **kwargs: Unpack[DiaProcessorKwargs],\n     ):\n         r\"\"\"\n@@ -267,7 +266,7 @@ def __call__(\n     def batch_decode(\n         self,\n         decoder_input_ids: \"torch.Tensor\",\n-        audio_prompt_len: Optional[int] = None,\n+        audio_prompt_len: int | None = None,\n         **kwargs: Unpack[DiaProcessorKwargs],\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n@@ -338,7 +337,7 @@ def batch_decode(\n     def decode(\n         self,\n         decoder_input_ids: \"torch.Tensor\",\n-        audio_prompt_len: Optional[int] = None,\n+        audio_prompt_len: int | None = None,\n         **kwargs: Unpack[DiaProcessorKwargs],\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -376,7 +375,7 @@ def get_audio_prompt_len(\n     def save_audio(\n         self,\n         audio: AudioInput,\n-        saving_path: Union[str, Path, list[Union[str, Path]]],\n+        saving_path: str | Path | list[str | Path],\n         **kwargs: Unpack[DiaProcessorKwargs],\n     ):\n         # TODO: @eustlb, this should be in AudioProcessor"
        },
        {
            "sha": "d21f2b5d2e5d97937a70096f4a8439b92985aa0a",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for DINOv3.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -56,10 +56,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "64f942f4008766e8e17857ad6ffc1cf3249df1d0",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 26,
            "deletions": 28,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Donut.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -107,16 +105,16 @@ class DonutImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_thumbnail: bool = True,\n         do_align_long_axis: bool = False,\n         do_pad: bool = True,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -143,8 +141,8 @@ def align_long_axis(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Align the long axis of the image to the longest axis of the specified size.\n@@ -191,8 +189,8 @@ def pad_image(\n         image: np.ndarray,\n         size: dict[str, int],\n         random_padding: bool = False,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad the image to the specified size.\n@@ -233,8 +231,8 @@ def thumbnail(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -283,8 +281,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -321,21 +319,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_thumbnail: Optional[bool] = None,\n-        do_align_long_axis: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_thumbnail: bool | None = None,\n+        do_align_long_axis: bool | None = None,\n+        do_pad: bool | None = None,\n         random_padding: bool = False,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "cef4b6d160c3d0a3c6974bc12786193e9aa0341e",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Donut.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -191,10 +191,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "1db500723f1ccff24215e95c42853897d78597f7",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n import re\n-from typing import Optional, Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -39,8 +38,8 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[DonutProcessorKwargs],\n     ):\n         if images is None and text is None:"
        },
        {
            "sha": "1584d23849fdc5a0b0c157b92791cfee0f726c2b",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 65,
            "deletions": 65,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from ...utils.import_utils import requires\n \n@@ -85,10 +85,10 @@ class DPTImageProcessorKwargs(ImagesKwargs, total=False):\n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    output_size: Union[int, Iterable[int]],\n+    output_size: int | Iterable[int],\n     keep_aspect_ratio: bool,\n     multiple: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n         x = round(val / multiple) * multiple\n@@ -176,17 +176,17 @@ class DPTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = False,\n-        size_divisor: Optional[int] = None,\n+        size_divisor: int | None = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -214,8 +214,8 @@ def resize(\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -264,8 +264,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         size_divisor: int,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Center pad an image to be a multiple of `multiple`.\n@@ -317,20 +317,20 @@ def reduce_label(self, label: ImageInput) -> np.ndarray:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_reduce_labels: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_reduce_labels: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_reduce_labels:\n             image = self.reduce_label(image)\n@@ -359,20 +359,20 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -410,13 +410,13 @@ def _preprocess_image(\n     def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        do_reduce_labels: bool | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"Preprocesses a single segmentation map.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -458,23 +458,23 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[int] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: int | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -605,7 +605,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->DPT\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`DPTForSemanticSegmentation`] into semantic segmentation maps.\n \n@@ -650,7 +650,7 @@ def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[lis\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "a9affae863942c66f17a55980a13bcd90c6a6a42",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -123,7 +123,7 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[DPTImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -135,10 +135,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n+        segmentation_maps: ImageInput | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[DPTImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -226,7 +226,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`DPTForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "3233a7e2523c8db57fefe8c6932a5ae5e0c203eb",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for SuperPoint.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ... import is_torch_available, is_vision_available\n@@ -62,7 +60,7 @@ class EfficientLoFTRImageProcessorKwargs(ImagesKwargs, total=False):\n # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n def is_grayscale(\n     image: np.ndarray,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:\n         if image.shape[0] == 1:\n@@ -77,7 +75,7 @@ def is_grayscale(\n # Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale\n def convert_to_grayscale(\n     image: ImageInput,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image.\n@@ -170,7 +168,7 @@ class EfficientLoFTRImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -193,8 +191,8 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -232,15 +230,15 @@ def resize(\n     def preprocess(\n         self,\n         images,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_grayscale: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_grayscale: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -345,7 +343,7 @@ def preprocess(\n     def post_process_keypoint_matching(\n         self,\n         outputs: \"EfficientLoFTRKeypointMatchingOutput\",\n-        target_sizes: Union[TensorType, list[tuple]],\n+        target_sizes: TensorType | list[tuple],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\""
        },
        {
            "sha": "4c69791f31bfe1424821408014a46614c07cb430",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_efficientloftr.py file directly. One of our CI enforces this.\n #                \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from PIL import Image, ImageDraw\n@@ -116,14 +116,14 @@ def _prepare_images_structure(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        size: Union[dict[str, int], SizeDict],\n+        size: dict[str, int] | SizeDict,\n         rescale_factor: float,\n         do_rescale: bool,\n         do_resize: bool,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_grayscale: bool,\n         disable_grouping: bool,\n-        return_tensors: Union[str, TensorType],\n+        return_tensors: str | TensorType,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "c6eb7b993d00247b7a71b589b38597c42be56b11",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 23,
            "deletions": 25,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for EfficientNet.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -100,16 +98,16 @@ class EfficientNetImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = False,\n-        crop_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        crop_size: dict[str, int] | None = None,\n+        rescale_factor: int | float = 1 / 255,\n         rescale_offset: bool = False,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         include_top: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -137,8 +135,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -183,10 +181,10 @@ def resize(\n     def rescale(\n         self,\n         image: np.ndarray,\n-        scale: Union[int, float],\n+        scale: int | float,\n         offset: bool = True,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -224,21 +222,21 @@ def rescale(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        rescale_offset: Optional[bool] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        include_top: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        rescale_offset: bool | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        include_top: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "ee772b01ebb014dd2bdec6b61fad7043ce7e36ce",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Fast Image processor class for EfficientNet.\"\"\"\n \n from functools import lru_cache\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -53,7 +53,7 @@ def rescale(\n         self,\n         image: \"torch.Tensor\",\n         scale: float,\n-        offset: Optional[bool] = True,\n+        offset: bool | None = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -88,13 +88,13 @@ def rescale(\n     @lru_cache(maxsize=10)\n     def _fuse_mean_std_and_rescale_factor(\n         self,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        rescale_offset: Optional[bool] = False,\n+        rescale_offset: bool | None = False,\n     ) -> tuple:\n         if do_rescale and do_normalize and not rescale_offset:\n             # Fused rescale and normalize\n@@ -109,8 +109,8 @@ def rescale_and_normalize(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, list[float]],\n-        image_std: Union[float, list[float]],\n+        image_mean: float | list[float],\n+        image_std: float | list[float],\n         rescale_offset: bool = False,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -146,10 +146,10 @@ def _preprocess(\n         rescale_offset: bool,\n         do_normalize: bool,\n         include_top: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "c1f677fa1a1a87ef41414bdb83d8f7a92b180e73",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 37,
            "deletions": 38,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -127,10 +126,10 @@ def __init__(\n         do_resize: bool = True,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         do_pad: bool = True,\n         min_pixels: int = 512 * 512,\n@@ -155,16 +154,16 @@ def __init__(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -252,8 +251,8 @@ def _pad_for_batching(\n         self,\n         pixel_values: list[np.ndarray],\n         image_sizes: list[list[int]],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n@@ -296,19 +295,19 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n         do_pad: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:\n@@ -415,13 +414,13 @@ def preprocess(\n     def postprocess(\n         self,\n         images: ImageInput,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Union[str, TensorType] = \"PIL.Image.Image\",\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType = \"PIL.Image.Image\",\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Postprocess an image or batch of images tensor. Postprocess is the reverse process of preprocess.\n@@ -491,9 +490,9 @@ def postprocess(\n     def unnormalize(\n         self,\n         image: np.ndarray,\n-        image_mean: Union[float, Iterable[float]],\n-        image_std: Union[float, Iterable[float]],\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        image_mean: float | Iterable[float],\n+        image_std: float | Iterable[float],\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`."
        },
        {
            "sha": "a0100e39c20b62ec4fdbc9a777fd2d5d558b2c28",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -79,8 +78,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[Emu3ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "9c41b78a6e0d898e2777756502e4cc355da91fe3",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 53,
            "deletions": 54,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for EoMT.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -66,14 +65,14 @@ class EomtImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     do_split_image: bool\n-    ignore_index: Optional[int]\n+    ignore_index: int | None\n \n \n # Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n     segmentation_map: np.ndarray,\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n ):\n     if ignore_index is not None:\n         segmentation_map = np.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n@@ -164,7 +163,7 @@ def compute_segments(\n     stuff_classes,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    target_size: Optional[tuple[int, int]] = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -270,17 +269,17 @@ class EomtImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n         do_split_image: bool = False,\n         do_pad: bool = False,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n-        num_labels: Optional[int] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n+        num_labels: int | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -307,7 +306,7 @@ def resize(\n         size: dict,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         data_format=None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -383,18 +382,18 @@ def _pad(self, image: ImageInput, size: dict) -> np.ndarray:\n     def _preprocess_images(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_split_image: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_split_image: bool | None = None,\n+        do_pad: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a batch of images.\"\"\"\n         images = [to_numpy_array(image) for image in images]\n@@ -443,12 +442,12 @@ def _preprocess_images(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = False,\n-        do_pad: Optional[bool] = False,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        data_format: Union[str, ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = False,\n+        do_pad: bool | None = False,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        data_format: str | ChannelDimension = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         # Add channel dimension if missing - needed for certain transformations\n@@ -481,22 +480,22 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        do_split_image: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        segmentation_maps: list[dict[int, int]] | dict[int, int] | None = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        do_split_image: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        do_pad: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocesses images or a batch of images.\n@@ -620,11 +619,11 @@ def preprocess(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        ignore_index: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -791,7 +790,7 @@ def post_process_semantic_segmentation(\n         self,\n         outputs,\n         target_sizes: list[tuple[int, int]],\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n     ) -> np.ndarray:\n         \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n \n@@ -838,8 +837,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.8,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        stuff_classes: Optional[list[int]] = None,\n-        size: Optional[dict[str, int]] = None,\n+        stuff_classes: list[int] | None = None,\n+        size: dict[str, int] | None = None,\n     ):\n         \"\"\"Post-processes model outputs into final panoptic segmentation prediction.\"\"\"\n \n@@ -894,7 +893,7 @@ def post_process_instance_segmentation(\n         outputs,\n         target_sizes: list[tuple[int, int]],\n         threshold: float = 0.5,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n     ):\n         \"\"\"Post-processes model outputs into Instance Segmentation Predictions.\"\"\"\n "
        },
        {
            "sha": "9b7adbf6aded724ee50c999baef2187e859e5522",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -51,8 +51,8 @@\n # Adapted from transformers.models.maskformer.image_processing_maskformer_fast.convert_segmentation_map_to_binary_masks_fast\n def convert_segmentation_map_to_binary_masks_fast(\n     segmentation_map: \"torch.Tensor\",\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n ):\n     if ignore_index is not None:\n         segmentation_map = torch.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n@@ -162,8 +162,8 @@ def _pad(self, images: torch.Tensor, size: dict) -> torch.Tensor:\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[list[torch.Tensor]] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+        segmentation_maps: list[torch.Tensor] | None = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n         **kwargs: Unpack[EomtImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -177,11 +177,11 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n-        instance_id_to_semantic_id: Optional[dict[int, int]],\n+        segmentation_maps: ImageInput | None,\n+        instance_id_to_semantic_id: dict[int, int] | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[EomtImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -253,10 +253,10 @@ def _preprocess(\n         do_normalize: bool,\n         do_split_image: bool,\n         do_pad: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ):\n         \"\"\"Preprocesses the input images and masks if provided.\"\"\"\n@@ -393,7 +393,7 @@ def post_process_semantic_segmentation(\n         self,\n         outputs,\n         target_sizes: list[tuple[int, int]],\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n     ) -> np.ndarray:\n         \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n \n@@ -440,8 +440,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.8,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        stuff_classes: Optional[list[int]] = None,\n-        size: Optional[dict[str, int]] = None,\n+        stuff_classes: list[int] | None = None,\n+        size: dict[str, int] | None = None,\n     ):\n         \"\"\"Post-processes model outputs into final panoptic segmentation prediction.\"\"\"\n \n@@ -496,7 +496,7 @@ def post_process_instance_segmentation(\n         outputs,\n         target_sizes: list[tuple[int, int]],\n         threshold: float = 0.8,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n     ):\n         \"\"\"Post-processes model outputs into Instance Segmentation Predictions.\"\"\"\n "
        },
        {
            "sha": "8822b80e21aeb712eb5dc2215374fc479663bfb9",
            "filename": "src/transformers/models/ernie4_5_vl_moe/image_processing_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,8 +17,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -297,21 +297,21 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "994ed65f9f3872549c9b115f39e1d867748098f6",
            "filename": "src/transformers/models/ernie4_5_vl_moe/image_processing_ernie4_5_vl_moe_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fimage_processing_ernie4_5_vl_moe_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -85,7 +85,7 @@ def __init__(self, **kwargs: Unpack[Ernie4_5_VL_MoeImageProcessorKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "e8699f5ec5f83609bbe053ecabd2219b82240499",
            "filename": "src/transformers/models/ernie4_5_vl_moe/processing_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fprocessing_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fprocessing_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fprocessing_ernie4_5_vl_moe.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n import os.path\n from pathlib import Path\n from shutil import SameFileError, copyfile\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -82,9 +81,9 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n \n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[Ernie4_5_VL_MoeProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "8142cba1fdca321a40206bc2b20ff1bd77e47647",
            "filename": "src/transformers/models/ernie4_5_vl_moe/video_processing_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fvideo_processing_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fvideo_processing_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fvideo_processing_ernie4_5_vl_moe.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n from functools import partial\n from pathlib import Path\n from shutil import SameFileError, copyfile\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import torch\n@@ -129,7 +129,7 @@ def __init__(self, **kwargs: Unpack[Ernie4_5_VL_MoeVideoProcessorInitKwargs]):\n \n     @classmethod\n     def get_video_processor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"Overriden to additionally load the font for drawing on frames.\"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n@@ -282,7 +282,7 @@ def to_dict(self) -> dict[str, Any]:\n \n         return output\n \n-    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n         \"\"\"We additionally save a copy of the font to the `save_directory` (if we found a file there)\"\"\"\n         os.makedirs(save_directory, exist_ok=True)\n \n@@ -296,7 +296,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -311,10 +311,10 @@ def _further_process_kwargs(\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        min_frames: Optional[int] = None,\n-        max_frames: Optional[int] = None,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        min_frames: int | None = None,\n+        max_frames: int | None = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         if fps is not None and num_frames is not None:\n@@ -386,9 +386,9 @@ def _render_image_with_timestamp(self, image: torch.Tensor, timestamp: str, size\n     def _prepare_input_videos(\n         self,\n         videos: VideoInput,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[str] = None,\n-        video_metadata: Optional[list[VideoMetadata]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        device: str | None = None,\n+        video_metadata: list[VideoMetadata] | None = None,\n         draw_on_frames: bool = True,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n@@ -447,16 +447,16 @@ def _preprocess(\n         videos: list[torch.Tensor],\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         # Group videos by size for batched resizing"
        },
        {
            "sha": "74a2def12093377bfe9aac6f369ccd7d8fdab5c0",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for EVOLLA.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import (\n     ProcessorMixin,\n@@ -89,10 +87,10 @@ def process_text(\n     @auto_docstring\n     def __call__(\n         self,\n-        proteins: Optional[Union[list[dict], dict]] = None,\n-        messages_list: Optional[Union[list[list[dict]], list[dict]]] = None,\n-        protein_max_length: Optional[int] = None,\n-        text_max_length: Optional[int] = None,\n+        proteins: list[dict] | dict | None = None,\n+        messages_list: list[list[dict]] | list[dict] | None = None,\n+        protein_max_length: int | None = None,\n+        text_max_length: int | None = None,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "5eb56b909552eca61ea85bb1523a89cd747361fa",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 66,
            "deletions": 66,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import random\n from collections.abc import Iterable\n from functools import lru_cache\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -132,23 +132,23 @@ class FlavaImageProcessorKwargs(ImagesKwargs, total=False):\n     codebook_do_center_crop: bool\n     codebook_crop_size: int\n     codebook_do_rescale: bool\n-    codebook_rescale_factor: Union[int, float]\n+    codebook_rescale_factor: int | float\n     codebook_do_map_pixels: bool\n     codebook_do_normalize: bool\n-    codebook_image_mean: Union[float, Iterable[float]]\n-    codebook_image_std: Union[float, Iterable[float]]\n+    codebook_image_mean: float | Iterable[float]\n+    codebook_image_std: float | Iterable[float]\n \n \n # Inspired from https://github.com/microsoft/unilm/blob/master/beit/masking_generator.py\n class FlavaMaskingGenerator:\n     def __init__(\n         self,\n-        input_size: Union[int, tuple[int, int]] = 14,\n+        input_size: int | tuple[int, int] = 14,\n         total_mask_patches: int = 75,\n-        mask_group_max_patches: Optional[int] = None,\n+        mask_group_max_patches: int | None = None,\n         mask_group_min_patches: int = 16,\n-        mask_group_min_aspect_ratio: Optional[float] = 0.3,\n-        mask_group_max_aspect_ratio: Optional[float] = None,\n+        mask_group_min_aspect_ratio: float | None = 0.3,\n+        mask_group_max_aspect_ratio: float | None = None,\n     ):\n         if not isinstance(input_size, tuple):\n             input_size = (input_size,) * 2\n@@ -313,36 +313,36 @@ class FlavaImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, Iterable[float]]] = None,\n-        image_std: Optional[Union[float, Iterable[float]]] = None,\n+        image_mean: float | Iterable[float] | None = None,\n+        image_std: float | Iterable[float] | None = None,\n         # Mask related params\n         return_image_mask: bool = False,\n         input_size_patches: int = 14,\n         total_mask_patches: int = 75,\n         mask_group_min_patches: int = 16,\n-        mask_group_max_patches: Optional[int] = None,\n+        mask_group_max_patches: int | None = None,\n         mask_group_min_aspect_ratio: float = 0.3,\n-        mask_group_max_aspect_ratio: Optional[float] = None,\n+        mask_group_max_aspect_ratio: float | None = None,\n         # Codebook related params\n         return_codebook_pixels: bool = False,\n         codebook_do_resize: bool = True,\n-        codebook_size: Optional[bool] = None,\n+        codebook_size: bool | None = None,\n         codebook_resample: int = PILImageResampling.LANCZOS,\n         codebook_do_center_crop: bool = True,\n-        codebook_crop_size: Optional[int] = None,\n+        codebook_crop_size: int | None = None,\n         codebook_do_rescale: bool = True,\n-        codebook_rescale_factor: Union[int, float] = 1 / 255,\n+        codebook_rescale_factor: int | float = 1 / 255,\n         codebook_do_map_pixels: bool = True,\n         codebook_do_normalize: bool = True,\n-        codebook_image_mean: Optional[Union[float, Iterable[float]]] = None,\n-        codebook_image_std: Optional[Union[float, Iterable[float]]] = None,\n+        codebook_image_mean: float | Iterable[float] | None = None,\n+        codebook_image_std: float | Iterable[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -427,8 +427,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -476,19 +476,19 @@ def map_pixels(self, image: np.ndarray) -> np.ndarray:\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_map_pixels: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[ChannelDimension] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_map_pixels: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n \n@@ -541,40 +541,40 @@ def _preprocess_image(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         # Mask related params\n-        return_image_mask: Optional[bool] = None,\n-        input_size_patches: Optional[int] = None,\n-        total_mask_patches: Optional[int] = None,\n-        mask_group_min_patches: Optional[int] = None,\n-        mask_group_max_patches: Optional[int] = None,\n-        mask_group_min_aspect_ratio: Optional[float] = None,\n-        mask_group_max_aspect_ratio: Optional[float] = None,\n+        return_image_mask: bool | None = None,\n+        input_size_patches: int | None = None,\n+        total_mask_patches: int | None = None,\n+        mask_group_min_patches: int | None = None,\n+        mask_group_max_patches: int | None = None,\n+        mask_group_min_aspect_ratio: float | None = None,\n+        mask_group_max_aspect_ratio: float | None = None,\n         # Codebook related params\n-        return_codebook_pixels: Optional[bool] = None,\n-        codebook_do_resize: Optional[bool] = None,\n-        codebook_size: Optional[dict[str, int]] = None,\n-        codebook_resample: Optional[int] = None,\n-        codebook_do_center_crop: Optional[bool] = None,\n-        codebook_crop_size: Optional[dict[str, int]] = None,\n-        codebook_do_rescale: Optional[bool] = None,\n-        codebook_rescale_factor: Optional[float] = None,\n-        codebook_do_map_pixels: Optional[bool] = None,\n-        codebook_do_normalize: Optional[bool] = None,\n-        codebook_image_mean: Optional[Iterable[float]] = None,\n-        codebook_image_std: Optional[Iterable[float]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_codebook_pixels: bool | None = None,\n+        codebook_do_resize: bool | None = None,\n+        codebook_size: dict[str, int] | None = None,\n+        codebook_resample: int | None = None,\n+        codebook_do_center_crop: bool | None = None,\n+        codebook_crop_size: dict[str, int] | None = None,\n+        codebook_do_rescale: bool | None = None,\n+        codebook_rescale_factor: float | None = None,\n+        codebook_do_map_pixels: bool | None = None,\n+        codebook_do_normalize: bool | None = None,\n+        codebook_image_mean: Iterable[float] | None = None,\n+        codebook_image_std: Iterable[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "71a5e80660f20a6a6c2c0ed8e56f61d2dfaf8878",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 42,
            "deletions": 42,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n import math\n import random\n from functools import lru_cache\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -46,12 +46,12 @@\n class FlavaMaskingGenerator:\n     def __init__(\n         self,\n-        input_size: Union[int, tuple[int, int]] = 14,\n+        input_size: int | tuple[int, int] = 14,\n         total_mask_patches: int = 75,\n-        mask_group_max_patches: Optional[int] = None,\n+        mask_group_max_patches: int | None = None,\n         mask_group_min_patches: int = 16,\n-        mask_group_min_aspect_ratio: Optional[float] = 0.3,\n-        mask_group_max_aspect_ratio: Optional[float] = None,\n+        mask_group_min_aspect_ratio: float | None = 0.3,\n+        mask_group_max_aspect_ratio: float | None = None,\n     ):\n         if not isinstance(input_size, tuple):\n             input_size = (input_size,) * 2\n@@ -199,17 +199,17 @@ def map_pixels(self, image: \"torch.Tensor\") -> \"torch.Tensor\":\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n-        crop_size: Optional[SizeDict] = None,\n-        default_to_square: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        codebook_size: Optional[SizeDict] = None,\n-        codebook_crop_size: Optional[SizeDict] = None,\n-        codebook_image_mean: Optional[Union[float, list[float]]] = None,\n-        codebook_image_std: Optional[Union[float, list[float]]] = None,\n-        codebook_resample: Optional[PILImageResampling] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        size: SizeDict | None = None,\n+        crop_size: SizeDict | None = None,\n+        default_to_square: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        codebook_size: SizeDict | None = None,\n+        codebook_crop_size: SizeDict | None = None,\n+        codebook_image_mean: float | list[float] | None = None,\n+        codebook_image_std: float | list[float] | None = None,\n+        codebook_resample: PILImageResampling | None = None,\n+        data_format: ChannelDimension | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -275,10 +275,10 @@ def _preprocess_image(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_map_pixels: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n     ) -> \"torch.Tensor\":\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n@@ -319,31 +319,31 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         # Mask related params\n-        return_image_mask: Optional[bool],\n-        input_size_patches: Optional[int],\n-        total_mask_patches: Optional[int],\n-        mask_group_min_patches: Optional[int],\n-        mask_group_max_patches: Optional[int],\n-        mask_group_min_aspect_ratio: Optional[float],\n-        mask_group_max_aspect_ratio: Optional[float],\n+        return_image_mask: bool | None,\n+        input_size_patches: int | None,\n+        total_mask_patches: int | None,\n+        mask_group_min_patches: int | None,\n+        mask_group_max_patches: int | None,\n+        mask_group_min_aspect_ratio: float | None,\n+        mask_group_max_aspect_ratio: float | None,\n         # Codebook related params\n-        return_codebook_pixels: Optional[bool],\n-        codebook_do_resize: Optional[bool],\n-        codebook_size: Optional[SizeDict],\n+        return_codebook_pixels: bool | None,\n+        codebook_do_resize: bool | None,\n+        codebook_size: SizeDict | None,\n         codebook_interpolation: Optional[\"F.InterpolationMode\"],\n-        codebook_do_center_crop: Optional[bool],\n-        codebook_crop_size: Optional[SizeDict],\n-        codebook_do_rescale: Optional[bool],\n-        codebook_rescale_factor: Optional[float],\n-        codebook_do_map_pixels: Optional[bool],\n-        codebook_do_normalize: Optional[bool],\n-        codebook_image_mean: Optional[Union[float, list[float]]],\n-        codebook_image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        codebook_do_center_crop: bool | None,\n+        codebook_crop_size: SizeDict | None,\n+        codebook_do_rescale: bool | None,\n+        codebook_rescale_factor: float | None,\n+        codebook_do_map_pixels: bool | None,\n+        codebook_do_normalize: bool | None,\n+        codebook_image_mean: float | list[float] | None,\n+        codebook_image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         processed_images = self._preprocess_image("
        },
        {
            "sha": "c8024210ed9cf359fdffe2746661a3d69ff285ff",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for Fuyu.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -57,7 +56,7 @@\n \n \n def make_list_of_list_of_images(\n-    images: Union[list[list[ImageInput]], list[ImageInput], ImageInput],\n+    images: list[list[ImageInput]] | list[ImageInput] | ImageInput,\n ) -> list[list[ImageInput]]:\n     if is_valid_image(images):\n         return [[images]]\n@@ -81,7 +80,7 @@ class FuyuImagesKwargs(ImagesKwargs, total=False):\n         The padding mode to use when padding the image.\n     \"\"\"\n \n-    patch_size: Optional[SizeDict]\n+    patch_size: SizeDict | None\n     padding_value: float\n     padding_mode: str\n \n@@ -93,7 +92,7 @@ class FuyuBatchFeature(BatchFeature):\n     The outputs dictionary from the processors contains a mix of tensors and lists of tensors.\n     \"\"\"\n \n-    def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = None, **kwargs):\n+    def convert_to_tensors(self, tensor_type: str | TensorType | None = None, **kwargs):\n         \"\"\"\n         Convert the inner content to tensors.\n \n@@ -253,17 +252,17 @@ class FuyuImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_pad: bool = True,\n         padding_value: float = 1.0,\n         padding_mode: str = \"constant\",\n         do_normalize: bool = True,\n-        image_mean: Union[float, list[float]] = 0.5,\n-        image_std: Union[float, list[float]] = 0.5,\n+        image_mean: float | list[float] = 0.5,\n+        image_std: float | list[float] = 0.5,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -285,8 +284,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -344,8 +343,8 @@ def pad_image(\n         size: dict[str, int],\n         mode: str = \"constant\",\n         constant_values: float = 1.0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image to `(size[\"height\"], size[\"width\"])`.\n@@ -380,21 +379,21 @@ def pad_image(\n     def preprocess(\n         self,\n         images,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_pad: Optional[bool] = None,\n-        padding_value: Optional[float] = None,\n-        padding_mode: Optional[str] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[float] = None,\n-        image_std: Optional[float] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        patch_size: Optional[dict[str, int]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        return_tensors: Optional[TensorType] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_pad: bool | None = None,\n+        padding_value: float | None = None,\n+        padding_mode: str | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | None = None,\n+        image_std: float | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        patch_size: dict[str, int] | None = None,\n+        data_format: str | ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        return_tensors: TensorType | None = None,\n     ):\n         \"\"\"\n \n@@ -556,7 +555,7 @@ def preprocess(\n         }\n         return FuyuBatchFeature(data=data, tensor_type=return_tensors)\n \n-    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[dict[str, int]] = None) -> int:\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: dict[str, int] | None = None) -> int:\n         \"\"\"\n         Calculate number of patches required to encode an image.\n \n@@ -581,7 +580,7 @@ def get_num_patches(self, image_height: int, image_width: int, patch_size: Optio\n         num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n         return num_patches\n \n-    def patchify_image(self, image: \"torch.Tensor\", patch_size: Optional[dict[str, int]] = None) -> \"torch.Tensor\":\n+    def patchify_image(self, image: \"torch.Tensor\", patch_size: dict[str, int] | None = None) -> \"torch.Tensor\":\n         \"\"\"\n         Convert an image into a tensor of patches.\n \n@@ -616,7 +615,7 @@ def preprocess_with_tokenizer_info(\n         image_placeholder_id: int,\n         image_newline_id: int,\n         variable_sized: bool,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n     ) -> FuyuBatchFeature:\n         \"\"\"Process images for model input. In particular, variable-sized images are handled here.\n "
        },
        {
            "sha": "dc64a5d509588798a9a1a1e7aae45df619cc760a",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Fast Image processor class for Fuyu.\"\"\"\n \n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n \n@@ -124,13 +124,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        padding_value: Optional[float],\n-        padding_mode: Optional[str],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        padding_value: float | None,\n+        padding_mode: str | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> FuyuBatchFeature:\n         # Group images by size for batched resizing\n@@ -185,7 +185,7 @@ def _preprocess(\n             tensor_type=return_tensors,\n         )\n \n-    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[SizeDict] = None) -> int:\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: SizeDict | None = None) -> int:\n         \"\"\"\n         Calculate number of patches required to encode an image.\n         Args:\n@@ -208,7 +208,7 @@ def get_num_patches(self, image_height: int, image_width: int, patch_size: Optio\n         num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n         return num_patches\n \n-    def patchify_image(self, image: torch.Tensor, patch_size: Optional[SizeDict] = None) -> torch.Tensor:\n+    def patchify_image(self, image: torch.Tensor, patch_size: SizeDict | None = None) -> torch.Tensor:\n         \"\"\"\n         Convert an image into a tensor of patches using PyTorch's unfold operation.\n         Args:\n@@ -241,7 +241,7 @@ def preprocess_with_tokenizer_info(\n         image_placeholder_id: int,\n         image_newline_id: int,\n         variable_sized: bool,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n     ) -> FuyuBatchFeature:\n         \"\"\"\n         Process images for model input. In particular, variable-sized images are handled here.\n@@ -365,7 +365,7 @@ def preprocess_with_tokenizer_info(\n \n     def _further_process_kwargs(\n         self,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "8eb480ca9188525d4fba591f08d375bbc8855f6c",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n import re\n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n \n@@ -235,7 +235,7 @@ def _transform_within_tags(text: str, scale_factor: float, tokenizer) -> list[in\n def _tokenize_prompts_with_image_and_batch(\n     tokenizer,\n     prompts: list[list[str]],\n-    scale_factors: Optional[list[list[\"torch.Tensor\"]]],\n+    scale_factors: list[list[\"torch.Tensor\"]] | None,\n     max_tokens_to_generate: int,\n     max_position_embeddings: int,\n     add_BOS: bool,  # Same issue with types as above\n@@ -484,8 +484,8 @@ def get_sample_encoding(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[FuyuProcessorKwargs],\n     ) -> \"FuyuBatchFeature\":\n         r\"\"\""
        },
        {
            "sha": "8a185eef8cd310b5d03ded701a977c0fab1756f1",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n \n import itertools\n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -114,18 +113,18 @@ class Gemma3ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = True,\n-        do_pan_and_scan: Optional[bool] = None,\n-        pan_and_scan_min_crop_size: Optional[int] = None,\n-        pan_and_scan_max_num_crops: Optional[int] = None,\n-        pan_and_scan_min_ratio_to_activate: Optional[float] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = True,\n+        do_pan_and_scan: bool | None = None,\n+        pan_and_scan_min_crop_size: int | None = None,\n+        pan_and_scan_max_num_crops: int | None = None,\n+        pan_and_scan_min_ratio_to_activate: float | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -154,8 +153,8 @@ def pan_and_scan(\n         pan_and_scan_min_crop_size: int,\n         pan_and_scan_max_num_crops: int,\n         pan_and_scan_min_ratio_to_activate: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pan and Scan and image, by cropping into smaller images when the aspect ratio exceeds\n@@ -237,8 +236,8 @@ def _process_images_for_pan_and_scan(\n         pan_and_scan_min_crop_size: int,\n         pan_and_scan_max_num_crops: int,\n         pan_and_scan_min_ratio_to_activate: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         pas_images_list = []\n         num_crops = []\n@@ -259,22 +258,22 @@ def _process_images_for_pan_and_scan(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_pan_and_scan: Optional[bool] = None,\n-        pan_and_scan_min_crop_size: Optional[int] = None,\n-        pan_and_scan_max_num_crops: Optional[int] = None,\n-        pan_and_scan_min_ratio_to_activate: Optional[float] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        do_convert_rgb: bool | None = None,\n+        do_pan_and_scan: bool | None = None,\n+        pan_and_scan_min_crop_size: int | None = None,\n+        pan_and_scan_max_num_crops: int | None = None,\n+        pan_and_scan_min_ratio_to_activate: float | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "dc50593a6dd04c9bcd9e8701125006b841c0b11d",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import itertools\n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -157,18 +157,18 @@ def _preprocess(\n         images: list[list[\"torch.Tensor\"]],\n         do_resize: bool,\n         size: SizeDict,\n-        do_pan_and_scan: Optional[bool],\n-        pan_and_scan_min_crop_size: Optional[int],\n-        pan_and_scan_max_num_crops: Optional[int],\n-        pan_and_scan_min_ratio_to_activate: Optional[float],\n+        do_pan_and_scan: bool | None,\n+        pan_and_scan_min_crop_size: int | None,\n+        pan_and_scan_max_num_crops: int | None,\n+        pan_and_scan_min_ratio_to_activate: float | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched processing"
        },
        {
            "sha": "479619c54ee871181d602f8ddd49748f024f9e0b",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -67,8 +66,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Gemma3ProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None and images is None:"
        },
        {
            "sha": "713734c2af82db2df970bc5285a9d184a155c89e",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -72,9 +71,9 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        audio: np.ndarray | list[float] | list[np.ndarray] | list[list[float]] | None = None,\n         **kwargs: Unpack[Gemma3nProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None and images is None and audio is None:"
        },
        {
            "sha": "7b9e1ddf40f8affd2c1a1d06ad9120bc96bc8584",
            "filename": "src/transformers/models/glm46v/image_processing_glm46v.py",
            "status": "modified",
            "additions": 34,
            "deletions": 35,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,7 +20,6 @@\n \n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -142,13 +141,13 @@ class Glm46VImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         patch_size: int = 14,\n         temporal_patch_size: int = 2,\n@@ -177,21 +176,21 @@ def __init__(\n \n     def _preprocess(\n         self,\n-        images: Union[ImageInput, VideoInput],\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | VideoInput,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n@@ -314,21 +313,21 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "1f9f446b6afbb324c58d2e576ddf8953fdc84232",
            "filename": "src/transformers/models/glm46v/image_processing_glm46v_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fimage_processing_glm46v_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -57,7 +57,7 @@ def __init__(self, **kwargs: Unpack[Glm46VImageProcessorKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -78,13 +78,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "bf72cf72fae98308830038611f3e36bf7ae24665",
            "filename": "src/transformers/models/glm46v/video_processing_glm46v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fvideo_processing_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm46v%2Fvideo_processing_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fvideo_processing_glm46v.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,8 +19,6 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n-\n import numpy as np\n import torch\n \n@@ -90,7 +88,7 @@ def __init__(self, **kwargs: Unpack[Glm46VVideoProcessorInitKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -183,17 +181,17 @@ def _preprocess(\n         videos: list[torch.Tensor],\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)"
        },
        {
            "sha": "a5ea56bddf89d872c70ac9d863cc6d924b3aa3ae",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 34,
            "deletions": 35,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for GLM-4.1V.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -140,13 +139,13 @@ class Glm4vImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         patch_size: int = 14,\n         temporal_patch_size: int = 2,\n@@ -175,21 +174,21 @@ def __init__(\n \n     def _preprocess(\n         self,\n-        images: Union[ImageInput, VideoInput],\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | VideoInput,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n@@ -312,21 +311,21 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "556d4e2364450880125f34967f25158c880ef520",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for GLM-4.1V.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -70,7 +70,7 @@ def __init__(self, **kwargs: Unpack[Glm4vImageProcessorKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -91,13 +91,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "fe39bccb6b3656564ebde664c94559aa7db2f18f",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"video processor class for GLM-4.1V.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -85,7 +84,7 @@ def __init__(self, **kwargs: Unpack[Glm4vVideoProcessorInitKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -100,7 +99,7 @@ def _further_process_kwargs(\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        fps: Optional[Union[int, float]] = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -152,17 +151,17 @@ def _preprocess(\n         videos: list[torch.Tensor],\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)"
        },
        {
            "sha": "315b8fb2a60d841d5169c48042980b17934f3bbf",
            "filename": "src/transformers/models/glmasr/processing_glmasr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,7 +20,6 @@\n \n \n import re\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -106,9 +105,9 @@ def _get_audio_token_length(self, audio_lengths: \"torch.Tensor\") -> \"torch.Tenso\n \n     def __call__(\n         self,\n-        text: Union[TextInput, list[TextInput]],\n-        audio: Optional[AudioInput] = None,\n-        output_labels: Optional[bool] = False,\n+        text: TextInput | list[TextInput],\n+        audio: AudioInput | None = None,\n+        output_labels: bool | None = False,\n         **kwargs: Unpack[GlmAsrProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "48c243640e977c7a3c936bf3f0b4092d0b711833",
            "filename": "src/transformers/models/glpn/image_processing_glpn.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for GLPN.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Union\n \n from ...utils.import_utils import requires\n \n@@ -90,7 +90,7 @@ def __init__(\n         size_divisor: int = 32,\n         resample=PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Optional[float] = 1 / 255,\n+        rescale_factor: float | None = 1 / 255,\n         **kwargs,\n     ) -> None:\n         self.do_resize = do_resize\n@@ -105,8 +105,8 @@ def resize(\n         image: np.ndarray,\n         size_divisor: int,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -154,14 +154,14 @@ def resize(\n     def preprocess(\n         self,\n         images: Union[\"PIL.Image.Image\", TensorType, list[\"PIL.Image.Image\"], list[TensorType]],\n-        do_resize: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n+        do_resize: bool | None = None,\n+        size_divisor: int | None = None,\n         resample=None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        return_tensors: TensorType | str | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess the given images.\n@@ -249,7 +249,7 @@ def preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "47185e425a8a316d164abad26863a464d5ea4645",
            "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for GLPN.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -82,16 +82,16 @@ def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n-        size_divisor: Optional[int] = None,\n+        size_divisor: int | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n         do_rescale: bool = True,\n-        rescale_factor: Optional[float] = 1 / 255,\n+        rescale_factor: float | None = 1 / 255,\n         do_normalize: bool = False,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        disable_grouping: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        resample: Optional[PILImageResampling] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        disable_grouping: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        resample: PILImageResampling | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "fa2234a165b3a93b4a9559c0689d2f8cac31c04a",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for Got-OCR-2.\"\"\"\n \n from functools import lru_cache\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -191,16 +190,16 @@ class GotOcr2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         crop_to_patches: bool = False,\n         min_patches: int = 1,\n         max_patches: int = 12,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -226,8 +225,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -273,21 +272,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        crop_to_patches: Optional[bool] = None,\n-        min_patches: Optional[int] = None,\n-        max_patches: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        crop_to_patches: bool | None = None,\n+        min_patches: int | None = None,\n+        max_patches: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        do_convert_rgb: bool | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -435,8 +434,8 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n-        patch_size: Optional[Union[tuple, int, dict]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        patch_size: tuple | int | dict | None = None,\n+        data_format: ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Crop the image to patches and return a list of cropped images."
        },
        {
            "sha": "f20c045dbd48c932e2d89c8451236d95b236d97d",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Got-OCR-2.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -61,7 +61,7 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n-        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        patch_size: tuple | int | dict | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n@@ -139,10 +139,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches:"
        },
        {
            "sha": "87f7169532f02d7e8672da5086e8215231fe50c6",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BatchFeature\n@@ -37,7 +35,7 @@ class GotOcr2TextKwargs(TextKwargs, total=False):\n         structured and formatted text output rather than raw OCR results.\n     \"\"\"\n \n-    format: Optional[bool]\n+    format: bool | None\n \n \n class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n@@ -70,8 +68,8 @@ class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n     crop_to_patches: bool\n     min_patches: int\n     max_patches: int\n-    box: Optional[Union[list, tuple[float, float], tuple[float, float, float, float]]]\n-    color: Optional[str]\n+    box: list | tuple[float, float] | tuple[float, float, float, float] | None\n+    color: str | None\n     num_image_tokens: int\n     multi_page: bool\n \n@@ -94,7 +92,7 @@ class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n-def preprocess_box_annotation(box: Union[list, tuple], image_size: tuple[int, int]) -> list:\n+def preprocess_box_annotation(box: list | tuple, image_size: tuple[int, int]) -> list:\n     \"\"\"\n     Convert box annotation to the format [x1, y1, x2, y2] in the range [0, 1000].\n     \"\"\"\n@@ -149,8 +147,8 @@ def _make_list_of_inputs(self, images, text, box, color, multi_page):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[GotOcr2ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "969ebc1c35965dd1ec90ae31027fd7afcabf7ef8",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -49,7 +49,7 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput],\n         audio: Union[\"torch.Tensor\", list[\"torch.Tensor\"]] = None,\n         device: str = \"cpu\",\n         **kwargs,\n@@ -94,7 +94,7 @@ def __call__(\n         text_inputs = self.tokenizer(prompt_strings, **kwargs)\n         return BatchFeature(data={**text_inputs, **audio_inputs})\n \n-    def _get_validated_text(self, text: Union[str, list]) -> list[str]:\n+    def _get_validated_text(self, text: str | list) -> list[str]:\n         if isinstance(text, str):\n             return [text]\n         elif isinstance(text, list) and isinstance(text[0], str):"
        },
        {
            "sha": "75c4ba93b5267c9073a3844db4b3656eb44f1db2",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 60,
            "deletions": 60,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n import pathlib\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n import numpy as np\n \n@@ -81,7 +81,7 @@\n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n-AnnotationType = dict[str, Union[int, str, list[dict]]]\n+AnnotationType = dict[str, int | str | list[dict]]\n \n \n class AnnotationFormat(ExplicitEnum):\n@@ -108,18 +108,18 @@ class GroundingDinoImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -148,7 +148,7 @@ def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -180,7 +180,7 @@ def get_image_size_for_max_height_width(\n \n \n # Copied from transformers.models.detr.image_processing_detr.safe_squeeze\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -218,7 +218,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -237,7 +237,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -294,7 +294,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by GroundingDino.\n@@ -389,9 +389,9 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for GroundingDino.\n@@ -710,8 +710,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -853,18 +853,18 @@ class GroundingDinoImageProcessor(BaseImageProcessor):\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n@@ -915,10 +915,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into GroundingDino model.\n@@ -949,8 +949,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1023,8 +1023,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -1104,10 +1104,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1137,14 +1137,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -1221,24 +1221,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        do_convert_annotations: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1471,7 +1471,7 @@ def post_process_object_detection(\n         self,\n         outputs: \"GroundingDinoObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "58928ba147a818cce88bcaf8024a029e4d49a2a3",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -24,7 +24,7 @@\n # limitations under the License.\n \n import pathlib\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional\n \n import torch\n from torchvision.io import read_image\n@@ -101,7 +101,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by GROUNDING_DINO.\n@@ -212,9 +212,9 @@ def rgb_to_id(color):\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for GROUNDING_DINO.\n@@ -322,10 +322,10 @@ def prepare_annotation(\n         self,\n         image: torch.Tensor,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into GROUNDING_DINO model.\n@@ -511,7 +511,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):\n@@ -540,8 +540,8 @@ def pad(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]],\n-        masks_path: Optional[Union[str, pathlib.Path]],\n+        annotations: AnnotationType | list[AnnotationType] | None,\n+        masks_path: str | pathlib.Path | None,\n         return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -550,12 +550,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_convert_annotations: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        pad_size: Optional[SizeDict],\n-        format: Optional[Union[str, AnnotationFormat]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        pad_size: SizeDict | None,\n+        format: str | AnnotationFormat | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "7835885fd42def5b68fc508ea41d18894653b027",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n import warnings\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from ...image_transforms import center_to_corners_format\n from ...image_utils import ImageInput\n@@ -32,7 +32,7 @@\n     from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n \n \n-AnnotationType = dict[str, Union[int, str, list[dict]]]\n+AnnotationType = dict[str, int | str | list[dict]]\n \n \n def get_phrases_from_posmap(posmaps, input_ids):\n@@ -123,8 +123,8 @@ def __init__(self, image_processor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[GroundingDinoProcessorKwargs],\n     ) -> BatchEncoding:\n         if text is not None:\n@@ -151,11 +151,11 @@ def _preprocess_input_text(self, text):\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"GroundingDinoObjectDetectionOutput\",\n-        input_ids: Optional[TensorType] = None,\n+        input_ids: TensorType | None = None,\n         threshold: float = 0.25,\n         text_threshold: float = 0.25,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n-        text_labels: Optional[list[list[str]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n+        text_labels: list[list[str]] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "660951a5df6fe473dd6ee801d8017b6486041e81",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for Idefics.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n from PIL import Image\n \n@@ -46,7 +45,7 @@ class IdeficsImageProcessorKwargs(ImagesKwargs, total=False):\n         Resize to image size\n     \"\"\"\n \n-    transform: Optional[Callable]\n+    transform: Callable | None\n     image_size: dict[str, int]\n \n \n@@ -94,11 +93,11 @@ class IdeficsImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         image_size: int = 224,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        image_num_channels: Optional[int] = 3,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        image_num_channels: int | None = 3,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -113,14 +112,14 @@ def __init__(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        image_num_channels: Optional[int] = 3,\n-        image_size: Optional[dict[str, int]] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        transform: Optional[Callable] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        image_num_channels: int | None = 3,\n+        image_size: dict[str, int] | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        transform: Callable | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        return_tensors: str | TensorType | None = TensorType.PYTORCH,\n         **kwargs,\n     ) -> TensorType:\n         \"\"\""
        },
        {
            "sha": "5d73a6a9c0b1558549d99bf41b3ab62e00ece5b4",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 9,
            "deletions": 12,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n Processor class for IDEFICS.\n \"\"\"\n \n-from typing import Optional, Union\n from urllib.parse import urlparse\n \n from ...feature_extraction_utils import BatchFeature\n@@ -48,8 +47,8 @@ class IdeficsTextKwargs(TextKwargs, total=False):\n         particularly important for chat-based models.\n     \"\"\"\n \n-    add_eos_token: Optional[bool]\n-    add_end_of_utterance_token: Optional[bool]\n+    add_eos_token: bool | None\n+    add_end_of_utterance_token: bool | None\n \n \n class IdeficsProcessorKwargs(ProcessingKwargs, total=False):\n@@ -172,15 +171,13 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Union[ImageInput, list[ImageInput], str, list[str], list[list[str]]] = None,\n-        text: Union[\n-            TextInput,\n-            PreTokenizedInput,\n-            list[TextInput],\n-            list[PreTokenizedInput],\n-            list[list[TextInput]],\n-            list[list[PreTokenizedInput]],\n-        ] = None,\n+        images: ImageInput | list[ImageInput] | str | list[str] | list[list[str]] = None,\n+        text: TextInput\n+        | PreTokenizedInput\n+        | list[TextInput]\n+        | list[PreTokenizedInput]\n+        | list[list[TextInput]]\n+        | list[list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[IdeficsProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "365407413792a70266ae4bbcdba043bf18cf2688",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \n \n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -95,7 +95,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n \n def get_max_height_width(\n-    images_list: list[list[np.ndarray]], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images_list: list[list[np.ndarray]], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -114,7 +114,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -200,13 +200,13 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         do_image_splitting: bool = False,\n         **kwargs,\n@@ -229,8 +229,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -266,9 +266,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -292,11 +292,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         For a list of images, for each images, pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width.\n@@ -363,7 +363,7 @@ def _crop(\n         h1: int,\n         w2: int,\n         h2: int,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         if input_data_format == ChannelDimension.FIRST:\n             return im[:, h1:h2, w1:w2]\n@@ -373,7 +373,7 @@ def _crop(\n     def split_image(\n         self,\n         image: np.ndarray,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Split an image into 4 equal sub-images, and the concatenate that sequence with the original image.\n@@ -401,20 +401,20 @@ def split_image(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        do_image_splitting: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[ChannelDimension] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        do_convert_rgb: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        do_image_splitting: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: ChannelDimension | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n     ):\n         \"\"\"\n         Preprocess a batch of images."
        },
        {
            "sha": "a945ac623b68593532865625b24ac91818cb2aa0",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n \n@@ -215,12 +215,12 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        do_image_splitting: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        do_image_splitting: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "dd87290838ff4d847c31d23f2ac5fe9affe512af",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n \n import re\n from itertools import accumulate\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n@@ -58,7 +58,7 @@ class Idefics2ProcessorKwargs(ProcessingKwargs, total=False):\n @auto_docstring\n class Idefics2Processor(ProcessorMixin):\n     def __init__(\n-        self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n+        self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: str | None = None, **kwargs\n     ):\n         r\"\"\"\n         image_seq_len (`int`, *optional*, defaults to 64):\n@@ -98,7 +98,7 @@ def _extract_images_from_prompts(self, prompts):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n+        images: ImageInput | list[ImageInput] | list[list[ImageInput]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n         **kwargs: Unpack[Idefics2ProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "5199b0fc042278e1e9e14dafe31d7e153e6131b4",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 42,
            "deletions": 42,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -64,7 +64,7 @@ class Idefics3ImageProcessorKwargs(ImagesKwargs, total=False):\n \n \n def _resize_output_size_rescale_to_max_len(\n-    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+    height: int, width: int, min_len: int | None = 1, max_len: int | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -101,7 +101,7 @@ def _resize_output_size_rescale_to_max_len(\n \n \n def _resize_output_size_scale_below_upper_bound(\n-    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+    height: int, width: int, max_len: dict[str, int] | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -134,7 +134,7 @@ def _resize_output_size_scale_below_upper_bound(\n def get_resize_output_image_size(\n     image,\n     resolution_max_side: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -167,7 +167,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n \n def get_max_height_width(\n-    images_list: list[list[np.ndarray]], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images_list: list[list[np.ndarray]], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -186,7 +186,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -204,9 +204,9 @@ def make_pixel_mask(\n \n def convert_to_rgb(\n     image: np.ndarray,\n-    palette: Optional[PIL.ImagePalette.ImagePalette] = None,\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    palette: PIL.ImagePalette.ImagePalette | None = None,\n+    data_format: str | ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to RGB format.\n@@ -250,7 +250,7 @@ def _crop(\n     h1: int,\n     w2: int,\n     h2: int,\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n+    data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     if data_format is None:\n         data_format = infer_channel_dimension_format(image, num_channels=(1, 3, 4))\n@@ -313,15 +313,15 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n         do_image_splitting: bool = True,\n-        max_image_size: Optional[dict[str, int]] = None,\n+        max_image_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -344,8 +344,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -401,8 +401,8 @@ def split_image(\n         image,\n         max_image_size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Split an image into squares of side max_image_size and the original image resized to max_image_size.\n@@ -480,8 +480,8 @@ def resize_for_vision_encoder(\n         image: np.ndarray,\n         vision_encoder_max_size: int,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.\n@@ -518,9 +518,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -544,11 +544,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[list[np.ndarray]],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         For a list of images, for each images, pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width.\n@@ -620,22 +620,22 @@ def empty_image(size, input_data_format):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_image_splitting: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        max_image_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_image_splitting: bool | None = None,\n+        do_rescale: bool | None = None,\n+        max_image_size: dict[str, int] | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         return_row_col_info: bool = False,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess a batch of images."
        },
        {
            "sha": "d8faf1cf202ebad7f948e1147cb399e2f52b26ae",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \n \n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n \n@@ -47,7 +47,7 @@\n \n \n def _resize_output_size_rescale_to_max_len(\n-    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+    height: int, width: int, min_len: int | None = 1, max_len: int | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -84,7 +84,7 @@ def _resize_output_size_rescale_to_max_len(\n \n \n def _resize_output_size_scale_below_upper_bound(\n-    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+    height: int, width: int, max_len: dict[str, int] | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -362,14 +362,14 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        do_image_splitting: Optional[bool],\n-        max_image_size: Optional[dict[str, int]],\n-        return_row_col_info: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        do_image_splitting: bool | None,\n+        max_image_size: dict[str, int] | None,\n+        return_row_col_info: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "aa61fe38904a3acdf624188b59c48132d1ed8638",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n \n import re\n from itertools import accumulate\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Union\n \n import numpy as np\n \n@@ -103,7 +103,7 @@ class Idefics3ProcessorKwargs(ProcessingKwargs, total=False):\n @auto_docstring\n class Idefics3Processor(ProcessorMixin):\n     def __init__(\n-        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n+        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: str | None = None, **kwargs\n     ):\n         r\"\"\"\n         image_seq_len (`int`, *optional*, defaults to 169):\n@@ -154,9 +154,9 @@ def _extract_images_from_prompts(self, prompts):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n+        images: ImageInput | list[ImageInput] | list[list[ImageInput]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n-        image_seq_len: Optional[int] = None,\n+        image_seq_len: int | None = None,\n         **kwargs: Unpack[Idefics3ProcessorKwargs],\n     ) -> BatchEncoding:\n         r\"\"\""
        },
        {
            "sha": "fcbfe04f3328bae7a7297734669e096518f1956d",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for ImageGPT.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n \n@@ -54,7 +54,7 @@ class ImageGPTImageProcessorKwargs(ImagesKwargs, total=False):\n         When True, each pixel is assigned to its nearest color cluster, enabling ImageGPT's discrete token modeling.\n     \"\"\"\n \n-    clusters: Optional[Union[np.ndarray, list[list[int]], \"torch.Tensor\"]]\n+    clusters: Union[np.ndarray, list[list[int]], \"torch.Tensor\"] | None\n     do_color_quantize: bool\n \n \n@@ -104,9 +104,9 @@ class ImageGPTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         # clusters is a first argument to maintain backwards compatibility with the old ImageGPTImageProcessor\n-        clusters: Optional[Union[list[list[int]], np.ndarray]] = None,\n+        clusters: list[list[int]] | np.ndarray | None = None,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_normalize: bool = True,\n         do_color_quantize: bool = True,\n@@ -128,8 +128,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -174,8 +174,8 @@ def resize(\n     def normalize(\n         self,\n         image: np.ndarray,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Normalizes an images' pixel values to between [-1, 1].\n@@ -196,15 +196,15 @@ def normalize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_color_quantize: Optional[bool] = None,\n-        clusters: Optional[Union[list[list[int]], np.ndarray]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_normalize: bool | None = None,\n+        do_color_quantize: bool | None = None,\n+        clusters: list[list[int]] | np.ndarray | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "f19de263da985302435390c08f42d4dc0ecc37b0",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for ImageGPT.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import numpy as np\n import torch\n@@ -81,7 +81,7 @@ class ImageGPTImageProcessorFast(BaseImageProcessorFast):\n \n     def __init__(\n         self,\n-        clusters: Optional[Union[list, np.ndarray, torch.Tensor]] = None,  # keep as arg for backwards compatibility\n+        clusters: list | np.ndarray | torch.Tensor | None = None,  # keep as arg for backwards compatibility\n         **kwargs: Unpack[ImageGPTImageProcessorKwargs],\n     ):\n         r\"\"\"\n@@ -103,12 +103,12 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_color_quantize: Optional[bool] = None,\n-        clusters: Optional[Union[list, np.ndarray, torch.Tensor]] = None,\n-        disable_grouping: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_color_quantize: bool | None = None,\n+        clusters: list | np.ndarray | torch.Tensor | None = None,\n+        disable_grouping: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         # Group images by size for batched resizing"
        },
        {
            "sha": "10c69dd79d1d4d7b2a65a6f326dbad9a3f40b3be",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for InstructBLIP. Largely copy of Blip2Processor with addition of a tokenizer for the Q-Former.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -65,8 +63,8 @@ def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_toke\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[InstructBlipProcessorKwargs],\n     ) -> BatchFeature:\n         if images is None and text is None:"
        },
        {
            "sha": "9aa85f692e529666f3a6a4656f5e510045495325",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for InstructBLIP. Largely copy of Blip2Processor with addition of a tokenizer for the Q-Former.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import (\n@@ -53,22 +51,22 @@ def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_toke\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[VideoInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: VideoInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n         return_token_type_ids: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         if images is None and text is None:"
        },
        {
            "sha": "2b4d8070a78ea1112972047be726e38967440df9",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n Video processor class for InstructBLIPVideo\n \"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -53,9 +53,9 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group videos by size for batched resizing"
        },
        {
            "sha": "80ce36fb78e25fd6d3ad6b887b7c26fa5f79ff62",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -134,9 +133,9 @@ def _insert_media_placeholders(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[InternVLProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "a088d4269e03943c303370f39d12db8da0d2b9a1",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Video processor class for InternVL.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -27,7 +27,7 @@\n \n \n class InternVLVideoProcessorInitKwargs(VideosKwargs, total=False):\n-    initial_shift: Union[bool, float, int]\n+    initial_shift: bool | float | int\n \n \n class InternVLVideoProcessor(BaseVideoProcessor):\n@@ -49,9 +49,9 @@ def __init__(self, **kwargs: Unpack[InternVLVideoProcessorInitKwargs]):\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n-        initial_shift: Optional[Union[bool, float, int]] = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n+        initial_shift: bool | float | int | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -109,9 +109,9 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group videos by size for batched resizing"
        },
        {
            "sha": "f0e80a78d577b4b806c59a2c3238de2b4a840640",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -90,7 +90,7 @@ def resize(\n     def pad_to_square(\n         self,\n         images: \"torch.Tensor\",\n-        background_color: Union[int, tuple[int, int, int]] = 0,\n+        background_color: int | tuple[int, int, int] = 0,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n@@ -147,10 +147,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> BatchFeature:\n@@ -185,12 +185,12 @@ def _preprocess(\n     def postprocess(\n         self,\n         images: ImageInput,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[list[float]] = None,\n-        image_std: Optional[list[float]] = None,\n-        return_tensors: Optional[str] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: list[float] | None = None,\n+        image_std: list[float] | None = None,\n+        return_tensors: str | None = None,\n     ) -> \"torch.Tensor\":\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = 1.0 / self.rescale_factor if rescale_factor is None else rescale_factor"
        },
        {
            "sha": "38d8df9e0af9149e2854760b1d041ac52cdde1f2",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Janus.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n@@ -70,8 +68,8 @@ def __init__(self, image_processor, tokenizer, chat_template=None, use_default_s\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: Optional[ImageInput] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        images: ImageInput | None = None,\n         **kwargs: Unpack[JanusProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "fbd8abb9ea9f7e25c0a93985b90cc4baca676a30",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n import math\n import re\n-from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -26,14 +25,15 @@\n from ...utils import auto_docstring\n \n \n-BboxInput = Union[\n-    list[tuple[int, int]],\n-    list[tuple[float, float, float, float]],\n-    list[list[tuple[int, int]]],\n-    list[list[tuple[float, float, float]]],\n-]\n+BboxInput = (\n+    list[tuple[int, int]]\n+    | list[tuple[float, float, float, float]]\n+    | list[list[tuple[int, int]]]\n+    | list[list[tuple[float, float, float]]]\n+)\n \n-NestedList = list[Union[Optional[int], \"NestedList\"]]\n+\n+NestedList = list[tuple | None | list[tuple | None | list[tuple | None | list[tuple | None]]]]\n \n \n class Kosmos2ImagesKwargs(ImagesKwargs, total=False):\n@@ -48,9 +48,9 @@ class Kosmos2ImagesKwargs(ImagesKwargs, total=False):\n         information. If unset, will default to `self.tokenizer.unk_token_id + 1`.\n     \"\"\"\n \n-    bboxes: Optional[NestedList]  # NOTE: hub validators can't accept `Sequence`\n+    bboxes: NestedList | None  # NOTE: hub validators can't accept `Sequence`\n     num_image_tokens: int\n-    first_image_token_id: Optional[int]\n+    first_image_token_id: int | None\n \n \n class Kosmos2TextKwargs(TextKwargs, total=False):\n@@ -137,8 +137,8 @@ def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwa\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, list[TextInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | list[TextInput] = None,\n         **kwargs: Unpack[Kosmos2ProcessorKwargs],\n     ) -> BatchFeature:\n         if images is None and text is None:\n@@ -322,11 +322,11 @@ def _preprocess_single_example(self, text, image, bboxes, img_info_tokens):\n \n     def preprocess_examples(\n         self,\n-        texts: Union[TextInput, list[TextInput]],\n-        images: Optional[ImageInput] = None,\n+        texts: TextInput | list[TextInput],\n+        images: ImageInput | None = None,\n         bboxes: BboxInput = None,\n-        num_image_tokens: Optional[int] = 64,\n-    ) -> Union[str, list[str]]:\n+        num_image_tokens: int | None = 64,\n+    ) -> str | list[str]:\n         \"\"\"Add image and bounding box information to `texts` as image and patch index tokens.\n \n         Args:\n@@ -417,7 +417,7 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return tokenizer_input_names + image_processor_input_names + [\"image_embeds_position_mask\"]\n \n-    def _insert_patch_index_tokens(self, text: str, bboxes: Union[list[tuple[int]], list[tuple[float]]]) -> str:\n+    def _insert_patch_index_tokens(self, text: str, bboxes: list[tuple[int]] | list[tuple[float]]) -> str:\n         if bboxes is None or len(bboxes) == 0:\n             return text\n \n@@ -463,7 +463,7 @@ def _insert_patch_index_tokens(self, text: str, bboxes: Union[list[tuple[int]],\n         return text\n \n     def _convert_bbox_to_patch_index_tokens(\n-        self, bbox: Union[tuple[int, int], tuple[float, float, float, float]]\n+        self, bbox: tuple[int, int] | tuple[float, float, float, float]\n     ) -> tuple[str, str]:\n         # already computed patch indices\n         if len(bbox) == 2:"
        },
        {
            "sha": "c228fdd2f8b74e49810702f2f78edb89bf490de9",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for Kosmos2_5.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -111,7 +110,7 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_normalize: bool = True,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n         max_patches: int = 4096,\n         **kwargs,\n     ) -> None:\n@@ -126,7 +125,7 @@ def extract_flattened_patches(\n         image: np.ndarray,\n         max_patches: int,\n         patch_size: dict,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -216,8 +215,8 @@ def extract_flattened_patches(\n     def normalize(\n         self,\n         image: np.ndarray,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -252,13 +251,13 @@ def normalize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_normalize: Optional[bool] = None,\n-        max_patches: Optional[int] = None,\n-        patch_size: Optional[dict[str, int]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: bool | None = None,\n+        do_normalize: bool | None = None,\n+        max_patches: int | None = None,\n+        patch_size: dict[str, int] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> ImageInput:\n         \"\"\""
        },
        {
            "sha": "8c9a972c60c50906873705c14eea00a8624c21b9",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Fast Image processor class for Kosmos2_5.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import torch\n \n@@ -204,8 +203,8 @@ def _preprocess(\n         do_normalize: bool,\n         max_patches: int,\n         patch_size: dict[str, int],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Q: should we have this?"
        },
        {
            "sha": "df4dc5ff2946a892fbbb803eb73812a2ec7976d3",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Kosmos2_5.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -59,8 +57,8 @@ def __init__(self, image_processor, tokenizer, num_image_tokens: int = 2048):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, list[TextInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | list[TextInput] = None,\n         **kwargs: Unpack[Kosmos2_5ProcessorKwargs],\n     ) -> BatchFeature:\n         if images is None and text is None:"
        },
        {
            "sha": "c1acaebaae078d568d8c8a79f2155bb8ee064634",
            "filename": "src/transformers/models/lasr/processing_lasr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,8 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n-\n from ...audio_utils import AudioInput, make_list_of_audio\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n@@ -54,8 +52,8 @@ def __init__(self, feature_extractor, tokenizer):\n     def __call__(\n         self,\n         audio: AudioInput,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n-        sampling_rate: Optional[int] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        sampling_rate: int | None = None,\n         **kwargs: Unpack[LasrProcessorKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "7715b28803829023b3220d2fe924916c230c677f",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 20,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for LayoutLMv2.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -66,8 +64,8 @@ class LayoutLMv2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     apply_ocr: bool\n-    ocr_lang: Optional[str]\n-    tesseract_config: Optional[str]\n+    ocr_lang: str | None\n+    tesseract_config: str | None\n \n \n def normalize_box(box, width, height):\n@@ -81,9 +79,9 @@ def normalize_box(box, width, height):\n \n def apply_tesseract(\n     image: np.ndarray,\n-    lang: Optional[str],\n-    tesseract_config: Optional[str] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    lang: str | None,\n+    tesseract_config: str | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ):\n     \"\"\"Applies Tesseract OCR on a document image, and returns recognized words + normalized bounding boxes.\"\"\"\n     tesseract_config = tesseract_config if tesseract_config is not None else \"\"\n@@ -149,11 +147,11 @@ class LayoutLMv2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         apply_ocr: bool = True,\n-        ocr_lang: Optional[str] = None,\n-        tesseract_config: Optional[str] = \"\",\n+        ocr_lang: str | None = None,\n+        tesseract_config: str | None = \"\",\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -173,8 +171,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -220,15 +218,15 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        apply_ocr: Optional[bool] = None,\n-        ocr_lang: Optional[str] = None,\n-        tesseract_config: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        apply_ocr: bool | None = None,\n+        ocr_lang: str | None = None,\n+        tesseract_config: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "aeb7e4c56e660b12a6f5e137c020b1ca194cd337",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for LayoutLMv2.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -59,10 +59,10 @@ def _preprocess(\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         apply_ocr: bool,\n-        ocr_lang: Optional[str],\n-        tesseract_config: Optional[str],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        ocr_lang: str | None,\n+        tesseract_config: str | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Tesseract OCR to get words + normalized bounding boxes"
        },
        {
            "sha": "84e6b8d621ef809da59dac3ad8f8f96a1ccf5c02",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for LayoutLMv2.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n from ...utils import TensorType, auto_docstring\n@@ -31,24 +29,24 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     def __call__(\n         self,\n         images,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        text_pair: Optional[Union[PreTokenizedInput, list[PreTokenizedInput]]] = None,\n-        boxes: Optional[Union[list[list[int]], list[list[list[int]]]]] = None,\n-        word_labels: Optional[Union[list[int], list[list[int]]]] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        text_pair: PreTokenizedInput | list[PreTokenizedInput] | None = None,\n+        boxes: list[list[int]] | list[list[list[int]]] | None = None,\n+        word_labels: list[int] | list[list[int]] | None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = False,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = False,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         # verify input"
        },
        {
            "sha": "820093a9023eb9aabb03054219c6de905ae437d1",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 24,
            "deletions": 25,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for LayoutLMv3.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -70,8 +69,8 @@ class LayoutLMv3ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     apply_ocr: bool\n-    ocr_lang: Optional[str]\n-    tesseract_config: Optional[str]\n+    ocr_lang: str | None\n+    tesseract_config: str | None\n \n \n def normalize_box(box, width, height):\n@@ -85,9 +84,9 @@ def normalize_box(box, width, height):\n \n def apply_tesseract(\n     image: np.ndarray,\n-    lang: Optional[str],\n-    tesseract_config: Optional[str],\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    lang: str | None,\n+    tesseract_config: str | None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"Applies Tesseract OCR on a document image, and returns recognized words + normalized bounding boxes.\"\"\"\n \n@@ -167,16 +166,16 @@ class LayoutLMv3ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, Iterable[float]]] = None,\n-        image_std: Optional[Union[float, Iterable[float]]] = None,\n+        image_mean: float | Iterable[float] | None = None,\n+        image_std: float | Iterable[float] | None = None,\n         apply_ocr: bool = True,\n-        ocr_lang: Optional[str] = None,\n-        tesseract_config: Optional[str] = \"\",\n+        ocr_lang: str | None = None,\n+        tesseract_config: str | None = \"\",\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -203,8 +202,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -250,20 +249,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, Iterable[float]]] = None,\n-        image_std: Optional[Union[float, Iterable[float]]] = None,\n-        apply_ocr: Optional[bool] = None,\n-        ocr_lang: Optional[str] = None,\n-        tesseract_config: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | Iterable[float] | None = None,\n+        image_std: float | Iterable[float] | None = None,\n+        apply_ocr: bool | None = None,\n+        ocr_lang: str | None = None,\n+        tesseract_config: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "a10d636bf8f8a1550387e7081cfa45f12a27120b",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for LayoutLMv3.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -66,13 +66,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         apply_ocr: bool,\n-        ocr_lang: Optional[str],\n-        tesseract_config: Optional[str],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        disable_grouping: Optional[bool],\n+        ocr_lang: str | None,\n+        tesseract_config: str | None,\n+        return_tensors: str | TensorType | None,\n+        disable_grouping: bool | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Tesseract OCR to get words + normalized bounding boxes"
        },
        {
            "sha": "f9433787175155c51756d4d13f14ed732c998eae",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for LayoutLMv3.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n from ...utils import TensorType, auto_docstring\n@@ -31,24 +29,24 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     def __call__(\n         self,\n         images,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        text_pair: Optional[Union[PreTokenizedInput, list[PreTokenizedInput]]] = None,\n-        boxes: Optional[Union[list[list[int]], list[list[list[int]]]]] = None,\n-        word_labels: Optional[Union[list[int], list[list[int]]]] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        text_pair: PreTokenizedInput | list[PreTokenizedInput] | None = None,\n+        boxes: list[list[int]] | list[list[list[int]]] | None = None,\n+        word_labels: list[int] | list[list[int]] | None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         if self.image_processor.apply_ocr and (boxes is not None):"
        },
        {
            "sha": "749336d301f56c0479b4bea405fda613f76cd95a",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for LayoutXLM.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n from ...utils import TensorType, auto_docstring\n@@ -31,24 +29,24 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     def __call__(\n         self,\n         images,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        text_pair: Optional[Union[PreTokenizedInput, list[PreTokenizedInput]]] = None,\n-        boxes: Optional[Union[list[list[int]], list[list[list[int]]]]] = None,\n-        word_labels: Optional[Union[list[int], list[list[int]]]] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        text_pair: PreTokenizedInput | list[PreTokenizedInput] | None = None,\n+        boxes: list[list[int]] | list[list[list[int]]] | None = None,\n+        word_labels: list[int] | list[list[int]] | None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         # verify input"
        },
        {
            "sha": "12f6b410fca0b00e4bfbae9eefecd7f3b10d3b8b",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for LeViT.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -90,15 +89,15 @@ class LevitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, Iterable[float]]] = IMAGENET_DEFAULT_MEAN,\n-        image_std: Optional[Union[float, Iterable[float]]] = IMAGENET_DEFAULT_STD,\n+        image_mean: float | Iterable[float] | None = IMAGENET_DEFAULT_MEAN,\n+        image_std: float | Iterable[float] | None = IMAGENET_DEFAULT_STD,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -123,8 +122,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -177,19 +176,19 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, Iterable[float]]] = None,\n-        image_std: Optional[Union[float, Iterable[float]]] = None,\n-        return_tensors: Optional[TensorType] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | Iterable[float] | None = None,\n+        image_std: float | Iterable[float] | None = None,\n+        return_tensors: TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images to be used as input to a LeViT model."
        },
        {
            "sha": "7645c2af36f560a3cde289e409ebddc1892ca1b5",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n import math\n from functools import lru_cache\n-from typing import Union\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -450,8 +449,8 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, list[float]],\n-        image_std: Union[float, list[float]],\n+        image_mean: float | list[float],\n+        image_std: float | list[float],\n         downsample_factor: int,\n         do_image_splitting: bool,\n         min_tiles: int,\n@@ -462,7 +461,7 @@ def _preprocess(\n         encoder_patch_size: int,\n         tile_size: int,\n         max_pixels_tolerance: float,\n-        return_tensors: Union[str, TensorType],\n+        return_tensors: str | TensorType,\n         disable_grouping: bool,\n         do_pad: bool,\n         return_row_col_info: bool,"
        },
        {
            "sha": "bf654310d0d3d45b11d8c901265129472e2384ec",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n@@ -37,7 +36,7 @@ class Lfm2VlTextKwargs(TextKwargs, total=False):\n         When disabled, only the image token itself is used without delimiters.\n     \"\"\"\n \n-    use_image_special_tokens: Optional[bool]\n+    use_image_special_tokens: bool | None\n \n \n class Lfm2VlProcessorKwargs(ProcessingKwargs, total=False):\n@@ -61,7 +60,7 @@ def __init__(\n         self,\n         image_processor,\n         tokenizer,\n-        chat_template: Optional[str] = None,\n+        chat_template: str | None = None,\n         **kwargs,\n     ):\n         self.image_token = getattr(tokenizer, \"image_token\", \"<image>\")\n@@ -78,8 +77,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[Union[ImageInput, list[ImageInput], list[list[ImageInput]]]] = None,\n-        text: Optional[Union[TextInput, list[TextInput]]] = None,\n+        images: ImageInput | list[ImageInput] | list[list[ImageInput]] | None = None,\n+        text: TextInput | list[TextInput] | None = None,\n         **kwargs: Unpack[Lfm2VlProcessorKwargs],\n     ) -> BatchEncoding:\n         if text is None and images is None:"
        },
        {
            "sha": "cf4d1b45f6115b6a43fcf083d65f7e2e0fe23e81",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional, Union\n+\n+from typing import TYPE_CHECKING\n \n import numpy as np\n import torch\n@@ -65,7 +66,7 @@ class LightGlueImageProcessorKwargs(ImagesKwargs, total=False):\n \n def is_grayscale(\n     image: np.ndarray,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:\n         if image.shape[0] == 1:\n@@ -79,7 +80,7 @@ def is_grayscale(\n \n def convert_to_grayscale(\n     image: ImageInput,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image.\n@@ -171,7 +172,7 @@ class LightGlueImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -193,8 +194,8 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -231,15 +232,15 @@ def resize(\n     def preprocess(\n         self,\n         images,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_grayscale: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_grayscale: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "7a3eab39d85c0ea3309c2e2047d905bf4c786b1a",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -137,14 +137,14 @@ def _prepare_images_structure(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        size: Union[dict[str, int], SizeDict],\n+        size: dict[str, int] | SizeDict,\n         rescale_factor: float,\n         do_rescale: bool,\n         do_resize: bool,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_grayscale: bool,\n         disable_grouping: bool,\n-        return_tensors: Union[str, TensorType],\n+        return_tensors: str | TensorType,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "cc584104c8c56e23e9e08eee5e6bc7f0bce72156",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n import math\n from collections import defaultdict\n from functools import lru_cache\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -152,7 +152,7 @@ def find_supported_resolutions(max_num_chunks: int, patch_size: SizeDict) -> tor\n def pad_to_best_fit(\n     images: \"torch.Tensor\",\n     target_size: tuple[int, int],\n-    background_color: Union[int, tuple[int, int, int]] = 0,\n+    background_color: int | tuple[int, int, int] = 0,\n ) -> \"torch.Tensor\":\n     \"\"\"\n     Pads an image to fit the target size.\n@@ -325,8 +325,8 @@ def rescale_and_normalize(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, list[float]],\n-        image_std: Union[float, list[float]],\n+        image_mean: float | list[float],\n+        image_std: float | list[float],\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Rescale and normalize images.\n@@ -356,10 +356,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         possible_resolutions = find_supported_resolutions(max_num_chunks=max_patches, patch_size=size)"
        },
        {
            "sha": "f67e37a1e80a228410510df10cd22455c23b56a9",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \n \n-from typing import Optional, Union\n-\n from transformers.processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n \n@@ -123,8 +121,8 @@ def _prompt_split_image(self, aspect_ratio, num_patches_per_chunk):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[Llama4ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "04c9e2351182352a7d3890c59b84b54ab1820e29",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 25,
            "deletions": 27,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for LLaVa.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -98,15 +96,15 @@ def __init__(\n         self,\n         do_pad: bool = False,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -150,9 +148,9 @@ def __init__(\n     def pad_to_square(\n         self,\n         image: np.ndarray,\n-        background_color: Union[int, tuple[int, int, int]] = 0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        background_color: int | tuple[int, int, int] = 0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n@@ -231,8 +229,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -278,21 +276,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_pad: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_pad: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> PIL.Image.Image:\n         \"\"\""
        },
        {
            "sha": "cb61af59f3ce228315110ccabe74770194286124",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for LLaVa.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -107,10 +107,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "c1044c65c701682de1f35abb01843e62342f6c00",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Llava.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -75,8 +73,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[LlavaProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "3eeb90d0f26163b76067d786c9eeb86a909fe3a9",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 43,
            "deletions": 44,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for LLaVa-NeXT.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -168,17 +167,17 @@ class LlavaNextImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n+        size: dict[str, int] | None = None,\n+        image_grid_pinpoints: list | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = True,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = True,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -213,8 +212,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -261,11 +260,11 @@ def resize(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n+        padding: int | tuple[int, int] | Iterable[tuple[int, int]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n-        constant_values: Union[float, Iterable[float]] = 0.0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0.0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n@@ -329,18 +328,18 @@ def pad(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n@@ -523,8 +522,8 @@ def get_image_patches(\n     def _pad_for_batching(\n         self,\n         pixel_values: list[np.ndarray],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n@@ -562,22 +561,22 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        image_grid_pinpoints: list | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "53edb0f9e255a772cd46c2647ae2a207c994e99e",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for LLaVa-NeXT.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -201,11 +201,11 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         processed_images = []"
        },
        {
            "sha": "73787e3b47617a993fbca06addd36454dfdd46d0",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for LLaVa-NeXT.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -86,8 +84,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[LlavaNextProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "543898f29fd1941ac20046a41d728dad0afd33a2",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for LLaVa-NeXT-Video.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -94,9 +92,9 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[LlavaNextVideoProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "f1303094295c4974855f1859e7e758f0651c9513",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for LLaVa-Onevision.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -162,15 +161,15 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n+        size: dict[str, int] | None = None,\n+        image_grid_pinpoints: list | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = True,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = True,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -236,11 +235,11 @@ def __init__(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n+        padding: int | tuple[int, int] | Iterable[tuple[int, int]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n-        constant_values: Union[float, Iterable[float]] = 0.0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0.0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n@@ -419,8 +418,8 @@ def get_image_patches(\n     def _pad_for_batching(\n         self,\n         pixel_values: list[np.ndarray],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n@@ -459,9 +458,9 @@ def _pad_for_batching(\n     def pad_to_square(\n         self,\n         image: np.ndarray,\n-        background_color: Union[int, tuple[int, int, int]] = 0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        background_color: int | tuple[int, int, int] = 0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n@@ -537,17 +536,17 @@ def pad_to_square(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> Image.Image:\n         \"\"\"\n         Args:\n@@ -612,20 +611,20 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        image_grid_pinpoints: list | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "3bd407123864388f0e0454fecfbdd33c53f7d081",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,6 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -92,9 +91,9 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -167,7 +166,7 @@ def __call__(\n     def _expand_image_tokens(\n         self,\n         text: list[TextInput],\n-        image_sizes: Iterable[Union[list[int], int]],\n+        image_sizes: Iterable[list[int] | int],\n         height: int,\n         width: int,\n         special_token: str,"
        },
        {
            "sha": "50ec89769d02d226947fe3c1382740735be79a8c",
            "filename": "src/transformers/models/markuplm/processing_markuplm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for MarkupLM.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...file_utils import TensorType\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, TruncationStrategy\n@@ -39,19 +37,19 @@ def __call__(\n         node_labels=None,\n         questions=None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         # first, create nodes and xpaths"
        },
        {
            "sha": "b22bddf8d044737b3531c119ecde97a2375d935f",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 84,
            "deletions": 84,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -74,9 +74,9 @@ class Mask2FormerImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     size_divisor: int\n-    ignore_index: Optional[int]\n+    ignore_index: int | None\n     do_reduce_labels: bool\n-    num_labels: Optional[int]\n+    num_labels: int | None\n \n \n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n@@ -88,7 +88,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -107,7 +107,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -224,8 +224,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -282,8 +282,8 @@ def compute_segments(\n # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n     segmentation_map: np.ndarray,\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -324,11 +324,11 @@ def convert_segmentation_map_to_binary_masks(\n # Copied from transformers.models.maskformer.image_processing_maskformer.get_maskformer_resize_output_image_size with maskformer->mask2former\n def get_mask2former_resize_output_image_size(\n     image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    max_size: Optional[int] = None,\n+    size: int | tuple[int, int] | list[int] | tuple[int],\n+    max_size: int | None = None,\n     size_divisor: int = 0,\n     default_to_square: bool = True,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output size given the desired size.\n@@ -424,18 +424,18 @@ class Mask2FormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        num_labels: Optional[int] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        num_labels: int | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -479,7 +479,7 @@ def resize(\n         size_divisor: int = 0,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         data_format=None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -534,8 +534,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -562,8 +562,8 @@ def rescale(\n     def convert_segmentation_map_to_binary_masks(\n         self,\n         segmentation_map: np.ndarray,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        ignore_index: Optional[int] = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n     ):\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n@@ -581,16 +581,16 @@ def __call__(self, images, segmentation_maps=None, **kwargs) -> BatchFeature:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_resize:\n             image = self.resize(\n@@ -605,17 +605,17 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -647,10 +647,10 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 0,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -685,23 +685,23 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n@@ -785,9 +785,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -812,12 +812,12 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -874,13 +874,13 @@ def pad(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        ignore_index: Optional[int] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -985,7 +985,7 @@ def encode_inputs(\n         return encoded_inputs\n \n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`Mask2FormerForUniversalSegmentation`] into semantic segmentation maps. Only supports\n@@ -1045,9 +1045,9 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n-        return_binary_maps: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n+        return_binary_maps: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into instance segmentation predictions.\n@@ -1166,8 +1166,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into image panoptic segmentation"
        },
        {
            "sha": "bc9d129b259ad866c1c1bbd76292184347388eee",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -58,8 +58,8 @@\n \n def convert_segmentation_map_to_binary_masks_fast(\n     segmentation_map: \"torch.Tensor\",\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -205,7 +205,7 @@ def pad(\n         self,\n         images: torch.Tensor,\n         padded_size: tuple[int, int],\n-        segmentation_maps: Optional[torch.Tensor] = None,\n+        segmentation_maps: torch.Tensor | None = None,\n         fill: int = 0,\n         ignore_index: int = 255,\n     ) -> BatchFeature:\n@@ -233,8 +233,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n         **kwargs: Unpack[Mask2FormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -254,10 +254,10 @@ def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n         segmentation_maps: ImageInput,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[Mask2FormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -282,21 +282,21 @@ def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         segmentation_maps: Optional[\"torch.Tensor\"],\n-        instance_id_to_semantic_id: Optional[dict[int, int]],\n-        do_resize: Optional[bool],\n-        size: Optional[SizeDict],\n-        pad_size: Optional[SizeDict],\n-        size_divisor: Optional[int],\n-        interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n-        do_rescale: Optional[bool],\n-        rescale_factor: Optional[float],\n-        do_normalize: Optional[bool],\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        ignore_index: Optional[int],\n-        do_reduce_labels: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        instance_id_to_semantic_id: dict[int, int] | None,\n+        do_resize: bool | None,\n+        size: SizeDict | None,\n+        pad_size: SizeDict | None,\n+        size_divisor: int | None,\n+        interpolation: Union[\"PILImageResampling\", \"F.InterpolationMode\"] | None,\n+        do_rescale: bool | None,\n+        rescale_factor: float | None,\n+        do_normalize: bool | None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        ignore_index: int | None,\n+        do_reduce_labels: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if segmentation_maps is not None and len(images) != len(segmentation_maps):"
        },
        {
            "sha": "44f5aa24e96ed4f1254492829cbe57a158e82f9f",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 84,
            "deletions": 84,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -75,9 +75,9 @@ class MaskFormerImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     size_divisor: int\n-    ignore_index: Optional[int]\n+    ignore_index: int | None\n     do_reduce_labels: bool\n-    num_labels: Optional[int]\n+    num_labels: int | None\n \n \n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n@@ -89,7 +89,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -108,7 +108,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -225,8 +225,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -282,8 +282,8 @@ def compute_segments(\n # TODO: (Amy) Move to image_transforms\n def convert_segmentation_map_to_binary_masks(\n     segmentation_map: np.ndarray,\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -323,11 +323,11 @@ def convert_segmentation_map_to_binary_masks(\n \n def get_maskformer_resize_output_image_size(\n     image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    max_size: Optional[int] = None,\n+    size: int | tuple[int, int] | list[int] | tuple[int],\n+    max_size: int | None = None,\n     size_divisor: int = 0,\n     default_to_square: bool = True,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output size given the desired size.\n@@ -425,18 +425,18 @@ class MaskFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        num_labels: Optional[int] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        num_labels: int | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -478,7 +478,7 @@ def resize(\n         size_divisor: int = 0,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         data_format=None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -533,8 +533,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -560,8 +560,8 @@ def rescale(\n     def convert_segmentation_map_to_binary_masks(\n         self,\n         segmentation_map: np.ndarray,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        ignore_index: Optional[int] = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n     ):\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n@@ -579,16 +579,16 @@ def __call__(self, images, segmentation_maps=None, **kwargs) -> BatchFeature:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_resize:\n             image = self.resize(\n@@ -603,17 +603,17 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -645,10 +645,10 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 0,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -683,23 +683,23 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n@@ -783,9 +783,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -809,12 +809,12 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -871,13 +871,13 @@ def pad(\n     def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        ignore_index: Optional[int] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -984,7 +984,7 @@ def encode_inputs(\n         return encoded_inputs\n \n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n@@ -1039,9 +1039,9 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n-        return_binary_maps: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n+        return_binary_maps: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into instance segmentation predictions. Only\n@@ -1155,8 +1155,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation"
        },
        {
            "sha": "4890b198bfff49f011b7ecbe492498a9789e577a",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -61,8 +61,8 @@\n \n def convert_segmentation_map_to_binary_masks_fast(\n     segmentation_map: \"torch.Tensor\",\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -208,7 +208,7 @@ def pad(\n         self,\n         images: torch.Tensor,\n         padded_size: tuple[int, int],\n-        segmentation_maps: Optional[torch.Tensor] = None,\n+        segmentation_maps: torch.Tensor | None = None,\n         fill: int = 0,\n         ignore_index: int = 255,\n     ) -> BatchFeature:\n@@ -236,8 +236,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n         **kwargs: Unpack[MaskFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -257,10 +257,10 @@ def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n         segmentation_maps: ImageInput,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[MaskFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -285,21 +285,21 @@ def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         segmentation_maps: Optional[\"torch.Tensor\"],\n-        instance_id_to_semantic_id: Optional[dict[int, int]],\n-        do_resize: Optional[bool],\n-        size: Optional[SizeDict],\n-        pad_size: Optional[SizeDict],\n-        size_divisor: Optional[int],\n-        interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n-        do_rescale: Optional[bool],\n-        rescale_factor: Optional[float],\n-        do_normalize: Optional[bool],\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        ignore_index: Optional[int],\n-        do_reduce_labels: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        instance_id_to_semantic_id: dict[int, int] | None,\n+        do_resize: bool | None,\n+        size: SizeDict | None,\n+        pad_size: SizeDict | None,\n+        size_divisor: int | None,\n+        interpolation: Union[\"PILImageResampling\", \"F.InterpolationMode\"] | None,\n+        do_rescale: bool | None,\n+        rescale_factor: float | None,\n+        do_normalize: bool | None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        ignore_index: int | None,\n+        do_reduce_labels: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if segmentation_maps is not None and len(images) != len(segmentation_maps):\n@@ -403,7 +403,7 @@ def _preprocess(\n \n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation\n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n@@ -459,9 +459,9 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n-        return_binary_maps: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n+        return_binary_maps: bool | None = False,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into instance segmentation predictions. Only\n@@ -576,8 +576,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation"
        },
        {
            "sha": "0a4eb0369434bbf1a4d7a2420b630c69afe79325",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 23,
            "deletions": 24,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \n import math\n from functools import lru_cache\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -438,8 +437,8 @@ def convert_aspect_ratios_to_ids(aspect_ratios: list[list[tuple[int, int]]], max\n \n def to_channel_dimension_format(\n     image: np.ndarray,\n-    channel_dim: Union[ChannelDimension, str],\n-    input_channel_dim: Optional[Union[ChannelDimension, str]] = None,\n+    channel_dim: ChannelDimension | str,\n+    input_channel_dim: ChannelDimension | str | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Converts `image` to the channel dimension format specified by `channel_dim`.\n@@ -557,13 +556,13 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         max_image_tiles: int = 4,\n         **kwargs,\n@@ -586,19 +585,19 @@ def __init__(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        max_image_tiles: Optional[int] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        max_image_tiles: int | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        return_tensors: str | TensorType | None = None,\n     ):\n         \"\"\"\n         Preprocess a batch of images.\n@@ -774,8 +773,8 @@ def pad(\n         image: np.ndarray,\n         size: dict[str, int],\n         aspect_ratio: tuple[int, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image to the `size` x `aspect_ratio`. For example, if size is {height: 224, width: 224} and aspect ratio is\n@@ -822,9 +821,9 @@ def resize(\n         size: dict[str, int],\n         max_image_tiles: int,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> Union[np.ndarray, tuple[int, int]]:\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+    ) -> np.ndarray | tuple[int, int]:\n         \"\"\"\n         Resizes an image to fit within a tiled canvas while maintaining its aspect ratio.\n         The optimal canvas size is calculated based on the maximum number of tiles and the tile size."
        },
        {
            "sha": "0144cd485fcbd07a92eb98530f39e9877ccf9dae",
            "filename": "src/transformers/models/mllama/image_processing_mllama_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -344,11 +344,11 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        max_image_tiles: Optional[int],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        disable_grouping: Optional[bool],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        max_image_tiles: int | None,\n+        return_tensors: str | TensorType | None,\n+        disable_grouping: bool | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "2a604b4cf0b0eaa1449482e6af48611dec5875ab",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,8 +14,6 @@\n \n \"\"\"Processor class for Mllama.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -184,8 +182,8 @@ def __init__(self, image_processor, tokenizer, chat_template=None):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[MllamaProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "533b3d777412af737500da5ebb4938a6171eee4f",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 20,
            "deletions": 22,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for MobileNetV1.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -88,15 +86,15 @@ class MobileNetV1ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -121,8 +119,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -169,19 +167,19 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "328bbf9616ef2666b589695888ec7dfc444f13a6",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 50,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for MobileNetV2.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -112,15 +110,15 @@ class MobileNetV2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -147,8 +145,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -217,13 +215,13 @@ def _preprocess(\n         do_rescale: bool,\n         do_center_crop: bool,\n         do_normalize: bool,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        rescale_factor: Optional[float] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        rescale_factor: float | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_reduce_labels:\n             image = self.reduce_label(image)\n@@ -245,18 +243,18 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -292,12 +290,12 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_reduce_labels: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_reduce_labels: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -335,21 +333,21 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -482,7 +480,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileNetV2\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "9537947ca0f9a74af1311e61fd2a2abb89c6fc42",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -74,7 +74,7 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[MobileNetV2ImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -86,10 +86,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n+        segmentation_maps: ImageInput | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[MobileNetV2ImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -135,14 +135,14 @@ def _preprocess(\n         do_rescale: bool,\n         do_center_crop: bool,\n         do_normalize: bool,\n-        size: Optional[SizeDict],\n+        size: SizeDict | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        rescale_factor: Optional[float],\n-        crop_size: Optional[SizeDict],\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        rescale_factor: float | None,\n+        crop_size: SizeDict | None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         disable_grouping: bool,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         processed_images = []\n@@ -183,7 +183,7 @@ def _preprocess(\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.post_process_semantic_segmentation with Beit->MobileNetV2\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "1d04a0451466c21f07d617ce07b326a8e14fdda4",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 41,
            "deletions": 43,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for MobileViT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -110,12 +108,12 @@ class MobileViTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_flip_channel_order: bool = True,\n         do_reduce_labels: bool = False,\n         **kwargs,\n@@ -142,8 +140,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -189,8 +187,8 @@ def resize(\n     def flip_channel_order(\n         self,\n         image: np.ndarray,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Flip the color channels from RGB to BGR or vice versa.\n@@ -231,11 +229,11 @@ def _preprocess(\n         do_rescale: bool,\n         do_center_crop: bool,\n         do_flip_channel_order: bool,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        rescale_factor: Optional[float] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        rescale_factor: float | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_reduce_labels:\n             image = self.reduce_label(image)\n@@ -257,16 +255,16 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_flip_channel_order: Optional[bool] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_flip_channel_order: bool | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -300,12 +298,12 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_reduce_labels: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_reduce_labels: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -341,19 +339,19 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_flip_channel_order: Optional[bool] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_flip_channel_order: bool | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -480,7 +478,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileViT\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`MobileViTForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "8f1ba1af2b34253e6191c9e9159be177cf3dcf1a",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -72,7 +72,7 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[MobileVitImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -84,10 +84,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n+        segmentation_maps: ImageInput | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[MobileVitImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -130,15 +130,15 @@ def _preprocess(\n         images: list[\"torch.Tensor\"],\n         do_reduce_labels: bool,\n         do_resize: bool,\n-        size: Optional[SizeDict],\n+        size: SizeDict | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n-        rescale_factor: Optional[float],\n+        rescale_factor: float | None,\n         do_center_crop: bool,\n-        crop_size: Optional[SizeDict],\n+        crop_size: SizeDict | None,\n         do_flip_channel_order: bool,\n         disable_grouping: bool,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         processed_images = []\n@@ -184,7 +184,7 @@ def _preprocess(\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "04eba8e1997d133c957dce42c4923baebdd6484a",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 29,
            "deletions": 31,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Nougat.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -108,16 +106,16 @@ def __init__(\n         self,\n         do_crop_margin: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_thumbnail: bool = True,\n         do_align_long_axis: bool = False,\n         do_pad: bool = True,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -158,8 +156,8 @@ def crop_margin(\n         self,\n         image: np.ndarray,\n         gray_threshold: int = 200,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the\n@@ -211,8 +209,8 @@ def align_long_axis(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Align the long axis of the image to the longest axis of the specified size.\n@@ -258,8 +256,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad the image to the specified size at the top, bottom, left and right.\n@@ -295,8 +293,8 @@ def thumbnail(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -346,8 +344,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -384,21 +382,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_crop_margin: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_thumbnail: Optional[bool] = None,\n-        do_align_long_axis: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_crop_margin: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_thumbnail: bool | None = None,\n+        do_align_long_axis: bool | None = None,\n+        do_pad: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "d4b2e2c866ddb620249a339d9bf4c6c12fd023d7",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Nougat.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -253,10 +253,10 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_crop_margin: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         disable_grouping: bool,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Crop images"
        },
        {
            "sha": "64b9d122e100a1f419b95482e07c3586d1d01b39",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -33,35 +33,33 @@ def __call__(\n         self,\n         images=None,\n         text=None,\n-        do_crop_margin: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        do_crop_margin: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample: \"PILImageResampling\" = None,  # noqa: F821\n-        do_thumbnail: Optional[bool] = None,\n-        do_align_long_axis: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        do_thumbnail: bool | None = None,\n+        do_align_long_axis: bool | None = None,\n+        do_pad: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         data_format: Optional[\"ChannelDimension\"] = \"channels_first\",  # noqa: F821\n-        input_data_format: Optional[Union[str, \"ChannelDimension\"]] = None,  # noqa: F821\n-        text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        text_pair_target: Optional[\n-            Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n-        ] = None,\n+        input_data_format: Union[str, \"ChannelDimension\"] | None = None,  # noqa: F821\n+        text_pair: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        text_target: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        text_pair_target: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy | None = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,"
        },
        {
            "sha": "58578ca69d90b593787432be53bc60f36d1f5242",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n import warnings\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_transforms import center_to_corners_format\n@@ -44,7 +44,7 @@ class OmDetTurboTextKwargs(TextKwargs, total=False):\n         or pre-tokenized input. The task description guides the model on what objects to detect in the images.\n     \"\"\"\n \n-    task: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]\n+    task: str | list[str] | TextInput | PreTokenizedInput | None\n \n \n if is_torch_available():\n@@ -136,7 +136,7 @@ def _post_process_boxes_for_image(\n     image_size: tuple[int, int],\n     threshold: float,\n     nms_threshold: float,\n-    max_num_det: Optional[int] = None,\n+    max_num_det: int | None = None,\n ) -> tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n     \"\"\"\n     Filter predicted results using given thresholds and NMS.\n@@ -214,8 +214,8 @@ def __init__(self, image_processor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[list[str], list[list[str]]]] = None,\n+        images: ImageInput | None = None,\n+        text: list[str] | list[list[str]] | None = None,\n         **kwargs: Unpack[OmDetTurboProcessorKwargs],\n     ) -> BatchFeature:\n         if images is None or text is None:\n@@ -284,11 +284,11 @@ def _get_default_image_size(self) -> tuple[int, int]:\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"OmDetTurboObjectDetectionOutput\",\n-        text_labels: Optional[Union[list[str], list[list[str]]]] = None,\n+        text_labels: list[str] | list[list[str]] | None = None,\n         threshold: float = 0.3,\n         nms_threshold: float = 0.5,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n-        max_num_det: Optional[int] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n+        max_num_det: int | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`OmDetTurboForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "a09aeb382d189c5562d20d7fd99d088d4ec8f221",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 83,
            "deletions": 83,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n import json\n import os\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n@@ -79,11 +79,11 @@ class OneFormerImageProcessorKwargs(ImagesKwargs, total=False):\n         Whether to decrement all label values by 1, mapping the background class to `ignore_index`.\n     \"\"\"\n \n-    repo_path: Optional[str]\n-    class_info_file: Optional[str]\n-    num_text: Optional[int]\n-    num_labels: Optional[int]\n-    ignore_index: Optional[int]\n+    repo_path: str | None\n+    class_info_file: str | None\n+    num_text: int | None\n+    num_labels: int | None\n+    ignore_index: int | None\n     do_reduce_labels: bool\n \n \n@@ -97,7 +97,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -116,7 +116,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -233,8 +233,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -290,8 +290,8 @@ def compute_segments(\n # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n     segmentation_map: np.ndarray,\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -331,10 +331,10 @@ def convert_segmentation_map_to_binary_masks(\n \n def get_oneformer_resize_output_image_size(\n     image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    max_size: Optional[int] = None,\n+    size: int | tuple[int, int] | list[int] | tuple[int],\n+    max_size: int | None = None,\n     default_to_square: bool = True,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple:\n     \"\"\"\n     Computes the output size given the desired size.\n@@ -453,19 +453,19 @@ class OneFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        repo_path: Optional[str] = \"shi-labs/oneformer_demo\",\n-        class_info_file: Optional[str] = None,\n-        num_text: Optional[int] = None,\n-        num_labels: Optional[int] = None,\n+        repo_path: str | None = \"shi-labs/oneformer_demo\",\n+        class_info_file: str | None = None,\n+        num_text: int | None = None,\n+        num_labels: int | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -512,7 +512,7 @@ def resize(\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         data_format=None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -547,8 +547,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -575,8 +575,8 @@ def rescale(\n     def convert_segmentation_map_to_binary_masks(\n         self,\n         segmentation_map: np.ndarray,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        ignore_index: Optional[int] = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n     ):\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n@@ -594,15 +594,15 @@ def __call__(self, images, task_inputs=None, segmentation_maps=None, **kwargs) -\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_resize:\n             image = self.resize(image, size=size, resample=resample, input_data_format=input_data_format)\n@@ -615,16 +615,16 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -655,9 +655,9 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -691,22 +691,22 @@ def _preprocess_mask(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        task_inputs: Optional[list[str]] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        ignore_index: Optional[int] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        task_inputs: list[str] | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        ignore_index: int | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         if task_inputs is None:\n             # Default value\n@@ -789,9 +789,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -816,11 +816,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -967,12 +967,12 @@ def encode_inputs(\n         self,\n         pixel_values_list: list[ImageInput],\n         task_inputs: list[str],\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        ignore_index: Optional[int] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n@@ -1099,7 +1099,7 @@ def encode_inputs(\n \n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation\n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n@@ -1156,8 +1156,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ):\n         \"\"\"\n         Converts the output of [`OneFormerForUniversalSegmentationOutput`] into image instance segmentation\n@@ -1274,8 +1274,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation"
        },
        {
            "sha": "a09519fb1db5aac8b747cc05412ac417f6177b75",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -155,8 +155,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -209,8 +209,8 @@ def compute_segments(\n \n def convert_segmentation_map_to_binary_masks_fast(\n     segmentation_map: \"torch.Tensor\",\n-    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-    ignore_index: Optional[int] = None,\n+    instance_id_to_semantic_id: dict[int, int] | None = None,\n+    ignore_index: int | None = None,\n     do_reduce_labels: bool = False,\n ):\n     if do_reduce_labels and ignore_index is None:\n@@ -249,8 +249,8 @@ def convert_segmentation_map_to_binary_masks_fast(\n \n def get_oneformer_resize_output_image_size(\n     image: \"torch.Tensor\",\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    max_size: Optional[int] = None,\n+    size: int | tuple[int, int] | list[int] | tuple[int],\n+    max_size: int | None = None,\n     default_to_square: bool = True,\n ) -> tuple:\n     \"\"\"\n@@ -330,9 +330,9 @@ def __init__(self, **kwargs: Unpack[OneFormerImageProcessorKwargs]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        task_inputs: Optional[list[str]] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        task_inputs: list[str] | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n         **kwargs: Unpack[OneFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -354,12 +354,12 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        task_inputs: Optional[list[str]],\n+        task_inputs: list[str] | None,\n         segmentation_maps: ImageInput,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[OneFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -383,21 +383,21 @@ def _preprocess_image_like_inputs(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        task_inputs: Optional[list[str]],\n+        task_inputs: list[str] | None,\n         segmentation_maps: list[\"torch.Tensor\"],\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        ignore_index: Optional[int],\n-        do_reduce_labels: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        ignore_index: int | None,\n+        do_reduce_labels: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n@@ -475,7 +475,7 @@ def pad(\n         self,\n         images: list[\"torch.Tensor\"],\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pad a batch of images to the same size using torch operations.\n@@ -509,8 +509,8 @@ def pad(\n     def convert_segmentation_map_to_binary_masks(\n         self,\n         segmentation_map: \"torch.Tensor\",\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        ignore_index: Optional[int] = None,\n+        instance_id_to_semantic_id: dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n     ):\n         return convert_segmentation_map_to_binary_masks_fast(\n@@ -619,12 +619,12 @@ def get_panoptic_annotations(self, label, num_class_obj):\n     def _encode_inputs_fast(\n         self,\n         pixel_values_list: list[\"torch.Tensor\"],\n-        task_inputs: Optional[list[str]] = None,\n-        segmentation_maps: Optional[list[\"torch.Tensor\"]] = None,\n-        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        ignore_index: Optional[int] = None,\n+        task_inputs: list[str] | None = None,\n+        segmentation_maps: list[\"torch.Tensor\"] | None = None,\n+        instance_id_to_semantic_id: list[dict[int, int]] | dict[int, int] | None = None,\n+        ignore_index: int | None = None,\n         do_reduce_labels: bool = False,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_tensors: str | TensorType | None = None,\n     ) -> BatchFeature:\n         if task_inputs is None:\n             task_inputs = [\"panoptic\"]\n@@ -687,7 +687,7 @@ def _encode_inputs_fast(\n         return encoded_inputs\n \n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n@@ -746,8 +746,8 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n-        return_coco_annotation: Optional[bool] = False,\n+        target_sizes: list[tuple[int, int]] | None = None,\n+        return_coco_annotation: bool | None = False,\n     ):\n         \"\"\"\n         Converts the output of [`OneFormerForUniversalSegmentationOutput`] into image instance segmentation\n@@ -864,8 +864,8 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[set[int]] = None,\n-        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        label_ids_to_fuse: set[int] | None = None,\n+        target_sizes: list[tuple[int, int]] | None = None,\n     ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation"
        },
        {
            "sha": "bd8ba616751195eb28edd5cb06d1a8529cc68d0f",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n from functools import lru_cache\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -252,16 +251,16 @@ class Ovis2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         crop_to_patches: bool = False,\n         min_patches: int = 1,\n         max_patches: int = 12,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         use_covering_area_grid: bool = True,\n         **kwargs,\n@@ -288,8 +287,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -335,21 +334,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        crop_to_patches: Optional[bool] = None,\n-        min_patches: Optional[int] = None,\n-        max_patches: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        crop_to_patches: bool | None = None,\n+        min_patches: int | None = None,\n+        max_patches: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        do_convert_rgb: bool | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         use_covering_area_grid: bool = True,\n     ) -> PIL.Image.Image:\n         \"\"\"\n@@ -499,8 +498,8 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_covering_area_grid: bool = True,\n-        patch_size: Optional[Union[tuple, int, dict]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        patch_size: tuple | int | dict | None = None,\n+        data_format: ChannelDimension | None = None,\n         covering_threshold: float = 0.9,\n     ):\n         \"\"\""
        },
        {
            "sha": "a9c4307cc34c6a1564703cf076605bf50dfbbe53",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -66,7 +66,7 @@ def crop_image_to_patches(\n         max_patches: int,\n         use_covering_area_grid: bool = True,\n         covering_threshold: float = 0.9,\n-        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        patch_size: tuple | int | dict | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\"\n@@ -162,10 +162,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches and max_patches > 1:"
        },
        {
            "sha": "acebbb4b2f84139ad5437477f81f35ae9702e7ff",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -62,8 +61,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Ovis2ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "daf12466f241e3a6d281a53057e46dcb49f0fc80",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Image processor class for OWLv2.\"\"\"\n \n import warnings\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -244,14 +244,14 @@ class Owlv2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_pad: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -269,8 +269,8 @@ def __init__(\n     def pad(\n         self,\n         image: np.ndarray,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pad an image to a square with gray pixels on the bottom and the right, as per the original OWLv2\n@@ -303,8 +303,8 @@ def resize(\n         size: dict[str, int],\n         anti_aliasing: bool = True,\n         anti_aliasing_sigma=None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -368,17 +368,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_pad: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_pad: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -498,7 +498,7 @@ def post_process_object_detection(\n         self,\n         outputs: \"Owlv2ObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "f234afd79e89b40289a557e13507da3699aa75a3",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \n import warnings\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -56,7 +56,7 @@ def post_process_object_detection(\n         self,\n         outputs: \"Owlv2ObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "39c8623937ea6abe9f5a67ccac119a805fbbfd2c",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n Image/Text processor class for OWLv2\n \"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -43,7 +43,7 @@ class Owlv2ImagesKwargs(ImagesKwargs, total=False):\n         performs image-to-image matching instead of text-to-image matching.\n     \"\"\"\n \n-    query_images: Optional[ImageInput]\n+    query_images: ImageInput | None\n \n \n class Owlv2ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -67,8 +67,8 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OwlViT->Owlv2\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Owlv2ProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -145,8 +145,8 @@ def post_process_grounded_object_detection(\n         self,\n         outputs: \"Owlv2ObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n-        text_labels: Optional[list[list[str]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n+        text_labels: list[list[str]] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -195,7 +195,7 @@ def post_process_image_guided_detection(\n         outputs: \"Owlv2ImageGuidedObjectDetectionOutput\",\n         threshold: float = 0.0,\n         nms_threshold: float = 0.3,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the output of [`Owlv2ForObjectDetection.image_guided_detection`] into the format expected by the COCO"
        },
        {
            "sha": "61d8bb08c61b4ba85ca12bcff85e0caa1da1805c",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for OwlViT\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -203,8 +203,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -240,8 +240,8 @@ def center_crop(\n         self,\n         image: np.ndarray,\n         crop_size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -275,8 +275,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -303,19 +303,19 @@ def rescale(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Prepares an image or batch of images for the model.\n@@ -442,7 +442,7 @@ def post_process_object_detection(\n         self,\n         outputs: \"OwlViTObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "f1f8f49098c53af7427e7a8d114625ef7c8e37c9",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for OwlViT\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import torch\n \n@@ -51,7 +51,7 @@ def post_process_object_detection(\n         self,\n         outputs: \"OwlViTObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "0863c5775fdceff64a11a319ae2110772606f6ef",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n Image/Text processor class for OWL-ViT\n \"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -43,7 +43,7 @@ class OwlViTImagesKwargs(ImagesKwargs, total=False):\n         performs image-to-image matching instead of text-to-image matching.\n     \"\"\"\n \n-    query_images: Optional[ImageInput]\n+    query_images: ImageInput | None\n \n \n class OwlViTProcessorKwargs(ProcessingKwargs, total=False):\n@@ -66,8 +66,8 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[OwlViTProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -150,8 +150,8 @@ def post_process_grounded_object_detection(\n         self,\n         outputs: \"OwlViTObjectDetectionOutput\",\n         threshold: float = 0.1,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n-        text_labels: Optional[list[list[str]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n+        text_labels: list[list[str]] | None = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -199,7 +199,7 @@ def post_process_image_guided_detection(\n         outputs: \"OwlViTImageGuidedObjectDetectionOutput\",\n         threshold: float = 0.0,\n         nms_threshold: float = 0.3,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Converts the output of [`OwlViTForObjectDetection.image_guided_detection`] into the format expected by the COCO"
        },
        {
            "sha": "a14852f9350bcc6b72f1bd536b14aea2c4c15c01",
            "filename": "src/transformers/models/paddleocr_vl/image_processing_paddleocr_vl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fimage_processing_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fimage_processing_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fimage_processing_paddleocr_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -24,7 +24,6 @@\n # limitations under the License.\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -331,23 +330,23 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        min_pixels: int | None = None,\n+        max_pixels: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "63ca98cb3dc132adbc2d6361f4c2b719f90a58e9",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for PaliGemma.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -45,7 +43,7 @@ class PaliGemmaTextKwargs(TextKwargs):\n         for more information. If your prompt is \"<image> What is on the image\", the suffix corresponds to the expected prediction \"a cow sitting on a bench\".\n     \"\"\"\n \n-    suffix: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n+    suffix: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None\n \n \n class PaliGemmaProcessorKwargs(ProcessingKwargs, total=False):\n@@ -130,8 +128,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[PaliGemmaProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "69734fb055afd0c72bbcd366dd61410d6cba37f4",
            "filename": "src/transformers/models/parakeet/processing_parakeet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n \n from ...audio_utils import AudioInput, make_list_of_audio\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -47,8 +46,8 @@ def __init__(self, feature_extractor, tokenizer):\n     def __call__(\n         self,\n         audio: AudioInput,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n-        sampling_rate: Optional[int] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        sampling_rate: int | None = None,\n         **kwargs: Unpack[ParakeetProcessorKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "82a6d0e1121e0b1c5051fed6ffb940ff1a2e18ad",
            "filename": "src/transformers/models/pe_video/video_processing_pe_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpe_video%2Fvideo_processing_pe_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpe_video%2Fvideo_processing_pe_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpe_video%2Fvideo_processing_pe_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n \n import torch\n \n@@ -28,8 +27,8 @@ class PeVideoVideoProcessor(BaseVideoProcessor):\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         if num_frames:"
        },
        {
            "sha": "fdcea93cea9f74800528e4ce0f7df52c56085473",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver.py",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Perceiver.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -87,15 +85,15 @@ class PerceiverImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -119,9 +117,9 @@ def center_crop(\n         self,\n         image: np.ndarray,\n         crop_size: dict[str, int],\n-        size: Optional[int] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        size: int | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -165,8 +163,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -212,19 +210,19 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "f21f8233f1fbeb71dcae38c291be00e40ff82bd3",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Perceiver.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -83,10 +83,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "440ff0487d71c64946a5ffe3cc22b698f019d5e3",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \n import math\n from functools import reduce\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -53,7 +52,7 @@ class PerceptionLMImageProcessorKwargs(ImagesKwargs, total=False):\n         Maximum number of tiles an image can be split into based on its aspect ratio.\n     \"\"\"\n \n-    vision_input_type: Optional[str]\n+    vision_input_type: str | None\n     tile_size: int\n     max_num_tiles: int\n \n@@ -244,7 +243,7 @@ def resize(\n         tile_size: int,\n         max_num_tiles: int,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         height, width = get_image_size(image, channel_dim=input_data_format)\n         if max_num_tiles > 1:\n@@ -262,15 +261,15 @@ def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n-        do_rescale: Optional[bool],\n-        rescale_factor: Optional[Union[int, float]],\n-        do_normalize: Optional[bool],\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        do_rescale: bool | None,\n+        rescale_factor: int | float | None,\n+        do_normalize: bool | None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         vision_input_type: str,\n         tile_size: int,\n         max_num_tiles: int,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         disable_grouping: bool,\n         **kwargs: Unpack[PerceptionLMImageProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "0af66b4536733f49dd3f9a3f883cb6cf358bde36",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -68,9 +67,9 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[PerceptionLMProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "0857a10db2c5e0fd82f498b07f9fbd2232e891a7",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -170,11 +170,11 @@ def _preprocess(\n         patch_size: int,\n         dynamic_hd: int,\n         do_rescale: bool,\n-        rescale_factor: Optional[float],\n+        rescale_factor: float | None,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         if size.height != size.width:"
        },
        {
            "sha": "dbfcc45bff5331383af4ee3ef335b934f1d9fa9f",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,7 +17,6 @@\n \"\"\"\n \n import re\n-from typing import Optional, Union\n \n from ...audio_utils import AudioInput\n from ...image_processing_utils import BatchFeature\n@@ -56,9 +55,9 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, list[TextInput]],\n-        images: Optional[ImageInput] = None,\n-        audio: Optional[AudioInput] = None,\n+        text: TextInput | list[TextInput],\n+        images: ImageInput | None = None,\n+        audio: AudioInput | None = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "45e68cdb788eca375a6dd674937c41ce2e779ca3",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 15,
            "deletions": 18,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,6 @@\n \n import io\n import math\n-from typing import Optional, Union\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n@@ -64,7 +63,7 @@ class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     max_patches: int\n     patch_size: dict[str, int]\n     is_vqa: bool\n-    header_text: Optional[Union[list[str], str]]\n+    header_text: list[str] | str | None\n \n \n # adapted from: https://discuss.pytorch.org/t/tf-image-extract-patches-in-pytorch/171409/2\n@@ -104,8 +103,8 @@ def render_text(\n     right_padding: int = 5,\n     top_padding: int = 5,\n     bottom_padding: int = 5,\n-    font_bytes: Optional[bytes] = None,\n-    font_path: Optional[str] = None,\n+    font_bytes: bytes | None = None,\n+    font_path: str | None = None,\n ) -> Image.Image:\n     \"\"\"\n     Render text. This script is entirely adapted from the original script that can be found here:\n@@ -163,9 +162,7 @@ def render_text(\n \n \n # Adapted from https://github.com/google-research/pix2struct/blob/0e1779af0f4db4b652c1d92b3bbd2550a7399123/pix2struct/preprocessing/preprocessing_utils.py#L87\n-def render_header(\n-    image: np.ndarray, header: str, input_data_format: Optional[Union[str, ChildProcessError]] = None, **kwargs\n-):\n+def render_header(image: np.ndarray, header: str, input_data_format: str | ChildProcessError | None = None, **kwargs):\n     \"\"\"\n     Renders the input text as a header on the input image.\n \n@@ -233,7 +230,7 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_normalize: bool = True,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n         max_patches: int = 2048,\n         is_vqa: bool = False,\n         **kwargs,\n@@ -250,7 +247,7 @@ def extract_flattened_patches(\n         image: np.ndarray,\n         max_patches: int,\n         patch_size: dict,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -329,8 +326,8 @@ def extract_flattened_patches(\n     def normalize(\n         self,\n         image: np.ndarray,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -365,14 +362,14 @@ def normalize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        header_text: Optional[str] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_normalize: Optional[bool] = None,\n-        max_patches: Optional[int] = None,\n-        patch_size: Optional[dict[str, int]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        header_text: str | None = None,\n+        do_convert_rgb: bool | None = None,\n+        do_normalize: bool | None = None,\n+        max_patches: int | None = None,\n+        patch_size: dict[str, int] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> ImageInput:\n         \"\"\""
        },
        {
            "sha": "85672db400e852b4974560c126c33e43e2e56da3",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Pix2Struct.\"\"\"\n \n-from typing import Optional, Union\n-\n import torch\n from PIL import Image\n from torchvision.transforms.v2 import functional as F\n@@ -64,7 +62,7 @@ class Pix2StructImageProcessorFast(BaseImageProcessorFast):\n \n     def _further_process_kwargs(\n         self,\n-        patch_size: Optional[dict[str, int]] = None,\n+        patch_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -88,8 +86,8 @@ def render_header(\n         self,\n         image: torch.Tensor,\n         header: str,\n-        font_bytes: Optional[bytes] = None,\n-        font_path: Optional[str] = None,\n+        font_bytes: bytes | None = None,\n+        font_path: str | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Render header text on image using torch tensors.\n@@ -236,7 +234,7 @@ def extract_flattened_patches(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        header_text: Optional[Union[str, list[str]]] = None,\n+        header_text: str | list[str] | None = None,\n         **kwargs: Unpack[Pix2StructImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -248,10 +246,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        header_text: Optional[Union[str, list[str]]] = None,\n+        header_text: str | list[str] | None = None,\n         do_convert_rgb: bool = True,\n         input_data_format: ChannelDimension = ChannelDimension.FIRST,\n-        device: Optional[Union[str, torch.device]] = None,\n+        device: str | torch.device | None = None,\n         **kwargs: Unpack[Pix2StructImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -291,7 +289,7 @@ def _preprocess(\n         do_normalize: bool,\n         max_patches: int,\n         patch_size: SizeDict,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         disable_grouping: bool,\n         **kwargs,\n     ) -> BatchFeature:"
        },
        {
            "sha": "189c539daaf0de0fd8e7cda7a099ad689f6a1f1d",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Pix2Struct.\n \"\"\"\n \n-from typing import Union\n-\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n@@ -55,9 +53,9 @@ def __init__(self, image_processor, tokenizer):\n     def __call__(\n         self,\n         images=None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Pix2StructProcessorKwargs],\n-    ) -> Union[BatchEncoding, BatchFeature]:\n+    ) -> BatchEncoding | BatchFeature:\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n "
        },
        {
            "sha": "a9ab86582869b08013b701e8a1c6092e60f56157",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 26,
            "deletions": 27,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for Pixtral.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -55,7 +54,7 @@ class PixtralImageProcessorKwargs(ImagesKwargs, total=False):\n         Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n     \"\"\"\n \n-    patch_size: Union[dict[str, int], int]\n+    patch_size: dict[str, int] | int\n \n \n # Adapted from function in image_transforms.py to ensure any transparent pixels are converted to white.\n@@ -107,9 +106,9 @@ def _num_image_tokens(image_size: tuple[int, int], patch_size: tuple[int, int])\n \n def get_resize_output_image_size(\n     input_image: ImageInput,\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    patch_size: Union[int, tuple[int, int], list[int], tuple[int]],\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int] | tuple[int],\n+    patch_size: int | tuple[int, int] | list[int] | tuple[int],\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple:\n     \"\"\"\n     Find the target (height, width) dimension of the output image after resizing given the input image and the desired\n@@ -185,14 +184,14 @@ class PixtralImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        patch_size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n+        patch_size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -234,8 +233,8 @@ def resize(\n         size: dict[str, int],\n         patch_size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -287,8 +286,8 @@ def _pad_for_batching(\n         self,\n         pixel_values: list[np.ndarray],\n         image_sizes: list[list[int]],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n@@ -329,19 +328,19 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        patch_size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        patch_size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> PIL.Image.Image:\n         \"\"\""
        },
        {
            "sha": "31924a34029fdc2cb6d2a73876fec3c03de00574",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Pixtral.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -134,10 +134,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         patch_size = get_size_dict(patch_size, default_to_square=True)"
        },
        {
            "sha": "569a73cf681d144294bbc3e18f4522f959264b10",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Pixtral.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -101,8 +99,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[PixtralProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "47c0dc177a203d085713515a21527aae9f37338d",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 23,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for PoolFormer.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -113,16 +111,16 @@ class PoolFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         crop_pct: int = 0.9,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        crop_size: dict[str, int] | None = None,\n+        rescale_factor: int | float = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -147,10 +145,10 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        crop_pct: Optional[float] = None,\n+        crop_pct: float | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -223,20 +221,20 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        crop_pct: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        crop_pct: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "fbed60891ee7884d9f822ac8dba25c68568651f9",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for PoolFormer.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -68,7 +68,7 @@ def resize(\n         self,\n         image: \"torch.Tensor\",\n         size: SizeDict,\n-        crop_pct: Optional[float] = None,\n+        crop_pct: float | None = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n         antialias: bool = True,\n         **kwargs,\n@@ -199,10 +199,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "e7588ad148163782b7fbb70b075b75424cfbf5e4",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Processor class for Pop2Piano.\"\"\"\n \n import os\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -34,18 +33,18 @@ def __init__(self, feature_extractor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        audio: Union[np.ndarray, list[float], list[np.ndarray]] = None,\n-        sampling_rate: Optional[Union[int, list[int]]] = None,\n+        audio: np.ndarray | list[float] | list[np.ndarray] = None,\n+        sampling_rate: int | list[int] | None = None,\n         steps_per_beat: int = 2,\n-        resample: Optional[bool] = True,\n-        notes: Union[list, TensorType] = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n+        resample: bool | None = True,\n+        notes: list | TensorType = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy = None,\n+        max_length: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n         verbose: bool = True,\n         **kwargs,\n-    ) -> Union[BatchFeature, BatchEncoding]:\n+    ) -> BatchFeature | BatchEncoding:\n         # Since Feature Extractor needs both audio and sampling_rate and tokenizer needs both token_ids and\n         # feature_extractor_output, we must check for both.\n         r\"\"\""
        },
        {
            "sha": "369afaf46998abde4b386119d5fd4e16a5d2f9c0",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n \"\"\"Image processor class for PromptDepthAnything.\"\"\"\n \n import math\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n \n if TYPE_CHECKING:\n@@ -87,7 +87,7 @@ def _get_resize_output_image_size(\n     output_size: tuple[int, int],\n     keep_aspect_ratio: bool,\n     multiple: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     input_height, input_width = get_image_size(input_image, input_data_format)\n     output_height, output_width = output_size\n@@ -158,17 +158,17 @@ class PromptDepthAnythingImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = False,\n-        size_divisor: Optional[int] = None,\n+        size_divisor: int | None = None,\n         prompt_scale_to_meter: float = 0.001,  # default unit is mm\n         **kwargs,\n     ):\n@@ -196,8 +196,8 @@ def resize(\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -245,8 +245,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         size_divisor: int,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Center pad an image to be a multiple of `multiple`.\n@@ -293,23 +293,23 @@ def _get_pad(size, size_divisor):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        prompt_depth: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[int] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        prompt_scale_to_meter: Optional[float] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        prompt_depth: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: int | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        prompt_scale_to_meter: float | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -472,7 +472,7 @@ def preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "67c833a7955f6c1b73859cba66ea57f1482652e0",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -113,7 +113,7 @@ def __init__(self, **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        prompt_depth: Optional[ImageInput] = None,\n+        prompt_depth: ImageInput | None = None,\n         **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -186,11 +186,11 @@ def _get_pad(size, size_divisor):\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        prompt_depth: Optional[ImageInput],\n+        prompt_depth: ImageInput | None,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n-        prompt_scale_to_meter: Optional[float] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n+        prompt_scale_to_meter: float | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -248,18 +248,18 @@ def _preprocess(\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n         size: SizeDict,\n-        keep_aspect_ratio: Optional[bool],\n+        keep_aspect_ratio: bool | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        ensure_multiple_of: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        size_divisor: Optional[int] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        disable_grouping: bool | None,\n+        ensure_multiple_of: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        size_divisor: int | None = None,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -305,7 +305,7 @@ def _preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"DepthEstimatorOutput\",\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "e374649f99c24d4fa7cd3735eec8d122202a7bea",
            "filename": "src/transformers/models/pvt/image_processing_pvt.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Pvt.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -74,13 +72,13 @@ class PvtImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -101,8 +99,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -148,17 +146,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "dcc98856ddc25fa41ca9d92bce1f84148bc3d632",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,7 +18,6 @@\n \n import logging\n import re\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -75,7 +74,7 @@ class Qwen2_5_OmniVideosKwargs(VideosKwargs, total=False):\n     max_frames: int\n     use_audio_in_video: bool\n     seconds_per_chunk: float\n-    position_id_per_seconds: Union[int, float]\n+    position_id_per_seconds: int | float\n \n \n class Qwen2_5OmniProcessorKwargs(ProcessingKwargs, total=False):\n@@ -120,10 +119,10 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: Optional[ImageInput] = None,\n-        videos: Optional[VideoInput] = None,\n-        audio: Optional[AudioInput] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        images: ImageInput | None = None,\n+        videos: VideoInput | None = None,\n+        audio: AudioInput | None = None,\n         **kwargs: Unpack[Qwen2_5OmniProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None:"
        },
        {
            "sha": "bd9a45a55c7d2d99df6ad089f3abbd8e975342b9",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for Qwen2Audio.\n \"\"\"\n \n-from typing import Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -64,8 +62,8 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio: Union[np.ndarray, list[np.ndarray]] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        audio: np.ndarray | list[np.ndarray] = None,\n         **kwargs: Unpack[Qwen2AudioProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None:"
        },
        {
            "sha": "630431584cb73788937af6f9510a42c075bc0788",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 38,
            "deletions": 39,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,6 @@\n \"\"\"Image processor class for Qwen2-VL.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -142,16 +141,16 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n+        min_pixels: int | None = None,\n+        max_pixels: int | None = None,\n         patch_size: int = 14,\n         temporal_patch_size: int = 2,\n         merge_size: int = 2,\n@@ -187,21 +186,21 @@ def __init__(\n \n     def _preprocess(\n         self,\n-        images: Union[ImageInput, VideoInput],\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | VideoInput,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n@@ -322,23 +321,23 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        min_pixels: int | None = None,\n+        max_pixels: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "48a7bec0fe8f42457c81f02cd4268b0c905f49cb",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -86,9 +86,9 @@ def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n+        size: SizeDict | None = None,\n+        min_pixels: int | None = None,\n+        max_pixels: int | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -120,7 +120,7 @@ def _preprocess_image_like_inputs(\n         images: ImageInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -145,13 +145,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ):\n         # Group images by size for batched resizing"
        },
        {
            "sha": "3cd29ad0e61034fa30a4d45b58fa66346b5d741e",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,8 +20,6 @@\n Processor class for Qwen2-VL.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -64,9 +62,9 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos: Optional[VideoInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[Qwen2VLProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\""
        },
        {
            "sha": "c932a250af20573551f12f28e47684610534e167",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,7 @@\n \"\"\"video processor class for Qwen2-VL.\"\"\"\n \n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -110,11 +110,11 @@ def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]):\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        temporal_patch_size: Optional[int] = None,\n-        min_frames: Optional[int] = None,\n-        max_frames: Optional[int] = None,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        temporal_patch_size: int | None = None,\n+        min_frames: int | None = None,\n+        max_frames: int | None = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -186,12 +186,12 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         # Group videos by size for batched resizing"
        },
        {
            "sha": "9ab134377829939cf0f8f34759c2d59398e70d57",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Union\n \n import numpy as np\n \n@@ -77,7 +76,7 @@ class Qwen3OmniMoeVideosKwargs(VideosKwargs, total=False):\n     max_frames: int\n     use_audio_in_video: bool\n     seconds_per_chunk: float\n-    position_id_per_seconds: Union[int, float]\n+    position_id_per_seconds: int | float\n \n \n class Qwen3OmniMoeProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "9f545d272891e566bcc84311df4e08cec99326ba",
            "filename": "src/transformers/models/qwen3_vl/video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"video processor class for Qwen3-VL.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -109,7 +108,7 @@ def __init__(self, **kwargs: Unpack[Qwen3VLVideoProcessorInitKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -124,8 +123,8 @@ def _further_process_kwargs(\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -175,17 +174,17 @@ def _preprocess(\n         videos: list[torch.Tensor],\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[SizeDict] = None,\n+        size: SizeDict | None = None,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)"
        },
        {
            "sha": "972e589a9c14a7d4839615cc677b77add2853abc",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 54,
            "deletions": 54,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import pathlib\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -83,18 +83,18 @@ class RTDetrImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -123,7 +123,7 @@ def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -153,7 +153,7 @@ def get_image_size_for_max_height_width(\n \n \n # Copied from transformers.models.detr.image_processing_detr.safe_squeeze\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -191,7 +191,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -210,7 +210,7 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -231,7 +231,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by RTDETR.\n@@ -393,18 +393,18 @@ class RTDetrImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = False,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_annotations: bool = True,\n         do_pad: bool = False,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         size = size if size is not None else {\"height\": 640, \"width\": 640}\n@@ -431,10 +431,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into RTDETR model.\n@@ -456,8 +456,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -530,8 +530,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -611,10 +611,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -644,14 +644,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -728,24 +728,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        do_convert_annotations: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model.\n@@ -969,7 +969,7 @@ def post_process_object_detection(\n         self,\n         outputs,\n         threshold: float = 0.5,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n         use_focal_loss: bool = True,\n     ):\n         \"\"\""
        },
        {
            "sha": "e2f13c7c5f544116d5a748e766c0e601e25f0d4b",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -307,7 +307,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):"
        },
        {
            "sha": "8d3fbf5b86bc7cba73dbba1935829a74ca2f8c8a",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 59,
            "deletions": 59,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n import math\n from copy import deepcopy\n from itertools import product\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import numpy as np\n \n@@ -125,17 +125,17 @@ class SamImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        mask_size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n+        mask_size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[int] = None,\n-        mask_pad_size: Optional[int] = None,\n+        pad_size: int | None = None,\n+        mask_pad_size: int | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -174,8 +174,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         pad_size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -223,8 +223,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -273,14 +273,14 @@ def _preprocess(\n         do_resize: bool,\n         do_rescale: bool,\n         do_normalize: bool,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        rescale_factor: Optional[float] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        rescale_factor: float | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        pad_size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_resize:\n             image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n@@ -300,19 +300,19 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        pad_size: dict[str, int] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> tuple[np.ndarray, tuple[int, int], tuple[int, int]]:\n         # PIL RGBA images are converted to RGB\n         if do_convert_rgb:\n@@ -355,11 +355,11 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        mask_size: Optional[dict[str, int]] = None,\n-        do_pad: Optional[bool] = None,\n-        mask_pad_size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        mask_size: dict[str, int] | None = None,\n+        do_pad: bool | None = None,\n+        mask_pad_size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         segmentation_map = to_numpy_array(segmentation_map)\n \n@@ -403,23 +403,23 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        mask_size: Optional[dict[str, int]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        mask_size: dict[str, int] | None = None,\n         resample: Optional[\"PILImageResampling\"] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n-        mask_pad_size: Optional[dict[str, int]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        pad_size: dict[str, int] | None = None,\n+        mask_pad_size: dict[str, int] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -695,10 +695,10 @@ def generate_crop_boxes(\n         target_size,\n         crop_n_layers: int = 0,\n         overlap_ratio: float = 512 / 1500,\n-        points_per_crop: Optional[int] = 32,\n-        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        points_per_crop: int | None = 32,\n+        crop_n_points_downscale_factor: list[int] | None = 1,\n         device: Optional[\"torch.device\"] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         return_tensors: str = \"pt\",\n     ):\n         \"\"\"\n@@ -932,9 +932,9 @@ def _generate_crop_boxes(\n     target_size: int,  # Is it tuple here?\n     crop_n_layers: int = 0,\n     overlap_ratio: float = 512 / 1500,\n-    points_per_crop: Optional[int] = 32,\n-    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    points_per_crop: int | None = 32,\n+    crop_n_points_downscale_factor: list[int] | None = 1,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[list[list[int]], list[int]]:\n     \"\"\"\n     Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer."
        },
        {
            "sha": "97d1ce9b0b90da5d0119aecfabe6582413bc3c37",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -108,14 +108,14 @@ def resize(\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n-        pad_size: Optional[SizeDict] = None,\n-        mask_size: Optional[SizeDict] = None,\n-        mask_pad_size: Optional[SizeDict] = None,\n-        default_to_square: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n+        size: SizeDict | None = None,\n+        pad_size: SizeDict | None = None,\n+        mask_size: SizeDict | None = None,\n+        mask_pad_size: SizeDict | None = None,\n+        default_to_square: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -162,7 +162,7 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[SamImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -174,10 +174,10 @@ def preprocess(\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput],\n+        segmentation_maps: ImageInput | None,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[SamImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -231,12 +231,12 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        pad_size: Optional[SizeDict],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        pad_size: SizeDict | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n@@ -277,8 +277,8 @@ def generate_crop_boxes(\n         target_size,\n         crop_n_layers: int = 0,\n         overlap_ratio: float = 512 / 1500,\n-        points_per_crop: Optional[int] = 32,\n-        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        points_per_crop: int | None = 32,\n+        crop_n_points_downscale_factor: list[int] | None = 1,\n         device: Optional[\"torch.device\"] = None,\n     ):\n         \"\"\"\n@@ -597,8 +597,8 @@ def _generate_crop_boxes(\n     target_size: int,  # Is it tuple here?\n     crop_n_layers: int = 0,\n     overlap_ratio: float = 512 / 1500,\n-    points_per_crop: Optional[int] = 32,\n-    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+    points_per_crop: int | None = 32,\n+    crop_n_points_downscale_factor: list[int] | None = 1,\n ) -> tuple[list[list[int]], list[int]]:\n     \"\"\"\n     Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer."
        },
        {
            "sha": "75e0d3ecd2b108002809326c5c44b956068b8273",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n from copy import deepcopy\n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n \n@@ -29,7 +29,7 @@\n if is_torch_available():\n     import torch\n \n-NestedList = list[Union[Optional[float | int], \"NestedList\"]]\n+NestedList = list[Union[float | int | None, \"NestedList\"]]\n \n \n class SamImagesKwargs(ImagesKwargs, total=False):\n@@ -60,11 +60,11 @@ class SamImagesKwargs(ImagesKwargs, total=False):\n         batching masks of different sizes to ensure consistent dimensions.\n     \"\"\"\n \n-    segmentation_maps: Optional[ImageInput]\n-    input_points: Optional[NestedList]\n-    input_labels: Optional[NestedList]\n-    input_boxes: Optional[NestedList]\n-    point_pad_value: Optional[int]\n+    segmentation_maps: ImageInput | None\n+    input_points: NestedList | None\n+    input_labels: NestedList | None\n+    input_boxes: NestedList | None\n+    point_pad_value: int | None\n     mask_size: dict[str, int]\n     mask_pad_size: dict[str, int]\n \n@@ -87,8 +87,8 @@ def __init__(self, image_processor):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "6b5c4edb17917e6ce153434f67385582ab62b60a",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -175,8 +175,8 @@ def _generate_crop_boxes(\n     target_size: int,  # Is it tuple here?\n     crop_n_layers: int = 0,\n     overlap_ratio: float = 512 / 1500,\n-    points_per_crop: Optional[int] = 32,\n-    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+    points_per_crop: int | None = 32,\n+    crop_n_points_downscale_factor: list[int] | None = 1,\n ) -> tuple[list[list[int]], list[int]]:\n     \"\"\"\n     Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n@@ -505,8 +505,8 @@ def generate_crop_boxes(\n         target_size,\n         crop_n_layers: int = 0,\n         overlap_ratio: float = 512 / 1500,\n-        points_per_crop: Optional[int] = 32,\n-        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        points_per_crop: int | None = 32,\n+        crop_n_points_downscale_factor: list[int] | None = 1,\n         device: Optional[\"torch.device\"] = None,\n     ):\n         \"\"\""
        },
        {
            "sha": "2d91799b96ac83733f3e9c493c45cb24bd501f71",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n from copy import deepcopy\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -36,7 +35,7 @@\n @requires(backends=(\"torch\",))\n @auto_docstring\n class Sam2Processor(ProcessorMixin):\n-    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+    def __init__(self, image_processor, target_size: int | None = None, point_pad_value: int = -10, **kwargs):\n         r\"\"\"\n         target_size (`int`, *optional*):\n             The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n@@ -54,13 +53,13 @@ def __init__(self, image_processor, target_size: Optional[int] = None, point_pad\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n-        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n-        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        input_points: list[list[list[list[float]]]] | torch.Tensor | None = None,\n+        input_labels: list[list[list[int]]] | torch.Tensor | None = None,\n+        input_boxes: list[list[list[float]]] | torch.Tensor | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -374,11 +373,11 @@ def _get_nesting_level(self, input_list):\n \n     def _validate_single_input(\n         self,\n-        data: Union[torch.Tensor, np.ndarray, list],\n+        data: torch.Tensor | np.ndarray | list,\n         expected_depth: int,\n         input_name: str,\n         expected_format: str,\n-        expected_coord_size: Optional[int] = None,\n+        expected_coord_size: int | None = None,\n     ) -> list:\n         \"\"\"\n                 Validate a single input by ensuring proper nesting and raising an error if the input is not valid."
        },
        {
            "sha": "416c5c543fe2325e6058dd166c07e1c2b8eb72a7",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from copy import deepcopy\n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n import torch\n@@ -55,13 +55,13 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n-        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n-        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        input_points: list[list[list[list[float]]]] | torch.Tensor | None = None,\n+        input_labels: list[list[list[int]]] | torch.Tensor | None = None,\n+        input_boxes: list[list[list[float]]] | torch.Tensor | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -375,11 +375,11 @@ def _get_nesting_level(self, input_list):\n \n     def _validate_single_input(\n         self,\n-        data: Union[torch.Tensor, np.ndarray, list],\n+        data: torch.Tensor | np.ndarray | list,\n         expected_depth: int,\n         input_name: str,\n         expected_format: str,\n-        expected_coord_size: Optional[int] = None,\n+        expected_coord_size: int | None = None,\n     ) -> list:\n         \"\"\"\n                 Validate a single input by ensuring proper nesting and raising an error if the input is not valid."
        },
        {
            "sha": "a1dad3a0f4df1dcba48bc8caf1d128a6f0a603f2",
            "filename": "src/transformers/models/sam2_video/video_processing_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for SAM2.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n import torch\n import torch.nn.functional as F\n@@ -40,7 +38,7 @@ def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n         size: SizeDict,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         original_sizes = [video.shape[-2:] for video in videos]"
        },
        {
            "sha": "e896322fedc5b967fa2fb0bc3ae527a6c2eab07c",
            "filename": "src/transformers/models/sam3/image_processing_sam3_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -177,8 +177,8 @@ def _generate_crop_boxes(\n     target_size: int,  # Is it tuple here?\n     crop_n_layers: int = 0,\n     overlap_ratio: float = 512 / 1500,\n-    points_per_crop: Optional[int] = 32,\n-    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+    points_per_crop: int | None = 32,\n+    crop_n_points_downscale_factor: list[int] | None = 1,\n ) -> tuple[list[list[int]], list[int]]:\n     \"\"\"\n     Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n@@ -535,8 +535,8 @@ def generate_crop_boxes(\n         target_size,\n         crop_n_layers: int = 0,\n         overlap_ratio: float = 512 / 1500,\n-        points_per_crop: Optional[int] = 32,\n-        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        points_per_crop: int | None = 32,\n+        crop_n_points_downscale_factor: list[int] | None = 1,\n         device: Optional[\"torch.device\"] = None,\n     ):\n         \"\"\""
        },
        {
            "sha": "d53270573af5cae0fb2d13b1488fa92ca036c673",
            "filename": "src/transformers/models/sam3/processing_sam3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n from copy import deepcopy\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -87,7 +86,7 @@ def box_area(boxes):\n @auto_docstring\n class Sam3Processor(ProcessorMixin):\n     def __init__(\n-        self, image_processor, tokenizer, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n+        self, image_processor, tokenizer, target_size: int | None = None, point_pad_value: int = -10, **kwargs\n     ):\n         r\"\"\"\n         target_size (`int`, *optional*):\n@@ -102,13 +101,13 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n-        input_boxes_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        input_boxes: list[list[list[float]]] | torch.Tensor | None = None,\n+        input_boxes_labels: list[list[list[int]]] | torch.Tensor | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -445,11 +444,11 @@ def _get_nesting_level(self, input_list):\n \n     def _validate_single_input(\n         self,\n-        data: Union[torch.Tensor, np.ndarray, list],\n+        data: torch.Tensor | np.ndarray | list,\n         expected_depth: int,\n         input_name: str,\n         expected_format: str,\n-        expected_coord_size: Optional[int] = None,\n+        expected_coord_size: int | None = None,\n     ) -> list:\n         \"\"\"\n                 Validate a single input by ensuring proper nesting and raising an error if the input is not valid."
        },
        {
            "sha": "d9beb6ee65da1f07747b8db9544e7838d08924d9",
            "filename": "src/transformers/models/sam3_tracker/processing_sam3_tracker.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,7 +20,6 @@\n \n \n from copy import deepcopy\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -35,7 +34,7 @@\n @requires(backends=(\"torch\",))\n @auto_docstring\n class Sam3TrackerProcessor(ProcessorMixin):\n-    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+    def __init__(self, image_processor, target_size: int | None = None, point_pad_value: int = -10, **kwargs):\n         r\"\"\"\n         target_size (`int`, *optional*):\n             The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n@@ -53,13 +52,13 @@ def __init__(self, image_processor, target_size: Optional[int] = None, point_pad\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n-        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n-        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        input_points: list[list[list[list[float]]]] | torch.Tensor | None = None,\n+        input_labels: list[list[list[int]]] | torch.Tensor | None = None,\n+        input_boxes: list[list[list[float]]] | torch.Tensor | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -373,11 +372,11 @@ def _get_nesting_level(self, input_list):\n \n     def _validate_single_input(\n         self,\n-        data: Union[torch.Tensor, np.ndarray, list],\n+        data: torch.Tensor | np.ndarray | list,\n         expected_depth: int,\n         input_name: str,\n         expected_format: str,\n-        expected_coord_size: Optional[int] = None,\n+        expected_coord_size: int | None = None,\n     ) -> list:\n         \"\"\"\n                 Validate a single input by ensuring proper nesting and raising an error if the input is not valid."
        },
        {
            "sha": "708be51bd5e32fb25297c130c8d5c26fb289d768",
            "filename": "src/transformers/models/sam3_tracker_video/processing_sam3_tracker_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,7 +20,7 @@\n \n \n from copy import deepcopy\n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n import torch\n@@ -57,13 +57,13 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n-        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n-        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        input_points: list[list[list[list[float]]]] | torch.Tensor | None = None,\n+        input_labels: list[list[list[int]]] | torch.Tensor | None = None,\n+        input_boxes: list[list[list[float]]] | torch.Tensor | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -377,11 +377,11 @@ def _get_nesting_level(self, input_list):\n \n     def _validate_single_input(\n         self,\n-        data: Union[torch.Tensor, np.ndarray, list],\n+        data: torch.Tensor | np.ndarray | list,\n         expected_depth: int,\n         input_name: str,\n         expected_format: str,\n-        expected_coord_size: Optional[int] = None,\n+        expected_coord_size: int | None = None,\n     ) -> list:\n         \"\"\"\n                 Validate a single input by ensuring proper nesting and raising an error if the input is not valid."
        },
        {
            "sha": "1a93f3f322bee3b1c72b596a7c9ff38c99ca94db",
            "filename": "src/transformers/models/sam3_video/processing_sam3_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n from torchvision.ops import masks_to_boxes\n@@ -33,7 +33,7 @@ def __init__(\n         image_processor,\n         video_processor,\n         tokenizer,\n-        target_size: Optional[int] = None,\n+        target_size: int | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -46,10 +46,10 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        images: ImageInput | None = None,\n+        segmentation_maps: ImageInput | None = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n@@ -89,7 +89,7 @@ def __call__(\n \n         return encoding_image_processor\n \n-    def add_text_prompt(self, inference_session: Sam3VideoInferenceSession, text: Union[str, list[str]]):\n+    def add_text_prompt(self, inference_session: Sam3VideoInferenceSession, text: str | list[str]):\n         \"\"\"\n         Add text prompt(s) to the inference session.\n \n@@ -123,11 +123,11 @@ def add_text_prompt(self, inference_session: Sam3VideoInferenceSession, text: Un\n \n     def init_video_session(\n         self,\n-        video: Optional[VideoInput] = None,\n+        video: VideoInput | None = None,\n         inference_device: Union[str, \"torch.device\"] = \"cpu\",\n-        inference_state_device: Optional[Union[str, \"torch.device\"]] = None,\n-        processing_device: Optional[Union[str, \"torch.device\"]] = None,\n-        video_storage_device: Optional[Union[str, \"torch.device\"]] = None,\n+        inference_state_device: Union[str, \"torch.device\"] | None = None,\n+        processing_device: Union[str, \"torch.device\"] | None = None,\n+        video_storage_device: Union[str, \"torch.device\"] | None = None,\n         max_vision_features_cache_size: int = 1,\n         dtype: torch.dtype = torch.float32,\n     ):\n@@ -239,7 +239,7 @@ def postprocess_outputs(\n         self,\n         inference_session,\n         model_outputs,\n-        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        original_sizes: list[list[float]] | torch.Tensor | None = None,\n     ):\n         \"\"\"\n         Post-process model outputs to get final masks, boxes, and scores."
        },
        {
            "sha": "c81a12b894bf9aef7bdd1ac8a908d974f8e72252",
            "filename": "src/transformers/models/sam_hq/processing_sam_hq.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n from copy import deepcopy\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -29,7 +28,7 @@\n if is_torch_available():\n     import torch\n \n-NestedList = list[Union[Optional[float | int], \"NestedList\"]]\n+NestedList = list[float | int | None | list[float | int | None]]\n \n \n class SamHQImagesKwargs(ImagesKwargs, total=False):\n@@ -61,11 +60,11 @@ class SamHQImagesKwargs(ImagesKwargs, total=False):\n         batching masks of different sizes to ensure consistent dimensions.\n     \"\"\"\n \n-    segmentation_maps: Optional[ImageInput]\n-    input_points: Optional[NestedList]\n-    input_labels: Optional[NestedList]\n-    input_boxes: Optional[NestedList]\n-    point_pad_value: Optional[int]\n+    segmentation_maps: ImageInput | None\n+    input_points: NestedList | None\n+    input_labels: NestedList | None\n+    input_boxes: NestedList | None\n+    point_pad_value: int | None\n     mask_size: dict[str, int]\n     mask_pad_size: dict[str, int]\n \n@@ -93,7 +92,7 @@ def __init__(self, image_processor):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n+        images: ImageInput | None = None,\n         **kwargs: Unpack[SamHQProcessorKwargs],\n     ) -> BatchEncoding:\n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "21b5d253a4f08d2cd5f75f9af34417952dae93d4",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Audio/Text processor class for SeamlessM4T\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...audio_utils import AudioInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n@@ -37,8 +35,8 @@ class SeamlessM4TTextKwargs(TextKwargs):\n         specify the desired output language for translation tasks. The model will generate text in this language.\n     \"\"\"\n \n-    src_lang: Optional[str]\n-    tgt_lang: Optional[str]\n+    src_lang: str | None\n+    tgt_lang: str | None\n \n \n class SeamlessM4TProcessorKwargs(ProcessingKwargs, total=False):\n@@ -56,8 +54,8 @@ def __init__(self, feature_extractor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio: Optional[AudioInput] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        audio: AudioInput | None = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "b0e04b3e4d1fda6bedba97c51450aeee5204d28d",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 39,
            "deletions": 41,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Segformer.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import INIT_SERVICE_KWARGS, BaseImageProcessor, BatchFeature, get_size_dict\n@@ -109,13 +107,13 @@ class SegformerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -138,8 +136,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -197,12 +195,12 @@ def _preprocess(\n         do_resize: bool,\n         do_rescale: bool,\n         do_normalize: bool,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        rescale_factor: Optional[float] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        rescale_factor: float | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         if do_reduce_labels:\n             image = self.reduce_label(image)\n@@ -221,16 +219,16 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         # All transformations expect numpy arrays.\n@@ -262,10 +260,10 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_reduce_labels: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_reduce_labels: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n         segmentation_map = to_numpy_array(segmentation_map)\n@@ -308,19 +306,19 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_reduce_labels: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        segmentation_maps: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_reduce_labels: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -431,7 +429,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->Segformer\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`SegformerForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "5aa93482842964d7100a5f6fff3a62609d92929d",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -72,7 +72,7 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n+        segmentation_maps: ImageInput | None = None,\n         **kwargs: Unpack[SegformerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -170,7 +170,7 @@ def _preprocess(\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] | None = None):\n         \"\"\"\n         Converts the output of [`SegformerForSemanticSegmentation`] into semantic segmentation maps.\n "
        },
        {
            "sha": "fccb00f9c428f9f2df1295d6facb512be80379b1",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 38,
            "deletions": 40,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for SegGPT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -64,7 +62,7 @@ def build_palette(num_labels: int) -> list[tuple[int, int]]:\n \n \n def mask_to_rgb(\n-    mask: np.ndarray, palette: Optional[list[tuple[int, int]]] = None, data_format: Optional[ChannelDimension] = None\n+    mask: np.ndarray, palette: list[tuple[int, int]] | None = None, data_format: ChannelDimension | None = None\n ) -> np.ndarray:\n     data_format = data_format if data_format is not None else ChannelDimension.FIRST\n \n@@ -130,13 +128,13 @@ class SegGptImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -168,8 +166,8 @@ def get_palette(self, num_labels: int) -> list[tuple[int, int]]:\n     def mask_to_rgb(\n         self,\n         image: np.ndarray,\n-        palette: Optional[list[tuple[int, int]]] = None,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        palette: list[tuple[int, int]] | None = None,\n+        data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Converts a segmentation map to RGB format.\n \n@@ -196,8 +194,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -242,18 +240,18 @@ def resize(\n     def _preprocess_step(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        num_labels: Optional[int] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        do_convert_rgb: bool | None = None,\n+        num_labels: int | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -381,22 +379,22 @@ def _preprocess_step(\n \n     def preprocess(\n         self,\n-        images: Optional[ImageInput] = None,\n-        prompt_images: Optional[ImageInput] = None,\n-        prompt_masks: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        num_labels: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | None = None,\n+        prompt_images: ImageInput | None = None,\n+        prompt_masks: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        num_labels: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -529,7 +527,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def post_process_semantic_segmentation(\n-        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None, num_labels: Optional[int] = None\n+        self, outputs, target_sizes: list[tuple[int, int]] | None = None, num_labels: int | None = None\n     ):\n         \"\"\"\n         Converts the output of [`SegGptImageSegmentationOutput`] into segmentation maps. Only supports"
        },
        {
            "sha": "04798f3774ee7e0090e5d7f90ae65d6138b81e66",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Mapping, Sequence\n-from typing import Optional\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -46,8 +45,8 @@\n \n \n class ShieldGemma2ProcessorKwargs(Gemma3ProcessorKwargs, total=False):\n-    policies: Optional[Sequence[str]]\n-    custom_policies: Optional[Mapping[str, str]]\n+    policies: Sequence[str] | None\n+    custom_policies: Mapping[str, str] | None\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": True,\n@@ -84,7 +83,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n+        images: ImageInput | None = None,\n         text=None,\n         **kwargs: Unpack[ShieldGemma2ProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "583b5f7202e275f8f48949aac9a102ad0787c568",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for SigLIP.\"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n     convert_to_rgb,\n@@ -81,14 +79,14 @@ class SiglipImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -110,18 +108,18 @@ def __init__(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        do_convert_rgb: bool | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "dd0daf87505029e870647362dd8db4d706f23432",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import math\n from functools import lru_cache\n-from typing import Optional, Union\n+from typing import Optional\n \n import numpy as np\n \n@@ -181,9 +181,9 @@ def __init__(\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n         patch_size: int = 16,\n         max_num_patches: int = 256,\n         **kwargs,\n@@ -208,18 +208,18 @@ def __init__(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n+        do_resize: bool | None = None,\n         resample: Optional[\"PILImageResampling\"] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        patch_size: Optional[int] = None,\n-        max_num_patches: Optional[int] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        do_convert_rgb: bool | None = None,\n+        patch_size: int | None = None,\n+        max_num_patches: int | None = None,\n     ) -> \"Image.Image\":\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "b8f80400bbcf288744794119cd6b89bc9703566a",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for SigLIP2.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -98,9 +98,9 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         pixel_masks = []"
        },
        {
            "sha": "60332e65597d34515992ae328063428da9500da8",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 41,
            "deletions": 42,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -21,7 +21,6 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -73,7 +72,7 @@ class SmolVLMImageProcessorKwargs(ImagesKwargs, total=False):\n \n \n def _resize_output_size_rescale_to_max_len(\n-    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+    height: int, width: int, min_len: int | None = 1, max_len: int | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -110,7 +109,7 @@ def _resize_output_size_rescale_to_max_len(\n \n \n def _resize_output_size_scale_below_upper_bound(\n-    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+    height: int, width: int, max_len: dict[str, int] | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -143,7 +142,7 @@ def _resize_output_size_scale_below_upper_bound(\n def get_resize_output_image_size(\n     image,\n     resolution_max_side: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -168,7 +167,7 @@ def get_resize_output_image_size(\n \n \n def get_max_height_width(\n-    images_list: list[list[np.ndarray]], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images_list: list[list[np.ndarray]], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -186,7 +185,7 @@ def get_max_height_width(\n \n \n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -204,9 +203,9 @@ def make_pixel_mask(\n \n def convert_to_rgb(\n     image: np.ndarray,\n-    palette: Optional[PIL.ImagePalette.ImagePalette] = None,\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    palette: PIL.ImagePalette.ImagePalette | None = None,\n+    data_format: str | ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to RGB format.\n@@ -250,7 +249,7 @@ def _crop(\n     h1: int,\n     w2: int,\n     h2: int,\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n+    data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     if data_format is None:\n         data_format = infer_channel_dimension_format(image, num_channels=(1, 3, 4))\n@@ -313,15 +312,15 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n         do_image_splitting: bool = True,\n-        max_image_size: Optional[dict[str, int]] = None,\n+        max_image_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -344,8 +343,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -401,8 +400,8 @@ def split_image(\n         image,\n         max_image_size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Split an image into squares of side max_image_size and the original image resized to max_image_size.\n@@ -480,8 +479,8 @@ def resize_for_vision_encoder(\n         image: np.ndarray,\n         vision_encoder_max_size: int,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.\n@@ -518,9 +517,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -544,11 +543,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[list[np.ndarray]],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         For a list of images, for each images, pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width.\n@@ -620,22 +619,22 @@ def empty_image(size, input_data_format):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_image_splitting: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        max_image_size: Optional[dict[str, int]] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: bool | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_image_splitting: bool | None = None,\n+        do_rescale: bool | None = None,\n+        max_image_size: dict[str, int] | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         return_row_col_info: bool = False,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess a batch of images."
        },
        {
            "sha": "5b398b6bb15f7e28adcb9eb9d59c1c1e81f2c46b",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n \n@@ -48,7 +48,7 @@\n \n \n def _resize_output_size_rescale_to_max_len(\n-    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+    height: int, width: int, min_len: int | None = 1, max_len: int | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -85,7 +85,7 @@ def _resize_output_size_rescale_to_max_len(\n \n \n def _resize_output_size_scale_below_upper_bound(\n-    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+    height: int, width: int, max_len: dict[str, int] | None = None\n ) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -347,14 +347,14 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: Optional[bool],\n-        do_image_splitting: Optional[bool],\n-        max_image_size: Optional[dict[str, int]],\n-        return_row_col_info: Optional[bool],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        do_pad: bool | None,\n+        do_image_splitting: bool | None,\n+        max_image_size: dict[str, int] | None,\n+        return_row_col_info: bool | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "21d7f24466a5cc5d821ef437188db9b7f42c02f8",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n \n from datetime import timedelta\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n@@ -119,7 +119,7 @@ def __init__(\n         tokenizer,\n         video_processor,\n         image_seq_len: int = 169,\n-        chat_template: Optional[str] = None,\n+        chat_template: str | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -211,9 +211,9 @@ def expand_text_with_video_tokens(self, text, video_inputs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n+        images: ImageInput | list[ImageInput] | list[list[ImageInput]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n-        videos: Optional[VideoInput] = None,\n+        videos: VideoInput | None = None,\n         **kwargs: Unpack[SmolVLMProcessorKwargs],\n     ) -> BatchEncoding:\n         if text is None and images is None and videos is None:\n@@ -290,8 +290,8 @@ def __call__(\n \n     def apply_chat_template(\n         self,\n-        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n-        chat_template: Optional[str] = None,\n+        conversation: list[dict[str, str]] | list[list[dict[str, str]]],\n+        chat_template: str | None = None,\n         **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:\n         \"\"\""
        },
        {
            "sha": "237d3afe000e538ab4b99b3872ba971f0f860959",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import numpy as np\n import torch\n@@ -215,9 +215,9 @@ def pad(\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n-        skip_secs: Optional[int] = 1,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n+        skip_secs: int | None = 1,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -288,9 +288,9 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_pad: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ):\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)"
        },
        {
            "sha": "718c7667586f336b6c254a3845061924bb40f5a5",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for SuperPoint.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -56,7 +56,7 @@\n # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n def is_grayscale(\n     image: np.ndarray,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:\n         if image.shape[0] == 1:\n@@ -71,7 +71,7 @@ def is_grayscale(\n # Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale\n def convert_to_grayscale(\n     image: ImageInput,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image.\n@@ -172,7 +172,7 @@ class SuperGlueImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -195,8 +195,8 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -233,15 +233,15 @@ def resize(\n     def preprocess(\n         self,\n         images,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_grayscale: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_grayscale: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -346,7 +346,7 @@ def preprocess(\n     def post_process_keypoint_matching(\n         self,\n         outputs: \"SuperGlueKeypointMatchingOutput\",\n-        target_sizes: Union[TensorType, list[tuple]],\n+        target_sizes: TensorType | list[tuple],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\""
        },
        {
            "sha": "9409b57ca223cf57d046117a911ef3fb0f2d8b9a",
            "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from PIL import Image, ImageDraw\n@@ -123,14 +123,14 @@ def _prepare_images_structure(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        size: Union[dict[str, int], SizeDict],\n+        size: dict[str, int] | SizeDict,\n         rescale_factor: float,\n         do_rescale: bool,\n         do_resize: bool,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_grayscale: bool,\n         disable_grouping: bool,\n-        return_tensors: Union[str, TensorType],\n+        return_tensors: str | TensorType,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n@@ -166,7 +166,7 @@ def _preprocess(\n     def post_process_keypoint_matching(\n         self,\n         outputs: \"SuperGlueKeypointMatchingOutput\",\n-        target_sizes: Union[TensorType, list[tuple]],\n+        target_sizes: TensorType | list[tuple],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\""
        },
        {
            "sha": "c88002e5703cc70864ac574b7f8c2fe184c9ab4e",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Image processor class for SuperPoint.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -57,7 +57,7 @@ class SuperPointImageProcessorKwargs(ImagesKwargs, total=False):\n \n def is_grayscale(\n     image: np.ndarray,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:\n         if image.shape[0] == 1:\n@@ -71,7 +71,7 @@ def is_grayscale(\n \n def convert_to_grayscale(\n     image: ImageInput,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> ImageInput:\n     \"\"\"\n     Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image.\n@@ -135,7 +135,7 @@ class SuperPointImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -157,8 +157,8 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -195,15 +195,15 @@ def resize(\n     def preprocess(\n         self,\n         images,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_grayscale: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_grayscale: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -305,7 +305,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def post_process_keypoint_detection(\n-        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: Union[TensorType, list[tuple]]\n+        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: TensorType | list[tuple]\n     ) -> list[dict[str, \"torch.Tensor\"]]:\n         \"\"\"\n         Converts the raw output of [`SuperPointForKeypointDetection`] into lists of keypoints, scores and descriptors"
        },
        {
            "sha": "a02f1f2697438f6200b5e0a4059ae8e0572cd351",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Superpoint.\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Optional\n \n import torch\n \n@@ -88,14 +88,14 @@ def __init__(self, **kwargs: Unpack[SuperPointImageProcessorKwargs]):\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        size: Union[dict[str, int], SizeDict],\n+        size: dict[str, int] | SizeDict,\n         rescale_factor: float,\n         do_rescale: bool,\n         do_resize: bool,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_grayscale: bool,\n         disable_grouping: bool,\n-        return_tensors: Union[str, TensorType],\n+        return_tensors: str | TensorType,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n@@ -112,7 +112,7 @@ def _preprocess(\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_keypoint_detection(\n-        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: Union[TensorType, list[tuple]]\n+        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: TensorType | list[tuple]\n     ) -> list[dict[str, \"torch.Tensor\"]]:\n         \"\"\"\n         Converts the raw output of [`SuperPointForKeypointDetection`] into lists of keypoints, scores and descriptors"
        },
        {
            "sha": "77675d041389df666f99dcadb2bfa2341ec94c7e",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Swin2SR.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature\n@@ -59,7 +57,7 @@ class Swin2SRImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_pad: bool = True,\n         size_divisor: int = 8,\n         **kwargs,\n@@ -76,8 +74,8 @@ def pad(\n         self,\n         image: np.ndarray,\n         size: int,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pad an image to make the height and width divisible by `size`.\n@@ -117,13 +115,13 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "139a19588c25087dd4028d1450f17b9d726c8d4c",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Swin2SR.\"\"\"\n \n-from typing import Optional, Union\n-\n import torch\n from torchvision.transforms.v2 import functional as F\n \n@@ -83,8 +81,8 @@ def _preprocess(\n         rescale_factor: float,\n         do_pad: bool,\n         size_divisor: int,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "129b1d47f595482e2de3d7521b9cf74c636e186a",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 22,
            "deletions": 24,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for TextNet.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -99,16 +97,16 @@ class TextNetImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = False,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = IMAGENET_DEFAULT_MEAN,\n-        image_std: Optional[Union[float, list[float]]] = IMAGENET_DEFAULT_STD,\n+        image_mean: float | list[float] | None = IMAGENET_DEFAULT_MEAN,\n+        image_std: float | list[float] | None = IMAGENET_DEFAULT_STD,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -155,8 +153,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -208,21 +206,21 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> PIL.Image.Image:\n         \"\"\""
        },
        {
            "sha": "2d474b0ae0be60fdfdfed3c09aacc74912271053",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for TextNet.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -105,10 +105,10 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing"
        },
        {
            "sha": "05c0694026ab010a23a2c23d74c301077ee31940",
            "filename": "src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n import os\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n \n@@ -52,7 +52,7 @@ class TimmWrapperImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         pretrained_cfg: dict[str, Any],\n-        architecture: Optional[str] = None,\n+        architecture: str | None = None,\n         **kwargs,\n     ):\n         requires_backends(self, \"timm\")\n@@ -83,7 +83,7 @@ def to_dict(self) -> dict[str, Any]:\n \n     @classmethod\n     def get_image_processor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         Get the image processor dict for the model.\n@@ -96,7 +96,7 @@ def get_image_processor_dict(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n+        return_tensors: str | TensorType | None = \"pt\",\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "d04b9852eb877fb56e4dd78ad5371c20f0c15821",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for TrOCR.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -36,8 +34,8 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n         **kwargs: Unpack[TrOCRProcessorKwargs],\n     ) -> BatchFeature:\n         if images is None and text is None:"
        },
        {
            "sha": "56848ef4cace8ca5f0aa7afc726a0bce2ddc5199",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 50,
            "deletions": 51,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Image processor class for TVP.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -60,8 +59,8 @@ class TvpImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n \n     do_flip_channel_order: bool\n-    constant_values: Optional[Union[float, list[float]]]\n-    pad_mode: Optional[str]\n+    constant_values: float | list[float] | None\n+    pad_mode: str | None\n \n \n # Copied from transformers.models.vivit.image_processing_vivit.make_batched\n@@ -81,7 +80,7 @@ def make_batched(videos) -> list[list[ImageInput]]:\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     max_size: int = 448,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     height, width = get_image_size(input_image, input_data_format)\n     if height >= width:\n@@ -153,20 +152,20 @@ class TvpImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        pad_size: dict[str, int] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         pad_mode: PaddingMode = PaddingMode.CONSTANT,\n         do_normalize: bool = True,\n         do_flip_channel_order: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -195,8 +194,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -236,11 +235,11 @@ def resize(\n     def pad_image(\n         self,\n         image: np.ndarray,\n-        pad_size: Optional[dict[str, int]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        pad_size: dict[str, int] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         pad_mode: PaddingMode = PaddingMode.CONSTANT,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -283,23 +282,23 @@ def pad_image(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n-        constant_values: Optional[Union[float, Iterable[float]]] = None,\n-        pad_mode: Optional[PaddingMode] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_flip_channel_order: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: dict[str, int] | None = None,\n+        constant_values: float | Iterable[float] | None = None,\n+        pad_mode: PaddingMode | None = None,\n+        do_normalize: bool | None = None,\n+        do_flip_channel_order: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n@@ -354,25 +353,25 @@ def _preprocess_image(\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n-        videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_pad: Optional[bool] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n-        constant_values: Optional[Union[float, Iterable[float]]] = None,\n-        pad_mode: Optional[PaddingMode] = None,\n-        do_normalize: Optional[bool] = None,\n-        do_flip_channel_order: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        videos: ImageInput | list[ImageInput] | list[list[ImageInput]],\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_pad: bool | None = None,\n+        pad_size: dict[str, int] | None = None,\n+        constant_values: float | Iterable[float] | None = None,\n+        pad_mode: PaddingMode | None = None,\n+        do_normalize: bool | None = None,\n+        do_flip_channel_order: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "be897094db0c3391319d3de4dbaacdbc781210f7",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for TVP.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -63,7 +63,7 @@ def __init__(self, **kwargs: Unpack[TvpImageProcessorKwargs]):\n     @auto_docstring\n     def preprocess(\n         self,\n-        videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n+        videos: ImageInput | list[ImageInput] | list[list[ImageInput]],\n         **kwargs: Unpack[TvpImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(videos, **kwargs)\n@@ -156,22 +156,22 @@ def _preprocess(\n         self,\n         images: list[list[\"torch.Tensor\"]],\n         do_resize: bool,\n-        size: Union[SizeDict, dict],\n+        size: SizeDict | dict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_center_crop: bool,\n-        crop_size: Union[SizeDict, dict],\n+        crop_size: SizeDict | dict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_pad: bool,\n         pad_size: SizeDict,\n-        constant_values: Union[float, list[float]],\n+        constant_values: float | list[float],\n         pad_mode: str,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_flip_channel_order: bool,\n-        return_tensors: Optional[Union[str, TensorType]],\n-        disable_grouping: Optional[bool],\n+        return_tensors: str | TensorType | None,\n+        disable_grouping: bool | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "707b5693a2d56ba7b46887a8ceed6e298eb45f8f",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for UDOP.\n \"\"\"\n \n-from typing import Optional, Union\n-\n from transformers import logging\n \n from ...image_processing_utils import BatchFeature\n@@ -30,8 +28,8 @@\n \n \n class UdopTextKwargs(TextKwargs, total=False):\n-    word_labels: Optional[Union[list[int], list[list[int]]]]\n-    boxes: Optional[Union[list[list[int]], list[list[list[int]]]]]\n+    word_labels: list[int] | list[list[int]] | None\n+    boxes: list[list[int]] | list[list[list[int]]] | None\n \n \n class UdopProcessorKwargs(ProcessingKwargs, total=False):\n@@ -74,8 +72,8 @@ def __init__(self, image_processor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[UdopProcessorKwargs],\n     ) -> BatchFeature:\n         # verify input"
        },
        {
            "sha": "901c8312f7c85757bc9b75397d9389e3ede7db74",
            "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -17,8 +17,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import math\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -187,21 +187,21 @@ def __init__(\n \n     def _preprocess(\n         self,\n-        images: Union[ImageInput, VideoInput],\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | VideoInput,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        patch_size: int | None = None,\n+        temporal_patch_size: int | None = None,\n+        merge_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`."
        },
        {
            "sha": "1cd03cc3bd367bd6282d8b9cf0c742d9a31b2237",
            "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -108,9 +108,9 @@ def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n \n     def _further_process_kwargs(\n         self,\n-        size: Optional[SizeDict] = None,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n+        size: SizeDict | None = None,\n+        min_pixels: int | None = None,\n+        max_pixels: int | None = None,\n         **kwargs,\n     ) -> dict:\n         \"\"\"\n@@ -174,13 +174,13 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ):\n         # Group images by size for batched resizing"
        },
        {
            "sha": "aaec51e620a09d297a76eb6150a3091a1b864e2c",
            "filename": "src/transformers/models/video_llama_3/video_processing_video_llama_3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n import torch.nn.functional as F\n@@ -112,11 +112,11 @@ def __init__(self, **kwargs: Unpack[VideoLlama3VideoProcessorInitKwargs]):\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        temporal_patch_size: Optional[int] = None,\n-        min_frames: Optional[int] = None,\n-        max_frames: Optional[int] = None,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        temporal_patch_size: int | None = None,\n+        min_frames: int | None = None,\n+        max_frames: int | None = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         \"\"\""
        },
        {
            "sha": "d462a00d026022575663cfb1491c6c37c4dc8eb9",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 35,
            "deletions": 37,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Video-LLaVA.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -87,15 +85,15 @@ class VideoLlavaImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -122,8 +120,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -169,21 +167,21 @@ def resize(\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n-        images: Optional[list[ImageInput]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: list[ImageInput] | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -281,20 +279,20 @@ def preprocess(\n \n     def _preprocess_image(\n         self,\n-        image: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        image: ImageInput | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: int | None = None,\n+        do_convert_rgb: bool | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         validate_preprocess_arguments(\n             do_rescale=do_rescale,"
        },
        {
            "sha": "4a5e3336b0e906a158492ce68c15abeec743ddfc",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Processor class for VideoLlava.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n@@ -70,13 +68,13 @@ def __init__(\n     @auto_docstring\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: Optional[ImageInput] = None,\n-        videos: Optional[ImageInput] = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n-        max_length: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n+        images: ImageInput | None = None,\n+        videos: ImageInput | None = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy | None = None,\n+        max_length: int | None = None,\n+        return_tensors: str | TensorType | None = TensorType.PYTORCH,\n     ) -> BatchFeature:\n         r\"\"\"\n         padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):"
        },
        {
            "sha": "b018faeaa3a7ad4edc34aa0d8c142d233c88aa64",
            "filename": "src/transformers/models/videomae/image_processing_videomae.py",
            "status": "modified",
            "additions": 31,
            "deletions": 33,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for VideoMAE.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -104,15 +102,15 @@ class VideoMAEImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -137,8 +135,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -179,18 +177,18 @@ def resize(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n         validate_preprocess_arguments(\n@@ -237,19 +235,19 @@ def _preprocess_image(\n     def preprocess(\n         self,\n         videos: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "a7a7ad6b15b2ae75e7ec746c19ce1a85994f3043",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Image processor class for Vilt.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -58,7 +58,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n \n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -76,7 +76,7 @@ def make_pixel_mask(\n \n \n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -98,7 +98,7 @@ def get_resize_output_image_size(\n     shorter: int = 800,\n     longer: int = 1333,\n     size_divisor: int = 32,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     input_height, input_width = get_image_size(input_image, input_data_format)\n     min_size, max_size = shorter, longer\n@@ -171,14 +171,14 @@ class ViltImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -203,8 +203,8 @@ def resize(\n         size: dict[str, int],\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -249,9 +249,9 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pad an image with zeros to the given size.\n@@ -275,11 +275,11 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -329,19 +329,19 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        size_divisor: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        size_divisor: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "b118faa79905dabd5fd74face82ed1cb48d8ff12",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Vilt.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -59,15 +59,15 @@ def _preprocess(\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        size_divisor: Optional[int],\n+        size_divisor: int | None,\n         do_pad: bool,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -118,7 +118,7 @@ def resize(\n         images: \"torch.Tensor\",\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n-        size_divisor: Optional[int] = None,\n+        size_divisor: int | None = None,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Resize an image or batch of images to specified size.\n@@ -170,8 +170,8 @@ def resize(\n     def _pad_batch(\n         self,\n         images: list[\"torch.Tensor\"],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        disable_grouping: Optional[bool],\n+        return_tensors: str | TensorType | None,\n+        disable_grouping: bool | None,\n     ) -> tuple:\n         \"\"\"\n         Pad a batch of images to the same size based on the maximum dimensions."
        },
        {
            "sha": "8c3c0d1c998545f60bf8d83c9db4136997ae01cd",
            "filename": "src/transformers/models/vit/image_processing_vit.py",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for ViT.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n@@ -78,14 +76,14 @@ class ViTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_rgb: bool | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -106,8 +104,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -153,18 +151,18 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        do_convert_rgb: bool | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "82a78229526572e10375d65aab2a8c124c1ede8f",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for ViTMatte.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature\n@@ -76,10 +74,10 @@ class VitMatteImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_pad: bool = True,\n         size_divisor: int = 32,\n         **kwargs,\n@@ -98,8 +96,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         size_divisor: int = 32,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Args:\n@@ -140,16 +138,16 @@ def preprocess(\n         self,\n         images: ImageInput,\n         trimaps: ImageInput,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "5eb3e89df679295ac8447d65378f0f94523e9921",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for ViTMatte.\"\"\"\n \n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -47,10 +47,10 @@\n @auto_docstring\n class VitMatteImageProcessorFast(BaseImageProcessorFast):\n     do_rescale: bool = True\n-    rescale_factor: Union[int, float] = 1 / 255\n+    rescale_factor: int | float = 1 / 255\n     do_normalize: bool = True\n-    image_mean: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_MEAN\n-    image_std: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_STD\n+    image_mean: float | list[float] | None = IMAGENET_STANDARD_MEAN\n+    image_std: float | list[float] | None = IMAGENET_STANDARD_STD\n     do_pad: bool = True\n     size_divisor: int = 32\n     valid_kwargs = VitMatteImageProcessorKwargs\n@@ -104,7 +104,7 @@ def _preprocess_image_like_inputs(\n         trimaps: ImageInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n-        device: Optional[Union[str, \"torch.device\"]] = None,\n+        device: Union[str, \"torch.device\"] | None = None,\n         **kwargs: Unpack[VitMatteImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -122,15 +122,15 @@ def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         trimaps: list[\"torch.Tensor\"],\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_pad: Optional[bool] = None,\n-        size_divisor: Optional[int] = None,\n-        disable_grouping: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_pad: bool | None = None,\n+        size_divisor: int | None = None,\n+        disable_grouping: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         grouped_trimaps, grouped_trimaps_index = group_images_by_shape(trimaps, disable_grouping=disable_grouping)"
        },
        {
            "sha": "b3202423623a5ea0b6e43333fccc846d280bf13c",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import itertools\n import math\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -60,13 +60,13 @@ class VitPoseImageProcessorKwargs(ImagesKwargs, total=False):\n         Width and height scale factor used for normalization when computing center and scale from bounding boxes.\n     \"\"\"\n \n-    do_affine_transform: Optional[bool]\n-    normalize_factor: Optional[float]\n+    do_affine_transform: bool | None\n+    normalize_factor: float | None\n \n \n # inspired by https://github.com/ViTAE-Transformer/ViTPose/blob/d5216452796c90c6bc29f5c5ec0bdba94366768a/mmpose/datasets/datasets/base/kpt_2d_sview_rgb_img_top_down_dataset.py#L132\n def box_to_center_and_scale(\n-    box: Union[tuple, list, np.ndarray],\n+    box: tuple | list | np.ndarray,\n     image_width: int,\n     image_height: int,\n     normalize_factor: float = 200.0,\n@@ -366,12 +366,12 @@ class VitPoseImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_affine_transform: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -391,8 +391,8 @@ def affine_transform(\n         scale: tuple[float],\n         rotation: float,\n         size: dict[str, int],\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Apply an affine transformation to an image.\n@@ -436,17 +436,17 @@ def affine_transform(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        boxes: Union[list[list[float]], np.ndarray],\n-        do_affine_transform: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        boxes: list[list[float]] | np.ndarray,\n+        do_affine_transform: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -605,10 +605,10 @@ def keypoints_from_heatmaps(\n     def post_process_pose_estimation(\n         self,\n         outputs: \"VitPoseEstimatorOutput\",\n-        boxes: Union[list[list[list[float]]], np.ndarray],\n+        boxes: list[list[list[float]]] | np.ndarray,\n         kernel_size: int = 11,\n-        threshold: Optional[float] = None,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        threshold: float | None = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Transform the heatmaps into keypoint predictions and transform them back to the image."
        },
        {
            "sha": "c9b6c27234e78fa0fff2d0d12b0031fddec89d6d",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -14,7 +14,7 @@\n \"\"\"Fast Image processor class for VitPose.\"\"\"\n \n import itertools\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n import torch\n@@ -95,7 +95,7 @@ def torch_affine_transform(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        boxes: Union[list[list[float]], np.ndarray],\n+        boxes: list[list[float]] | np.ndarray,\n         **kwargs: Unpack[VitPoseImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n@@ -108,16 +108,16 @@ def preprocess(\n     def _preprocess(\n         self,\n         images: list[torch.Tensor],\n-        boxes: Union[list, np.ndarray],\n+        boxes: list | np.ndarray,\n         do_affine_transform: bool,\n         size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Union[float, tuple[float]],\n-        image_std: Union[float, tuple[float]],\n+        image_mean: float | tuple[float],\n+        image_std: float | tuple[float],\n         disable_grouping: bool,\n-        return_tensors: Optional[Union[str, TensorType]],\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -203,10 +203,10 @@ def keypoints_from_heatmaps(\n     def post_process_pose_estimation(\n         self,\n         outputs: \"VitPoseEstimatorOutput\",\n-        boxes: Union[list[list[list[float]]], np.ndarray],\n+        boxes: list[list[list[float]]] | np.ndarray,\n         kernel_size: int = 11,\n-        threshold: Optional[float] = None,\n-        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+        threshold: float | None = None,\n+        target_sizes: TensorType | list[tuple] | None = None,\n     ):\n         \"\"\"\n         Transform the heatmaps into keypoint predictions and transform them back to the image."
        },
        {
            "sha": "0e7659079d96a24e0e834f28209f297a475a957c",
            "filename": "src/transformers/models/vivit/image_processing_vivit.py",
            "status": "modified",
            "additions": 36,
            "deletions": 38,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Image processor class for Vivit.\"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from transformers.utils import is_vision_available\n@@ -108,16 +106,16 @@ class VivitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n+        crop_size: dict[str, int] | None = None,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 127.5,\n+        rescale_factor: int | float = 1 / 127.5,\n         offset: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -143,8 +141,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -186,10 +184,10 @@ def resize(\n     def rescale(\n         self,\n         image: np.ndarray,\n-        scale: Union[int, float],\n+        scale: int | float,\n         offset: bool = True,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -226,19 +224,19 @@ def rescale(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        offset: Optional[bool] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        offset: bool | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single image.\"\"\"\n \n@@ -289,20 +287,20 @@ def _preprocess_image(\n     def preprocess(\n         self,\n         videos: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        offset: Optional[bool] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_center_crop: bool | None = None,\n+        crop_size: dict[str, int] | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        offset: bool | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "adf9dc07c0d65fd47469b53b79a37c4219041ee5",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import io\n-from typing import Optional, Union\n \n from ...utils import auto_docstring, is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n \n@@ -42,7 +41,7 @@ class VoxtralAudioKwargs(AudioKwargs, total=False):\n         Maximum number of positions per chunk when splitting mel spectrogram features along the time dimension.\n     \"\"\"\n \n-    max_source_positions: Optional[int]\n+    max_source_positions: int | None\n \n \n class VoxtralProcessorKwargs(ProcessingKwargs, total=False):\n@@ -97,7 +96,7 @@ def _retrieve_input_features(self, audio, max_source_positions, **kwargs):\n \n     def apply_chat_template(\n         self,\n-        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        conversation: list[dict[str, str]] | list[list[dict[str, str]]],\n         **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:\n         \"\"\"\n@@ -232,7 +231,7 @@ def apply_chat_template(\n     )\n     def __call__(\n         self,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]],\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None,\n         **kwargs: Unpack[VoxtralProcessorKwargs],\n     ):\n         if isinstance(text, str):\n@@ -251,11 +250,11 @@ def __call__(\n     # TODO: @eustlb, this should be moved to mistral_common + testing\n     def apply_transcription_request(\n         self,\n-        audio: Union[str, list[str], AudioInput],\n+        audio: str | list[str] | AudioInput,\n         model_id: str,\n-        language: Optional[Union[str, list[Union[str, None]]]] = None,\n-        sampling_rate: Optional[int] = None,\n-        format: Optional[Union[str, list[str]]] = None,\n+        language: str | list[str | None] | None = None,\n+        sampling_rate: int | None = None,\n+        format: str | list[str] | None = None,\n         **kwargs: Unpack[VoxtralProcessorKwargs],\n     ):\n         \"\"\""
        },
        {
            "sha": "2560fcbd550b28ef259f74a3291706dc11c5f5d4",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"\n \n import warnings\n-from typing import Optional, Union\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n@@ -35,8 +34,8 @@ def __init__(self, feature_extractor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        audio: Optional[AudioInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        audio: AudioInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[Wav2Vec2ProcessorKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "6602c6ec60a8f48973ac8ba172281afb74429eb5",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,8 +15,6 @@\n Speech processor class for Wav2Vec2-BERT\n \"\"\"\n \n-from typing import Optional, Union\n-\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n from ...utils import auto_docstring\n@@ -34,8 +32,8 @@ def __init__(self, feature_extractor, tokenizer):\n     @auto_docstring\n     def __call__(\n         self,\n-        audio: Optional[AudioInput] = None,\n-        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n+        audio: AudioInput | None = None,\n+        text: str | list[str] | TextInput | PreTokenizedInput | None = None,\n         **kwargs: Unpack[Wav2Vec2BertProcessorKwargs],\n     ):\n         r\"\"\""
        },
        {
            "sha": "74eea24f0f529978d9c9f04ce13d470853382a43",
            "filename": "src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 28,
            "deletions": 27,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -20,8 +20,9 @@\n from collections.abc import Iterable\n from contextlib import nullcontext\n from dataclasses import dataclass\n-from multiprocessing import Pool, get_context, get_start_method\n-from typing import TYPE_CHECKING, Optional, Union\n+from multiprocessing import get_context, get_start_method\n+from multiprocessing.pool import Pool\n+from typing import TYPE_CHECKING\n \n import numpy as np\n \n@@ -39,7 +40,7 @@\n     from ...tokenization_python import PreTrainedTokenizerBase\n \n \n-ListOfDict = list[dict[str, Union[int, str]]]\n+ListOfDict = list[dict[str, int | str]]\n \n \n @dataclass\n@@ -59,10 +60,10 @@ class Wav2Vec2DecoderWithLMOutput(ModelOutput):\n             can be used to compute time stamps for each word.\n     \"\"\"\n \n-    text: Union[list[list[str]], list[str], str]\n-    logit_score: Union[list[list[float]], list[float], float] = None\n-    lm_score: Union[list[list[float]], list[float], float] = None\n-    word_offsets: Union[list[list[ListOfDict]], list[ListOfDict], ListOfDict] = None\n+    text: list[list[str]] | list[str] | str\n+    logit_score: list[list[float]] | list[float] | float = None\n+    lm_score: list[list[float]] | list[float] | float = None\n+    word_offsets: list[list[ListOfDict]] | list[ListOfDict] | ListOfDict = None\n \n \n @auto_docstring\n@@ -264,17 +265,17 @@ def pad(self, *args, **kwargs):\n     def batch_decode(\n         self,\n         logits: np.ndarray,\n-        pool: Optional[Pool] = None,\n-        num_processes: Optional[int] = None,\n-        beam_width: Optional[int] = None,\n-        beam_prune_logp: Optional[float] = None,\n-        token_min_logp: Optional[float] = None,\n-        hotwords: Optional[Iterable[str]] = None,\n-        hotword_weight: Optional[float] = None,\n-        alpha: Optional[float] = None,\n-        beta: Optional[float] = None,\n-        unk_score_offset: Optional[float] = None,\n-        lm_score_boundary: Optional[bool] = None,\n+        pool: Pool | None = None,\n+        num_processes: int | None = None,\n+        beam_width: int | None = None,\n+        beam_prune_logp: float | None = None,\n+        token_min_logp: float | None = None,\n+        hotwords: Iterable[str] | None = None,\n+        hotword_weight: float | None = None,\n+        alpha: float | None = None,\n+        beta: float | None = None,\n+        unk_score_offset: float | None = None,\n+        lm_score_boundary: bool | None = None,\n         output_word_offsets: bool = False,\n         n_best: int = 1,\n     ):\n@@ -449,15 +450,15 @@ def batch_decode(\n     def decode(\n         self,\n         logits: np.ndarray,\n-        beam_width: Optional[int] = None,\n-        beam_prune_logp: Optional[float] = None,\n-        token_min_logp: Optional[float] = None,\n-        hotwords: Optional[Iterable[str]] = None,\n-        hotword_weight: Optional[float] = None,\n-        alpha: Optional[float] = None,\n-        beta: Optional[float] = None,\n-        unk_score_offset: Optional[float] = None,\n-        lm_score_boundary: Optional[bool] = None,\n+        beam_width: int | None = None,\n+        beam_prune_logp: float | None = None,\n+        token_min_logp: float | None = None,\n+        hotwords: Iterable[str] | None = None,\n+        hotword_weight: float | None = None,\n+        alpha: float | None = None,\n+        beta: float | None = None,\n+        unk_score_offset: float | None = None,\n+        lm_score_boundary: bool | None = None,\n         output_word_offsets: bool = False,\n         n_best: int = 1,\n     ):"
        },
        {
            "sha": "e09e0e014aed0c700190ee724837204713341cf1",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 60,
            "deletions": 60,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import pathlib\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -96,16 +96,16 @@ class YolosImageProcessorKwargs(ImagesKwargs, total=False):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Union[str, AnnotationFormat]\n+    format: str | AnnotationFormat\n     do_convert_annotations: bool\n     return_segmentation_masks: bool\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n+    annotations: AnnotationType | list[AnnotationType] | None\n+    masks_path: str | pathlib.Path | None\n \n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    images: list[np.ndarray], input_data_format: str | ChannelDimension | None = None\n ) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n@@ -123,7 +123,7 @@ def get_max_height_width(\n \n \n def get_size_with_aspect_ratio(\n-    image_size: tuple[int, int], size: int, max_size: Optional[int] = None, mod_size: int = 16\n+    image_size: tuple[int, int], size: int, max_size: int | None = None, mod_size: int = 16\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size with multiple of divisible_size.\n@@ -176,7 +176,7 @@ def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,\n     max_width: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n@@ -210,9 +210,9 @@ def get_image_size_for_max_height_width(\n # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int]],\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    size: int | tuple[int, int] | list[int],\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n@@ -237,7 +237,7 @@ def get_resize_output_image_size(\n \n \n # Copied from transformers.models.detr.image_processing_detr.safe_squeeze\n-def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n+def safe_squeeze(arr: np.ndarray, axis: int | None = None) -> np.ndarray:\n     \"\"\"\n     Squeezes an array, but only if the axis specified has dim 1.\n     \"\"\"\n@@ -275,7 +275,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: str | ChannelDimension | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n@@ -332,7 +332,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by DETR.\n@@ -427,9 +427,9 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for YOLOS.\n@@ -662,8 +662,8 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[set[int]] = None,\n-    target_size: Optional[tuple[int, int]] = None,\n+    label_ids_to_fuse: set[int] | None = None,\n+    target_size: tuple[int, int] | None = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -771,18 +771,18 @@ class YolosImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        format: str | AnnotationFormat = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n         do_pad: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> None:\n         max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n@@ -833,10 +833,10 @@ def prepare_annotation(\n         self,\n         image: np.ndarray,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into DETR model.\n@@ -867,8 +867,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -941,8 +941,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         rescale_factor: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Rescale the image by the given factor. image = image * rescale_factor.\n@@ -1022,10 +1022,10 @@ def _pad_image(\n         self,\n         image: np.ndarray,\n         output_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        annotation: dict[str, Any] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -1054,14 +1054,14 @@ def _pad_image(\n     def pad(\n         self,\n         images: list[np.ndarray],\n-        annotations: Optional[list[dict[str, Any]]] = None,\n-        constant_values: Union[float, Iterable[float]] = 0,\n+        annotations: list[dict[str, Any]] | None = None,\n+        constant_values: float | Iterable[float] = 0,\n         return_pixel_mask: bool = False,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        return_tensors: str | TensorType | None = None,\n+        data_format: ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[dict[str, int]] = None,\n+        pad_size: dict[str, int] | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -1138,24 +1138,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        annotations: AnnotationType | list[AnnotationType] | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n         resample=None,  # PILImageResampling\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[Union[int, float]] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_annotations: Optional[bool] = None,\n-        do_pad: Optional[bool] = None,\n-        format: Optional[Union[str, AnnotationFormat]] = None,\n-        return_tensors: Optional[Union[TensorType, str]] = None,\n-        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[dict[str, int]] = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: int | float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_convert_annotations: bool | None = None,\n+        do_pad: bool | None = None,\n+        format: str | AnnotationFormat | None = None,\n+        return_tensors: TensorType | str | None = None,\n+        data_format: str | ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n+        pad_size: dict[str, int] | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1380,7 +1380,7 @@ def preprocess(\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection with Detr->Yolos\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: TensorType | list[tuple] | None = None\n     ):\n         \"\"\"\n         Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "2e3a2b3e672f5051183c670a0968b1e03f3849ee",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_yolos.py file directly. One of our CI enforces this.\n #                \n import pathlib\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n from torchvision.io import read_image\n@@ -79,7 +79,7 @@ def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n-    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+    input_data_format: ChannelDimension | str | None = None,\n ):\n     \"\"\"\n     Convert the target in COCO format into the format expected by YOLOS.\n@@ -190,9 +190,9 @@ def rgb_to_id(color):\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: dict,\n-    masks_path: Union[str, pathlib.Path],\n+    masks_path: str | pathlib.Path,\n     return_masks: bool = True,\n-    input_data_format: Union[ChannelDimension, str] = None,\n+    input_data_format: ChannelDimension | str = None,\n ) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for YOLOS.\n@@ -321,10 +321,10 @@ def prepare_annotation(\n         self,\n         image: torch.Tensor,\n         target: dict,\n-        format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: Optional[bool] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        format: AnnotationFormat | None = None,\n+        return_segmentation_masks: bool | None = None,\n+        masks_path: str | pathlib.Path | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into YOLOS model.\n@@ -510,7 +510,7 @@ def pad(\n         self,\n         image: torch.Tensor,\n         padded_size: tuple[int, int],\n-        annotation: Optional[dict[str, Any]] = None,\n+        annotation: dict[str, Any] | None = None,\n         update_bboxes: bool = True,\n         fill: int = 0,\n     ):\n@@ -539,8 +539,8 @@ def pad(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]],\n-        masks_path: Optional[Union[str, pathlib.Path]],\n+        annotations: AnnotationType | list[AnnotationType] | None,\n+        masks_path: str | pathlib.Path | None,\n         return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -549,12 +549,12 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_convert_annotations: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n         do_pad: bool,\n-        pad_size: Optional[SizeDict],\n-        format: Optional[Union[str, AnnotationFormat]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        pad_size: SizeDict | None,\n+        format: str | AnnotationFormat | None,\n+        return_tensors: str | TensorType | None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "bb75e60bca7758d0de2348a79577bd401917a29a",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 28,
            "deletions": 28,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -15,7 +15,7 @@\n \n import math\n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Union\n \n import numpy as np\n \n@@ -82,10 +82,10 @@ class ZoeDepthImageProcessorKwargs(ImagesKwargs, total=False):\n \n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    output_size: Union[int, Iterable[int]],\n+    output_size: int | Iterable[int],\n     keep_aspect_ratio: bool,\n     multiple: int,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple[int, int]:\n     def constrain_to_multiple_of(val, multiple, min_val=0):\n         x = (np.round(val / multiple) * multiple).astype(int)\n@@ -170,12 +170,12 @@ def __init__(\n         self,\n         do_pad: bool = True,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         keep_aspect_ratio: bool = True,\n         ensure_multiple_of: int = 32,\n@@ -203,8 +203,8 @@ def resize(\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Resize an image to target size `(size[\"height\"], size[\"width\"])`. If `keep_aspect_ratio` is `True`, the image\n@@ -269,8 +269,8 @@ def pad_image(\n         self,\n         image: np.ndarray,\n         mode: PaddingMode = PaddingMode.REFLECT,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Pad an image as done in the original ZoeDepth implementation.\n@@ -319,20 +319,20 @@ def pad_image(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_pad: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[int] = None,\n-        keep_aspect_ratio: Optional[bool] = None,\n-        ensure_multiple_of: Optional[int] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_pad: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        do_resize: bool | None = None,\n+        size: int | None = None,\n+        keep_aspect_ratio: bool | None = None,\n+        ensure_multiple_of: int | None = None,\n+        resample: PILImageResampling | None = None,\n+        return_tensors: str | TensorType | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -468,10 +468,10 @@ def preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"ZoeDepthDepthEstimatorOutput\",\n-        source_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n-        outputs_flipped: Optional[Union[\"ZoeDepthDepthEstimatorOutput\", None]] = None,\n-        do_remove_padding: Optional[Union[bool, None]] = None,\n+        source_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n+        outputs_flipped: Union[\"ZoeDepthDepthEstimatorOutput\", None] | None = None,\n+        do_remove_padding: bool | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`ZoeDepthDepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "b26deaf07ef25d74e15ee71a9af4da90fb25e73e",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -142,17 +142,17 @@ def _preprocess(\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n         size: SizeDict,\n-        keep_aspect_ratio: Optional[bool],\n-        ensure_multiple_of: Optional[int],\n+        keep_aspect_ratio: bool | None,\n+        ensure_multiple_of: int | None,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_pad: bool,\n         do_rescale: bool,\n-        rescale_factor: Optional[float],\n+        rescale_factor: float | None,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        disable_grouping: bool | None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n@@ -177,10 +177,10 @@ def _preprocess(\n     def post_process_depth_estimation(\n         self,\n         outputs: \"ZoeDepthDepthEstimatorOutput\",\n-        source_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n-        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n-        outputs_flipped: Optional[Union[\"ZoeDepthDepthEstimatorOutput\", None]] = None,\n-        do_remove_padding: Optional[Union[bool, None]] = None,\n+        source_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n+        target_sizes: TensorType | list[tuple[int, int]] | None | None = None,\n+        outputs_flipped: Union[\"ZoeDepthDepthEstimatorOutput\", None] | None = None,\n+        do_remove_padding: bool | None | None = None,\n     ) -> list[dict[str, TensorType]]:\n         \"\"\"\n         Converts the raw output of [`ZoeDepthDepthEstimatorOutput`] into final depth predictions and depth PIL images."
        },
        {
            "sha": "8d54a50c992700ee6cd3299e8dd37e35441d0923",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 95,
            "deletions": 95,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -24,7 +24,7 @@\n import typing\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Annotated, Any, Literal, Optional, TypedDict, TypeVar, Union\n+from typing import Annotated, Any, Literal, TypedDict, TypeVar, Union\n \n import numpy as np\n import typing_extensions\n@@ -199,26 +199,26 @@ class TextKwargs(TypedDict, total=False):\n             - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n-    text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n-    text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n-    text_pair_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n-    add_special_tokens: Optional[bool]\n-    padding: Annotated[Optional[Union[bool, str, PaddingStrategy]], padding_validator()]\n-    truncation: Annotated[Optional[Union[bool, str, TruncationStrategy]], truncation_validator()]\n-    max_length: Annotated[Optional[int], positive_int()]\n-    stride: Annotated[Optional[int], positive_int()]\n-    is_split_into_words: Optional[bool]\n-    pad_to_multiple_of: Annotated[Optional[int], positive_int()]\n-    return_token_type_ids: Optional[bool]\n-    return_attention_mask: Optional[bool]\n-    return_overflowing_tokens: Optional[bool]\n-    return_special_tokens_mask: Optional[bool]\n-    return_offsets_mapping: Optional[bool]\n-    return_length: Optional[bool]\n-    verbose: Optional[bool]\n-    padding_side: Optional[Literal[\"left\", \"right\"]]\n-    return_mm_token_type_ids: Optional[bool]\n-    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n+    text_pair: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None\n+    text_target: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None\n+    text_pair_target: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None\n+    add_special_tokens: bool | None\n+    padding: Annotated[bool | str | PaddingStrategy | None, padding_validator()]\n+    truncation: Annotated[bool | str | TruncationStrategy | None, truncation_validator()]\n+    max_length: Annotated[int | None, positive_int()]\n+    stride: Annotated[int | None, positive_int()]\n+    is_split_into_words: bool | None\n+    pad_to_multiple_of: Annotated[int | None, positive_int()]\n+    return_token_type_ids: bool | None\n+    return_attention_mask: bool | None\n+    return_overflowing_tokens: bool | None\n+    return_special_tokens_mask: bool | None\n+    return_offsets_mapping: bool | None\n+    return_length: bool | None\n+    verbose: bool | None\n+    padding_side: Literal[\"left\", \"right\"] | None\n+    return_mm_token_type_ids: bool | None\n+    return_tensors: Annotated[str | TensorType | None, tensor_type_validator()]\n \n \n class ImagesKwargs(TypedDict, total=False):\n@@ -272,25 +272,25 @@ class methods and docstrings.\n             Added for backward compatibility but this should be set as a processor attribute in future models.\n     \"\"\"\n \n-    do_convert_rgb: Optional[bool]\n-    do_resize: Optional[bool]\n-    size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n-    crop_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n-    resample: Annotated[Optional[Union[\"PILImageResampling\", int]], resampling_validator()]\n-    do_rescale: Optional[bool]\n-    rescale_factor: Optional[float]\n-    do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, list[float], tuple[float, ...]]]\n-    image_std: Optional[Union[float, list[float], tuple[float, ...]]]\n-    do_pad: Optional[bool]\n-    pad_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n-    do_center_crop: Optional[bool]\n-    data_format: Optional[Union[str, ChannelDimension]]\n-    input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Annotated[Optional[Union[str, \"torch.device\"]], device_validator()]\n-    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n-    disable_grouping: Optional[bool]\n-    image_seq_length: Optional[int]\n+    do_convert_rgb: bool | None\n+    do_resize: bool | None\n+    size: Annotated[int | list[int] | tuple[int, ...] | dict[str, int] | None, image_size_validator()]\n+    crop_size: Annotated[int | list[int] | tuple[int, ...] | dict[str, int] | None, image_size_validator()]\n+    resample: Annotated[Union[\"PILImageResampling\", int] | None, resampling_validator()]\n+    do_rescale: bool | None\n+    rescale_factor: float | None\n+    do_normalize: bool | None\n+    image_mean: float | list[float] | tuple[float, ...] | None\n+    image_std: float | list[float] | tuple[float, ...] | None\n+    do_pad: bool | None\n+    pad_size: Annotated[int | list[int] | tuple[int, ...] | dict[str, int] | None, image_size_validator()]\n+    do_center_crop: bool | None\n+    data_format: str | ChannelDimension | None\n+    input_data_format: str | ChannelDimension | None\n+    device: Annotated[Union[str, \"torch.device\"] | None, device_validator()]\n+    return_tensors: Annotated[str | TensorType | None, tensor_type_validator()]\n+    disable_grouping: bool | None\n+    image_seq_length: int | None\n \n \n class VideosKwargs(TypedDict, total=False):\n@@ -346,28 +346,28 @@ class VideosKwargs(TypedDict, total=False):\n             - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n-    do_convert_rgb: Optional[bool]\n-    do_resize: Optional[bool]\n-    size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n-    default_to_square: Optional[bool]\n-    resample: Annotated[Optional[Union[\"PILImageResampling\", int]], resampling_validator()]\n-    do_rescale: Optional[bool]\n-    rescale_factor: Optional[float]\n-    do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, list[float], tuple[float, ...]]]\n-    image_std: Optional[Union[float, list[float], tuple[float, ...]]]\n-    do_center_crop: Optional[bool]\n-    do_pad: Optional[bool]\n-    crop_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n-    data_format: Optional[Union[str, ChannelDimension]]\n-    input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Annotated[Optional[Union[str, \"torch.device\"]], device_validator()]\n-    do_sample_frames: Optional[bool]\n-    video_metadata: Annotated[Optional[VideoMetadataType], video_metadata_validator()]\n-    fps: Annotated[Optional[Union[int, float]], positive_any_number()]\n-    num_frames: Annotated[Optional[int], positive_int()]\n-    return_metadata: Optional[bool]\n-    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n+    do_convert_rgb: bool | None\n+    do_resize: bool | None\n+    size: Annotated[int | list[int] | tuple[int, ...] | dict[str, int] | None, image_size_validator()]\n+    default_to_square: bool | None\n+    resample: Annotated[Union[\"PILImageResampling\", int] | None, resampling_validator()]\n+    do_rescale: bool | None\n+    rescale_factor: float | None\n+    do_normalize: bool | None\n+    image_mean: float | list[float] | tuple[float, ...] | None\n+    image_std: float | list[float] | tuple[float, ...] | None\n+    do_center_crop: bool | None\n+    do_pad: bool | None\n+    crop_size: Annotated[int | list[int] | tuple[int, ...] | dict[str, int] | None, image_size_validator()]\n+    data_format: str | ChannelDimension | None\n+    input_data_format: str | ChannelDimension | None\n+    device: Annotated[Union[str, \"torch.device\"] | None, device_validator()]\n+    do_sample_frames: bool | None\n+    video_metadata: Annotated[VideoMetadataType | None, video_metadata_validator()]\n+    fps: Annotated[int | float | None, positive_any_number()]\n+    num_frames: Annotated[int | None, positive_int()]\n+    return_metadata: bool | None\n+    return_tensors: Annotated[str | TensorType | None, tensor_type_validator()]\n \n \n class AudioKwargs(TypedDict, total=False):\n@@ -404,14 +404,14 @@ class AudioKwargs(TypedDict, total=False):\n             - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n-    sampling_rate: Annotated[Optional[int], positive_int()]\n-    raw_speech: Optional[Union[\"np.ndarray\", list[float], list[\"np.ndarray\"], list[list[float]]]]\n-    padding: Annotated[Optional[Union[bool, str, PaddingStrategy]], padding_validator()]\n-    max_length: Annotated[Optional[int], positive_int()]\n-    truncation: Annotated[Optional[Union[bool, str, TruncationStrategy]], truncation_validator()]\n-    pad_to_multiple_of: Annotated[Optional[int], positive_int()]\n-    return_attention_mask: Optional[bool]\n-    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n+    sampling_rate: Annotated[int | None, positive_int()]\n+    raw_speech: Union[\"np.ndarray\", list[float], list[\"np.ndarray\"], list[list[float]]] | None\n+    padding: Annotated[bool | str | PaddingStrategy | None, padding_validator()]\n+    max_length: Annotated[int | None, positive_int()]\n+    truncation: Annotated[bool | str | TruncationStrategy | None, truncation_validator()]\n+    pad_to_multiple_of: Annotated[int | None, positive_int()]\n+    return_attention_mask: bool | None\n+    return_tensors: Annotated[str | TensorType | None, tensor_type_validator()]\n \n \n class ProcessingKwargs(TypedDict, total=False):\n@@ -501,11 +501,11 @@ class TokenizerChatTemplateKwargs(TypedDict, total=False):\n         This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n     \"\"\"\n \n-    tools: Optional[list[dict]] = None\n-    documents: Optional[list[dict[str, str]]] = None\n-    add_generation_prompt: Optional[bool] = False\n-    continue_final_message: Optional[bool] = False\n-    return_assistant_tokens_mask: Optional[bool] = False\n+    tools: list[dict] | None = None\n+    documents: list[dict[str, str]] | None = None\n+    add_generation_prompt: bool | None = False\n+    continue_final_message: bool | None = False\n+    return_assistant_tokens_mask: bool | None = False\n \n \n class ChatTemplateLoadKwargs(TypedDict, total=False):\n@@ -519,8 +519,8 @@ class ChatTemplateLoadKwargs(TypedDict, total=False):\n             processor. This flag has no effect if the model doesn't support audio modality.\n     \"\"\"\n \n-    sampling_rate: Optional[int] = 16_000\n-    load_audio_from_video: Optional[bool] = False\n+    sampling_rate: int | None = 16_000\n+    load_audio_from_video: bool | None = False\n \n \n class ProcessorChatTemplateKwargs(ChatTemplateLoadKwargs, TokenizerChatTemplateKwargs, total=False):\n@@ -533,8 +533,8 @@ class ProcessorChatTemplateKwargs(ChatTemplateLoadKwargs, TokenizerChatTemplateK\n         Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n     \"\"\"\n \n-    tokenize: Optional[bool] = False\n-    return_dict: Optional[bool] = False\n+    tokenize: bool | None = False\n+    return_dict: bool | None = False\n \n \n class AllKwargsForChatTemplate(TypedDict, total=False):\n@@ -555,10 +555,10 @@ class MultiModalData:\n     and we might change its API in the future.\n     \"\"\"\n \n-    num_image_tokens: Optional[list[int]] = None\n-    num_video_tokens: Optional[list[int]] = None\n-    num_audio_tokens: Optional[list[int]] = None\n-    num_image_patches: Optional[list[int]] = None\n+    num_image_tokens: list[int] | None = None\n+    num_video_tokens: list[int] | None = None\n+    num_audio_tokens: list[int] | None = None\n+    num_image_patches: list[int] | None = None\n \n     def __contains__(self, key):\n         return hasattr(self, key) and getattr(self, key) is not None\n@@ -616,10 +616,10 @@ def __init__(self, *args, **kwargs):\n \n     def __call__(\n         self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        videos: Optional[VideoInput] = None,\n-        audio: Optional[AudioInput] = None,\n+        images: ImageInput | None = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] | None = None,\n+        videos: VideoInput | None = None,\n+        audio: AudioInput | None = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n         \"\"\"\n@@ -773,7 +773,7 @@ def to_json_string(self) -> str:\n \n         return json.dumps(dictionary, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: str | os.PathLike):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -897,7 +897,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n \n     @classmethod\n     def get_processor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n@@ -1190,7 +1190,7 @@ def from_args_and_dict(cls, args, processor_dict: dict[str, Any], **kwargs):\n     def _merge_kwargs(\n         self,\n         ModelProcessorKwargs: ProcessingKwargs,\n-        tokenizer_init_kwargs: Optional[dict] = None,\n+        tokenizer_init_kwargs: dict | None = None,\n         **kwargs,\n     ) -> dict[str, dict]:\n         \"\"\"\n@@ -1364,11 +1364,11 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n     @classmethod\n     def from_pretrained(\n         cls: type[SpecificProcessorType],\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name_or_path: str | os.PathLike,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ) -> SpecificProcessorType:\n@@ -1630,8 +1630,8 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n \n     def apply_chat_template(\n         self,\n-        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n-        chat_template: Optional[str] = None,\n+        conversation: list[dict[str, str]] | list[list[dict[str, str]]],\n+        chat_template: str | None = None,\n         **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:\n         \"\"\""
        },
        {
            "sha": "996ea02fb01a345dbc77cc4a1bae06facc52f1f2",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Callable\n from copy import deepcopy\n from functools import partial\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import numpy as np\n from huggingface_hub import create_repo, is_offline_mode\n@@ -233,8 +233,8 @@ def convert_to_rgb(\n     def sample_frames(\n         self,\n         metadata: VideoMetadata,\n-        num_frames: Optional[int] = None,\n-        fps: Optional[Union[int, float]] = None,\n+        num_frames: int | None = None,\n+        fps: int | float | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -286,9 +286,9 @@ def sample_frames(\n     def _decode_and_sample_videos(\n         self,\n         videos: VideoInput,\n-        video_metadata: Union[VideoMetadata, dict],\n-        do_sample_frames: Optional[bool] = None,\n-        sample_indices_fn: Optional[Callable] = None,\n+        video_metadata: VideoMetadata | dict,\n+        do_sample_frames: bool | None = None,\n+        sample_indices_fn: Callable | None = None,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n         Decode input videos and sample frames if needed.\n@@ -326,8 +326,8 @@ def _decode_and_sample_videos(\n     def _prepare_input_videos(\n         self,\n         videos: VideoInput,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[str] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n+        device: str | None = None,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n         Prepare the input videos for processing.\n@@ -411,9 +411,9 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        image_mean: float | list[float] | None,\n+        image_std: float | list[float] | None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group videos by size for batched resizing\n@@ -447,11 +447,11 @@ def _preprocess(\n     @classmethod\n     def from_pretrained(\n         cls,\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name_or_path: str | os.PathLike,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ):\n@@ -543,7 +543,7 @@ def from_pretrained(\n \n         return cls.from_dict(video_processor_dict, **kwargs)\n \n-    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save an video processor object to the directory `save_directory`, so that it can be re-loaded using the\n         [`~video_processing_utils.VideoProcessorBase.from_pretrained`] class method.\n@@ -593,7 +593,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n \n     @classmethod\n     def get_video_processor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n@@ -801,7 +801,7 @@ def to_json_string(self) -> str:\n \n         return json.dumps(dictionary, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: str | os.PathLike):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -816,7 +816,7 @@ def __repr__(self):\n         return f\"{self.__class__.__name__} {self.to_json_string()}\"\n \n     @classmethod\n-    def from_json_file(cls, json_file: Union[str, os.PathLike]):\n+    def from_json_file(cls, json_file: str | os.PathLike):\n         \"\"\"\n         Instantiates a video processor of type [`~video_processing_utils.VideoProcessorBase`] from the path to a JSON\n         file of parameters.\n@@ -860,7 +860,7 @@ def register_for_auto_class(cls, auto_class=\"AutoVideoProcessor\"):\n \n         cls._auto_class = auto_class\n \n-    def fetch_videos(self, video_url_or_urls: Union[str, list[str], list[list[str]]], sample_indices_fn=None):\n+    def fetch_videos(self, video_url_or_urls: str | list[str] | list[list[str]], sample_indices_fn=None):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `np.array` objects.\n "
        },
        {
            "sha": "691a82bba792831c766aa6b7920c1fbb81fb0b5e",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -118,9 +118,7 @@ def update(self, dictionary):\n                 setattr(self, key, value)\n \n \n-VideoMetadataType = Union[\n-    VideoMetadata, dict, list[Union[dict, VideoMetadata]], list[list[Union[dict, VideoMetadata]]]\n-]\n+VideoMetadataType = VideoMetadata | dict | list[dict | VideoMetadata] | list[list[dict | VideoMetadata]]\n \n \n def is_valid_video_frame(frame):"
        }
    ],
    "stats": {
        "total": 7998,
        "additions": 3900,
        "deletions": 4098
    }
}