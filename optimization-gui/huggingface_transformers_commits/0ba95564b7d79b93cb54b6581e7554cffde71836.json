{
    "author": "yonigozlan",
    "message": "Add args support for fast image processors (#37018)\n\n* add args support to fast image processors\n\n* add comment for clarity\n\n* fix-copies\n\n* Handle child class args passed as both args or kwargs in call and preprocess functions\n\n* revert support args passed as kwargs in overwritten preprocess\n\n* fix image processor errors",
    "sha": "0ba95564b7d79b93cb54b6581e7554cffde71836",
    "files": [
        {
            "sha": "b1e26141220398870cb8f75cdad053bde8765837",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -18,11 +18,7 @@\n \n import numpy as np\n \n-from .image_processing_utils import (\n-    BaseImageProcessor,\n-    BatchFeature,\n-    get_size_dict,\n-)\n+from .image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from .image_transforms import (\n     convert_to_rgb,\n     get_resize_output_image_size,\n@@ -233,6 +229,9 @@ def __init__(\n             else:\n                 setattr(self, key, getattr(self, key, None))\n \n+        # get valid kwargs names\n+        self._valid_kwargs_names = list(self.valid_kwargs.__annotations__.keys())\n+\n     def resize(\n         self,\n         image: \"torch.Tensor\",\n@@ -566,12 +565,16 @@ def _validate_preprocess_kwargs(\n             data_format=data_format,\n         )\n \n+    def __call__(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n+        return self.preprocess(images, *args, **kwargs)\n+\n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+    def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n+        # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_kwargs_names)\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n+        for kwarg_name in self._valid_kwargs_names:\n             kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n \n         # Extract parameters that are only used for preparing the input images\n@@ -603,7 +606,7 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProces\n         kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n \n-        return self._preprocess(images=images, **kwargs)\n+        return self._preprocess(images, *args, **kwargs)\n \n     def _preprocess(\n         self,\n@@ -651,6 +654,7 @@ def _preprocess(\n     def to_dict(self):\n         encoder_dict = super().to_dict()\n         encoder_dict.pop(\"_valid_processor_keys\", None)\n+        encoder_dict.pop(\"_valid_kwargs_names\", None)\n         return encoder_dict\n \n "
        },
        {
            "sha": "c51356ddf565abf574a32ad22657f155b99c8edd",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -587,8 +587,6 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n@@ -606,19 +604,17 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -629,6 +625,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "5bdf903f211b2cbe76fe2b625cbf1aafebf888d7",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -578,8 +578,6 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n@@ -597,19 +595,17 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -620,6 +616,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "6840d367c638696fe6d5fe712a2bcf69ec842a76",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -28,11 +28,7 @@\n     get_max_height_width,\n     safe_squeeze,\n )\n-from ...image_transforms import (\n-    center_to_corners_format,\n-    corners_to_center_format,\n-    id_to_rgb,\n-)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format, id_to_rgb\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -603,8 +599,6 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n@@ -622,19 +616,17 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -645,6 +637,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "775f648def1a15ab9073a956010b255d73e83db4",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -609,8 +609,6 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n@@ -628,19 +626,17 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -651,6 +647,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "6bc57efb0a5e328dad86e5600c93ac87b16d7d46",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -26,11 +26,7 @@\n     group_images_by_shape,\n     reorder_images,\n )\n-from ...image_utils import (\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_utils import ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -320,7 +316,7 @@ def get_best_fit(\n     else:\n         optimal_canvas = chosen_canvas[0]\n \n-    return tuple(optimal_canvas.tolist())\n+    return optimal_canvas\n \n \n class Llama4ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n@@ -356,6 +352,8 @@ class Llama4ImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[Llama4ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n+    # Disable compilation here as conversion to bfloat16 causes differences in the output of the compiled and non-compiled versions\n+    @torch.compiler.disable\n     def rescale_and_normalize(\n         self,\n         images: \"torch.Tensor\",\n@@ -399,7 +397,7 @@ def _preprocess(\n         **kwargs,\n     ) -> BatchFeature:\n         possible_resolutions = find_supported_resolutions(max_num_chunks=max_patches, patch_size=size)\n-        possible_resolutions = torch.tensor(possible_resolutions)\n+        possible_resolutions = torch.tensor(possible_resolutions, device=images[0].device)\n         # process images by batch, grouped by shape\n         grouped_images, grouped_images_index = group_images_by_shape(images)\n         grouped_processed_images = {}\n@@ -438,7 +436,9 @@ def _preprocess(\n             # split into tiles\n             processed_images = split_to_tiles(processed_images, ratio_h, ratio_w)\n             grouped_processed_images[shape] = processed_images\n-            grouped_aspect_ratios[shape] = torch.tensor([[ratio_h, ratio_w]] * stacked_images.shape[0])\n+            grouped_aspect_ratios[shape] = torch.tensor(\n+                [[ratio_h, ratio_w]] * stacked_images.shape[0], device=images[0].device\n+            )\n \n             # add a global tile to the processed tile if there are more than one tile\n             if ratio_h * ratio_w > 1:"
        },
        {
            "sha": "2742e228bb108a0c19b717b8ed47fbbdaf9a913d",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -397,24 +397,20 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -425,6 +421,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "b987603192c7de78d9f0b964641b481a9691a564",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -137,7 +137,7 @@ def preprocess(\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return BaseImageProcessorFast().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return BaseImageProcessorFast().preprocess(images, annotations, masks_path, **kwargs)\n \n     def prepare_annotation(\n         self,\n@@ -163,13 +163,11 @@ def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -180,6 +178,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "174bebb45fc4706222b95954df78ab0f11aeac52",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -131,7 +131,7 @@ def preprocess(\n         kwargs.pop(\"size\")\n         kwargs.pop(\"crop_size\")\n \n-        return self._preprocess(images=images, trimaps=trimaps, **kwargs)\n+        return self._preprocess(images, trimaps, **kwargs)\n \n     def _prepare_input_trimaps(\n         self, trimaps: ImageInput, device: Optional[\"torch.device\"] = None"
        },
        {
            "sha": "120722f05a4ac15b0e2e48c6896c44097a026263",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -626,8 +626,6 @@ def preprocess(\n             - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                 An image can have no segments, in which case the list should be empty.\n             - \"file_name\" (`str`): The file name of the image.\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n         \"\"\"\n@@ -645,19 +643,17 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+        return super().preprocess(images, annotations, masks_path, **kwargs)\n \n     def _preprocess(\n         self,\n         images: List[\"torch.Tensor\"],\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n-        return_segmentation_masks: bool,\n         masks_path: Optional[Union[str, pathlib.Path]],\n+        return_segmentation_masks: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -668,6 +664,7 @@ def _preprocess(\n         pad_size: Optional[Dict[str, int]],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or a batch of images so that it can be used by the model."
        },
        {
            "sha": "afe0c4674d33ed04086a271cce24ce7e39404ea1",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -19,13 +19,7 @@\n import requests\n from packaging import version\n \n-from transformers.testing_utils import (\n-    require_torch,\n-    require_torch_gpu,\n-    require_vision,\n-    slow,\n-    torch_device,\n-)\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs"
        },
        {
            "sha": "49b8ff281a5011e5895d7a7babd1339df15488db",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba95564b7d79b93cb54b6581e7554cffde71836/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba95564b7d79b93cb54b6581e7554cffde71836/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=0ba95564b7d79b93cb54b6581e7554cffde71836",
            "patch": "@@ -19,8 +19,9 @@\n \n import numpy as np\n import requests\n+from packaging import version\n \n-from transformers.testing_utils import is_flaky, require_torch, require_vision\n+from transformers.testing_utils import is_flaky, require_torch, require_torch_gpu, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n@@ -334,3 +335,24 @@ def test_slow_fast_equivalence_batched(self):\n         self.assertLessEqual(\n             torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n         )\n+\n+    @slow\n+    @require_torch_gpu\n+    @require_vision\n+    def test_can_compile_fast_image_processor(self):\n+        # override as trimaps are needed for the image processor\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        dummy_trimap = np.random.randint(0, 3, size=input_image.shape[1:])\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, dummy_trimap, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, dummy_trimap, device=torch_device, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(output_eager.pixel_values, output_compiled.pixel_values, rtol=1e-4, atol=1e-4)"
        }
    ],
    "stats": {
        "total": 139,
        "additions": 68,
        "deletions": 71
    }
}