{
    "author": "yaswanth19",
    "message": "ðŸš¨ðŸš¨ðŸš¨ [eomt] make EoMT compatible with pipeline (#39122)\n\n* Make EoMT compatible with pipeline\n\n* Implicit patch offsets\n\n* remove patch offsets from arg\n\n* Modify tests\n\n* Update example\n\n* fix proc testcase\n\n* Add few more args\n\n* add pipeline test suite\n\n* fix\n\n* docstring fixes\n\n* add pipeline test\n\n* changes w.r.t review\n\n* ðŸ™ˆ MB\n\n* should fix device mismatch\n\n* debug\n\n* Fixes device mismatch\n\n* use decorator\n\n* we can split mlp\n\n* expected values update\n\n---------\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>",
    "sha": "b61023a1b760b207d99b699dafc1fbfde992c12c",
    "files": [
        {
            "sha": "86816a475fb04692326d23d4df30359ab8a0309b",
            "filename": "docs/source/en/model_doc/eomt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -74,20 +74,16 @@ inputs = processor(\n     return_tensors=\"pt\",\n )\n \n-# Remove Patch Offsets from inputs â€” only used later for post-processing.\n-patch_offsets = inputs.pop(\"patch_offsets\")\n-\n with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Prepare the original image size in the format (height, width)\n-original_image_sizes = [(image.height, image.width)]\n+target_sizes = [(image.height, image.width)]\n \n # Post-process the model outputs to get final segmentation prediction\n preds = processor.post_process_semantic_segmentation(\n     outputs,\n-    patch_offsets=patch_offsets,\n-    original_image_sizes=original_image_sizes,\n+    target_sizes=target_sizes,\n )\n \n # Visualize the segmentation mask\n@@ -130,12 +126,12 @@ with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Prepare the original image size in the format (height, width)\n-original_image_sizes = [(image.height, image.width)]\n+target_sizes = [(image.height, image.width)]\n \n # Post-process the model outputs to get final segmentation prediction\n preds = processor.post_process_instance_segmentation(\n     outputs,\n-    original_image_sizes=original_image_sizes,\n+    target_sizes=target_sizes,\n )\n \n # Visualize the segmentation mask\n@@ -173,12 +169,12 @@ with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Prepare the original image size in the format (height, width)\n-original_image_sizes = [(image.height, image.width)]\n+target_sizes = [(image.height, image.width)]\n \n # Post-process the model outputs to get final segmentation prediction\n preds = processor.post_process_panoptic_segmentation(\n     outputs,\n-    original_image_sizes=original_image_sizes,\n+    target_sizes=target_sizes,\n )\n \n # Visualize the panoptic segmentation mask"
        },
        {
            "sha": "e63a1be95fe4db52e6d2be685a364ca2f2b518da",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 31,
            "deletions": 30,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -97,7 +97,7 @@ def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, in\n     Computes the output image size given the input image size and the desired output size.\n \n     Args:\n-        image_size (`Tuple[int, int]`):\n+        image_size (`tuple[int, int]`):\n             The input image size.\n         size (`int`):\n             The desired output size.\n@@ -531,13 +531,13 @@ def preprocess(\n                 Image or batch of images to preprocess.\n             segmentation_maps (`ImageInput`, *optional*):\n                 The corresponding semantic segmentation maps with the pixel-wise annotations.\n-            instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+            instance_id_to_semantic_id (`list[dict[int, int]]` or `dict[int, int]`, *optional*):\n                 A mapping between object instance ids and class ids.\n             do_split_image (`bool`, *optional*, defaults to `self.do_split_image`):\n                 Whether to split the input images into overlapping patches for semantic segmentation.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the input images.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Target size as a dictionary with `\"shortest_edge\"` and `\"longest_edge\"` keys.\n             resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use when resizing.\n@@ -550,9 +550,9 @@ def preprocess(\n             do_pad (`bool`, *optional*, defaults to `False`):\n                 Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n                 number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Mean for normalization. Single value or list for each channel.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Standard deviation for normalization. Single value or list for each channel.\n             ignore_index (`int`, *optional*):\n                 Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n@@ -640,7 +640,7 @@ def preprocess(\n         )\n \n         if do_split_image and patch_offsets:\n-            encoded_inputs[\"patch_offsets\"] = patch_offsets\n+            encoded_inputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in patch_offsets]\n \n         return encoded_inputs\n \n@@ -663,8 +663,8 @@ def encode_inputs(\n         each mask.\n \n         Args:\n-            pixel_values_list (`List[ImageInput]`):\n-                List of images (pixel values) to be padded. Each image should be a tensor of shape `(channels, height,\n+            pixel_values_list (`list[ImageInput]`):\n+                list of images (pixel values) to be padded. Each image should be a tensor of shape `(channels, height,\n                 width)`.\n \n             segmentation_maps (`ImageInput`, *optional*):\n@@ -678,7 +678,7 @@ def encode_inputs(\n                 - 1 for pixels that are real (i.e. **not masked**),\n                 - 0 for pixels that are padding (i.e. **masked**).\n \n-            instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+            instance_id_to_semantic_id (`list[dict[int, int]]` or `dict[int, int]`, *optional*):\n                 A mapping between object instance ids and class ids. If passed, `segmentation_maps` is treated as an\n                 instance segmentation map where each pixel represents an instance id. Can be provided as a single\n                 dictionary with a global/dataset-level mapping or as a list of dictionaries (one per image), to map\n@@ -740,7 +740,7 @@ def merge_image_patches(\n         self,\n         segmentation_logits: torch.Tensor,\n         patch_offsets: list[tuple[int, int, int]],\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: dict[str, int],\n     ) -> list[torch.Tensor]:\n         \"\"\"\n@@ -750,28 +750,28 @@ def merge_image_patches(\n             segmentation_logits (`torch.Tensor`):\n                 A tensor of shape `(num_patches, num_classes, patch_height, patch_width)` representing predicted logits\n                 for each image patch.\n-            patch_offsets (`List[Tuple[int, int, int]]`):\n+            patch_offsets (`list[tuple[int, int, int]]`):\n                 A list of tuples where each tuple contains:\n                 - `image_index` (int): Index of the original image this patch belongs to.\n                 - `start` (int): Start pixel index of the patch along the long dimension (height or width).\n                 - `end` (int): End pixel index of the patch along the long dimension.\n-            original_image_sizes (`List[Tuple[int, int]]`):\n-                List of original (height, width) dimensions for each image before preprocessing.\n-            size (`Dict[str, int]`):\n+            target_sizes (`list[tuple[int, int]]`):\n+                list of original (height, width) dimensions for each image before preprocessing.\n+            size (`dict[str, int]`):\n                 A size dict which was used to resize.\n         \"\"\"\n         num_classes = segmentation_logits.shape[1]\n         aggregated_logits = []\n         patch_counts = []\n \n-        for image_size in original_image_sizes:\n+        for image_size in target_sizes:\n             height, width = get_size_with_aspect_ratio(image_size, size[\"shortest_edge\"], size[\"longest_edge\"])\n             aggregated_logits.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n             patch_counts.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n \n         # Stitch patches back into full-sized logit maps\n         for patch_idx, (image_idx, patch_start, patch_end) in enumerate(patch_offsets):\n-            if original_image_sizes[image_idx][0] > original_image_sizes[image_idx][1]:\n+            if target_sizes[image_idx][0] > target_sizes[image_idx][1]:\n                 aggregated_logits[image_idx][:, patch_start:patch_end, :] += segmentation_logits[patch_idx]\n                 patch_counts[image_idx][:, patch_start:patch_end, :] += 1\n             else:\n@@ -784,7 +784,7 @@ def merge_image_patches(\n             averaged_logits = logit_sum / count.clamp(min=1)\n             resized_logits = F.interpolate(\n                 averaged_logits[None, ...],\n-                size=original_image_sizes[idx],\n+                size=target_sizes[idx],\n                 mode=\"bilinear\",\n                 align_corners=False,\n             )[0]\n@@ -796,14 +796,14 @@ def merge_image_patches(\n     def unpad_image(\n         self,\n         segmentation_logits: torch.Tensor,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: dict[str, int],\n     ) -> list[torch.Tensor]:\n         \"\"\"Restores panoptic segmentation logits to their original image resolutions.\"\"\"\n \n         resized_logits = []\n \n-        for idx, original_size in enumerate(original_image_sizes):\n+        for idx, original_size in enumerate(target_sizes):\n             target_height, target_width = get_size_with_aspect_ratio(\n                 original_size, size[\"shortest_edge\"], size[\"longest_edge\"]\n             )\n@@ -817,8 +817,7 @@ def unpad_image(\n     def post_process_semantic_segmentation(\n         self,\n         outputs,\n-        patch_offsets: list[tuple[int, int, int]],\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: Optional[dict[str, int]] = None,\n     ) -> np.ndarray:\n         \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n@@ -827,6 +826,7 @@ def post_process_semantic_segmentation(\n \n         masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n         class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        patch_offsets = outputs.patch_offsets\n \n         output_size = get_target_size(size)\n         masks_queries_logits = F.interpolate(\n@@ -841,15 +841,15 @@ def post_process_semantic_segmentation(\n \n         segmentation_logits = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n \n-        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, original_image_sizes, size)\n+        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, target_sizes, size)\n \n-        preds = torch.stack(output_logits).argmax(dim=1)\n+        preds = [logit.argmax(dim=0) for logit in output_logits]\n         return preds\n \n     def post_process_panoptic_segmentation(\n         self,\n         outputs,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         threshold: float = 0.8,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n@@ -873,7 +873,7 @@ def post_process_panoptic_segmentation(\n             mode=\"bilinear\",\n         )\n \n-        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, target_sizes, size)\n         pred_scores_batch, pred_labels_batch = class_queries_logits.softmax(dim=-1).max(-1)\n \n         results: list = []\n@@ -885,7 +885,7 @@ def post_process_panoptic_segmentation(\n \n             # No mask found\n             if mask_probs.shape[0] <= 0:\n-                height, width = original_image_sizes[i] if original_image_sizes is not None else mask_probs.shape[1:]\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs.shape[1:]\n                 segmentation = torch.zeros((height, width)) - 1\n                 results.append({\"segmentation\": segmentation, \"segments_info\": []})\n                 continue\n@@ -897,16 +897,17 @@ def post_process_panoptic_segmentation(\n                 stuff_classes=stuff_classes,\n                 mask_threshold=mask_threshold,\n                 overlap_mask_area_threshold=overlap_mask_area_threshold,\n-                target_size=original_image_sizes[i] if original_image_sizes is not None else None,\n+                target_size=target_sizes[i] if target_sizes is not None else None,\n             )\n \n             results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n         return results\n \n+    @filter_out_non_signature_kwargs()\n     def post_process_instance_segmentation(\n         self,\n         outputs,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         threshold: float = 0.5,\n         size: Optional[dict[str, int]] = None,\n     ):\n@@ -924,7 +925,7 @@ def post_process_instance_segmentation(\n             mode=\"bilinear\",\n         )\n \n-        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, target_sizes, size)\n \n         device = masks_queries_logits.device\n         batch_size = class_queries_logits.shape[0]\n@@ -946,7 +947,7 @@ def post_process_instance_segmentation(\n             )\n             pred_scores = scores * mask_scores\n \n-            segmentation = torch.zeros(original_image_sizes[i], device=device) - 1\n+            segmentation = torch.zeros(target_sizes[i], device=device) - 1\n \n             instance_maps, segments = [], []\n             current_segment_id = 0"
        },
        {
            "sha": "343c6ae2cf1aefa02daed7a382196e8a6f94dfbe",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 25,
            "deletions": 23,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -41,6 +41,7 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n+    filter_out_non_signature_kwargs,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -268,7 +269,7 @@ def preprocess(\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n             The segmentation maps to preprocess for corresponding images.\n-        instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+        instance_id_to_semantic_id (`list[dict[int, int]]` or `dict[int, int]`, *optional*):\n             A mapping between object instance ids and class ids.\n         \"\"\"\n         # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n@@ -340,15 +341,15 @@ def preprocess(\n             outputs[\"class_labels\"] = class_labels\n \n         if patch_offsets:\n-            outputs[\"patch_offsets\"] = patch_offsets\n+            outputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in patch_offsets]\n \n         return outputs\n \n     def merge_image_patches(\n         self,\n         segmentation_logits: torch.Tensor,\n         patch_offsets: list[tuple[int, int, int]],\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: dict[str, int],\n     ) -> list[torch.Tensor]:\n         \"\"\"\n@@ -358,28 +359,28 @@ def merge_image_patches(\n             segmentation_logits (`torch.Tensor`):\n                 A tensor of shape `(num_patches, num_classes, patch_height, patch_width)` representing predicted logits\n                 for each image patch.\n-            patch_offsets (`List[Tuple[int, int, int]]`):\n+            patch_offsets (`list[tuple[int, int, int]]`):\n                 A list of tuples where each tuple contains:\n                 - `image_index` (int): Index of the original image this patch belongs to.\n                 - `start` (int): Start pixel index of the patch along the long dimension (height or width).\n                 - `end` (int): End pixel index of the patch along the long dimension.\n-            original_image_sizes (`List[Tuple[int, int]]`):\n-                List of original (height, width) dimensions for each image before preprocessing.\n-            size (`Dict[str, int]`):\n+            target_sizes (`list[tuple[int, int]]`):\n+                list of original (height, width) dimensions for each image before preprocessing.\n+            size (`dict[str, int]`):\n                 A size dict which was used to resize.\n         \"\"\"\n         num_classes = segmentation_logits.shape[1]\n         aggregated_logits = []\n         patch_counts = []\n \n-        for image_size in original_image_sizes:\n+        for image_size in target_sizes:\n             height, width = get_size_with_aspect_ratio(image_size, size[\"shortest_edge\"], size[\"longest_edge\"])\n             aggregated_logits.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n             patch_counts.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n \n         # Stitch patches back into full-sized logit maps\n         for patch_idx, (image_idx, patch_start, patch_end) in enumerate(patch_offsets):\n-            if original_image_sizes[image_idx][0] > original_image_sizes[image_idx][1]:\n+            if target_sizes[image_idx][0] > target_sizes[image_idx][1]:\n                 aggregated_logits[image_idx][:, patch_start:patch_end, :] += segmentation_logits[patch_idx]\n                 patch_counts[image_idx][:, patch_start:patch_end, :] += 1\n             else:\n@@ -392,7 +393,7 @@ def merge_image_patches(\n             averaged_logits = logit_sum / count.clamp(min=1)\n             resized_logits = torch.nn.functional.interpolate(\n                 averaged_logits[None, ...],\n-                size=original_image_sizes[idx],\n+                size=target_sizes[idx],\n                 mode=\"bilinear\",\n                 align_corners=False,\n             )[0]\n@@ -404,14 +405,14 @@ def merge_image_patches(\n     def unpad_image(\n         self,\n         segmentation_logits: torch.Tensor,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: dict[str, int],\n     ) -> list[torch.Tensor]:\n         \"\"\"Restores panoptic segmentation logits to their original image resolutions.\"\"\"\n \n         resized_logits = []\n \n-        for idx, original_size in enumerate(original_image_sizes):\n+        for idx, original_size in enumerate(target_sizes):\n             target_height, target_width = get_size_with_aspect_ratio(\n                 original_size, size[\"shortest_edge\"], size[\"longest_edge\"]\n             )\n@@ -425,8 +426,7 @@ def unpad_image(\n     def post_process_semantic_segmentation(\n         self,\n         outputs,\n-        patch_offsets: list[tuple[int, int, int]],\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         size: Optional[dict[str, int]] = None,\n     ) -> np.ndarray:\n         \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n@@ -435,6 +435,7 @@ def post_process_semantic_segmentation(\n \n         masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n         class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        patch_offsets = outputs.patch_offsets\n \n         output_size = get_target_size(size)\n         masks_queries_logits = torch.nn.functional.interpolate(\n@@ -449,15 +450,15 @@ def post_process_semantic_segmentation(\n \n         segmentation_logits = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n \n-        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, original_image_sizes, size)\n+        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, target_sizes, size)\n \n-        preds = torch.stack(output_logits).argmax(dim=1)\n+        preds = [logit.argmax(dim=0) for logit in output_logits]\n         return preds\n \n     def post_process_panoptic_segmentation(\n         self,\n         outputs,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         threshold: float = 0.8,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n@@ -481,7 +482,7 @@ def post_process_panoptic_segmentation(\n             mode=\"bilinear\",\n         )\n \n-        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, target_sizes, size)\n         pred_scores_batch, pred_labels_batch = class_queries_logits.softmax(dim=-1).max(-1)\n \n         results: list = []\n@@ -493,7 +494,7 @@ def post_process_panoptic_segmentation(\n \n             # No mask found\n             if mask_probs.shape[0] <= 0:\n-                height, width = original_image_sizes[i] if original_image_sizes is not None else mask_probs.shape[1:]\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs.shape[1:]\n                 segmentation = torch.zeros((height, width)) - 1\n                 results.append({\"segmentation\": segmentation, \"segments_info\": []})\n                 continue\n@@ -505,16 +506,17 @@ def post_process_panoptic_segmentation(\n                 stuff_classes=stuff_classes,\n                 mask_threshold=mask_threshold,\n                 overlap_mask_area_threshold=overlap_mask_area_threshold,\n-                target_size=original_image_sizes[i] if original_image_sizes is not None else None,\n+                target_size=target_sizes[i] if target_sizes is not None else None,\n             )\n \n             results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n         return results\n \n+    @filter_out_non_signature_kwargs()\n     def post_process_instance_segmentation(\n         self,\n         outputs,\n-        original_image_sizes: list[tuple[int, int]],\n+        target_sizes: list[tuple[int, int]],\n         threshold: float = 0.8,\n         size: Optional[dict[str, int]] = None,\n     ):\n@@ -532,7 +534,7 @@ def post_process_instance_segmentation(\n             mode=\"bilinear\",\n         )\n \n-        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, target_sizes, size)\n \n         device = masks_queries_logits.device\n         batch_size = class_queries_logits.shape[0]\n@@ -554,7 +556,7 @@ def post_process_instance_segmentation(\n             )\n             pred_scores = scores * mask_scores\n \n-            segmentation = torch.zeros(original_image_sizes[i], device=device) - 1\n+            segmentation = torch.zeros(target_sizes[i], device=device) - 1\n \n             instance_maps, segments = [], []\n             current_segment_id = 0"
        },
        {
            "sha": "bc865988ca692414107f07b650dafbc7fe48c4d4",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -74,6 +74,8 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n     attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n         Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n         sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    patch_offsets (`list[torch.Tensor]`, *optional*):\n+        list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -82,6 +84,7 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n+    patch_offsets: Optional[list[torch.Tensor]] = None\n \n \n # Adapted from https://github.com/facebookresearch/detectron2/blob/main/projects/PointRend/point_rend/point_features.py\n@@ -996,7 +999,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n-    _no_split_modules = [\"EomtMLP\"]\n+    _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n \n@@ -1097,13 +1100,16 @@ def forward(\n         class_labels: Optional[list[Tensor]] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n+        patch_offsets: Optional[list[Tensor]] = None,\n     ) -> EomtForUniversalSegmentationOutput:\n         r\"\"\"\n-        mask_labels (`List[torch.Tensor]`, *optional*):\n-            List of mask labels of shape `(num_labels, height, width)` to be fed to a model\n-        class_labels (`List[torch.LongTensor]`, *optional*):\n+        mask_labels (`list[torch.Tensor]`, *optional*):\n+            list of mask labels of shape `(num_labels, height, width)` to be fed to a model\n+        class_labels (`list[torch.LongTensor]`, *optional*):\n             list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n             labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n+        patch_offsets (`list[torch.Tensor]`, *optional*):\n+            list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n         \"\"\"\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1126,7 +1132,7 @@ def forward(\n                 all_hidden_states += (hidden_states,)\n \n             if idx == self.num_hidden_layers - self.config.num_blocks:\n-                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1)\n+                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1).to(hidden_states.device)\n                 hidden_states = torch.cat((query, hidden_states), dim=1)\n \n             if idx >= self.num_hidden_layers - self.config.num_blocks and (\n@@ -1206,6 +1212,7 @@ def forward(\n             last_hidden_state=sequence_output,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n+            patch_offsets=patch_offsets,\n         )\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "44ecb69eca67bb1c351670ecb063e1d2a191ff44",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -226,6 +226,8 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n     attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n         Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n         sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    patch_offsets (`list[torch.Tensor]`, *optional*):\n+        list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -234,6 +236,7 @@ class EomtForUniversalSegmentationOutput(ModelOutput):\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n+    patch_offsets: Optional[list[torch.Tensor]] = None\n \n \n class EomtLoss(Mask2FormerLoss):\n@@ -368,7 +371,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n-    _no_split_modules = [\"EomtMLP\"]\n+    _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n \n@@ -473,13 +476,16 @@ def forward(\n         class_labels: Optional[list[Tensor]] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n+        patch_offsets: Optional[list[Tensor]] = None,\n     ):\n         r\"\"\"\n-        mask_labels (`List[torch.Tensor]`, *optional*):\n-            List of mask labels of shape `(num_labels, height, width)` to be fed to a model\n-        class_labels (`List[torch.LongTensor]`, *optional*):\n+        mask_labels (`list[torch.Tensor]`, *optional*):\n+            list of mask labels of shape `(num_labels, height, width)` to be fed to a model\n+        class_labels (`list[torch.LongTensor]`, *optional*):\n             list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n             labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n+        patch_offsets (`list[torch.Tensor]`, *optional*):\n+            list of tuples indicating the image index and start and end positions of patches for semantic segementation.\n         \"\"\"\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -502,7 +508,7 @@ def forward(\n                 all_hidden_states += (hidden_states,)\n \n             if idx == self.num_hidden_layers - self.config.num_blocks:\n-                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1)\n+                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1).to(hidden_states.device)\n                 hidden_states = torch.cat((query, hidden_states), dim=1)\n \n             if idx >= self.num_hidden_layers - self.config.num_blocks and (\n@@ -582,6 +588,7 @@ def forward(\n             last_hidden_state=sequence_output,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n+            patch_offsets=patch_offsets,\n         )\n \n "
        },
        {
            "sha": "594a1d9fe867fa71a36ec18322bf9d75694f9af2",
            "filename": "tests/models/eomt/test_image_processing_eomt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -84,10 +84,11 @@ def prepare_image_processor_dict(self):\n             \"num_labels\": self.num_labels,\n         }\n \n-    def prepare_fake_eomt_outputs(self, batch_size):\n+    def prepare_fake_eomt_outputs(self, batch_size, patch_offsets=None):\n         return EomtForUniversalSegmentationOutput(\n             masks_queries_logits=torch.randn((batch_size, self.num_queries, self.height, self.width)),\n             class_queries_logits=torch.randn((batch_size, self.num_queries, self.num_classes + 1)),\n+            patch_offsets=patch_offsets,\n         )\n \n     def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n@@ -263,13 +264,13 @@ def test_post_process_semantic_segmentation(self):\n         image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n         inputs = processor(images=image, do_split_image=True, return_tensors=\"pt\")\n-        patch_offsets = inputs.pop(\"patch_offsets\")\n+        patch_offsets = inputs[\"patch_offsets\"]\n \n-        original_sizes = [image.size[::-1]]\n+        target_sizes = [image.size[::-1]]\n \n         # For semantic segmentation, the BS of output is 2 coz, two patches are created for the image.\n-        outputs = self.image_processor_tester.prepare_fake_eomt_outputs(inputs[\"pixel_values\"].shape[0])\n-        segmentation = processor.post_process_semantic_segmentation(outputs, patch_offsets, original_sizes)\n+        outputs = self.image_processor_tester.prepare_fake_eomt_outputs(inputs[\"pixel_values\"].shape[0], patch_offsets)\n+        segmentation = processor.post_process_semantic_segmentation(outputs, target_sizes)\n \n         self.assertEqual(segmentation[0].shape, (image.height, image.width))\n "
        },
        {
            "sha": "c4b026cc18e20af5d8d4fb58be919ea39b3d46fe",
            "filename": "tests/models/eomt/test_modeling_eomt.py",
            "status": "modified",
            "additions": 21,
            "deletions": 14,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/b61023a1b760b207d99b699dafc1fbfde992c12c/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b61023a1b760b207d99b699dafc1fbfde992c12c/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py?ref=b61023a1b760b207d99b699dafc1fbfde992c12c",
            "patch": "@@ -17,12 +17,13 @@\n \n import requests\n \n-from transformers import AutoImageProcessor, EomtConfig, EomtForUniversalSegmentation\n+from transformers import AutoImageProcessor, EomtConfig, EomtForUniversalSegmentation, pipeline\n from transformers.testing_utils import require_torch, require_torch_accelerator, require_torch_fp16, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -100,8 +101,9 @@ def prepare_config_and_inputs_for_training(self):\n \n \n @require_torch\n-class EomtForUniversalSegmentationTest(ModelTesterMixin, unittest.TestCase):\n+class EomtForUniversalSegmentationTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (EomtForUniversalSegmentation,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-segmentation\": EomtForUniversalSegmentation} if is_torch_available() else {}\n     is_encoder_decoder = False\n     test_pruning = False\n     test_head_masking = False\n@@ -340,19 +342,16 @@ def test_semantic_segmentation_inference(self):\n         image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n         inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n-        patch_offsets = inputs.pop(\"patch_offsets\", None)\n \n         with torch.inference_mode():\n             outputs = model(**inputs)\n \n         self.assertTrue(outputs.class_queries_logits.shape == (2, 100, 151))\n         self.assertTrue(outputs.masks_queries_logits.shape == (2, 100, 128, 128))\n \n-        preds = processor.post_process_semantic_segmentation(\n-            outputs, original_image_sizes=[(image.size[1], image.size[0])], patch_offsets=patch_offsets\n-        )\n+        preds = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1], image.size[0])])[0]\n \n-        self.assertTrue(preds.shape[1:] == (image.size[1], image.size[0]))\n+        self.assertTrue(preds.shape == (image.size[1], image.size[0]))\n \n         # fmt: off\n         EXPECTED_SLICE = torch.tensor([\n@@ -369,7 +368,7 @@ def test_semantic_segmentation_inference(self):\n         ], device=model.device)\n         # fmt: on\n \n-        output_slice = preds[0, :10, :10]\n+        output_slice = preds[:10, :10]\n         torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n@@ -387,9 +386,7 @@ def test_panoptic_segmentation_inference(self):\n         self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 134))\n         self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n \n-        preds = processor.post_process_panoptic_segmentation(\n-            outputs, original_image_sizes=[(image.size[1], image.size[0])]\n-        )[0]\n+        preds = processor.post_process_panoptic_segmentation(outputs, target_sizes=[(image.size[1], image.size[0])])[0]\n         segmentation, segments_info = preds[\"segmentation\"], preds[\"segments_info\"]\n \n         # fmt: off\n@@ -438,9 +435,7 @@ def test_instance_segmentation_inference(self):\n         self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 81))\n         self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n \n-        preds = processor.post_process_instance_segmentation(\n-            outputs, original_image_sizes=[(image.size[1], image.size[0])]\n-        )[0]\n+        preds = processor.post_process_instance_segmentation(outputs, target_sizes=[(image.size[1], image.size[0])])[0]\n         segmentation, segments_info = preds[\"segmentation\"], preds[\"segments_info\"]\n \n         # fmt: off\n@@ -473,3 +468,15 @@ def test_instance_segmentation_inference(self):\n             self.assertEqual(actual[\"id\"], expected[\"id\"])\n             self.assertEqual(actual[\"label_id\"], expected[\"label_id\"])\n             self.assertAlmostEqual(actual[\"score\"], expected[\"score\"], delta=1e-3)\n+\n+    @slow\n+    def test_segmentation_pipeline(self):\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        pipe = pipeline(model=self.model_id, subtask=\"panoptic\", device=torch_device)\n+        output = pipe(image)\n+\n+        EXPECTED_OUTPUT_LABELS = [\"cat\", \"cat\", \"couch\", \"remote\", \"remote\"]\n+\n+        output_labels = [segment[\"label\"] for segment in output]\n+        self.assertEqual(output_labels, EXPECTED_OUTPUT_LABELS)"
        }
    ],
    "stats": {
        "total": 205,
        "additions": 113,
        "deletions": 92
    }
}