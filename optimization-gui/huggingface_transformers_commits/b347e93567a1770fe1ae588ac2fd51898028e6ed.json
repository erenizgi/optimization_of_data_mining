{
    "author": "qubvel",
    "message": "[typing] Fix return typehint for decoder and inv_freq annotation (#39610)\n\n* fix return typehint for decoder and annotate inv_freq\n\n* fix modular\n\n* Fix consistency\n\n* Move annotation on class level\n\n* missing annotations\n\n* add comment",
    "sha": "b347e93567a1770fe1ae588ac2fd51898028e6ed",
    "files": [
        {
            "sha": "bb7d419a146ceefb583661874df52979fee300c5",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -82,6 +82,8 @@ def extra_repr(self):\n \n \n class ArceeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: ArceeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -278,7 +280,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "9a87c4cc502a8cf3c76b309e321c48e2b58d36f0",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -598,7 +598,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -668,6 +668,8 @@ def _init_weights(self, module):\n \n \n class AriaTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: AriaTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "e31c0a53ed81a144d42233ca217a0641918195ff",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -189,6 +189,8 @@ def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTens\n \n \n class BambaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: BambaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "dee9edfc2e52c911c61c9afbf009c811aa0455f7",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -184,7 +184,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -243,7 +243,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -268,6 +268,8 @@ def forward(\n \n \n class BitNetRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: BitNetConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "e8f29db38dc6a590a001b8403c56c19d47d23602",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -66,7 +66,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "91bbc7e87f2e225d146dbf83e96942adcac2711a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -67,6 +67,8 @@ def extra_repr(self):\n # copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Chameleon\n # TODO(joao): add me back asap :)\n class ChameleonRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n         super().__init__()\n         self.scaling_factor = scaling_factor"
        },
        {
            "sha": "a07d9b8d1bb87b39a5869e6bdd0c68cf4f86fb70",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -65,6 +65,8 @@ def forward(self, hidden_states):\n \n \n class CohereRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: CohereConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -233,7 +235,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "9cc22c27320d12d7c2101c21f333ef4d1053232f",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -153,7 +153,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "8b31b2f3495c120c96811b3b486302cd93e87a59",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -40,6 +40,8 @@\n \n \n class Cohere2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Cohere2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "cde72840375d68c606d5760302a51f422fa59f99",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -118,6 +118,8 @@ def extra_repr(self):\n \n \n class CsmRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: CsmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -330,7 +332,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "2440abf3b64f0563602946ed19f61a29ab774657",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -46,6 +46,8 @@\n \n \n class DbrxRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n "
        },
        {
            "sha": "bd41e7ba7563f9b54fd0b888b702766a1c04634f",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -194,6 +194,8 @@ def extra_repr(self):\n \n \n class DeepseekV2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: DeepseekV2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -421,7 +423,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "aa15c5ee8775d25b6a0ff14ea176127e72413699",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -49,6 +49,8 @@ def extra_repr(self):\n \n \n class DeepseekV3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: DeepseekV3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -463,7 +465,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "9447e17473dfb1ea3d8e43caf46c2bb193331405",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -68,6 +68,10 @@ def extra_repr(self):\n \n \n class OpenLlamaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+    cos_cached: torch.Tensor\n+    sin_cached: torch.Tensor\n+\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n "
        },
        {
            "sha": "8b31ddfb7f636348f6ef46644b8ccc6cc56530d6",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -140,6 +140,8 @@ def extra_repr(self):\n \n \n class DiaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: DiaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b0e56f702c240206c87f6802becdc6352259cbd3",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -241,7 +241,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, None]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n@@ -498,7 +498,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -550,6 +550,8 @@ def _init_weights(self, module):\n \n \n class DiffLlamaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: DiffLlamaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "1452ebda28dd4f33373bc2fa7f8380d85591e769",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -177,7 +177,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, None]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \""
        },
        {
            "sha": "d83b6f1796c5a5d8151cc157fb40a43040cb0bd6",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -70,6 +70,8 @@ def extra_repr(self):\n \n \n class DogeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: DogeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "d3de04024121b6554f91152ff83d602d71a70019",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -62,6 +62,8 @@ def extra_repr(self):\n \n \n class Dots1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Dots1Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -204,7 +206,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -383,7 +385,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "4725d919dbdcec11c983df1319ffcd4796870d5c",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -69,6 +69,8 @@ class KeypointMatchingOutput(ModelOutput):\n \n \n class EfficientLoFTRRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: EfficientLoFTRConfig, device=None):\n         super().__init__()\n         self.config = config"
        },
        {
            "sha": "6f4cafab7cf12ac6375ecc236912207f4dc8b015",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -244,7 +244,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n@@ -1105,6 +1105,8 @@ class Emu3PreTrainedModel(PreTrainedModel):\n \n \n class Emu3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Emu3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b714e8c7320367a08157f6d07ca2a0dfe24bdc11",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -61,7 +61,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n "
        },
        {
            "sha": "38a3e5f56355335891968a3defae370b4d70506f",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -39,6 +39,8 @@\n \n \n class Ernie4_5RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Ernie4_5Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -276,7 +278,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "3c579d5bd16fad9bbbd33c3557e3634659d8da6d",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -79,6 +79,8 @@ def forward(self, x):\n \n \n class Ernie4_5_MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Ernie4_5_MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "3bf625e924773d4252e3292b97510d9ade13c0f6",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -87,6 +87,8 @@ class RotaryEmbedding(torch.nn.Module):\n     matrices which depend on their relative positions.\n     \"\"\"\n \n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)"
        },
        {
            "sha": "61746685eee43dc62dd85983f56bbc482f39222e",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -195,6 +195,8 @@ class EvollaSaProtRotaryEmbedding(nn.Module):\n     matrices which depend on their relative positions.\n     \"\"\"\n \n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)\n@@ -1228,6 +1230,8 @@ def extra_repr(self):\n \n \n class EvollaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: EvollaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -1452,7 +1456,7 @@ def forward(\n         msa_batch_mask: Optional[torch.Tensor] = None,\n         query_attn_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)"
        },
        {
            "sha": "7abd74e43516b86f97ae56099c6058d305798028",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -88,6 +88,8 @@ class EvollaSaProtRotaryEmbedding(nn.Module):\n     matrices which depend on their relative positions.\n     \"\"\"\n \n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)"
        },
        {
            "sha": "7dbd77a0c08f4c3207f371ccffb7c0364e65c484",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -68,6 +68,8 @@ def extra_repr(self):\n \n \n class Exaone4RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Exaone4Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -286,7 +288,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,"
        },
        {
            "sha": "ad7b44a2508e80213e70d7be48e0ca42cb682b4f",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -101,6 +101,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Falcon\n class FalconRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: FalconConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "5e60f1a8bb7a143153619d95d9c872a34f3f2b87",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -217,6 +217,8 @@ def reset(self):\n \n \n class FalconH1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: FalconH1Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -357,7 +359,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "afbacc5666c242314025a233d6ffafcaf277c645",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -212,7 +212,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "ce6b7955276c7977c6c9b158482db24fd4629d84",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -79,6 +79,8 @@ def forward(self, x):\n \n \n class GemmaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GemmaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -275,7 +277,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "80b3c647b140cc06efdddee84ef5dc4b896ea1c0",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -297,6 +297,8 @@ def forward(\n \n \n class Gemma2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Gemma2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "95aa3ab6f061974b734e9372d92ab612e761cc3d",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -150,6 +150,8 @@ def extra_repr(self):\n \n \n class Gemma3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Gemma3TextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "59aefc77426f43ecd6019bb6e03782240d835ba7",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -1142,6 +1142,8 @@ def scale_corrected_output(self, corrected: torch.Tensor) -> torch.Tensor:\n \n \n class Gemma3nTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Gemma3nTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "cc2de4f9e5f1e1db26c4ac14d7bc350c0056d236",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -238,6 +238,8 @@ def extra_repr(self):\n \n \n class GlmRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GlmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -292,7 +294,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "76193a6132cc53e93378d7485f1b8874b944ae49",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -287,6 +287,8 @@ def extra_repr(self):\n \n \n class Glm4RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Glm4Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "81e161179590528d4bac30ab256c3523bd783279",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -160,7 +160,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -369,7 +369,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -417,6 +417,8 @@ def _init_weights(self, module):\n \n \n class Glm4MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Glm4MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "c93ffe8bd17b2abd8d73515943ac12c6d9c0cf23",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -101,6 +101,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class Glm4vVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n@@ -521,6 +523,8 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n \n \n class Glm4vTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Glm4vTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "908be720fdd8e036a0e0ef41dabc68757a054510",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -254,6 +254,8 @@ def forward(\n \n \n class GPTNeoXRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GPTNeoXConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -329,7 +331,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "facd63521844a693cb1ed030d583b829162b16c8",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -220,6 +220,8 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n \n # Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoX->GPTNeoXJapanese\n class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "c614c1bcbaa34e445f842d91a5dbe49f2df5be3b",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -318,6 +318,8 @@ class GranitePreTrainedModel(PreTrainedModel):\n \n \n class GraniteRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GraniteConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "13ab0dae0c64328c9cc89b77af2f0b538e51386c",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -150,6 +150,8 @@ def extra_repr(self):\n \n # Copied from transformers.models.granite.modeling_granite.GraniteRotaryEmbedding with Granite->GraniteMoe\n class GraniteMoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GraniteMoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "109c60eab917238301e5a96ea11179a225b78ca2",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -1255,6 +1255,8 @@ def _init_weights(self, module):\n \n \n class GraniteMoeHybridRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GraniteMoeHybridConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "1acac400cc204d1dbb14f04262255ede659dddd2",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -545,6 +545,8 @@ def _init_weights(self, module):\n \n \n class GraniteMoeSharedRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GraniteMoeSharedConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "44d687d46d8aabf5460344c1ec91fbdfce6ec004",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -61,6 +61,8 @@ def extra_repr(self):\n \n \n class HeliumRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: HeliumConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -277,7 +279,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "37095197bcf0ee2abccec2120073f9d103d5c713",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -388,6 +388,8 @@ def extra_repr(self):\n \n # Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with Gemma->JetMoe\n class JetMoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: JetMoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "304f7261de47c122cdc557fcb874e74bb491eda9",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -267,6 +267,8 @@ def forward(self, x, layer_idx=None):\n \n \n class KyutaiSpeechToTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: KyutaiSpeechToTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "5e8e6eaeb6e91c19a5c7708a2c1647700e221225",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -66,6 +66,8 @@ def extra_repr(self):\n \n \n class Lfm2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Lfm2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -365,7 +367,7 @@ def forward(\n         past_key_value: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "d91b82926a6177884d63694da158b597f537eb1b",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -248,7 +248,7 @@ def forward(\n         past_key_value: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "744c397c60d1a1fd3702637b072e7c8fcf4948a4",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -207,7 +207,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "0b487256aa5997ac9b5f792e7add970669e3c753",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -370,7 +370,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "71753d170815666fd29d45883aacbd3427d26469",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -70,6 +70,8 @@ def extra_repr(self):\n \n \n class LlamaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -282,7 +284,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "62a942ef3faf6665757036cce3fce9774e3a6156",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -165,6 +165,8 @@ def forward(self, hidden_states):\n \n \n class Llama4TextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Llama4TextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b5e52b13f0ce2f510ff06564d7f206f262a1b00b",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -503,6 +503,8 @@ def forward(self, x: torch.Tensor):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Mimi\n class MimiRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MimiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "8a4ae7f356e8ac14dce9b7f79447fb0be4bfabab",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -360,7 +360,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -591,6 +591,8 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n \n \n class MiniMaxRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MiniMaxConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "212151206ae36be480edc6410347a7a9e929eca9",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -144,7 +144,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -221,7 +221,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -265,6 +265,8 @@ class MistralPreTrainedModel(PreTrainedModel):\n \n \n class MistralRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MistralConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "ece58e6e78c6fe1facd745c062ebff96b94662e1",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -58,7 +58,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "1ee30045932bd19325101069458b2eb4819fb98e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -256,7 +256,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -340,6 +340,8 @@ def forward(\n \n \n class MixtralRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MixtralConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "5379c5313726dd2da0401b766580b569e354e277",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -49,6 +49,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class MLCDRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))"
        },
        {
            "sha": "615eb34f6d584a3e9387a614dee98e3383605c18",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -718,6 +718,8 @@ def forward(\n \n \n class MllamaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MllamaTextConfig, device=None):\n         super().__init__()\n         self.rope_type = config.rope_scaling[\"rope_type\"]"
        },
        {
            "sha": "48e1e823950bf69192a3a34ae7d63e2cfba29d21",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -243,6 +243,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class ModernBertRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: ModernBertConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "aa7dc909a6d18f49fe964bd153ebdf424e309bc5",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -294,6 +294,8 @@ def forward(\n \n \n class MoonshineRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MoonshineConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -354,7 +356,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "2e1581fbdbacf90624075904abddc91f22017c53",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -273,6 +273,8 @@ def forward(self, x, layer_idx=None):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Moshi\n class MoshiRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: MoshiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "71a2e2c8766463e6f910f8e580b188d3f57049f7",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -89,6 +89,8 @@ def forward(self, input: Tensor) -> Tensor:\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n class NemotronRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     # Ignore copy\n     def __init__(\n         self,"
        },
        {
            "sha": "80a1c47b98bf2402b02a761cdcad22eed37797eb",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -161,7 +161,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -226,7 +226,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -251,6 +251,8 @@ def forward(\n \n \n class OlmoRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: OlmoConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "56690a2c1ad74690c1083044028ee6fe07092383",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -83,7 +83,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "5fa1aaeeacf818a64708160cf20de771c7465529",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -156,7 +156,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -232,7 +232,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n@@ -256,6 +256,8 @@ def forward(\n \n \n class Olmo2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Olmo2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "56635628aa239373791ecc04043221ea625e6dfb",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -202,7 +202,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -263,7 +263,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,"
        },
        {
            "sha": "7c9cdc983faccca8b2b5e5e03816a0473ff73d75",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -148,6 +148,8 @@ def extra_repr(self):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmoe\n class OlmoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: OlmoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "3143c50bdb891fe52e45128ff87b534943b69100",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -57,6 +57,8 @@\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Persimmon\n class PersimmonRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: PersimmonConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "265520eecf9a81fb946bb24639e8eea96976a68d",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -136,7 +136,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -254,6 +254,8 @@ def forward(\n \n \n class PhiRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: PhiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "c028ee32a0d89578f14c979b858a3465935f1d46",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -58,7 +58,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "743b9eae5897a01115eec33ad715e08621d66e92",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -297,6 +297,8 @@ class Phi3PreTrainedModel(PreTrainedModel):\n \n \n class Phi3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Phi3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "e52fd88e3c664d11af8107e187a4679cca97dcc5",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -1538,6 +1538,8 @@ def forward(\n \n \n class Phi4MultimodalRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Phi4MultimodalConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b46a399e095b12f66b0e54615247cd3b68b6863d",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -58,6 +58,8 @@ class PixtralRotaryEmbedding(nn.Module):\n     a corresponding positional embedding, based on its index in the grid.\n     \"\"\"\n \n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config, device=None):\n         super().__init__()\n         self.rope_type = \"default\""
        },
        {
            "sha": "6f47d3ae9a764a074143b3eb8c97cafee1df1573",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -144,7 +144,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -224,7 +224,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -268,6 +268,8 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n \n \n class Qwen2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "0421e5bace917fc0ef00eb83752315622ac9b802",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -58,7 +58,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "b12aec70b245d38d1427c931146f1e0f566b88ef",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -1057,6 +1057,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n@@ -1244,6 +1246,8 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n \n \n class Qwen2_5OmniRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen2_5OmniThinkerConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -2499,6 +2503,8 @@ def _update_model_kwargs_for_generation(\n \n # Using custom RoPE, will use LlamaRotaryEmbedding next version\n class Qwen2_5OmniDiTRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim, base=10000):\n         super().__init__()\n "
        },
        {
            "sha": "bdb983df8487f536387fbfa35ac8a3fdbccb052a",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -2796,6 +2796,8 @@ def _update_model_kwargs_for_generation(\n \n # Using custom RoPE, will use LlamaRotaryEmbedding next version\n class Qwen2_5OmniDiTRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim, base=10000):\n         super().__init__()\n "
        },
        {
            "sha": "25e6be0213dbb771a515910900a2ace848f33197",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -89,6 +89,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n@@ -517,6 +519,8 @@ class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n \n \n class Qwen2_5_VLRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen2_5_VLTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "d51a4951e109c7389c3404d4d77d9ff27273e71a",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -168,6 +168,8 @@ def extra_repr(self):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2Moe\n class Qwen2MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen2MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "472a8b525a1d856e4a8dbc9288b3301557d8fc87",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -107,6 +107,8 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n \n \n class Qwen2VLRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen2VLTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -210,6 +212,8 @@ def apply_rotary_pos_emb_vision(\n \n \n class VisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim: int, theta: float = 10000.0) -> None:\n         super().__init__()\n         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))"
        },
        {
            "sha": "ce9e0fbbeac5e8a665b3d855727195dbb8ce36a5",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -191,7 +191,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -250,7 +250,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -294,6 +294,8 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n \n \n class Qwen3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "1a729ab0b44b2247fd66f9d67a3e933f2277ee26",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -71,7 +71,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "e9cffdb523992ba5ea881d23521449497a7b3b7a",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -155,7 +155,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -363,6 +363,8 @@ def forward(\n \n \n class Qwen3MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Qwen3MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "fec14c49331493c57805ef5bd3e5586c416eba1a",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -59,6 +59,8 @@ def extra_repr(self):\n \n \n class RecurrentGemmaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, dim, base=10000, device=None):\n         super().__init__()\n         self.dim = dim"
        },
        {
            "sha": "3a891cff6c84f83da5fceee0bc2050be2cbfd141",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -158,7 +158,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -254,7 +254,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -298,6 +298,8 @@ class SmolLM3PreTrainedModel(PreTrainedModel):\n \n \n class SmolLM3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: SmolLM3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b19083f650acc306c0cf429271aa7deda7bc3e92",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -281,7 +281,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "978d9e3ecd2a9c5c7934dc360e9ac33903fd72ce",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -61,6 +61,8 @@\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->StableLm\n class StableLmRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: StableLmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "281bacf4d5c13104a982c87d7756e53d56d4e798",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -224,7 +224,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n@@ -249,6 +249,8 @@ def forward(\n \n \n class Starcoder2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Starcoder2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "0a1b62396e1339e1fb0c79c8756aef9716522f16",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -90,6 +90,8 @@ def forward(self, x):\n \n \n class T5GemmaRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "1ce66bf9628af8eb6ca0e8eb8dd644e67111c25c",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b347e93567a1770fe1ae588ac2fd51898028e6ed/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=b347e93567a1770fe1ae588ac2fd51898028e6ed",
            "patch": "@@ -216,6 +216,8 @@ def reset(self):\n \n \n class Zamba2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: Zamba2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\""
        }
    ],
    "stats": {
        "total": 288,
        "additions": 229,
        "deletions": 59
    }
}