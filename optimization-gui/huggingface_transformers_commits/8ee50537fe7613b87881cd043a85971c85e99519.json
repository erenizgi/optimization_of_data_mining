{
    "author": "ArdalanM",
    "message": "Qwen2VL fix cos,sin dtypes to float when used with deepspeed (#36188)\n\n* fix dtype of cos,sin when used with deepspeed\n\n* move sin,cos casting withing flash attention functions\n\n* fix cos,sin float casting in modular\n\n---------\n\nCo-authored-by: ardalan.mehrani <ardalan.mehrani@ardalanmehranis-MacBook-Pro.local>\nCo-authored-by: ardalan.mehrani <ardalan.mehrani@bytedance.com>",
    "sha": "8ee50537fe7613b87881cd043a85971c85e99519",
    "files": [
        {
            "sha": "c21264a3980427a5926ee3ef8b9494bd02ebef08",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=8ee50537fe7613b87881cd043a85971c85e99519",
            "patch": "@@ -165,8 +165,8 @@ def apply_rotary_pos_emb_flashatt(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     cos = cos.chunk(2, dim=-1)[0].contiguous()\n     sin = sin.chunk(2, dim=-1)[0].contiguous()\n-    q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)\n-    k_embed = apply_rotary_emb(k.float(), cos, sin).type_as(k)\n+    q_embed = apply_rotary_emb(q.float(), cos.float(), sin.float()).type_as(q)\n+    k_embed = apply_rotary_emb(k.float(), cos.float(), sin.float()).type_as(k)\n     return q_embed, k_embed\n \n \n@@ -194,8 +194,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n@@ -223,7 +223,7 @@ def apply_rotary_pos_emb_vision(\n     orig_q_dtype = q.dtype\n     orig_k_dtype = k.dtype\n     q, k = q.float(), k.float()\n-    cos, sin = cos.unsqueeze(-2), sin.unsqueeze(-2)\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     q_embed = q_embed.to(orig_q_dtype)\n@@ -256,8 +256,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n@@ -305,8 +305,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)"
        },
        {
            "sha": "7dd8a91a2028211757d4170123b43212900aa16f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=8ee50537fe7613b87881cd043a85971c85e99519",
            "patch": "@@ -70,8 +70,8 @@ def apply_rotary_pos_emb_flashatt(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     cos = cos.chunk(2, dim=-1)[0].contiguous()\n     sin = sin.chunk(2, dim=-1)[0].contiguous()\n-    q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)\n-    k_embed = apply_rotary_emb(k.float(), cos, sin).type_as(k)\n+    q_embed = apply_rotary_emb(q.float(), cos.float(), sin.float()).type_as(q)\n+    k_embed = apply_rotary_emb(k.float(), cos.float(), sin.float()).type_as(k)\n     return q_embed, k_embed\n \n \n@@ -170,8 +170,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)"
        },
        {
            "sha": "7a22d75b6ae6fb8527e50ffb86fbd0bcf3f1dab7",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ee50537fe7613b87881cd043a85971c85e99519/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=8ee50537fe7613b87881cd043a85971c85e99519",
            "patch": "@@ -220,7 +220,7 @@ def apply_rotary_pos_emb_vision(\n     orig_q_dtype = q.dtype\n     orig_k_dtype = k.dtype\n     q, k = q.float(), k.float()\n-    cos, sin = cos.unsqueeze(-2), sin.unsqueeze(-2)\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     q_embed = q_embed.to(orig_q_dtype)\n@@ -318,8 +318,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n@@ -367,8 +367,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n@@ -405,8 +405,8 @@ def forward(\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos().float()\n-            sin = emb.sin().float()\n+            cos = emb.cos()\n+            sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n         q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 20,
        "deletions": 20
    }
}