{
    "author": "gante",
    "message": "[chat] use the checkpoint's `generation_config.json` as base parameterization (#38330)\n\n* use model gen config\n\n* unwanted diff",
    "sha": "80902ae9b108fd40fa6fcfbd0a188c0858b2b5b4",
    "files": [
        {
            "sha": "7ade958149a41b1047c880621f38f12ff639ea0d",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 28,
            "deletions": 13,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/80902ae9b108fd40fa6fcfbd0a188c0858b2b5b4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80902ae9b108fd40fa6fcfbd0a188c0858b2b5b4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=80902ae9b108fd40fa6fcfbd0a188c0858b2b5b4",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n \n+import copy\n import json\n import os\n import platform\n@@ -28,7 +29,13 @@\n import yaml\n from huggingface_hub.utils import disable_progress_bars\n \n-from transformers import AutoTokenizer, GenerationConfig, TextIteratorStreamer, logging\n+from transformers import (\n+    AutoTokenizer,\n+    GenerationConfig,\n+    PreTrainedTokenizer,\n+    TextIteratorStreamer,\n+    logging,\n+)\n from transformers.utils import is_rich_available, is_torch_available\n \n from . import BaseTransformersCLICommand\n@@ -45,7 +52,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n+    from transformers import AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedModel\n \n \n ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n@@ -248,7 +255,9 @@ class ChatArguments:\n     repetition_penalty: float = field(default=1.0, metadata={\"help\": \"Repetition penalty.\"})\n     eos_tokens: Optional[str] = field(\n         default=None,\n-        metadata={\"help\": \"EOS tokens to stop the generation. If multiple they should be comma separated.\"},\n+        metadata={\n+            \"help\": \"EOS tokens (text format) to stop the generation. If multiple they should be comma separated.\"\n+        },\n     )\n     eos_token_ids: Optional[str] = field(\n         default=None,\n@@ -469,16 +478,19 @@ def is_number(s: str) -> bool:\n         return processed_generate_flags\n \n     def get_generation_parameterization(\n-        self, args: ChatArguments, tokenizer: AutoTokenizer\n+        self, args: ChatArguments, tokenizer: AutoTokenizer, model: PreTrainedModel\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         Returns a GenerationConfig object holding the generation parameters for the CLI command.\n         \"\"\"\n-        # No generation config arg provided -> use base generation config, apply CLI defaults\n+        # No generation config arg provided -> use default generation config, apply CLI defaults\n         if args.generation_config is None:\n-            generation_config = GenerationConfig()\n+            # We start off from the checkpoint's generation config\n+            generation_config = copy.deepcopy(model.generation_config)\n             # Apply deprecated CLI args on top of the default generation config\n-            pad_token_id, eos_token_ids = self.parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n+            pad_token_id, eos_token_ids = self.parse_eos_tokens(\n+                tokenizer, generation_config, args.eos_tokens, args.eos_token_ids\n+            )\n             deprecated_kwargs = {\n                 \"max_new_tokens\": args.max_new_tokens,\n                 \"do_sample\": args.do_sample,\n@@ -509,13 +521,16 @@ def get_generation_parameterization(\n \n     @staticmethod\n     def parse_eos_tokens(\n-        tokenizer: AutoTokenizer, eos_tokens: Optional[str], eos_token_ids: Optional[str]\n+        tokenizer: PreTrainedTokenizer,\n+        generation_config: GenerationConfig,\n+        eos_tokens: Optional[str],\n+        eos_token_ids: Optional[str],\n     ) -> tuple[int, list[int]]:\n         \"\"\"Retrieves the pad token ID and all possible EOS token IDs.\"\"\"\n-        if tokenizer.pad_token_id is None:\n-            pad_token_id = tokenizer.eos_token_id\n+        if generation_config.pad_token_id is None:\n+            pad_token_id = generation_config.eos_token_id\n         else:\n-            pad_token_id = tokenizer.pad_token_id\n+            pad_token_id = generation_config.pad_token_id\n \n         all_eos_token_ids = []\n \n@@ -526,7 +541,7 @@ def parse_eos_tokens(\n             all_eos_token_ids.extend([int(token_id) for token_id in eos_token_ids.split(\",\")])\n \n         if len(all_eos_token_ids) == 0:\n-            all_eos_token_ids.append(tokenizer.eos_token_id)\n+            all_eos_token_ids.append(generation_config.eos_token_id)\n \n         return pad_token_id, all_eos_token_ids\n \n@@ -683,7 +698,7 @@ def run(self):\n \n         model, tokenizer = self.load_model_and_tokenizer(args)\n         generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n-        generation_config, model_kwargs = self.get_generation_parameterization(args, tokenizer)\n+        generation_config, model_kwargs = self.get_generation_parameterization(args, tokenizer, model)\n \n         # if not verbose -> disable warnings, progress bars, etc in the chat interface\n         if not args.verbose:"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 28,
        "deletions": 13
    }
}