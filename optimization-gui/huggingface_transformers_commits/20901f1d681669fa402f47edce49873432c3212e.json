{
    "author": "ArkVex",
    "message": "[typing] LlamaAttention return typehint  (#38998)\n\n* helo llama\n\n* helo llama\n\n* helo llama\n\n* apply modular\n\n* fix dia\n\n---------\n\nCo-authored-by: qubvel <qubvel@gmail.com>",
    "sha": "20901f1d681669fa402f47edce49873432c3212e",
    "files": [
        {
            "sha": "da233918123c46c0cfb4132f31e7465c429a9352",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -225,7 +225,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "591fcd935d429f5517e6f0355c9534eae7c6c822",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -532,7 +532,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "698a74449ba575661ba4f9f0c67b922d12561cdd",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -296,7 +296,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "3b96c272c0dbb9e607e7495b0cec72571dfe5a06",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -311,7 +311,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "b87712852fd6310b1559cd0566c5f54974ae7934",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -281,7 +281,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "e68f8d4a49d662f4df7978d544627fc0b6aaf31e",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -189,7 +189,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "399c809d126b4ddbc3070f02357b95b80958b951",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -222,7 +222,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "17deed6bc708f69f6e973c3ddb328515d26a7733",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -184,7 +184,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "ddb15923886cb24ff5b1b51939102d4561fedf1f",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -242,7 +242,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "d5e7e4fe517ea5830173fa0fd975cc7da61f5574",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -148,7 +148,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "fb8fc6f3be2b7778515bf62c904466bb276b282e",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -224,7 +224,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "74c9651aca01898a238d8d87109455580c2f5693",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20901f1d681669fa402f47edce49873432c3212e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=20901f1d681669fa402f47edce49873432c3212e",
            "patch": "@@ -224,7 +224,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        }
    ],
    "stats": {
        "total": 24,
        "additions": 12,
        "deletions": 12
    }
}