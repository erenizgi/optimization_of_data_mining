{
    "author": "JustinVanHeek",
    "message": "Fix eval thread fork bomb (#39717)",
    "sha": "6e4a9a5b431f09f4a853ad1808b89315e09b972c",
    "files": [
        {
            "sha": "bb944097243bb251003d9f76d22e9ccb9a55d058",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e4a9a5b431f09f4a853ad1808b89315e09b972c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e4a9a5b431f09f4a853ad1808b89315e09b972c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6e4a9a5b431f09f4a853ad1808b89315e09b972c",
            "patch": "@@ -1034,17 +1034,16 @@ def _get_dataloader(\n                     seed_worker, num_workers=self.args.dataloader_num_workers, rank=self.args.process_index\n                 )\n \n-        dataloader = DataLoader(dataset, **dataloader_params)\n+        dataloader = self.accelerator.prepare(DataLoader(dataset, **dataloader_params))\n \n-        # Accelerator.free_memory() will destroy the references, so\n-        # we need to store the non-prepared version for eval dataloaders.\n+        # Store the prepared dataloader for subsequent evaluations if using persistent workers.\n         if dataloader_key is not None and self.args.dataloader_persistent_workers:\n             if hasattr(self, \"_eval_dataloaders\"):\n                 self._eval_dataloaders[dataloader_key] = dataloader\n             else:\n                 self._eval_dataloaders = {dataloader_key: dataloader}\n \n-        return self.accelerator.prepare(dataloader)\n+        return dataloader\n \n     def get_train_dataloader(self) -> DataLoader:\n         \"\"\"\n@@ -1132,7 +1131,7 @@ def get_eval_dataloader(self, eval_dataset: Optional[Union[str, Dataset]] = None\n             and dataloader_key in self._eval_dataloaders\n             and self.args.dataloader_persistent_workers\n         ):\n-            return self.accelerator.prepare(self._eval_dataloaders[dataloader_key])\n+            return self._eval_dataloaders[dataloader_key]\n \n         eval_dataset = (\n             self.eval_dataset[eval_dataset]"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 4,
        "deletions": 5
    }
}