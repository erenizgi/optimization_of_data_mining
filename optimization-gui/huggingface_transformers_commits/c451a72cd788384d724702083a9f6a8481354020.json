{
    "author": "A-Duss",
    "message": "Add French translation of task_summary and tasks_explained (#33407)\n\n* Add French translation of task_summary and tasks_explained\r\n\r\n---------\r\n\r\nCo-authored-by: Aymeric Roucher <69208727+aymeric-roucher@users.noreply.github.com>",
    "sha": "c451a72cd788384d724702083a9f6a8481354020",
    "files": [
        {
            "sha": "a16283c719671a3bbd51e8a11a0239b813287d96",
            "filename": "docs/source/fr/_toctree.yml",
            "status": "modified",
            "additions": 32,
            "deletions": 26,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2F_toctree.yml?ref=c451a72cd788384d724702083a9f6a8481354020",
            "patch": "@@ -1,30 +1,36 @@\n - sections:\n-    - local: index\n-      title: ü§ó Transformers\n-    - local: quicktour\n-      title: Visite rapide\n-    - local: installation\n-      title: Installation\n+  - local: index\n+    title: ü§ó Transformers\n+  - local: quicktour\n+    title: Visite rapide\n+  - local: installation\n+    title: Installation\n   title: D√©marrer\n - sections:\n-    - local: tutoriel_pipeline\n-      title: Pipelines pour l'inf√©rence\n-    - local: autoclass_tutorial\n-      title: Chargement d'instances pr√©-entra√Æn√©es avec une AutoClass\n-    - local: in_translation\n-      title: Pr√©paration des donn√©es\n-    - local: in_translation\n-      title: Fine-tune un mod√®le pr√©-entra√Æn√©\n-    - local: run_scripts_fr\n-      title: Entra√Ænement avec un script\n-    - local: in_translation\n-      title: Entra√Ænement distribu√© avec ü§ó Accelerate\n-    - local: in_translation\n-      title: Chargement et entra√Ænement des adaptateurs avec ü§ó PEFT\n-    - local: in_translation\n-      title: Partager un mod√®le\n-    - local: in_translation\n-      title: Agents\n-    - local: in_translation\n-      title: G√©n√©ration avec LLMs\n+  - local: tutoriel_pipeline\n+    title: Pipelines pour l'inf√©rence\n+  - local: autoclass_tutorial\n+    title: Chargement d'instances pr√©-entra√Æn√©es avec une AutoClass\n+  - local: in_translation\n+    title: Pr√©paration des donn√©es\n+  - local: in_translation\n+    title: Fine-tune un mod√®le pr√©-entra√Æn√©\n+  - local: run_scripts_fr\n+    title: Entra√Ænement avec un script\n+  - local: in_translation\n+    title: Entra√Ænement distribu√© avec ü§ó Accelerate\n+  - local: in_translation\n+    title: Chargement et entra√Ænement des adaptateurs avec ü§ó PEFT\n+  - local: in_translation\n+    title: Partager un mod√®le\n+  - local: in_translation\n+    title: Agents\n+  - local: in_translation\n+    title: G√©n√©ration avec LLMs\n   title: Tutoriels\n+- sections:\n+  - local: task_summary\n+    title: Ce que ü§ó Transformers peut faire\n+  - local: tasks_explained\n+    title: Comment ü§ó Transformers r√©sout ces t√¢ches\n+  title: Guides conceptuels\n\\ No newline at end of file"
        },
        {
            "sha": "f730264f95e86fe8bb44cbb338f72e48af607125",
            "filename": "docs/source/fr/task_summary.md",
            "status": "added",
            "additions": 341,
            "deletions": 0,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2Ftask_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2Ftask_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Ftask_summary.md?ref=c451a72cd788384d724702083a9f6a8481354020",
            "patch": "@@ -0,0 +1,341 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Ce que ü§ó Transformers peut faire\n+\n+ü§ó Transformers est une biblioth√®que de mod√®les pr√©entra√Æn√©s √† la pointe de la technologie pour les t√¢ches de traitement du langage naturel (NLP), de vision par ordinateur et de traitement audio et de la parole. Non seulement la biblioth√®que contient des mod√®les Transformer, mais elle inclut √©galement des mod√®les non-Transformer comme des r√©seaux convolutionnels modernes pour les t√¢ches de vision par ordinateur. Si vous regardez certains des produits grand public les plus populaires aujourd'hui, comme les smartphones, les applications et les t√©l√©viseurs, il est probable qu'une technologie d'apprentissage profond soit derri√®re. Vous souhaitez supprimer un objet de fond d'une photo prise avec votre smartphone ? C'est un exemple de t√¢che de segmentation panoptique (ne vous inqui√©tez pas si vous ne savez pas encore ce que cela signifie, nous le d√©crirons dans les sections suivantes !).\n+\n+Cette page fournit un aper√ßu des diff√©rentes t√¢ches de traitement de la parole et de l'audio, de vision par ordinateur et de NLP qui peuvent √™tre r√©solues avec la biblioth√®que ü§ó Transformers en seulement trois lignes de code !\n+\n+## Audio\n+\n+Les t√¢ches de traitement audio et de la parole sont l√©g√®rement diff√©rentes des autres modalit√©s principalement parce que l'audio en tant que donn√©e d'entr√©e est un signal continu. Contrairement au texte, un signal audio brut ne peut pas discr√©tis√© de la mani√®re dont une phrase peut √™tre divis√©e en mots. Pour contourner cela, le signal audio brut est g√©n√©ralement √©chantillonn√© √† intervalles r√©guliers. Si vous prenez plus d'√©chantillons dans un intervalle, le taux d'√©chantillonnage est plus √©lev√© et l'audio ressemble davantage √† la source audio originale.\n+\n+Les approches pr√©c√©dentes pr√©traitaient l'audio pour en extraire des caract√©ristiques utiles. Il est maintenant plus courant de commencer les t√¢ches de traitement audio et de la parole en donnant directement le signal audio brut √† un encodeur de caract√©ristiques (*feature encoder* en anglais) pour extraire une repr√©sentation de l'audio. Cela correspond √† l'√©tape de pr√©traitement et permet au mod√®le d'apprendre les caract√©ristiques les plus essentielles du signal.\n+\n+### Classification audio\n+\n+La classification audio est une t√¢che qui consiste √† attribuer une classe, parmi un ensemble de classes pr√©d√©fini, √† un audio. La classification audio englobe de nombreuses applications sp√©cifiques, dont certaines incluent :\n+\n+* la classification d'environnements sonores : attribuer une classe (cat√©gorie) √† l'audio pour indiquer l'environnement associ√©, tel que \"bureau\", \"plage\" ou \"stade\". \n+* la d√©tection d'√©v√©nements sonores : √©tiqueter l'audio avec une √©tiquette d'√©v√©nement sonore (\"klaxon de voiture\", \"appel de baleine\", \"verre bris√©\")\n+* l'identification d'√©l√©ments sonores : attribuer des tags (*√©tiquettes* en fran√ßais) √† l'audio pour marquer des sons sp√©cifiques, comme \"chant des oiseaux\" ou \"identification du locuteur lors d'une r√©union\".\n+* la classification musicale : attribuer un genre √† la musique, comme \"metal\", \"hip-hop\" ou \"country\".\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n+>>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n+>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n+>>> preds\n+[{'score': 0.4532, 'label': 'hap'},\n+ {'score': 0.3622, 'label': 'sad'},\n+ {'score': 0.0943, 'label': 'neu'},\n+ {'score': 0.0903, 'label': 'ang'}]\n+```\n+\n+### Reconnaissance vocale\n+\n+La reconnaissance vocale (*Automatic Speech Recognition* ou ASR en anglais) transcrit la parole en texte. C'est l'une des t√¢ches audio les plus courantes en partie parce que la parole est une forme de communication la plus naturelle pour nous, humains. Aujourd'hui, les syst√®mes ASR sont int√©gr√©s dans des produits technologiques \"intelligents\" comme les enceintes, les t√©l√©phones et les voitures. Il est d√©sormais possible de demander √† nos assistants virtuels de jouer de la musique, de d√©finir des rappels et de nous indiquer la m√©t√©o.\n+\n+Mais l'un des principaux d√©fis auxquels les architectures Transformer contribuent √† r√©soudre est celui des langues √† faibles ressources, c'est-√†-dire des langues pour lesquelles il existe peu de donn√©es √©tiquet√©es. En pr√©entra√Ænant sur de grandes quantit√©s de donn√©es vocales d'un autre language plus ou moins similaire, le r√©glage fin (*fine-tuning* en anglais) du mod√®le avec seulement une heure de donn√©es vocales √©tiquet√©es dans une langue √† faibles ressources peut tout de m√™me produire des r√©sultats de haute qualit√© compar√©s aux syst√®mes ASR pr√©c√©dents entra√Æn√©s sur 100 fois plus de donn√©es √©tiquet√©es.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n+>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n+{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n+```\n+\n+## Vision par ordinateur\n+\n+L'une des premi√®res r√©ussites en vision par ordinateur a √©t√© la reconnaissance des num√©ros de code postal √† l'aide d'un [r√©seau de neurones convolutionnel (CNN)](glossary#convolution). Une image est compos√©e de pixels, chacun ayant une valeur num√©rique, ce qui permet de repr√©senter facilement une image sous forme de matrice de valeurs de pixels. Chaque combinaison de valeurs de pixels correspond aux couleurs d'une image.\n+\n+Il existe deux approches principales pour r√©soudre les t√¢ches de vision par ordinateur :\n+\n+1. Utiliser des convolutions pour apprendre les caract√©ristiques hi√©rarchiques d'une image, des d√©tails de bas niveau aux √©l√©ments abstraits de plus haut niveau.\n+2. Diviser l'image en morceaux (*patches* en anglais) et utiliser un Transformer pour apprendre progressivement comment chaque morceau est li√© aux autres pour former l'image compl√®te. Contrairement √† l'approche ascendante des CNNs, cette m√©thode ressemble √† un processus o√π l'on d√©marre avec une image floue pour ensuite la mettre au point petit √† petit. \n+\n+### Classification d'images\n+\n+La classification d'images consiste √† attribuer une classe, parmi un ensemble de classes pr√©d√©fini, √† toute une image. Comme pour la plupart des t√¢ches de classification, les cas d'utilisation pratiques sont nombreux, notamment :\n+\n+- Sant√© : classification d'images m√©dicales pour d√©tecter des maladies ou surveiller l'√©tat de sant√© des patients.\n+- Environnement : classification d'images satellites pour suivre la d√©forestation, aider √† la gestion des terres ou d√©tecter les incendies de for√™t.\n+- Agriculture : classification d'images de cultures pour surveiller la sant√© des plantes ou des images satellites pour analyser l'utilisation des terres.\n+- √âcologie : classification d'images d'esp√®ces animales ou v√©g√©tales pour suivre les populations fauniques ou les esp√®ces menac√©es.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(task=\"image-classification\")\n+>>> preds = classifier(\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+... )\n+>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n+>>> print(*preds, sep=\"\\n\")\n+{'score': 0.4335, 'label': 'lynx, catamount'}\n+{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n+{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n+{'score': 0.0239, 'label': 'Egyptian cat'}\n+{'score': 0.0229, 'label': 'tiger cat'}\n+```\n+\n+### D√©tection d'objets\n+\n+La d√©tection d'objets, √† la diff√©rence de la classification d'images, identifie plusieurs objets dans une image ainsi que leurs positions, g√©n√©ralement d√©finies par des bo√Ætes englobantes (*bounding boxes* en anglais). Voici quelques exemples d'applications :\n+\n+- V√©hicules autonomes : d√©tection des objets de la circulation, tels que les v√©hicules, pi√©tons et feux de signalisation.\n+- T√©l√©d√©tection : surveillance des catastrophes, planification urbaine et pr√©visions m√©t√©orologiques.\n+- D√©tection de d√©fauts : identification des fissures ou dommages structurels dans les b√¢timents, ainsi que des d√©fauts de fabrication.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> detector = pipeline(task=\"object-detection\")\n+>>> preds = detector(\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+... )\n+>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n+>>> preds\n+[{'score': 0.9865,\n+  'label': 'cat',\n+  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n+```\n+\n+### Segmentation d'images\n+\n+La segmentation d'images est une t√¢che qui consiste √† attribuer une classe √† chaque pixel d'une image, ce qui la rend plus pr√©cise que la d√©tection d'objets, qui se limite aux bo√Ætes englobantes (*bounding boxes* en anglais). Elle permet ainsi de d√©tecter les objets √† la pr√©cision du pixel. Il existe plusieurs types de segmentation d'images :\n+\n+- Segmentation d'instances : en plus de classifier un objet, elle identifie chaque instance distincte d'un m√™me objet (par exemple, \"chien-1\", \"chien-2\").\n+- Segmentation panoptique : combine segmentation s√©mantique et segmentation d'instances, attribuant √† chaque pixel une classe s√©mantique **et** une instance sp√©cifique.\n+\n+Ces techniques sont utiles pour les v√©hicules autonomes, qui doivent cartographier leur environnement pixel par pixel pour naviguer en toute s√©curit√© autour des pi√©tons et des v√©hicules. Elles sont √©galement pr√©cieuses en imagerie m√©dicale, o√π la pr√©cision au niveau des pixels permet de d√©tecter des anomalies cellulaires ou des caract√©ristiques d'organes. Dans le commerce en ligne, la segmentation est utilis√©e pour des essayages virtuels de v√™tements ou des exp√©riences de r√©alit√© augment√©e, en superposant des objets virtuels sur des images du monde r√©el via la cam√©ra.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> segmenter = pipeline(task=\"image-segmentation\")\n+>>> preds = segmenter(\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+... )\n+>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n+>>> print(*preds, sep=\"\\n\")\n+{'score': 0.9879, 'label': 'LABEL_184'}\n+{'score': 0.9973, 'label': 'snow'}\n+{'score': 0.9972, 'label': 'cat'}\n+```\n+\n+### Estimation de la profondeur\n+\n+L'estimation de la profondeur consiste √† pr√©dire la distance de chaque pixel d'une image par rapport √† la cam√©ra. Cette t√¢che est cruciale pour comprendre et reconstruire des sc√®nes r√©elles. Par exemple, pour les voitures autonomes, il est essentiel de d√©terminer la distance des objets tels que les pi√©tons, les panneaux de signalisation et les autres v√©hicules pour √©viter les collisions. L'estimation de la profondeur permet √©galement de cr√©er des mod√®les 3D √† partir d'images 2D, ce qui est utile pour g√©n√©rer des repr√©sentations d√©taill√©es de structures biologiques ou de b√¢timents.\n+\n+Il existe deux principales approches pour estimer la profondeur :\n+\n+- St√©r√©o : la profondeur est estim√©e en comparant deux images d'une m√™me sc√®ne prises sous des angles l√©g√®rement diff√©rents.\n+- Monoculaire : la profondeur est estim√©e √† partir d'une seule image.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> depth_estimator = pipeline(task=\"depth-estimation\")\n+>>> preds = depth_estimator(\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+... )\n+```\n+\n+## Traitement du langage naturel\n+\n+Les t√¢ches de traitement du langage naturel (*Natural Language Processing* ou *NLP* en anglais) sont courantes car le texte est une forme naturelle de communication pour nous. Pour qu'un mod√®le puisse traiter le texte, celui-ci doit √™tre *tokenis√©*, c'est-√†-dire divis√© en mots ou sous-mots appel√©s \"*tokens*\", puis converti en nombres. Ainsi, une s√©quence de texte peut √™tre repr√©sent√©e comme une s√©quence de nombres, qui peut ensuite √™tre utilis√©e comme donn√©es d'entr√©e pour un mod√®le afin de r√©soudre diverses t√¢ches  de traitement du langage naturel.\n+\n+### Classification de texte\n+\n+La classification de texte attribue une classe √† une s√©quence de texte (au niveau d'une phrase, d'un paragraphe ou d'un document) √† partir d'un ensemble de classes pr√©d√©fini. Voici quelques applications pratiques :\n+\n+- **Analyse des sentiments** : √©tiqueter le texte avec une polarit√© telle que `positive` ou `n√©gative`, ce qui aide √† la prise de d√©cision dans des domaines comme la politique, la finance et le marketing.\n+- **Classification de contenu** : organiser et filtrer les informations en attribuant des *tags* sur des sujets sp√©cifiques, comme `m√©t√©o`, `sports` ou `finance`, dans les flux d'actualit√©s et les r√©seaux sociaux.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(task=\"sentiment-analysis\")\n+>>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n+>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n+>>> preds\n+[{'score': 0.9991, 'label': 'POSITIVE'}]\n+```\n+\n+### Classification des tokens\n+\n+Dans les t√¢ches de traitement du language naturel, le texte est d'abord pr√©trait√© en le s√©parant en mots ou sous-mots individuels, appel√©s *[tokens](glossary#token)*. La classification des tokens attribue une classe √† chaque token √† partir d'un ensemble de classes pr√©d√©fini.\n+\n+Voici deux types courants de classification des tokens :\n+\n+- **Reconnaissance d'entit√©s nomm√©es (*Named Entity Recognition* ou *NER* en anglais)** : √©tiqueter un token selon une cat√©gorie d'entit√©, telle qu'organisation, personne, lieu ou date. La NER est particuli√®rement utilis√©e dans les contextes biom√©dicaux pour identifier des g√®nes, des prot√©ines et des noms de m√©dicaments.\n+- **√âtiquetage des parties du discours (*Part of Speech* ou *POS* en anglais)** : √©tiqueter un token en fonction de sa partie du discours, comme nom, verbe ou adjectif. Le POS est utile pour les syst√®mes de traduction afin de comprendre comment deux mots identiques peuvent avoir des r√¥les grammaticaux diff√©rents (par exemple, \"banque\" comme nom versus \"banque\" comme verbe).\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(task=\"ner\")\n+>>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n+>>> preds = [\n+...     {\n+...         \"entity\": pred[\"entity\"],\n+...         \"score\": round(pred[\"score\"], 4),\n+...         \"index\": pred[\"index\"],\n+...         \"word\": pred[\"word\"],\n+...         \"start\": pred[\"start\"],\n+...         \"end\": pred[\"end\"],\n+...     }\n+...     for pred in preds\n+... ]\n+>>> print(*preds, sep=\"\\n\")\n+{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n+{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n+{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n+{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n+{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n+{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n+{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n+```\n+\n+### R√©ponse √† des questions - (*Question Answering*)\n+\n+La r√©ponse √† des questions (*Question Answering* ou *QA* en anglais) est une t√¢che de traitement du language naturel qui consiste √† fournir une r√©ponse √† une question, parfois avec l'aide d'un contexte (domaine ouvert) et d'autres fois sans contexte (domaine ferm√©). Cette t√¢che intervient lorsqu'on interroge un assistant virtuel, par exemple pour savoir si un restaurant est ouvert. Elle est √©galement utilis√©e pour le support client, technique, et pour aider les moteurs de recherche √† fournir des informations pertinentes.\n+\n+Il existe deux types courants de r√©ponse √† des questions :\n+\n+- **Extractive** : pour une question donn√©e et un contexte fourni, la r√©ponse est extraite directement du texte du contexte par le mod√®le.\n+- **Abstractive** : pour une question donn√©e et un contexte, la r√©ponse est g√©n√©r√©e √† partir du contexte. Cette approche utilise le [`Text2TextGenerationPipeline`] plut√¥t que le [`QuestionAnsweringPipeline`] montr√© ci-dessous.\n+\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> question_answerer = pipeline(task=\"question-answering\")\n+>>> preds = question_answerer(\n+...     question=\"What is the name of the repository?\",\n+...     context=\"The name of the repository is huggingface/transformers\",\n+... )\n+>>> print(\n+...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n+... )\n+score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n+```\n+\n+### R√©sum√© de texte - (*Summarization*)\n+\n+Le r√©sum√© de text consiste √† cr√©er une version plus courte d'un texte tout en conservant l'essentiel du sens du document original. C'est une t√¢che de s√©quence √† s√©quence qui produit un texte plus condens√© √† partir du texte initial. Cette technique est utile pour aider les lecteurs √† saisir rapidement les points cl√©s de longs documents, comme les projets de loi, les documents juridiques et financiers, les brevets, et les articles scientifiques.\n+\n+Il existe deux types courants de summarization :\n+\n+- **Extractive** : identifier et extraire les phrases les plus importantes du texte original.\n+- **Abstractive** : g√©n√©rer un r√©sum√© qui peut inclure des mots nouveaux non pr√©sents dans le texte d'origine. Le [`SummarizationPipeline`] utilise l'approche abstractive.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> summarizer = pipeline(task=\"summarization\")\n+>>> summarizer(\n+...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n+... )\n+[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n+```\n+\n+### Traduction\n+\n+La traduction convertit un texte d'une langue √† une autre. Elle facilite la communication entre personnes de diff√©rentes langues, permet de toucher des audiences plus larges et peut aussi servir d'outil d'apprentissage pour ceux qui apprennent une nouvelle langue. Comme le r√©sum√© de texte, la traduction est une t√¢che de s√©quence √† s√©quence, o√π le mod√®le re√ßoit une s√©quence d'entr√©e (un texte est ici vu comme une s√©quence de mots, ou plus pr√©cis√©ment de tokens) et produit une s√©quence de sortie dans la langue cible.\n+\n+Initialement, les mod√®les de traduction √©taient principalement monolingues, mais il y a eu r√©cemment un int√©r√™t croissant pour les mod√®les multilingues capables de traduire entre plusieurs paires de langues.\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n+>>> translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n+>>> translator(text)\n+[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n+```\n+\n+### Mod√©lisation du langage\n+\n+La mod√©lisation du langage consiste √† pr√©dire un mot dans un texte. Cette t√¢che est devenue tr√®s populaire en traitement du language naturel, car un mod√®le de langage pr√©entra√Æn√© sur cette t√¢che peut ensuite √™tre ajust√© (*finetuned*) pour accomplir de nombreuses autres t√¢ches. R√©cemment, les grands mod√®les de langage (LLMs) ont suscit√© beaucoup d'int√©r√™t pour leur capacit√© √† apprendre avec peu ou pas de donn√©es sp√©cifiques √† une t√¢che, ce qui leur permet de r√©soudre des probl√®mes pour lesquels ils n'ont pas √©t√© explicitement entra√Æn√©s. Ces mod√®les peuvent g√©n√©rer du texte fluide et convaincant, bien qu'il soit important de v√©rifier leur pr√©cision.\n+\n+Il existe deux types de mod√©lisation du langage :\n+\n+- **Causale** : le mod√®le pr√©dit le token suivant dans une s√©quence, avec les tokens futurs masqu√©s.\n+\n+    ```py\n+    >>> from transformers import pipeline\n+\n+    >>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n+    >>> generator = pipeline(task=\"text-generation\")\n+    >>> generator(prompt)  # doctest: +SKIP\n+    ```\n+\n+- **Masqu√©e** : le mod√®le pr√©dit un token masqu√© dans une s√©quence en ayant acc√®s √† tous les autres tokens de la s√©quence (pass√© et futur).\n+\n+    ```py\n+    >>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n+    >>> fill_mask = pipeline(task=\"fill-mask\")\n+    >>> preds = fill_mask(text, top_k=1)\n+    >>> preds = [\n+    ...     {\n+    ...         \"score\": round(pred[\"score\"], 4),\n+    ...         \"token\": pred[\"token\"],\n+    ...         \"token_str\": pred[\"token_str\"],\n+    ...         \"sequence\": pred[\"sequence\"],\n+    ...     }\n+    ...     for pred in preds\n+    ... ]\n+    >>> preds\n+    [{'score': 0.2236,\n+      'token': 1761,\n+      'token_str': ' platform',\n+      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]\n+    ```\n+\n+## Multimodal\n+\n+Les t√¢ches multimodales n√©cessitent qu'un mod√®le traite plusieurs types de donn√©es (texte, image, audio, vid√©o) pour r√©soudre un probl√®me sp√©cifique. Par exemple, la g√©n√©ration de l√©gendes pour les images est une t√¢che multimodale o√π le mod√®le prend une image en entr√©e et produit une s√©quence de texte d√©crivant l'image ou ses propri√©t√©s.\n+\n+Bien que les mod√®les multimodaux traitent divers types de donn√©es, ils convertissent toutes ces donn√©es en *embeddings* (vecteurs ou listes de nombres contenant des informations significatives). Pour des t√¢ches comme la g√©n√©ration de l√©gendes pour les images, le mod√®le apprend les relations entre les *embeddings* d'images et ceux de texte.\n+\n+### R√©ponse √† des questions sur des documents - (*Document Question Answering*)\n+\n+La r√©ponse √† des questions sur des documents consiste √† r√©pondre √† des questions en langage naturel en utilisant un document comme r√©f√©rence. Contrairement √† la r√©ponse √† des questions au niveau des tokens, qui prend du texte en entr√©e, cette t√¢che prend une image d'un document ainsi qu'une question concernant ce document, et fournit une r√©ponse. Elle est utile pour analyser des donn√©es structur√©es et extraire des informations cl√©es. Par exemple, √† partir d'un re√ßu, on peut extraire des informations telles que le montant total et le change d√ª.\n+\n+```py\n+>>> from transformers import pipeline\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> url = \"https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> doc_question_answerer = pipeline(\"document-question-answering\", model=\"magorshunov/layoutlm-invoices\")\n+>>> preds = doc_question_answerer(\n+...     question=\"What is the total amount?\",\n+...     image=image,\n+... )\n+>>> preds\n+[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]\n+```\n+\n+En esp√©rant que cette page vous ait donn√© plus d'informations sur les diff√©rents types de t√¢ches dans chaque modalit√© et l'importance pratique de chacune d'elles. Dans la [section suivante](tasks_explained), vous d√©couvrirez **comment** ü§ó Transformers fonctionne pour r√©soudre ces t√¢ches."
        },
        {
            "sha": "a39096b2b86258ac4aaf03086385f70ba7d68d2a",
            "filename": "docs/source/fr/tasks_explained.md",
            "status": "added",
            "additions": 294,
            "deletions": 0,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c451a72cd788384d724702083a9f6a8481354020/docs%2Fsource%2Ffr%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Ftasks_explained.md?ref=c451a72cd788384d724702083a9f6a8481354020",
            "patch": "@@ -0,0 +1,294 @@\n+<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Comment ü§ó Transformers r√©sout ces t√¢ches\n+\n+Dans [Ce que ü§ó Transformers peut faire](task_summary), vous avez d√©couvert les t√¢ches de traitement du langage naturel (NLP), de traitement de la parole et de l'audio, de vision par ordinateur, ainsi que certaines de leurs applications importantes. Cette page se penche sur la mani√®re dont les mod√®les r√©solvent ces t√¢ches et explique les processus en arri√®re-plan. Bien que diff√©rents mod√®les puissent utiliser diverses techniques ou approches innovantes, les mod√®les Transformer suivent g√©n√©ralement une id√©e commune. Gr√¢ce √† leur architecture flexible, la plupart des mod√®les sont bas√©s sur un encodeur, un d√©codeur ou une combinaison encodeur-d√©codeur. En plus des mod√®les Transformer, notre biblioth√®que comprend √©galement des r√©seaux de neurones convolutifs (CNN), qui restent utilis√©s pour les t√¢ches de vision par ordinateur. Nous expliquerons aussi le fonctionnement d'un CNN moderne.\n+\n+Voici comment diff√©rents mod√®les r√©solvent des t√¢ches sp√©cifiques :\n+\n+- [Wav2Vec2](model_doc/wav2vec2) pour la classification audio et la reconnaissance vocale (*ASR* en anglais)\n+- [Vision Transformer (ViT)](model_doc/vit) et [ConvNeXT](model_doc/convnext) pour la classification d'images\n+- [DETR](model_doc/detr) pour la d√©tection d'objets\n+- [Mask2Former](model_doc/mask2former) pour la segmentation d'images\n+- [GLPN](model_doc/glpn) pour l'estimation de la profondeur\n+- [BERT](model_doc/bert) pour les t√¢ches de traitement du language naturel telles que la classification de texte, la classification des tokens et la r√©ponse √† des questions utilisant un encodeur\n+- [GPT2](model_doc/gpt2) pour les t√¢ches de traitement du language naturel telles que la g√©n√©ration de texte utilisant un d√©codeur\n+- [BART](model_doc/bart) pour les t√¢ches de traitement du language naturel telles que le r√©sum√© de texte et la traduction utilisant un encodeur-d√©codeur\n+\n+<Tip>\n+\n+Avant de poursuivre, il est utile d'avoir quelques connaissances de base sur l'architecture des Transformers. Comprendre le fonctionnement des encodeurs, des d√©codeurs et du m√©canisme d'attention vous aidera √† saisir comment les diff√©rents mod√®les Transformer fonctionnent. Si vous d√©butez ou avez besoin d'un rappel, consultez notre [cours](https://huggingface.co/course/chapter1/4?fw=pt) pour plus d'informations !\n+\n+</Tip>\n+\n+## Paroles et audio\n+\n+[Wav2Vec2](model_doc/wav2vec2) est un mod√®le auto-supervis√© qui est pr√©entra√Æn√© sur des donn√©es de parole non √©tiquet√©es et ajust√© sur des donn√©es √©tiquet√©es pour des t√¢ches telles que la classification audio et la reconnaissance vocale (ASR).\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n+</div>\n+\n+Ce mod√®le comporte quatre composants principaux :\n+\n+1. **Encodeur de caract√©ristiques** (*feature encoder*): Il prend le signal audio brut, le normalise pour avoir une moyenne nulle et une variance unitaire, et le convertit en une s√©quence de vecteurs de caract√©ristiques, chacun repr√©sentant une dur√©e de 20 ms.\n+\n+2. **Module de quantification** (*quantization module*): Les vecteurs de caract√©ristiques sont pass√©s √† ce module pour apprendre des unit√©s de parole discr√®tes. Chaque vecteur est associ√© √† un *codebook* (une collection de mots-cl√©s), et l'unit√© de parole la plus repr√©sentative est s√©lectionn√©e parmi celles du codebook et transmise au mod√®le.\n+\n+3. **R√©seau de contexte** (*context network*): Environ la moiti√© des vecteurs de caract√©ristiques sont masqu√©s al√©atoirement. Les vecteurs masqu√©s sont ensuite envoy√©s √† un *r√©seau de contexte*, qui est un encodeur qui ajoute des embeddings positionnels relatifs.\n+\n+4. **T√¢che contrastive** (*contrastive task*): Le r√©seau de contexte est pr√©entra√Æn√© avec une t√¢che contrastive. Le mod√®le doit pr√©dire la v√©ritable unit√© de parole quantifi√©e √† partir de la pr√©diction masqu√©e parmi un ensemble de fausses, ce qui pousse le mod√®le √† trouver l'unit√© de parole quantifi√©e la plus proche de la pr√©diction.\n+\n+Une fois pr√©entra√Æn√©, wav2vec2 peut √™tre ajust√© sur vos propres donn√©es pour des t√¢ches comme la classification audio ou la reconnaissance automatique de la parole !\n+\n+### Classification audio\n+\n+Pour utiliser le mod√®le pr√©entra√Æn√© pour la classification audio, ajoutez une t√™te de classification de s√©quence au-dessus du mod√®le Wav2Vec2 de base. Cette t√™te de classification est une couche lin√©aire qui re√ßoit les √©tats cach√©s (*hidden states*) de l'encodeur. Ces √©tats cach√©s, qui repr√©sentent les caract√©ristiques apprises de chaque trame audio, peuvent avoir des longueurs variables. Pour obtenir un vecteur de longueur fixe, les √©tats cach√©s sont d'abord regroup√©s, puis transform√©s en logits correspondant aux √©tiquettes de classe. La perte d'entropie crois√©e est calcul√©e entre les logits et la cible pour d√©terminer la classe la plus probable.\n+\n+Pr√™t √† vous lancer dans la classification audio ? Consultez notre [guide complet de classification audio](tasks/audio_classification) pour apprendre √† ajuster Wav2Vec2 et √† l'utiliser pour l'inf√©rence !\n+\n+### Reconnaissance vocale\n+\n+Pour utiliser le mod√®le pr√©entra√Æn√© pour la reconnaissance vocale, ajoutez une t√™te de mod√©lisation du langage au-dessus du mod√®le Wav2Vec2 de base pour la [classification temporelle connexionniste (CTC)](glossary#connectionist-temporal-classification-ctc). Cette t√™te de mod√©lisation du langage est une couche lin√©aire qui prend les √©tats cach√©s (*hidden states*) de l'encodeur et les convertit en logits. Chaque logit correspond √† une classe de token (le nombre de tokens provient du vocabulaire de la t√¢che). La perte CTC est calcul√©e entre les logits et les cibles (*targets*) pour identifier la s√©quence de tokens la plus probable, qui est ensuite d√©cod√©e en transcription.\n+\n+Pr√™t √† vous lancer dans la reconnaissance automatique de la parole ? Consultez notre [guide complet de reconnaissance automatique de la parole](tasks/asr) pour apprendre √† ajuster Wav2Vec2 et √† l'utiliser pour l'inf√©rence !\n+\n+## Vision par ordinateur\n+\n+Il existe deux fa√ßons d'aborder les t√¢ches de vision par ordinateur :\n+\n+1. **Diviser une image en une s√©quence de patches** et les traiter en parall√®le avec un Transformer.\n+2. **Utiliser un CNN moderne**, comme [ConvNeXT](model_doc/convnext), qui repose sur des couches convolutionnelles mais adopte des conceptions de r√©seau modernes.\n+\n+<Tip>\n+\n+Une troisi√®me approche combine les Transformers avec des convolutions (par exemple, [Convolutional Vision Transformer](model_doc/cvt) ou [LeViT](model_doc/levit)). Nous ne discuterons pas de ces approches ici, car elles m√©langent simplement les deux approches que nous examinons.\n+\n+</Tip>\n+\n+ViT et ConvNeXT sont couramment utilis√©s pour la classification d'images. Pour d'autres t√¢ches de vision par ordinateur comme la d√©tection d'objets, la segmentation et l'estimation de la profondeur, nous examinerons respectivement DETR, Mask2Former et GLPN, qui sont mieux adapt√©s √† ces t√¢ches.\n+\n+### Classification d'images\n+\n+ViT et ConvNeXT peuvent tous deux √™tre utilis√©s pour la classification d'images ; la principale diff√©rence r√©side dans leurs approches : ViT utilise un m√©canisme d'attention tandis que ConvNeXT repose sur des convolutions.\n+\n+#### Transformer\n+\n+[ViT](model_doc/vit) remplace enti√®rement les convolutions par une architecture Transformer pure. Si vous √™tes d√©j√† familiaris√© avec le Transformer original, vous trouverez que ViT suit des principes similaires, mais adapt√©s pour traiter les images comme des s√©quences de patches.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\n+</div>\n+\n+Le principal changement introduit par ViT concerne la fa√ßon dont les images sont fournies √† un Transformer :\n+\n+1. **Tokenisation des images** : L'image est divis√©e en patches carr√©s non chevauchants, chacun √©tant transform√© en un vecteur ou *embedding de patch*. Ces embeddings de patch sont g√©n√©r√©s √† partir d'une couche convolutionnelle 2D pour adapter les dimensions d'entr√©e (par exemple, 768 valeurs pour chaque embedding de patch). Si vous avez une image de 224x224 pixels, elle peut √™tre divis√©e en 196 patches de 16x16 pixels. Ainsi, une image est \"tokenis√©e\" en une s√©quence de patches.\n+\n+2. **Token `[CLS]`** : Un *embedding apprenables* sp√©cial, appel√© token `[CLS]`, est ajout√© au d√©but des embeddings de patch, similaire √† BERT. L'√©tat cach√© final du token `[CLS]` est utilis√© comme entr√©e pour la t√™te de classification attach√©e, tandis que les autres sorties sont ignor√©es. Ce token aide le mod√®le √† encoder une repr√©sentation globale de l'image.\n+\n+3. **Embeddings de position** : Pour que le mod√®le comprenne l'ordre des patches, des *embeddings de position* sont ajout√©s aux embeddings de patch. Ces embeddings de position, √©galement apprenables et de la m√™me taille que les embeddings de patch, permettent au mod√®le de saisir la structure spatiale de l'image.\n+\n+4. **Classification** : Les embeddings, enrichis des embeddings de position, sont ensuite trait√©s par l'encodeur Transformer. La sortie associ√©e au token `[CLS]` est pass√©e √† une t√™te de perceptron multicouche (MLP) pour la classification. La t√™te MLP convertit cette sortie en logits pour chaque √©tiquette de classe, et la perte d'entropie crois√©e est calcul√©e pour d√©terminer la classe la plus probable.\n+\n+Pr√™t √† vous essayer √† la classification d'images ? Consultez notre [guide complet de classification d'images](tasks/image_classification) pour apprendre √† ajuster ViT et √† l'utiliser pour l'inf√©rence !\n+\n+#### CNN\n+\n+<Tip>\n+\n+Cette section explique bri√®vement les convolutions, mais il serait utile d'avoir une compr√©hension pr√©alable de la fa√ßon dont elles modifient la forme et la taille d'une image. Si vous n'√™tes pas familier avec les convolutions, consultez le [chapitre sur les r√©seaux de neurones convolutionnels](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) du livre fastai !\n+\n+</Tip>\n+\n+[ConvNeXT](model_doc/convnext) est une architecture CNN qui adopte des conceptions de r√©seau modernes pour am√©liorer les performances. Cependant, les convolutions restent au c≈ìur du mod√®le. D'un point de vue g√©n√©ral, une [convolution](glossary#convolution) est une op√©ration o√π une matrice plus petite (*noyau*) est multipli√©e par une petite fen√™tre de pixels de l'image. Elle calcule certaines caract√©ristiques √† partir de cette fen√™tre, comme une texture particuli√®re ou la courbure d'une ligne. Ensuite, elle se d√©place vers la fen√™tre suivante de pixels ; la distance parcourue par la convolution est appel√©e le *stride*.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n+</div>\n+\n+<small>Une convolution de base sans padding ni stride, tir√©e de <a href=\"https://arxiv.org/abs/1603.07285\">Un guide des calculs de convolution pour l'apprentissage profond.</a></small>\n+\n+Vous pouvez alimenter la sortie d'une couche convolutionnelle √† une autre couche convolutionnelle. √Ä chaque couche successive, le r√©seau apprend des caract√©ristiques de plus en plus complexes et abstraites, telles que des objets sp√©cifiques comme des hot-dogs ou des fus√©es. Entre les couches convolutionnelles, il est courant d'ajouter des couches de pooling pour r√©duire la dimensionnalit√© et rendre le mod√®le plus robuste aux variations de position des caract√©ristiques.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png\"/>\n+</div>\n+\n+ConvNeXT modernise un CNN de cinq mani√®res :\n+\n+1. **Modification du nombre de blocs** : ConvNeXT utilise une approche similaire √† ViT en \"patchifiant\" l'image avec un stride plus grand et une taille de noyau correspondante, divisant ainsi l'image en patches non chevauchants.\n+\n+2. **Couche de goulot d'√©tranglement** (*bottleneck layer*) : Cette couche r√©duit puis restaure le nombre de canaux pour acc√©l√©rer les convolutions 1x1, permettant une plus grande profondeur du r√©seau. Un goulot d'√©tranglement invers√© augmente d'abord le nombre de canaux avant de les r√©duire, optimisant ainsi l'utilisation de la m√©moire.\n+\n+3. **Convolution en profondeur** (*depthwise convolution*): Remplace la convolution 3x3 traditionnelle par une convolution appliqu√©e √† chaque canal d'entr√©e s√©par√©ment, am√©liorant ainsi la largeur du r√©seau et ses performances.\n+\n+4. **Augmentation de la taille du noyau** : ConvNeXT utilise un noyau de 7x7 pour imiter le champ r√©ceptif global de ViT, ce qui permet de capturer des informations sur une plus grande partie de l'image.\n+\n+5. **Changements de conception des couches** : Le mod√®le adopte des modifications inspir√©es des Transformers, telles que moins de couches d'activation et de normalisation, l'utilisation de GELU au lieu de ReLU, et LayerNorm plut√¥t que BatchNorm.\n+\n+La sortie des blocs de convolution est ensuite pass√©e √† une t√™te de classification, qui convertit les sorties en logits et calcule la perte d'entropie crois√©e pour d√©terminer l'√©tiquette la plus probable.\n+\n+### Object detection\n+\n+[DETR](model_doc/detr), *DEtection TRansformer*, est un mod√®le de d√©tection d'objets de bout en bout qui combine un CNN avec un encodeur-d√©codeur Transformer.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\n+</div>\n+\n+D√©composons le fonctionnement de DETR (DEtection TRansformer) pour la d√©tection d'objets :\n+\n+1. **Extraction des caract√©ristiques avec le CNN** : Un CNN pr√©entra√Æn√©, appel√© *backbone*, prend une image et g√©n√®re une carte de caract√©ristiques (*feature map*) √† basse r√©solution. Une convolution 1x1 est ensuite appliqu√©e pour r√©duire la dimensionnalit√© et cr√©er une nouvelle carte de caract√©ristiques qui repr√©sente des abstractions de plus haut niveau de l'image. Cette derni√®re est ensuite aplatie en une s√©quence de vecteurs de caract√©ristiques, qui sont combin√©s avec des embeddings positionnels.\n+\n+2. **Traitement avec l'encodeur et le d√©codeur** : Les vecteurs de caract√©ristiques sont pass√©s √† l'encodeur, qui apprend les repr√©sentations de l'image avec ses couches d'attention. Les √©tats cach√©s de l'encodeur sont ensuite combin√©s avec des *objects queries* dans le d√©codeur. Ces *objects queries* sont des embeddings appris qui se concentrent sur diff√©rentes r√©gions de l'image et sont mis √† jour √† chaque couche d'attention. Les √©tats cach√©s du d√©codeur sont utilis√©s pour pr√©dire les coordonn√©es de la bo√Æte englobante (*bounding box*) et le label de la classe pour chaque objet query, ou `pas d'objet` si aucun objet n'est d√©tect√©.\n+\n+3. **Perte de correspondance bipartite** : Lors de l'entra√Ænement, DETR utilise une *perte de correspondance bipartite* pour comparer un nombre fixe de pr√©dictions avec un ensemble fixe de labels de v√©rit√© terrain. Si le nombre de labels de v√©rit√© terrain est inf√©rieur au nombre de *N* labels, ils sont compl√©t√©s avec une classe `pas d'objet`. Cette fonction de perte encourage DETR √† trouver une correspondance un √† un entre les pr√©dictions et les labels de v√©rit√© terrain. Si les bo√Ætes englobantes ou les labels de classe ne sont pas corrects, une perte est encourue. De m√™me, si DETR pr√©dit un objet inexistant, il est p√©nalis√©. Cela encourage DETR √† trouver d'autres objets dans l'image au lieu de se concentrer sur un seul objet tr√®s pro√©minent.\n+\n+Une t√™te de d√©tection d'objets est ajout√©e au-dessus de DETR pour trouver le label de la classe et les coordonn√©es de la bo√Æte englobante. Cette t√™te de d√©tection d'objets comprend deux composants : une couche lin√©aire pour transformer les √©tats cach√©s du d√©codeur en logits sur les labels de classe, et un MLP pour pr√©dire la bo√Æte englobante.\n+\n+Pr√™t √† essayer la d√©tection d'objets ? Consultez notre guide complet sur la [d√©tection d'objets](tasks/object_detection) pour apprendre √† affiner DETR et √† l'utiliser pour l'inf√©rence !\n+\n+### Segmentation d'image\n+\n+[Mask2Former](model_doc/mask2former) est une architecture polyvalente con√ßue pour traiter tous les types de t√¢ches de segmentation d'image. Contrairement aux mod√®les de segmentation traditionnels, qui sont g√©n√©ralement sp√©cialis√©s dans des sous-t√¢ches sp√©cifiques comme la segmentation d'instances, s√©mantique ou panoptique, Mask2Former aborde chaque t√¢che comme un probl√®me de *classification de masques*. Cette approche regroupe les pixels en *N* segments et pr√©dit pour chaque image *N* masques ainsi que leur √©tiquette de classe correspondante. Dans cette section, nous vous expliquerons le fonctionnement de Mask2Former et vous aurez la possibilit√© d'effectuer un r√©glage fin (*fine-tuning*) de SegFormer √† la fin.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png\"/>\n+</div>\n+\n+Il y a trois composants principaux dans Mask2Former :\n+\n+1. Un [backbone Swin](model_doc/swin) qui prend une image en entr√©e et g√©n√®re une carte de caract√©ristiques (*feature map*) √† basse r√©solution apr√®s trois convolutions successives de 3x3.\n+\n+2. Cette carte de caract√©ristiques est ensuite envoy√©e √† un *d√©codeur de pixels*, qui augmente progressivement la r√©solution des caract√©ristiques pour obtenir des embeddings par pixel en haute r√©solution. Le d√©codeur de pixels produit des caract√©ristiques multi-√©chelles, comprenant des r√©solutions de 1/32, 1/16, et 1/8 de l'image originale.\n+\n+3. Les cartes de caract√©ristiques √† diff√©rentes √©chelles sont successivement trait√©es par une couche de d√©codeur Transformer, permettant de capturer les petits objets √† partir des caract√©ristiques haute r√©solution. Le point central de Mask2Former est le m√©canisme de *masquage d'attention* dans le d√©codeur. Contrairement √† l'attention crois√©e, qui peut se concentrer sur l'ensemble de l'image, l'attention masqu√©e se focalise uniquement sur certaines zones sp√©cifiques. Cette approche est plus rapide et am√©liore les performances en permettant au mod√®le de se concentrer sur les d√©tails locaux de l'image.\n+\n+4. √Ä l'instar de [DETR](tasks_explained#object-detection), Mask2Former utilise √©galement des requ√™tes d'objet apprises, qu'il combine avec les caract√©ristiques de l'image du d√©codeur de pixels pour faire des pr√©dictions globales (c'est-√†-dire, `√©tiquette de classe`, `pr√©diction de masque`). Les √©tats cach√©s du d√©codeur sont pass√©s dans une couche lin√©aire pour √™tre transform√©s en logits correspondant aux √©tiquettes de classe. La perte d'entropie crois√©e est alors calcul√©e entre les logits et l'√©tiquette de classe pour d√©terminer la plus probable.\n+\n+   Les pr√©dictions de masque sont g√©n√©r√©es en combinant les embeddings de pixels avec les √©tats cach√©s finaux du d√©codeur. La perte d'entropie crois√©e sigmo√Øde et la perte de Dice sont calcul√©es entre les logits et le masque de v√©rit√© terrain pour d√©terminer le masque le plus probable.\n+\n+Pr√™t √† vous lancer dans la d√©tection d'objets ? Consultez notre [guide complet sur la segmentation d'image](tasks/semantic_segmentation) pour apprendre √† affiner SegFormer et l'utiliser pour l'inf√©rence !\n+\n+### Estimation de la profondeur\n+\n+[GLPN](model_doc/glpn), *Global-Local Path Network*, est un Transformer pour l'estimation de profondeur qui combine un encodeur [SegFormer](model_doc/segformer) avec un d√©codeur l√©ger.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"/>\n+</div>\n+1. Comme avec ViT, une image est divis√©e en une s√©quence de patches, mais ces patches sont plus petits. Cette approche est particuli√®rement adapt√©e aux t√¢ches de pr√©diction dense telles que la segmentation ou l'estimation de profondeur. Les patches d'image sont transform√©s en embeddings (voir la section [classification d'image](#image-classification) pour plus de d√©tails sur la cr√©ation des embeddings), puis envoy√©s √† l'encodeur.\n+\n+2. L'encodeur traite les embeddings de patches √† travers plusieurs blocs d'encodeur. Chaque bloc comprend des couches d'attention et de Mix-FFN, con√ßues pour fournir des informations positionnelles. √Ä la fin de chaque bloc, une couche de *fusion de patches* cr√©e des repr√©sentations hi√©rarchiques. Les caract√©ristiques des groupes de patches voisins sont concat√©n√©es, et une couche lin√©aire est appliqu√©e pour r√©duire le nombre de patches √† une r√©solution de 1/4. Ce processus est r√©p√©t√© dans les blocs suivants jusqu'√† obtenir des caract√©ristiques d'image avec des r√©solutions de 1/8, 1/16, et 1/32.\n+\n+3. Un d√©codeur l√©ger prend la derni√®re carte de caract√©ristiques (√† l'√©chelle 1/32) de l'encodeur et l'agrandit √† l'√©chelle 1/16. Ensuite, cette caract√©ristique passe par un module de *Fusion de Caract√©ristiques S√©lective (SFF)*, qui s√©lectionne et combine les caract√©ristiques locales et globales √† partir d'une carte d'attention pour chaque caract√©ristique, puis l'agrandit √† 1/8. Ce processus est r√©p√©t√© jusqu'√† ce que les caract√©ristiques d√©cod√©es aient la m√™me taille que l'image originale. La sortie est ensuite trait√©e par deux couches de convolution, suivies d'une activation sigmo√Øde pour pr√©dire la profondeur de chaque pixel.\n+\n+## Traitement du langage naturel\n+\n+Le Transformer a √©t√© initialement con√ßu pour la traduction automatique, et depuis, il est devenu pratiquement l'architecture par d√©faut pour r√©soudre toutes les t√¢ches de traitement du langage naturel (NLP). Certaines t√¢ches se pr√™tent bien √† la structure d'encodeur du Transformer, tandis que d'autres sont mieux adapt√©es au d√©codeur. D'autres t√¢ches encore utilisent √† la fois la structure encodeur-d√©codeur du Transformer.\n+\n+### Classification de texte\n+\n+[BERT](model_doc/bert) est un mod√®le bas√© uniquement sur l'encodeur, qui a √©t√© le premier √† int√©grer efficacement la bidirectionnalit√© profonde pour obtenir des repr√©sentations plus riches du texte en tenant compte des mots en amont et en aval.\n+\n+1. BERT utilise la tokenisation [WordPiece](tokenizer_summary#wordpiece) pour g√©n√©rer des embeddings de tokens √† partir du texte. Pour diff√©rencier une seule phrase d'une paire de phrases, un token sp√©cial `[SEP]` est ajout√©. De plus, un token sp√©cial `[CLS]` est plac√© au d√©but de chaque s√©quence de texte. La sortie finale associ√©e au token `[CLS]` est utilis√©e comme entr√©e pour la t√™te de classification des t√¢ches. BERT ajoute √©galement un embedding de segment pour indiquer si un token appartient √† la premi√®re ou √† la deuxi√®me phrase dans une paire.\n+\n+2. BERT est pr√©entra√Æn√© avec deux objectifs : le masquage de mots (masked language modeling) et la pr√©diction de la phrase suivante. Pour le masquage de mots, un pourcentage des tokens d'entr√©e est masqu√© al√©atoirement, et le mod√®le doit pr√©dire ces mots. Cela permet de surmonter le probl√®me de la bidirectionnalit√©, o√π le mod√®le pourrait autrement tricher en voyant tous les mots et en \"pr√©dire\" le mot suivant. Les √©tats cach√©s finaux des tokens masqu√©s sont pass√©s √† un r√©seau feedforward avec une fonction softmax sur le vocabulaire pour pr√©dire le mot masqu√©.\n+\n+   Le deuxi√®me objectif de pr√©entra√Ænement est la pr√©diction de la phrase suivante. Le mod√®le doit d√©terminer si la phrase B suit la phrase A. Dans la moiti√© des cas, la phrase B est la phrase suivante, et dans l'autre moiti√©, elle est al√©atoire. Cette pr√©diction (phrase suivante ou non) est envoy√©e √† un r√©seau feedforward avec une softmax sur les deux classes (`IsNext` et `NotNext`).\n+\n+3. Les embeddings d'entr√©e sont trait√©s par plusieurs couches d'encodeur pour produire des √©tats cach√©s finaux.\n+\n+Pour utiliser le mod√®le pr√©entra√Æn√© pour la classification de texte, ajoutez une t√™te de classification de s√©quence au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui prend les √©tats cach√©s finaux et les transforme en logits. La perte d'entropie crois√©e est ensuite calcul√©e entre les logits et les cibles pour d√©terminer l'√©tiquette la plus probable.\n+\n+Pr√™t √† essayer la classification de texte ? Consultez notre [guide complet sur la classification de texte](tasks/sequence_classification) pour apprendre √† effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !\n+\n+### Classification de tokens\n+\n+Pour utiliser BERT dans des t√¢ches de classification de tokens, comme la reconnaissance d'entit√©s nomm√©es (NER), ajoutez une t√™te de classification de tokens au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui prend les √©tats cach√©s finaux et les transforme en logits. La perte d'entropie crois√©e est ensuite calcul√©e entre les logits et les labels de chaque token pour d√©terminer l'√©tiquette la plus probable.\n+\n+Pr√™t √† essayer la classification de tokens ? Consultez notre [guide complet sur la classification de tokens](tasks/token_classification) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !\n+\n+### R√©ponse aux questions - (*Question Answering*)\n+\n+Pour utiliser BERT pour la r√©ponse aux questions, ajoutez une t√™te de classification de span au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui transforme les √©tats cach√©s finaux en logits pour les positions de d√©but et de fin du `span` correspondant √† la r√©ponse. La perte d'entropie crois√©e est calcul√©e entre les logits et les positions r√©elles pour d√©terminer le span de texte le plus probable en tant que r√©ponse.\n+\n+Pr√™t √† essayer la r√©ponse aux questions ? Consultez notre [guide complet sur la r√©ponse aux questions](tasks/question_answering) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !\n+\n+<Tip>\n+\n+üí° Une fois BERT pr√©entra√Æn√©, il est incroyablement facile de l‚Äôadapter √† diverses t√¢ches ! Il vous suffit d‚Äôajouter une t√™te sp√©cifique au mod√®le pr√©entra√Æn√© pour transformer les √©tats cach√©s en la sortie souhait√©e.\n+\n+</Tip>\n+\n+### G√©n√©ration de texte\n+\n+[GPT-2](model_doc/gpt2) est un mod√®le bas√© uniquement sur le d√©codeur, pr√©entra√Æn√© sur une grande quantit√© de texte. Il peut g√©n√©rer du texte convaincant (bien que parfois inexact !) √† partir d'une invite et accomplir d'autres t√¢ches de NLP, comme la r√©ponse aux questions, m√™me s'il n'a pas √©t√© sp√©cifiquement entra√Æn√© pour ces t√¢ches.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png\"/>\n+</div>\n+\n+1. GPT-2 utilise le [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) pour tokeniser les mots et g√©n√©rer des embeddings de tokens. Des encodages positionnels sont ajout√©s pour indiquer la position de chaque token dans la s√©quence. Les embeddings d'entr√©e passent √† travers plusieurs blocs de d√©codeur pour produire des √©tats cach√©s finaux. Chaque bloc de d√©codeur utilise une couche d'*attention masqu√©e*, ce qui signifie que GPT-2 ne peut pas se concentrer sur les tokens futurs et est uniquement autoris√© √† se focaliser sur les tokens √† gauche dans le texte. Cela diff√®re du token [`mask`] de BERT, car ici, dans l'attention masqu√©e, un masque d'attention est utilis√© pour attribuer un score de `0` aux tokens futurs.\n+\n+2. La sortie du d√©codeur est ensuite envoy√©e √† une t√™te de mod√©lisation du langage, qui effectue une transformation lin√©aire pour convertir les √©tats cach√©s en logits. L'√©tiquette est le token suivant dans la s√©quence, obtenue en d√©calant les logits vers la droite d'une position. La perte d'entropie crois√©e est calcul√©e entre les logits d√©cal√©s et les √©tiquettes pour d√©terminer le token suivant le plus probable.\n+\n+L'objectif de pr√©entra√Ænement de GPT-2 est bas√© sur la [mod√©lisation du langage causale](glossary#causal-language-modeling), qui consiste √† pr√©dire le mot suivant dans une s√©quence. Cette approche rend GPT-2 particuli√®rement efficace pour les t√¢ches de g√©n√©ration de texte.\n+\n+Pr√™t √† essayer la g√©n√©ration de texte ? Consultez notre [guide complet sur la mod√©lisation du langage causale](tasks/language_modeling#causal-language-modeling) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilGPT-2 et l'utiliser pour l'inf√©rence !\n+\n+<Tip>\n+\n+Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !\n+\n+</Tip>\n+\n+### R√©sum√© de texte\n+\n+Les mod√®les encodeur-d√©codeur tels que [BART](model_doc/bart) et [T5](model_doc/t5) sont con√ßus pour les t√¢ches de r√©sum√© en mode s√©quence-√†-s√©quence. Dans cette section, nous expliquerons le fonctionnement de BART, puis vous aurez l'occasion de d√©couvrir comment r√©aliser un r√©glagle fin (*fine-tuning*) de T5.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n+</div>\n+\n+1. L'architecture de l'encodeur de BART est tr√®s similaire √† celle de BERT, acceptant des embeddings de tokens et des embeddings positionnels du texte. BART est pr√©entra√Æn√© en corrompant l'entr√©e et en la reconstruisant avec le d√©codeur. Contrairement √† d'autres encodeurs utilisant des strat√©gies de corruption sp√©cifiques, BART peut appliquer divers types de corruption, parmi lesquelles la strat√©gie de *text infilling* est la plus efficace. Dans le text infilling, plusieurs segments de texte sont remplac√©s par un **seul** token [`mask`]. Cette approche est cruciale car elle force le mod√®le √† pr√©dire les tokens masqu√©s et √† estimer le nombre de tokens manquants. Les embeddings d'entr√©e et les spans masqu√©s sont pass√©s √† l'encodeur pour produire des √©tats cach√©s finaux. Contrairement √† BERT, BART ne comporte pas de r√©seau feedforward final pour pr√©dire un mot.\n+\n+2. La sortie de l'encodeur est transmise au d√©codeur, qui doit pr√©dire √† la fois les tokens masqu√©s et les tokens non corrompus. Ce contexte suppl√©mentaire aide le d√©codeur √† restaurer le texte original. La sortie du d√©codeur est ensuite envoy√©e √† une t√™te de mod√©lisation du langage, qui transforme les √©tats cach√©s en logits. La perte d'entropie crois√©e est calcul√©e entre les logits et l'√©tiquette, qui est simplement le token d√©cal√© vers la droite.\n+\n+Pr√™t √† essayer le r√©sum√© ? Consultez notre [guide complet sur le r√©sum√©](tasks/summarization) pour apprendre √† effectuer un r√©glage fin (*fine-tuning*) de T5 et l'utiliser pour l'inf√©rence !\n+\n+<Tip>\n+\n+Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !\n+\n+</Tip>\n+\n+### Traduction\n+\n+La traduction est un autre exemple de t√¢che s√©quence-√†-s√©quence, ce qui signifie qu'un mod√®le encodeur-d√©codeur comme [BART](model_doc/bart) ou [T5](model_doc/t5) peut √™tre utilis√© pour cette t√¢che. Nous expliquerons ici comment BART fonctionne pour la traduction, puis vous pourrez d√©couvrir comment affiner T5.\n+\n+BART adapte le mod√®le √† la traduction en ajoutant un encodeur s√©par√©, initialis√© al√©atoirement, pour mapper la langue source en une entr√©e qui peut √™tre d√©cod√©e dans la langue cible. Les embeddings de cet encodeur sont ensuite pass√©s √† l'encodeur pr√©entra√Æn√© au lieu des embeddings de mots originaux. L'encodeur source est entra√Æn√© en mettant √† jour l'encodeur source, les embeddings positionnels et les embeddings d'entr√©e avec la perte d'entropie crois√©e provenant de la sortie du mod√®le. Les param√®tres du mod√®le sont fig√©s lors de cette premi√®re √©tape, et tous les param√®tres du mod√®le sont entra√Æn√©s ensemble lors de la deuxi√®me √©tape.\n+\n+BART a √©t√© suivi par une version multilingue, mBART, qui est sp√©cifiquement con√ßue pour la traduction et pr√©entra√Æn√©e sur de nombreuses langues diff√©rentes.\n+\n+Pr√™t √† essayer la traduction ? Consultez notre [guide complet sur la traduction](tasks/translation) pour apprendre √† affiner T5 et l'utiliser pour l'inf√©rence !\n+\n+<Tip>\n+\n+Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !\n+\n+</Tip>"
        }
    ],
    "stats": {
        "total": 693,
        "additions": 667,
        "deletions": 26
    }
}