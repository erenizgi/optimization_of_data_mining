{
    "author": "cyyever",
    "message": "Simplify conditional code (#39781)\n\n* Use !=\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Use get\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Format\n\n* Simplify bool operations\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "1e0665a191f73f6b002209c3dfcda478baac6bac",
    "files": [
        {
            "sha": "e1286c325697cf81a929977d0fe9e5e9d4fbfa05",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -202,7 +202,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = TestDetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "b1049772b7d872a451cb9124d6e284fb90398b71",
            "filename": "examples/tensorflow/text-classification/run_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -495,7 +495,7 @@ def compute_metrics(preds, label_ids):\n \n         # region Training and validation\n         if training_args.do_train:\n-            if training_args.do_eval and not data_args.task_name == \"mnli\":\n+            if training_args.do_eval and data_args.task_name != \"mnli\":\n                 # Do both evaluation and training in the Keras fit loop, unless the task is MNLI\n                 # because MNLI has two validation sets\n                 validation_data = tf_data[\"validation\"]"
        },
        {
            "sha": "920b1cf44dafd320729eef1e6a36b9a41741c83c",
            "filename": "src/transformers/debug_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fdebug_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fdebug_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdebug_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -248,7 +248,7 @@ def forward_hook(self, module, input, output):\n \n         last_frame_of_batch = False\n \n-        trace_mode = True if self.batch_number in self.trace_batch_nums else False\n+        trace_mode = self.batch_number in self.trace_batch_nums\n         if trace_mode:\n             self.reset_saved_frames()\n "
        },
        {
            "sha": "b6647760b79034f0c36372d49f2ebb2993617121",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -228,7 +228,7 @@ def process(\n         cur_len = input_ids.shape[-1] + 1\n         batch_size = len(self._beam_hyps) // self.num_beam_groups\n \n-        if not (batch_size == (input_ids.shape[0] // self.group_size)):\n+        if batch_size != (input_ids.shape[0] // self.group_size):\n             if self.num_beam_groups > 1:\n                 raise ValueError(\n                     f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n@@ -564,7 +564,7 @@ def process(\n         # add up to the length which the next_scores is calculated on (including decoder prompt)\n         cur_len = input_ids.shape[-1] + 1\n         batch_size = len(self._beam_hyps)\n-        if not (batch_size == (input_ids.shape[0] // self.group_size)):\n+        if batch_size != (input_ids.shape[0] // self.group_size):\n             if self.num_beam_groups > 1:\n                 raise ValueError(\n                     f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \""
        },
        {
            "sha": "436793c402ea0c6b3e06c12b6763fecee6a14b64",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -435,9 +435,7 @@ def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.\n         # create banned_tokens boolean mask\n         banned_tokens_indices_mask = []\n         for banned_tokens_slice in banned_tokens:\n-            banned_tokens_indices_mask.append(\n-                [True if token in banned_tokens_slice else False for token in range(vocab_size)]\n-            )\n+            banned_tokens_indices_mask.append([token in banned_tokens_slice for token in range(vocab_size)])\n \n         scores = tf.where(tf.convert_to_tensor(banned_tokens_indices_mask, dtype=tf.bool), -float(\"inf\"), scores)\n "
        },
        {
            "sha": "9d22ee818e0b859c7e17a60afaa3242dd5890713",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -833,7 +833,7 @@ def center_crop(self, image, size):\n             return image.crop((left, top, right, bottom))\n \n         # Check if image is in (n_channels, height, width) or (height, width, n_channels) format\n-        channel_first = True if image.shape[0] in [1, 3] else False\n+        channel_first = image.shape[0] in [1, 3]\n \n         # Transpose (height, width, n_channels) format images\n         if not channel_first:"
        },
        {
            "sha": "bdb1dd64ce9aca271ef04997bacc2b2013ca2b19",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -508,13 +508,13 @@ def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optiona\n     short_factor = rope_scaling.get(\"short_factor\")\n     if not isinstance(short_factor, list) and all(isinstance(x, (int, float)) for x in short_factor):\n         logger.warning(f\"`rope_scaling`'s short_factor field must be a list of numbers, got {short_factor}\")\n-    if not len(short_factor) == dim // 2:\n+    if len(short_factor) != dim // 2:\n         logger.warning(f\"`rope_scaling`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\")\n \n     long_factor = rope_scaling.get(\"long_factor\")\n     if not isinstance(long_factor, list) and all(isinstance(x, (int, float)) for x in long_factor):\n         logger.warning(f\"`rope_scaling`'s long_factor field must be a list of numbers, got {long_factor}\")\n-    if not len(long_factor) == dim // 2:\n+    if len(long_factor) != dim // 2:\n         logger.warning(f\"`rope_scaling`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\")\n \n     # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over"
        },
        {
            "sha": "a9e6fa5c7cda8d93780044833154e0f917050b20",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -394,7 +394,7 @@ def __init__(\n     ):\n         super().__init__()\n         self.expand_ratio = expand_ratio\n-        self.expand = True if self.expand_ratio != 1 else False\n+        self.expand = self.expand_ratio != 1\n         expand_in_dim = in_dim * expand_ratio\n \n         if self.expand:\n@@ -464,10 +464,10 @@ def round_repeats(repeats):\n             expand_ratio = config.expand_ratios[i]\n \n             for j in range(round_repeats(config.num_block_repeats[i])):\n-                id_skip = True if j == 0 else False\n+                id_skip = j == 0\n                 stride = 1 if j > 0 else stride\n                 in_dim = out_dim if j > 0 else in_dim\n-                adjust_padding = False if curr_block_num in config.depthwise_padding else True\n+                adjust_padding = curr_block_num not in config.depthwise_padding\n                 drop_rate = config.drop_connect_rate * curr_block_num / num_blocks\n \n                 block = AlignVisionBlock("
        },
        {
            "sha": "8db238d66f15a3b958f4956d5439f0ed9f8066e5",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -515,8 +515,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        split_image = images_kwargs[\"split_image\"] if \"split_image\" in images_kwargs else self.split_image\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        split_image = images_kwargs.get(\"split_image\", self.split_image)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n \n         resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n         num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size"
        },
        {
            "sha": "4a19c3387dbf4d08c6db47ce20d83623c78ceccf",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -901,8 +901,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        split_image = images_kwargs[\"split_image\"] if \"split_image\" in images_kwargs else self.split_image\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        split_image = images_kwargs.get(\"split_image\", self.split_image)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n \n         resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n         num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size"
        },
        {
            "sha": "41c472b909c4a9f6cfb77cd0912552ad73698539",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -81,9 +81,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -99,7 +97,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"cohere2\"]()"
        },
        {
            "sha": "25787a90d61cf15a41bea24e9555f85c34a92ac8",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -269,7 +269,7 @@ def __init__(\n         self.semantic_config = BarkSemanticConfig(**semantic_config)\n         self.coarse_acoustics_config = BarkCoarseConfig(**coarse_acoustics_config)\n         self.fine_acoustics_config = BarkFineConfig(**fine_acoustics_config)\n-        codec_model_type = codec_config[\"model_type\"] if \"model_type\" in codec_config else \"encodec\"\n+        codec_model_type = codec_config.get(\"model_type\", \"encodec\")\n         self.codec_config = CONFIG_MAPPING[codec_model_type](**codec_config)\n \n         self.initializer_range = initializer_range"
        },
        {
            "sha": "ba9433126f8916a7ac77a8d33a13216fc4cd3b28",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -414,9 +414,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,"
        },
        {
            "sha": "8b83d9e439003f8d829d4ca9a2d73346510ce35e",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -96,9 +96,7 @@ def load_tf_weights_trivia_qa(init_vars):\n \n                 name_items[0] = f\"bert/encoder/layer_{layer_name_items[2]}\"\n \n-            name = \"/\".join([_TRIVIA_QA_MAPPING[x] if x in _TRIVIA_QA_MAPPING else x for x in name_items])[\n-                :-2\n-            ]  # remove last :0 in variable\n+            name = \"/\".join([_TRIVIA_QA_MAPPING.get(x, x) for x in name_items])[:-2]  # remove last :0 in variable\n \n             if \"self/attention/output\" in name:\n                 name = name.replace(\"self/attention/output\", \"output\")"
        },
        {
            "sha": "d0a312ebc11f7d5cf7779a305206c242dcd0ec5c",
            "filename": "src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -103,7 +103,7 @@ def convert_bigbird_pegasus(tf_weights: dict, config_update: dict) -> BigBirdPeg\n         new_k = rename_state_dict_key(k, patterns)\n         if new_k not in state_dict:\n             raise ValueError(f\"could not find new key {new_k} in state dict. (converted from {k})\")\n-        if any(True if i in k else False for i in [\"dense\", \"query\", \"key\", \"value\"]):\n+        if any(i in k for i in [\"dense\", \"query\", \"key\", \"value\"]):\n             v = v.T\n         mapping[new_k] = torch.from_numpy(v)\n         assert v.shape == state_dict[new_k].shape, f\"{new_k}, {k}, {v.shape}, {state_dict[new_k].shape}\"\n@@ -116,7 +116,7 @@ def convert_bigbird_pegasus(tf_weights: dict, config_update: dict) -> BigBirdPeg\n         new_k = rename_state_dict_key(k, patterns)\n         if new_k not in state_dict and k != \"pegasus/embeddings/position_embeddings\":\n             raise ValueError(f\"could not find new key {new_k} in state dict. (converted from {k})\")\n-        if any(True if i in k else False for i in [\"dense\", \"query\", \"key\", \"value\"]):\n+        if any(i in k for i in [\"dense\", \"query\", \"key\", \"value\"]):\n             v = v.T\n         mapping[new_k] = torch.from_numpy(v)\n         if k != \"pegasus/embeddings/position_embeddings\":"
        },
        {
            "sha": "98faa9ebea6d772c67b5f5f20286ba31b87b611d",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -252,7 +252,7 @@ def __init__(self, config: BitConfig):\n         else:\n             self.pad = nn.ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n \n-        if not config.layer_type == \"preactivation\":\n+        if config.layer_type != \"preactivation\":\n             self.norm = BitGroupNormActivation(config, num_channels=config.embedding_size)\n         else:\n             self.norm = nn.Identity()"
        },
        {
            "sha": "ff7e629887c1f14590de19d6dca73b207edc125d",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -304,7 +304,7 @@ def __init__(\n \n         self.vision_config = Blip2VisionConfig(**vision_config)\n         self.qformer_config = Blip2QFormerConfig(**qformer_config)\n-        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n+        text_model_type = text_config.get(\"model_type\", \"opt\")\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         self.num_query_tokens = num_query_tokens"
        },
        {
            "sha": "f5467c6c1f40cf1fe917c722d5b8d93364a9f4cd",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -365,9 +365,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,"
        },
        {
            "sha": "7f026c9a306e2efb11dc44f6c223f7e8b7f4b307",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -323,8 +323,8 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         state_dict[f\"model.vqmodel.{k}\"] = v\n \n     # Write configs\n-    ffn_dim_multiplier = params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in params else 1\n-    multiple_of = params[\"multiple_of\"] if \"multiple_of\" in params else 256\n+    ffn_dim_multiplier = params.get(\"ffn_dim_multiplier\", 1)\n+    multiple_of = params.get(\"multiple_of\", 256)\n \n     with open(os.path.join(input_base_path, \"tokenizer/text_tokenizer.json\")) as tokenizer_file:\n         tokenizer_config = json.load(tokenizer_file)"
        },
        {
            "sha": "7ea82bce515c6f28756433ec4f2e36028c1da2d1",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -36,7 +36,7 @@ def get_clipseg_config(model_name):\n     text_config = CLIPSegTextConfig()\n     vision_config = CLIPSegVisionConfig(patch_size=16)\n \n-    use_complex_transposed_convolution = True if \"refined\" in model_name else False\n+    use_complex_transposed_convolution = \"refined\" in model_name\n     reduce_dim = 16 if \"rd16\" in model_name else 64\n \n     config = CLIPSegConfig.from_text_vision_configs("
        },
        {
            "sha": "84be59aef09bdea08c50de2fb96249ade9c989b6",
            "filename": "src/transformers/models/colpali/configuration_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -93,7 +93,7 @@ def __init__(\n         self.vlm_config = vlm_config\n         self.text_config = text_config if text_config is not None else vlm_config.text_config\n         if isinstance(self.text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"gemma\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"gemma\")\n             self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n \n         self.embedding_dim = embedding_dim"
        },
        {
            "sha": "d36d59d44f887b95f78a8406fbf69bc4d1f310cb",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -139,7 +139,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = True if suffix is not None else False\n+        return_token_type_ids = suffix is not None\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")"
        },
        {
            "sha": "2bbbf46d5d71a27dea1969b058d01b306b4e4493",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -183,7 +183,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = True if suffix is not None else False\n+        return_token_type_ids = suffix is not None\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")"
        },
        {
            "sha": "18107a366e21e85e1a503e01808e30986cafa982",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -143,7 +143,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = True if suffix is not None else False\n+        return_token_type_ids = suffix is not None\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")"
        },
        {
            "sha": "d339c5ab7b5edb5d0a3eed860cc6ada081966a65",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -142,7 +142,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = True if suffix is not None else False\n+        return_token_type_ids = suffix is not None\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")"
        },
        {
            "sha": "e3de085ee51525f09a21cbbc99520504629ef41b",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -352,7 +352,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "31512d46e88574d05850eaf83aad83443e623a50",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -236,7 +236,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = ConditionalDetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "94b28f1087855a73a1065055c270418bd2371c3c",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -619,7 +619,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = DFineFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "8b767ef9cacea5aa1adba02fd878e5d1ff1715b6",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -187,7 +187,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = DabDetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "c9b3f01f42d4ee528c11e12865d0e020ed5f0cf7",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -399,7 +399,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "461d4ce0c4bf67e6875d68c78e6b23d0cff5b49a",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -507,7 +507,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "e192474c3dcdcc184b7041a44953df740540cf76",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -370,7 +370,7 @@ def id(self, sym):\n         logger.warning_once(\n             \"The `DebertaTokenizer.id` method is deprecated and will be removed in `transformers==4.35`\"\n         )\n-        return self.vocab[sym] if sym in self.vocab else 1\n+        return self.vocab.get(sym, 1)\n \n     def _encode_as_pieces(self, text):\n         text = convert_to_unicode(text)"
        },
        {
            "sha": "358a2da919e6f7203a19d7967656aab4ef6038fa",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -350,7 +350,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "2627af13cbed70b29ff2c922d24024ad8b0f1573",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -298,7 +298,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = DeformableDetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "434d25a1ab5152b0de5e74463c3fe15eb16f901b",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -332,7 +332,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "90688cffe89a5f481b2114dd02ec2345384ed1ba",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -393,7 +393,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = DetaFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "a6686c1eb2a1177c83c3b66c3c45d2a30e43ce36",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -589,7 +589,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = encoder_layer("
        },
        {
            "sha": "039ba1781cbeaaa4c629035c66463fc8e11e3297",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -345,7 +345,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "9a448784f9a25e01ed937f36b7160398ac0027ed",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -232,7 +232,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = DetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "66cc8d5c3e560d8c07175c4120a2d8d5dfa43a97",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -420,7 +420,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "7b91739c04dcd49d2dc5795567f429675dd00ce9",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -356,7 +356,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "a945a6b50a045d806bac686d9d5eab0971940dd4",
            "filename": "src/transformers/models/dit/convert_dit_unilm_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -136,7 +136,7 @@ def convert_dit_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub\n     \"\"\"\n \n     # define default BEiT configuration\n-    has_lm_head = False if \"rvlcdip\" in checkpoint_url else True\n+    has_lm_head = \"rvlcdip\" not in checkpoint_url\n     config = BeitConfig(use_absolute_position_embeddings=True, use_mask_token=has_lm_head)\n \n     # size of the architecture"
        },
        {
            "sha": "4de89316b759c211d6318040962b609f7f60bab9",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -300,7 +300,7 @@ def __init__(\n     ):\n         super().__init__()\n         self.expand_ratio = expand_ratio\n-        self.expand = True if self.expand_ratio != 1 else False\n+        self.expand = self.expand_ratio != 1\n         expand_in_dim = in_dim * expand_ratio\n \n         if self.expand:\n@@ -371,10 +371,10 @@ def round_repeats(repeats):\n             expand_ratio = config.expand_ratios[i]\n \n             for j in range(round_repeats(config.num_block_repeats[i])):\n-                id_skip = True if j == 0 else False\n+                id_skip = j == 0\n                 stride = 1 if j > 0 else stride\n                 in_dim = out_dim if j > 0 else in_dim\n-                adjust_padding = False if curr_block_num in config.depthwise_padding else True\n+                adjust_padding = curr_block_num not in config.depthwise_padding\n                 drop_rate = config.drop_connect_rate * curr_block_num / num_blocks\n \n                 block = EfficientNetBlock("
        },
        {
            "sha": "d0338983dc1303facf50a6a14499a3c062f6d00e",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -143,7 +143,7 @@ def _pad1d(hidden_states: torch.Tensor, paddings: tuple[int, int], mode: str = \"\n         \"\"\"\n         length = hidden_states.shape[-1]\n         padding_left, padding_right = paddings\n-        if not mode == \"reflect\":\n+        if mode != \"reflect\":\n             return nn.functional.pad(hidden_states, paddings, mode, value)\n \n         max_pad = max(padding_left, padding_right)"
        },
        {
            "sha": "fd6b3223a0b47a7f37c0174bcd58238dd3cf393e",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -616,7 +616,7 @@ def prepare_inputs_for_generation(\n         self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n     ):\n         decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)\n-        decoder_attention_mask = decoder_inputs[\"attention_mask\"] if \"attention_mask\" in decoder_inputs else None\n+        decoder_attention_mask = decoder_inputs.get(\"attention_mask\", None)\n         past_key_values = decoder_inputs.get(\"past_key_values\")\n         if past_key_values is None:\n             past_key_values = decoder_inputs.get(\"past\")  # e.g. on TF GPT2"
        },
        {
            "sha": "86d7bb8a283ad6e45ff1cb243915ffd43efc0ff8",
            "filename": "src/transformers/models/esm/convert_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -130,7 +130,7 @@ def convert_esm_checkpoint_to_pytorch(\n         num_attention_heads = esm.args.attention_heads\n         intermediate_size = esm.args.ffn_embed_dim\n         token_dropout = esm.args.token_dropout\n-        emb_layer_norm_before = True if esm.emb_layer_norm_before else False\n+        emb_layer_norm_before = bool(esm.emb_layer_norm_before)\n         position_embedding_type = \"absolute\"\n         is_folding_model = False\n         esmfold_config = None"
        },
        {
            "sha": "d8271371ca1b16437a79ec1689cd5d6ddf81fe06",
            "filename": "src/transformers/models/esm/openfold_utils/chunk_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -233,7 +233,7 @@ def chunk_layer(\n \n     def _prep_inputs(t: torch.Tensor) -> torch.Tensor:\n         if not low_mem:\n-            if not sum(t.shape[:no_batch_dims]) == no_batch_dims:\n+            if sum(t.shape[:no_batch_dims]) != no_batch_dims:\n                 t = t.expand(orig_batch_dims + t.shape[no_batch_dims:])\n             t = t.reshape(-1, *t.shape[no_batch_dims:])\n         else:"
        },
        {
            "sha": "bcd67aacab8e5ee6d141166d1ec7fa94cba84e6d",
            "filename": "src/transformers/models/esm/openfold_utils/data_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fdata_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fdata_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fdata_transforms.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -31,9 +31,7 @@ def make_atom14_masks(protein: dict[str, torch.Tensor]) -> dict[str, torch.Tenso\n         atom_names = rc.restype_name_to_atom14_names[rc.restype_1to3[rt]]\n         restype_atom14_to_atom37_list.append([(rc.atom_order[name] if name else 0) for name in atom_names])\n         atom_name_to_idx14 = {name: i for i, name in enumerate(atom_names)}\n-        restype_atom37_to_atom14_list.append(\n-            [(atom_name_to_idx14[name] if name in atom_name_to_idx14 else 0) for name in rc.atom_types]\n-        )\n+        restype_atom37_to_atom14_list.append([(atom_name_to_idx14.get(name, 0)) for name in rc.atom_types])\n \n         restype_atom14_mask_list.append([(1.0 if name else 0.0) for name in atom_names])\n "
        },
        {
            "sha": "c4bf2dcc3bbd4c6c2722a0d70d6f3a8701cd2a21",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -334,7 +334,7 @@ def forward(\n                 # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n                 # The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not\n                 # create a causal mask in case query_length == 1.\n-                is_causal = True if self.is_causal and attention_mask is None and query_length > 1 else False\n+                is_causal = self.is_causal and attention_mask is None and query_length > 1\n                 attn_output = torch.nn.functional.scaled_dot_product_attention(\n                     query_layer,\n                     key_layer,\n@@ -364,7 +364,7 @@ def forward(\n             if self._use_sdpa and not output_attentions and head_mask is None:\n                 # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n                 # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n-                is_causal = True if self.is_causal and attention_mask is None and query_length > 1 else False\n+                is_causal = self.is_causal and attention_mask is None and query_length > 1\n                 attn_output = torch.nn.functional.scaled_dot_product_attention(\n                     query_layer,\n                     key_layer,"
        },
        {
            "sha": "ead9950e2a61885e00520538409a14b9afb7344f",
            "filename": "src/transformers/models/focalnet/convert_focalnet_to_hf_format.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -29,9 +29,9 @@\n \n def get_focalnet_config(model_name):\n     depths = [2, 2, 6, 2] if \"tiny\" in model_name else [2, 2, 18, 2]\n-    use_conv_embed = True if \"large\" in model_name or \"huge\" in model_name else False\n-    use_post_layernorm = True if \"large\" in model_name or \"huge\" in model_name else False\n-    use_layerscale = True if \"large\" in model_name or \"huge\" in model_name else False\n+    use_conv_embed = bool(\"large\" in model_name or \"huge\" in model_name)\n+    use_post_layernorm = bool(\"large\" in model_name or \"huge\" in model_name)\n+    use_layerscale = bool(\"large\" in model_name or \"huge\" in model_name)\n \n     if \"large\" in model_name or \"xlarge\" in model_name or \"huge\" in model_name:\n         if \"fl3\" in model_name:"
        },
        {
            "sha": "40da84e2e780821f26765333a2cee51030e0bea4",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -158,7 +158,7 @@ def __init__(\n                 \"tie_word_embeddings\": tie_word_embeddings,\n             }\n             logger.info(\"text_config is None. initializing the text model with default values.\")\n-        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"persimmon\"\n+        text_model_type = text_config.get(\"model_type\", \"persimmon\")\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         self._vocab_size = vocab_size"
        },
        {
            "sha": "d1b0636a99ab31a6ec4b7f484575dcf31cf21820",
            "filename": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -222,7 +222,7 @@ def main():\n \n         spm_path = os.path.join(args.tokenizer_checkpoint)\n         write_tokenizer(spm_path, args.output_dir, args.push_to_hub)\n-    if not args.model_size == \"tokenizer_only\":\n+    if args.model_size != \"tokenizer_only\":\n         config = CONFIG_MAPPING[args.model_size]\n         dtype = getattr(torch, args.dtype)\n         write_model("
        },
        {
            "sha": "699b8c5ad7bec5283dd0a8713660cad3b08e771f",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -449,8 +449,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of image patches per image.\n         \"\"\"\n-        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n-        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n+        patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n+        merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "eb039f958950c3fbc3cd4401b425348181b13a00",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -179,7 +179,7 @@ def __init__(\n             self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"qwen2\"]("
        },
        {
            "sha": "b6414b1c61ea1a8ae32ddad9ee604b040d6cbe5a",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -505,12 +505,10 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        min_patches = images_kwargs[\"min_patches\"] if \"min_patches\" in images_kwargs else self.min_patches\n-        max_patches = images_kwargs[\"max_patches\"] if \"max_patches\" in images_kwargs else self.max_patches\n-        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.size\n-        crop_to_patches = (\n-            images_kwargs[\"crop_to_patches\"] if \"crop_to_patches\" in images_kwargs else self.crop_to_patches\n-        )\n+        min_patches = images_kwargs.get(\"min_patches\", self.min_patches)\n+        max_patches = images_kwargs.get(\"max_patches\", self.max_patches)\n+        patch_size = images_kwargs.get(\"patch_size\", self.size)\n+        crop_to_patches = images_kwargs.get(\"crop_to_patches\", self.crop_to_patches)\n \n         num_patches = 1\n         if crop_to_patches and max_patches > 1:"
        },
        {
            "sha": "813bb6061ba25474bc9892e72e3caa57eb49a7cc",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -237,12 +237,10 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        min_patches = images_kwargs[\"min_patches\"] if \"min_patches\" in images_kwargs else self.min_patches\n-        max_patches = images_kwargs[\"max_patches\"] if \"max_patches\" in images_kwargs else self.max_patches\n-        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.size\n-        crop_to_patches = (\n-            images_kwargs[\"crop_to_patches\"] if \"crop_to_patches\" in images_kwargs else self.crop_to_patches\n-        )\n+        min_patches = images_kwargs.get(\"min_patches\", self.min_patches)\n+        max_patches = images_kwargs.get(\"max_patches\", self.max_patches)\n+        patch_size = images_kwargs.get(\"patch_size\", self.size)\n+        crop_to_patches = images_kwargs.get(\"crop_to_patches\", self.crop_to_patches)\n \n         num_patches = 1\n         if crop_to_patches and max_patches > 1:"
        },
        {
            "sha": "9e017659e41f09bea60de7075e6357e57a7c6a1c",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -201,7 +201,7 @@ def __init__(\n             self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"qwen2\"]("
        },
        {
            "sha": "3403ac3196fc11a57fd112600ae78c79ab844b63",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -893,7 +893,7 @@ def forward(\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     mask=encoder_attention_mask, dtype=inputs_embeds.dtype, tgt_len=input_shape[-1]\n                 )\n-            elif not self._attn_implementation == \"flash_attention_2\":\n+            elif self._attn_implementation != \"flash_attention_2\":\n                 encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_attention_mask = None"
        },
        {
            "sha": "fede07b7b7e820e78f44538313a85d39afc811d7",
            "filename": "src/transformers/models/granite_speech/configuration_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -169,15 +169,13 @@ def __init__(\n         **kwargs,\n     ):\n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"granite\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"granite\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"granite\"]()\n \n         if isinstance(projector_config, dict):\n-            projector_config[\"model_type\"] = (\n-                projector_config[\"model_type\"] if \"model_type\" in projector_config else \"blip_2_qformer\"\n-            )\n+            projector_config[\"model_type\"] = projector_config.get(\"model_type\", \"blip_2_qformer\")\n             projector_config = CONFIG_MAPPING[projector_config[\"model_type\"]](**projector_config)\n         elif projector_config is None:\n             projector_config = CONFIG_MAPPING[\"blip_2_qformer\"]()"
        },
        {
            "sha": "a45848b4c47c6ff4cdd769a5874d0a2c4e1c274f",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -261,7 +261,7 @@ def __init__(\n         self.disable_custom_kernels = disable_custom_kernels\n         # Text backbone\n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"bert\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"bert\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"bert\"]()"
        },
        {
            "sha": "8ba1f34ae9cc0f2c8e99785ee1fee08706bb732b",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -359,7 +359,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "310aaf8e0ca7f01fe5df163c9b9bf95e953243c3",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -354,7 +354,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = GroundingDinoFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "71da4f055a7a1af5b810929989aac74c1afc8aa1",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -284,7 +284,7 @@ def __init__(self, config: HGNetV2Config, stage_index: int, drop_path: float = 0\n                     mid_channels,\n                     out_channels,\n                     num_layers,\n-                    residual=False if i == 0 else True,\n+                    residual=(i != 0),\n                     kernel_size=kernel_size,\n                     light_block=light_block,\n                     drop_path=drop_path,"
        },
        {
            "sha": "4f898718e33de319e80978b2431d28827b46b495",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -407,7 +407,7 @@ def __init__(self, config: HGNetV2Config, stage_index: int, drop_path: float = 0\n                     mid_channels,\n                     out_channels,\n                     num_layers,\n-                    residual=False if i == 0 else True,\n+                    residual=(i != 0),\n                     kernel_size=kernel_size,\n                     light_block=light_block,\n                     drop_path=drop_path,"
        },
        {
            "sha": "a0e0b5cd566b23ab3334fb32c4f8ddef70d3ac5c",
            "filename": "src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -217,7 +217,7 @@ def convert_hubert_checkpoint(\n                 word_delimiter_token=\"|\",\n                 do_lower_case=False,\n             )\n-            return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+            return_attention_mask = config.feat_extract_norm == \"layer\"\n             feature_extractor = Wav2Vec2FeatureExtractor(\n                 feature_size=1,\n                 sampling_rate=16000,"
        },
        {
            "sha": "060b715e8d499a13906092133dab2bdddae216df",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -459,7 +459,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer(\n@@ -624,7 +624,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication"
        },
        {
            "sha": "fb0d129c506ed51d85ea5b81dd0c66f2f4fc9e0a",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -234,9 +234,7 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n         )\n \n         self.tokenizer_was_trained_with_end_of_utterance_token = (\n-            True\n-            if \"<end_of_utterance>\" in self.tokenizer.special_tokens_map.get(\"additional_special_tokens\", [])\n-            else False\n+            \"<end_of_utterance>\" in self.tokenizer.special_tokens_map.get(\"additional_special_tokens\", [])\n         )\n \n     @deprecate_kwarg(old_name=\"prompts\", version=\"5.0.0\", new_name=\"text\", raise_if_both_names=True)\n@@ -402,7 +400,7 @@ def image_tokens(last_was_image):\n             last_was_text = False\n             for i, item in enumerate(sample):\n                 if i > 0:\n-                    last_was_text = True if not last_was_image else False\n+                    last_was_text = bool(not last_was_image)\n \n                 if isinstance(item, str):\n                     item = item.strip(\" \")"
        },
        {
            "sha": "a8fa442a1dbc67276864f033623c5526f3fed750",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -241,7 +241,7 @@ def __init__(\n             self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"mistral\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"mistral\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             logger.info(\"text_config is None, using default text config\")"
        },
        {
            "sha": "97a2e57f1d8dc4cc2b8c27ebade7ef1a5109f97f",
            "filename": "src/transformers/models/idefics3/configuration_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -171,7 +171,7 @@ def __init__(\n             self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             logger.info(\"text_config is None, using default text config\")"
        },
        {
            "sha": "f98413d13350eace56d41ce9f0fbc99406544e4e",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -866,11 +866,9 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = (\n-            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n-        )\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n-        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", self.do_image_splitting)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n+        size = images_kwargs.get(\"size\", self.size)\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "b70829f5b43f60eb5c76ba078848d4693c1f67b3",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -514,11 +514,9 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = (\n-            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n-        )\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n-        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", self.do_image_splitting)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n+        size = images_kwargs.get(\"size\", self.size)\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "9b8323f15f0525887d178239b6b221dbb488d952",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -302,7 +302,7 @@ def __init__(\n \n         self.vision_config = InstructBlipVisionConfig(**vision_config)\n         self.qformer_config = InstructBlipQFormerConfig(**qformer_config)\n-        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n+        text_model_type = text_config.get(\"model_type\", \"opt\")\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         self.num_query_tokens = num_query_tokens"
        },
        {
            "sha": "af2acc83387675e5bac3fcfa7c6ffe5c793838a0",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -308,7 +308,7 @@ def __init__(\n \n         self.vision_config = InstructBlipVideoVisionConfig(**vision_config)\n         self.qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n-        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n+        text_model_type = text_config.get(\"model_type\", \"opt\")\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         self.num_query_tokens = num_query_tokens"
        },
        {
            "sha": "e7bcfaba82fb0f5592bd39617a22f473fab833d6",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -142,7 +142,7 @@ def __init__(\n \n         self.vision_config = InstructBlipVideoVisionConfig(**vision_config)\n         self.qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n-        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n+        text_model_type = text_config.get(\"model_type\", \"opt\")\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         self.num_query_tokens = num_query_tokens"
        },
        {
            "sha": "17be5388b6ab7dcbf596c2deee69b4a467755fe2",
            "filename": "src/transformers/models/internvl/configuration_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -212,7 +212,7 @@ def __init__(\n             self.vision_config = InternVLVisionConfig()\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"qwen2\"]()"
        },
        {
            "sha": "e4a376e90af1c7e1fd88d8a707a34d77891bc997",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -533,7 +533,7 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and q_len > 1 else False\n+        is_causal = self.is_causal and causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "5156ac59742e4e5eba05ececf5e19ce94c7bd23f",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -620,7 +620,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "0489fc91d1762317b8a7bc6892eb5e819e8015a6",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -690,7 +690,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "d0f228ae4b468dcd3e575d713d1ad0874d0c4303",
            "filename": "src/transformers/models/lightglue/configuration_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -129,9 +129,7 @@ def __init__(\n         # Keypoint Detector is forced into eager attention mode because SuperPoint does not have Attention\n         # See https://github.com/huggingface/transformers/pull/31718#discussion_r2109733153\n         if isinstance(keypoint_detector_config, dict):\n-            keypoint_detector_config[\"model_type\"] = (\n-                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n-            )\n+            keypoint_detector_config[\"model_type\"] = keypoint_detector_config.get(\"model_type\", \"superpoint\")\n             if keypoint_detector_config[\"model_type\"] not in CONFIG_MAPPING:\n                 keypoint_detector_config = AutoConfig.from_pretrained(\n                     keypoint_detector_config[\"_name_or_path\"], trust_remote_code=self.trust_remote_code"
        },
        {
            "sha": "cefb235fcb2acfbfb597b3b6e3378bff18dacb1b",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -143,9 +143,7 @@ def __init__(\n         # Keypoint Detector is forced into eager attention mode because SuperPoint does not have Attention\n         # See https://github.com/huggingface/transformers/pull/31718#discussion_r2109733153\n         if isinstance(keypoint_detector_config, dict):\n-            keypoint_detector_config[\"model_type\"] = (\n-                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n-            )\n+            keypoint_detector_config[\"model_type\"] = keypoint_detector_config.get(\"model_type\", \"superpoint\")\n             if keypoint_detector_config[\"model_type\"] not in CONFIG_MAPPING:\n                 keypoint_detector_config = AutoConfig.from_pretrained(\n                     keypoint_detector_config[\"_name_or_path\"], trust_remote_code=self.trust_remote_code"
        },
        {
            "sha": "ed7a89f6f32e7955285476ad38e60aa8a098cbf8",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -360,8 +360,8 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         # Write configs\n         index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n         write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n-        ffn_dim_multiplier = params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in params else 1\n-        multiple_of = params[\"multiple_of\"] if \"multiple_of\" in params else 256\n+        ffn_dim_multiplier = params.get(\"ffn_dim_multiplier\", 1)\n+        multiple_of = params.get(\"multiple_of\", 256)\n \n         if is_llama_3(llama_version):\n             bos_token_id = 128000\n@@ -398,7 +398,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n             max_position_embeddings=max_position_embeddings,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n-            tie_word_embeddings=True if llama_version in [\"3.2\"] else False,\n+            tie_word_embeddings=llama_version in [\"3.2\"],\n         )\n \n         config.save_pretrained(tmp_model_path)"
        },
        {
            "sha": "9ae710c011986f422d965adcdb758b29e3113690",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -106,9 +106,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n@@ -125,7 +123,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "17ea71b1aa6421c8e2007cbb9399a69e9894288e",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -120,9 +120,7 @@ def __init__(\n         self.image_grid_pinpoints = image_grid_pinpoints\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n@@ -139,7 +137,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "41fc226783652321429b9c1befc58c2185f25c20",
            "filename": "src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -127,7 +127,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     torch.set_default_dtype(torch.float16)\n     text_config = AutoConfig.from_pretrained(text_model_id)\n \n-    use_fast = False if model_id == \"liuhaotian/llava-v1.6-34b\" else True\n+    use_fast = model_id != \"liuhaotian/llava-v1.6-34b\"\n     tokenizer = AutoTokenizer.from_pretrained(text_model_id, use_fast=use_fast)\n     tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n "
        },
        {
            "sha": "1eb1078b6aceb27128001eac79e0161396cc0387",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -136,9 +136,7 @@ def __init__(\n         self.image_grid_pinpoints = image_grid_pinpoints\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n@@ -155,7 +153,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "fecd2320f9ef37e5ba6cd5e6cd09377dffc3d08e",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -153,9 +153,7 @@ def __init__(\n         self.image_grid_pinpoints = image_grid_pinpoints\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n@@ -172,7 +170,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "21ead3df17061f3c3545a9d997396babe6d7fbd6",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -165,9 +165,7 @@ def __init__(\n         self.image_grid_pinpoints = image_grid_pinpoints\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -183,7 +181,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"qwen2\"]()"
        },
        {
            "sha": "89344e6ac1e1e4a820b159f62fde16c4bf14af0b",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -871,7 +871,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n \n@@ -1120,7 +1120,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n "
        },
        {
            "sha": "8ea35b6da56f12f5f835b4da03e298aef344b297",
            "filename": "src/transformers/models/markuplm/feature_extraction_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ffeature_extraction_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ffeature_extraction_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ffeature_extraction_markuplm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -159,7 +159,7 @@ def __call__(self, html_strings) -> BatchFeature:\n                 f\"but is of type {type(html_strings)}.\"\n             )\n \n-        is_batched = bool(isinstance(html_strings, (list, tuple)) and (isinstance(html_strings[0], str)))\n+        is_batched = isinstance(html_strings, (list, tuple)) and (isinstance(html_strings[0], str))\n \n         if not is_batched:\n             html_strings = [html_strings]"
        },
        {
            "sha": "43fbd234fb2a34820017361acc50a5a4b27bc02a",
            "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -295,7 +295,7 @@ def convert_maskformer_checkpoint(\n         ignore_index = 65535\n     else:\n         ignore_index = 255\n-    do_reduce_labels = True if \"ade\" in model_name else False\n+    do_reduce_labels = \"ade\" in model_name\n     image_processor = MaskFormerImageProcessor(ignore_index=ignore_index, do_reduce_labels=do_reduce_labels)\n \n     inputs = image_processor(image, return_tensors=\"pt\")"
        },
        {
            "sha": "4b6e32e5cc138aad3953de5a2755afcff3d3a57a",
            "filename": "src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -276,7 +276,7 @@ def convert_maskformer_checkpoint(\n         ignore_index = 65535\n     else:\n         ignore_index = 255\n-    do_reduce_labels = True if \"ade\" in model_name else False\n+    do_reduce_labels = \"ade\" in model_name\n     image_processor = MaskFormerImageProcessor(ignore_index=ignore_index, do_reduce_labels=do_reduce_labels)\n \n     inputs = image_processor(image, return_tensors=\"pt\")"
        },
        {
            "sha": "584f3c43e008632d088b6e15364470bfd41c50f1",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -280,7 +280,7 @@ def _pad1d(hidden_states: torch.Tensor, paddings: tuple[int, int], mode: str = \"\n         \"\"\"\n         length = hidden_states.shape[-1]\n         padding_left, padding_right = paddings\n-        if not mode == \"reflect\":\n+        if mode != \"reflect\":\n             return nn.functional.pad(hidden_states, paddings, mode, value)\n \n         max_pad = max(padding_left, padding_right)\n@@ -888,7 +888,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "2dd55860879315f6985a7f8fc88c589162a4822d",
            "filename": "src/transformers/models/mistral3/configuration_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -92,7 +92,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = vision_config[\"model_type\"] if \"model_type\" in vision_config else \"pixtral\"\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"pixtral\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"pixtral\"](\n@@ -110,7 +110,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"mistral\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"mistral\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"mistral\"]("
        },
        {
            "sha": "b26b3038598243253aa685264affd50ddc6052db",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -266,7 +266,7 @@ def forward(\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+        is_causal = self.is_causal and attention_mask is None and q_len > 1\n \n         if self.head_dim_padding > 0:\n             query_states = torch.nn.functional.pad(query_states, (0, self.head_dim_padding))"
        },
        {
            "sha": "8452283e9c5cfc30ff9457578d74dfad0a32ee9e",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -365,7 +365,7 @@ def forward(\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+        is_causal = self.is_causal and attention_mask is None and q_len > 1\n \n         if self.head_dim_padding > 0:\n             query_states = torch.nn.functional.pad(query_states, (0, self.head_dim_padding))"
        },
        {
            "sha": "c9717e49be30ff690f482c13dfed8fec7aa2f72e",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -693,7 +693,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "d16b12d054a3c21ac6ea09eb7a7dbee9845ce18a",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -473,7 +473,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "5523aeda58a2e4d93aeeecac90ceb0014bb0e4f8",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -1285,7 +1285,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 layer_head_mask = head_mask[idx] if head_mask is not None else None\n                 cross_attn_layer_head_mask = cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None"
        },
        {
            "sha": "eacb56c064b607114ffaf7f0d5f3fab4b08d6bc4",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -538,7 +538,7 @@ def forward(\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "ab6f0573023cef8ef2949ee9487ba6b408e2135f",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -55,7 +55,7 @@ def get_omdet_turbo_config(model_name, use_timm_backbone):\n         text_config={\"model_type\": \"clip_text_model\"},\n         use_timm_backbone=use_timm_backbone,\n         backbone=\"swin_tiny_patch4_window7_224\" if use_timm_backbone else None,\n-        apply_layernorm_after_vision_backbone=True if use_timm_backbone else False,\n+        apply_layernorm_after_vision_backbone=bool(use_timm_backbone),\n         use_pretrained_backbone=False,\n     )\n "
        },
        {
            "sha": "e4ee4b3b45c2637d804f7eafa7f9cc7afa2638f8",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -92,9 +92,7 @@ def __init__(\n         self.is_encoder_decoder = False\n \n         if isinstance(self.vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n             self.vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             self.vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -110,7 +108,7 @@ def __init__(\n \n         self.text_config = text_config\n         if isinstance(self.text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"gemma\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"gemma\")\n             self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             self.text_config = CONFIG_MAPPING[\"gemma\"]("
        },
        {
            "sha": "3a7d06b6adeab457305cca1d1f281390187e3290",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -223,7 +223,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = True if suffix is not None else False\n+        return_token_type_ids = suffix is not None\n \n         if images is None:\n             raise ValueError(\"`images` are expected as arguments to a `PaliGemmaProcessor` instance.\")"
        },
        {
            "sha": "4b94652e20842059f5fcde77d58b89c37c412c28",
            "filename": "src/transformers/models/perception_lm/configuration_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -75,7 +75,7 @@ def __init__(\n         self.vision_use_cls_token = vision_use_cls_token\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "6174309303142d07fb4298e0035858e1e177f36f",
            "filename": "src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -351,8 +351,8 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         # Write configs\n         index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n         write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n-        ffn_dim_multiplier = model_params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in model_params else 1\n-        multiple_of = model_params[\"multiple_of\"] if \"multiple_of\" in model_params else 256\n+        ffn_dim_multiplier = model_params.get(\"ffn_dim_multiplier\", 1)\n+        multiple_of = model_params.get(\"multiple_of\", 256)\n \n         bos_token_id = tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")\n         eos_token_id = [tokenizer.convert_tokens_to_ids(t) for t in [\"<|end_of_text|>\", \"<|eot_id|>\"]]"
        },
        {
            "sha": "d2c587ccec4b9e0698157b850a0a40bd75008b45",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -577,7 +577,7 @@ def forward(\n         else:\n             attention_mask = (\n                 _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n-                if not self.config._attn_implementation == \"flash_attention_2\"\n+                if self.config._attn_implementation != \"flash_attention_2\"\n                 else patch_attention_mask\n             )\n "
        },
        {
            "sha": "8772796b0349cf3ec2f5b9413da0ebbdec975156",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -702,7 +702,7 @@ def forward(\n         else:\n             attention_mask = (\n                 _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n-                if not self.config._attn_implementation == \"flash_attention_2\"\n+                if self.config._attn_implementation != \"flash_attention_2\"\n                 else patch_attention_mask\n             )\n "
        },
        {
            "sha": "2207793dcaeadb922d05a4b161212f343c7fa221",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -488,7 +488,7 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = causal_mask is None and q_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "1eed6dc18bddd7b9ba0fa68dc31186fd35c03a99",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -425,7 +425,7 @@ def set_tgt_lang_special_tokens(self, lang: str) -> None:\n \n     def _convert_lang_code_special_format(self, lang: str) -> str:\n         \"\"\"Convert Language Codes to format tokenizer uses if required\"\"\"\n-        lang = FAIRSEQ_LANGUAGE_CODES_MAP[lang] if lang in FAIRSEQ_LANGUAGE_CODES_MAP else lang\n+        lang = FAIRSEQ_LANGUAGE_CODES_MAP.get(lang, lang)\n         return lang\n \n "
        },
        {
            "sha": "97255f969aa5f2b35db1bfd38001cf0bdd1e6730",
            "filename": "src/transformers/models/pop2piano/feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -377,7 +377,7 @@ def __call__(\n         \"\"\"\n \n         requires_backends(self, [\"librosa\"])\n-        is_batched = bool(isinstance(audio, (list, tuple)) and isinstance(audio[0], (np.ndarray, tuple, list)))\n+        is_batched = isinstance(audio, (list, tuple)) and isinstance(audio[0], (np.ndarray, tuple, list))\n         if is_batched:\n             # This enables the user to process files of different sampling_rate at same time\n             if not isinstance(sampling_rate, list):"
        },
        {
            "sha": "88e930a94fd696d8faf2348c3bd4448d252d0e20",
            "filename": "src/transformers/models/qwen2_audio/configuration_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -172,9 +172,7 @@ def __init__(\n         self.audio_token_index = audio_token_index\n \n         if isinstance(audio_config, dict):\n-            audio_config[\"model_type\"] = (\n-                audio_config[\"model_type\"] if \"model_type\" in audio_config else \"qwen2_audio_encoder\"\n-            )\n+            audio_config[\"model_type\"] = audio_config.get(\"model_type\", \"qwen2_audio_encoder\")\n             audio_config = CONFIG_MAPPING[audio_config[\"model_type\"]](**audio_config)\n         elif audio_config is None:\n             audio_config = CONFIG_MAPPING[\"qwen2_audio_encoder\"](\n@@ -192,7 +190,7 @@ def __init__(\n         self.audio_config = audio_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"qwen2\"]()"
        },
        {
            "sha": "108aa96176148a6351305f9f18b830adf6f0257d",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -553,7 +553,7 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = bool(causal_mask is None and q_len > 1)\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "e1529594d0765bee3ea7993dbdcef9cfdf98b6d0",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -504,8 +504,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         \"\"\"\n         min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n         max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n-        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n-        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n+        patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n+        merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "cc3e0ae6936540981d9a408a931f46cc9d6fd2a4",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -301,8 +301,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         \"\"\"\n         min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n         max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n-        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n-        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n+        patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n+        merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "fa8e638ef37eac0d65831fe9ba33c6ee452258fc",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -364,9 +364,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,"
        },
        {
            "sha": "42d8efef38a9a47b3993c87b768078576f7e9369",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -297,7 +297,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "eb20d4c2d17931790b2d2c5dd13f7cdfa2505e73",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -341,7 +341,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = RTDetrFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "f9588eac5a9e4209a3e6d85ba7e3640522c7e7b9",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -670,7 +670,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = RTDetrV2FrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "8e17cb17b26923eba7d7cfc7ea29a6f0460045b9",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -742,9 +742,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = (\n-                True if self.training and (dropout_probability < self.config.speech_encoder_layerdrop) else False\n-            )\n+            skip_the_layer = self.training and dropout_probability < self.config.speech_encoder_layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "e35278b48edb913f5ee2c10ee00cecb79f1097bc",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -611,9 +611,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = (\n-                True if self.training and (dropout_probability < self.config.speech_encoder_layerdrop) else False\n-            )\n+            skip_the_layer = self.training and dropout_probability < self.config.speech_encoder_layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "f39e14e409a442b60bfdad8de095acdff1da5e87",
            "filename": "src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -240,7 +240,7 @@ def convert_sew_checkpoint(\n         config = convert_config(model[0], is_finetuned)\n     model = model[0].eval()\n \n-    return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+    return_attention_mask = config.feat_extract_norm == \"layer\"\n     feature_extractor = Wav2Vec2FeatureExtractor(\n         feature_size=1,\n         sampling_rate=16000,"
        },
        {
            "sha": "d5bd617e842be2406ae30fedf700895a210a9665",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -479,7 +479,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "5b4ee00c4a654bd086fc86ab866261176b92a2fc",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -227,7 +227,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "bc638e6b7c52e129c3a318e6b9870324ccaa7cdc",
            "filename": "src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -252,7 +252,7 @@ def convert_sew_checkpoint(\n         config = convert_config(model[0], is_finetuned)\n     model = model[0].eval()\n \n-    return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+    return_attention_mask = config.feat_extract_norm == \"layer\"\n     feature_extractor = Wav2Vec2FeatureExtractor(\n         feature_size=1,\n         sampling_rate=16000,"
        },
        {
            "sha": "63d7f49bf33302e97192540ccd3fa102e883222c",
            "filename": "src/transformers/models/shieldgemma2/configuration_shieldgemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -91,17 +91,15 @@ def __init__(\n         **kwargs,\n     ):\n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"]()\n \n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"gemma3_text\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"gemma3_text\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"gemma3_text\"]()"
        },
        {
            "sha": "2dca4721c02b858f0883bd328bc98f9eb1a98094",
            "filename": "src/transformers/models/smolvlm/configuration_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -177,7 +177,7 @@ def __init__(\n             self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             logger.info(\"text_config is None, using default text config\")"
        },
        {
            "sha": "2c7e34a9864f6e4abe4394d7f98e8e4de6e02540",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -863,11 +863,9 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = (\n-            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n-        )\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n-        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", self.do_image_splitting)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n+        size = images_kwargs.get(\"size\", self.size)\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "a070cd87bf864f424da3516f978db42be40d543e",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -504,11 +504,9 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = (\n-            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n-        )\n-        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n-        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", self.do_image_splitting)\n+        max_image_size = images_kwargs.get(\"max_image_size\", self.max_image_size)\n+        size = images_kwargs.get(\"size\", self.size)\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "559c1a5f957b208c390ebe1fce9f85bcefccd047",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -393,7 +393,7 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        is_causal = bool(causal_mask is None and q_len > 1)\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,"
        },
        {
            "sha": "de6086114f45d8db339040201fce58c3a1ad6bf4",
            "filename": "src/transformers/models/superglue/configuration_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -100,9 +100,7 @@ def __init__(\n         self.matching_threshold = matching_threshold\n \n         if isinstance(keypoint_detector_config, dict):\n-            keypoint_detector_config[\"model_type\"] = (\n-                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n-            )\n+            keypoint_detector_config[\"model_type\"] = keypoint_detector_config.get(\"model_type\", \"superpoint\")\n             keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n                 **keypoint_detector_config\n             )"
        },
        {
            "sha": "71bd7e951558c86b53be594cb6bac7e858f17928",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -185,7 +185,7 @@ def replace_batch_norm(model):\n         if isinstance(module, nn.BatchNorm2d):\n             new_module = TableTransformerFrozenBatchNorm2d(module.num_features)\n \n-            if not module.weight.device == torch.device(\"meta\"):\n+            if module.weight.device != torch.device(\"meta\"):\n                 new_module.weight.data.copy_(module.weight)\n                 new_module.bias.data.copy_(module.bias)\n                 new_module.running_mean.data.copy_(module.running_mean)"
        },
        {
            "sha": "f0e05cbe1502321d1ab91a58dd2c02849df9b93d",
            "filename": "src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -229,7 +229,7 @@ def convert_unispeech_checkpoint(\n                 word_delimiter_token=\"|\",\n                 do_lower_case=False,\n             )\n-            return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+            return_attention_mask = config.feat_extract_norm == \"layer\"\n             feature_extractor = Wav2Vec2FeatureExtractor(\n                 feature_size=1,\n                 sampling_rate=16000,"
        },
        {
            "sha": "76f8072883e4246fa6f28be75297732c43c7ffa6",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -491,7 +491,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer(\n@@ -656,7 +656,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication"
        },
        {
            "sha": "0b771ec08f69d3cbd7ee3607f553e8d41eda44d1",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -496,7 +496,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer(\n@@ -661,7 +661,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication"
        },
        {
            "sha": "cdeb7823ad4638951f2569849612d88aa1552a00",
            "filename": "src/transformers/models/vipllava/configuration_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -94,9 +94,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(self.vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"clip_vision_model\")\n             self.vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             self.vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n@@ -111,7 +109,7 @@ def __init__(\n             )\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"llama\"]()"
        },
        {
            "sha": "4368690d091ac29e21ebd836a505775ac24f84bb",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -658,7 +658,7 @@ def prepare_inputs_for_generation(\n         self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n     ):\n         decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)\n-        decoder_attention_mask = decoder_inputs[\"attention_mask\"] if \"attention_mask\" in decoder_inputs else None\n+        decoder_attention_mask = decoder_inputs.get(\"attention_mask\", None)\n         past_key_values = decoder_inputs.get(\"past_key_values\")\n         input_dict = {\n             \"pixel_values\": None,  # needs to be passed to make Keras.layer.__call__ happy"
        },
        {
            "sha": "8cdd499cdeaae9d1e28d8c8981f7261e7d09564a",
            "filename": "src/transformers/models/voxtral/configuration_voxtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -175,16 +175,14 @@ def __init__(\n         **kwargs,\n     ):\n         if isinstance(audio_config, dict):\n-            audio_config[\"model_type\"] = (\n-                audio_config[\"model_type\"] if \"model_type\" in audio_config else \"voxtral_encoder\"\n-            )\n+            audio_config[\"model_type\"] = audio_config.get(\"model_type\", \"voxtral_encoder\")\n             audio_config = CONFIG_MAPPING[audio_config[\"model_type\"]](**audio_config)\n         elif audio_config is None:\n             audio_config = CONFIG_MAPPING[\"voxtral_encoder\"]()\n         self.audio_config = audio_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](\n                 **{**self._default_text_config_kwargs, **text_config}\n             )"
        },
        {
            "sha": "95236310970de51e3422251459de0bcafa8787b9",
            "filename": "src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -326,7 +326,7 @@ def convert_wav2vec2_checkpoint(\n                 word_delimiter_token=\"|\",\n                 do_lower_case=False,\n             )\n-            return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+            return_attention_mask = config.feat_extract_norm == \"layer\"\n             feature_extractor = Wav2Vec2FeatureExtractor(\n                 feature_size=1,\n                 sampling_rate=16000,"
        },
        {
            "sha": "41a8a872c2b290446b2c04e86dcc751705c9c96d",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -729,7 +729,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer(\n@@ -824,7 +824,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication"
        },
        {
            "sha": "e8f67e2d73cde40d0d61608bb49298b2cf7e6bad",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -518,7 +518,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "1f9ce07f0f9e4fee845b2968c8c2398a0945a843",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -416,7 +416,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "eca851f3a0edde8faf6a9883adb7026ad92485df",
            "filename": "src/transformers/models/wav2vec2_conformer/convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -262,7 +262,7 @@ def convert_wav2vec2_conformer_checkpoint(\n                 word_delimiter_token=\"|\",\n                 do_lower_case=False,\n             )\n-            return_attention_mask = True if config.feat_extract_norm == \"layer\" else False\n+            return_attention_mask = config.feat_extract_norm == \"layer\"\n             feature_extractor = Wav2Vec2FeatureExtractor(\n                 feature_size=1,\n                 sampling_rate=16000,"
        },
        {
            "sha": "459d42f2f3efb90f01e4af71cb3391fb9f43b9f9",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -694,7 +694,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "89048b363d88ba9bb1627fb2dbc9f0ce7fe39c96",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -502,7 +502,7 @@ def forward(\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n \n-            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            skip_the_layer = self.training and dropout_probability < self.config.layerdrop\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = layer("
        },
        {
            "sha": "929ee370fe5aca0fa804912b552a17ce68b56ee3",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -365,9 +365,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,"
        },
        {
            "sha": "2a3665f87fd358ddaee8a722b6b5be25114eda33",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -362,9 +362,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,"
        },
        {
            "sha": "4a716036d4003ce8b63ebe917f2d79849fac780a",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -357,7 +357,7 @@ def prepare_coco_detection_annotation(\n \n     # for conversion to coco api\n     area = np.asarray([obj[\"area\"] for obj in annotations], dtype=np.float32)\n-    iscrowd = np.asarray([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=np.int64)\n+    iscrowd = np.asarray([obj.get(\"iscrowd\", 0) for obj in annotations], dtype=np.int64)\n \n     boxes = [obj[\"bbox\"] for obj in annotations]\n     # guard against no boxes via resizing"
        },
        {
            "sha": "778fa7046f7d5dda67140e42210e36d54f319f75",
            "filename": "src/transformers/onnx/convert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconvert.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -416,7 +416,7 @@ def validate_model_outputs(\n         logger.info(f'\\t- Validating ONNX Model output \"{name}\":')\n \n         # Shape\n-        if not ort_value.shape == ref_value.shape:\n+        if ort_value.shape != ref_value.shape:\n             logger.info(f\"\\t\\t-[x] shape {ort_value.shape} doesn't match {ref_value.shape}\")\n             raise ValueError(\n                 \"Outputs shape doesn't match between reference model and ONNX exported model: \""
        },
        {
            "sha": "2823dc36fcd9d9d714baf544680d2ce190ba6688",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -341,7 +341,7 @@ def preprocess(\n                 raise ValueError(\"If you are using a VisionEncoderDecoderModel, you must provide a feature extractor\")\n \n         words, boxes = None, None\n-        if not self.model_type == ModelType.VisionEncoderDecoder:\n+        if self.model_type != ModelType.VisionEncoderDecoder:\n             if \"word_boxes\" in input:\n                 words = [x[0] for x in input[\"word_boxes\"]]\n                 boxes = [x[1] for x in input[\"word_boxes\"]]"
        },
        {
            "sha": "ee86074a4c5804a890c5d488f35cfa3106b58ca4",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -594,7 +594,7 @@ def postprocess(\n                 # Start: Index of the first character of the answer in the context string\n                 # End: Index of the character following the last character of the answer in the context string\n                 # Answer: Plain text of the answer\n-                question_first = bool(self.tokenizer.padding_side == \"right\")\n+                question_first = self.tokenizer.padding_side == \"right\"\n                 enc = output[\"encoding\"]\n \n                 # Encoding was *not* padded, input_ids *might*."
        },
        {
            "sha": "2ff8982b8350aa0eb26a961d9733b6a446deeda3",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -143,8 +143,8 @@ def __init__(self, args_parser=TableQuestionAnsweringArgumentHandler(), *args, *\n             mapping.update(MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES)\n         self.check_model_type(mapping)\n \n-        self.aggregate = bool(getattr(self.model.config, \"aggregation_labels\", None)) and bool(\n-            getattr(self.model.config, \"num_aggregation_labels\", None)\n+        self.aggregate = getattr(self.model.config, \"aggregation_labels\", None) and getattr(\n+            self.model.config, \"num_aggregation_labels\", None\n         )\n         self.type = \"tapas\" if hasattr(self.model.config, \"aggregation_labels\") else None\n "
        },
        {
            "sha": "22faadedddbfc41e1e642d2990a96ccc4bfa9f90",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -280,7 +280,7 @@ def __call__(\n \n     def preprocess(self, sentence, offset_mapping=None, **preprocess_params):\n         tokenizer_params = preprocess_params.pop(\"tokenizer_params\", {})\n-        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n+        truncation = self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0\n \n         word_to_chars_map = None\n         is_split_into_words = preprocess_params[\"is_split_into_words\"]"
        },
        {
            "sha": "a807e65e7ea6690ce2f394ba8673604061e6bfe8",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -1107,7 +1107,7 @@ def from_args_and_dict(cls, args, processor_dict: dict[str, Any], **kwargs):\n             for i, arg in enumerate(accepted_args_and_kwargs)\n             if (arg in valid_kwargs and i < len(args))\n         }\n-        args = [arg if i not in args_to_update else args_to_update[i] for i, arg in enumerate(args)]\n+        args = [args_to_update.get(i, arg) for i, arg in enumerate(args)]\n \n         # instantiate processor with used (and valid) kwargs only\n         processor = cls(*args, **valid_kwargs)"
        },
        {
            "sha": "36e57c0713e37bba773095873b8d1e2a1d14baf6",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -3465,7 +3465,7 @@ def prepare_for_model(\n             **kwargs,\n         )\n \n-        pair = bool(pair_ids is not None)\n+        pair = pair_ids is not None\n         len_ids = len(ids)\n         len_pair_ids = len(pair_ids) if pair else 0\n "
        },
        {
            "sha": "87a0e2b94a6526b9277123405e4c35cd209921d7",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -615,7 +615,7 @@ def __init__(\n         # Bnb Quantized models doesn't support `.to` operation.\n         if (\n             self.place_model_on_device\n-            and not getattr(model, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES\n+            and getattr(model, \"quantization_method\", None) != QuantizationMethod.BITS_AND_BYTES\n         ):\n             self._move_model_to_device(model, args.device)\n \n@@ -2363,7 +2363,7 @@ def _inner_training_loop(\n         # as the model is wrapped, don't use `accelerator.prepare`\n         # this is for unhandled cases such as\n         # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n-        use_accelerator_prepare = True if model is self.model else False\n+        use_accelerator_prepare = model is self.model\n \n         if use_accelerator_prepare and self.is_fsdp_enabled:\n             # In case of auto_find_batch_size=True\n@@ -4621,7 +4621,7 @@ def prediction_step(\n         return_loss = inputs.get(\"return_loss\")\n         if return_loss is None:\n             return_loss = self.can_return_loss\n-        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n+        loss_without_labels = len(self.label_names) == 0 and return_loss\n \n         inputs = self._prepare_inputs(inputs)\n         if ignore_keys is None:"
        },
        {
            "sha": "fb0cc33dce2e3707858eda21b115f13dea478bcc",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -894,7 +894,7 @@ def check_target_module_exists(optim_target_modules, key: str, return_is_regex:\n \n     if isinstance(optim_target_modules, str):\n         target_module_found = bool(re.fullmatch(optim_target_modules, key))\n-        is_regex = True if not optim_target_modules == key else False\n+        is_regex = optim_target_modules != key\n     elif key in optim_target_modules:  # from here, target_module_found must be a list of str\n         # this module is specified directly in target_modules\n         target_module_found = True"
        },
        {
            "sha": "2989f0230c0640db19cf329c8eb16f24374cbfc0",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -413,11 +413,7 @@ def __post_init__(self):\n             # set the associated fields\n             if first_field_iterator:\n                 for idx, element in enumerate(iterator):\n-                    if (\n-                        not isinstance(element, (list, tuple))\n-                        or not len(element) == 2\n-                        or not isinstance(element[0], str)\n-                    ):\n+                    if not isinstance(element, (list, tuple)) or len(element) != 2 or not isinstance(element[0], str):\n                         if idx == 0:\n                             # If we do not have an iterator of key/values, set it as attribute\n                             self[class_fields[0].name] = first_field"
        },
        {
            "sha": "ac3eba924c24ff6c5ebe03b3ac0384be3823d539",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -204,7 +204,7 @@ def define_sagemaker_information():\n         dlc_tag = None\n \n     sagemaker_params = json.loads(os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\"))\n-    runs_distributed_training = True if \"sagemaker_distributed_dataparallel_enabled\" in sagemaker_params else False\n+    runs_distributed_training = \"sagemaker_distributed_dataparallel_enabled\" in sagemaker_params\n     account_id = os.getenv(\"TRAINING_JOB_ARN\").split(\":\")[4] if \"TRAINING_JOB_ARN\" in os.environ else None\n \n     sagemaker_object = {"
        },
        {
            "sha": "eaed3228ec191d65c061327e529d24268beef500",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -101,7 +101,7 @@ def _configure_library_root_logger() -> None:\n             _default_handler.setFormatter(formatter)\n \n         is_ci = os.getenv(\"CI\") is not None and os.getenv(\"CI\").upper() in {\"1\", \"ON\", \"YES\", \"TRUE\"}\n-        library_root_logger.propagate = True if is_ci else False\n+        library_root_logger.propagate = is_ci\n \n \n def _reset_library_root_logger() -> None:"
        },
        {
            "sha": "397aa3e3ff044dbfdb869ff1166cf45f1861216f",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -253,7 +253,7 @@ def write_line(self, values):\n                 first_column = self.inner_table[0][0]\n                 if last_values[0] != values[first_column]:\n                     # write new line\n-                    self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\n+                    self.inner_table.append([values.get(c, \"No Log\") for c in columns])\n                 else:\n                     # update last line\n                     new_values = values"
        },
        {
            "sha": "3527e1d6b8cf60ed4033037831d2881017079e6c",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -147,7 +147,7 @@ class DepthAnythingModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torch_exportable = True\n-    test_torch_exportable_strictly = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = DepthAnythingModelTester(self)"
        },
        {
            "sha": "165da4be6be50365da8855c2801ddce5fff95880",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -141,7 +141,7 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torch_exportable = True\n-    test_torch_exportable_strictly = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = DPTModelTester(self)"
        },
        {
            "sha": "8a79e9afa8a651c5cfde6c14d3e611dbb785e492",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -371,7 +371,7 @@ def check_hidden_states_output(inputs_dict, config, model_class, image_size):\n                 [num_patches, self.model_tester.embed_dim],\n             )\n \n-            if not model_class.__name__ == \"HieraBackbone\":\n+            if model_class.__name__ != \"HieraBackbone\":\n                 reshaped_hidden_states = outputs.reshaped_hidden_states\n                 self.assertEqual(len(reshaped_hidden_states), expected_num_layers)\n "
        },
        {
            "sha": "a7cdb6a8de3b8f510defa4d2093f03cb3621a54d",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -382,7 +382,7 @@ def check_hidden_states_output(self, inputs_dict, config, model_class, image_siz\n             [num_patches, self.model_tester.embed_dim],\n         )\n \n-        if not model_class.__name__ == \"SwinBackbone\":\n+        if model_class.__name__ != \"SwinBackbone\":\n             reshaped_hidden_states = outputs.reshaped_hidden_states\n             self.assertEqual(len(reshaped_hidden_states), expected_num_layers)\n "
        },
        {
            "sha": "6c4e648a7493093a5dc8126a6a2c37b9a355d33c",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -365,7 +365,7 @@ def check_hidden_states_output(self, inputs_dict, config, model_class, image_siz\n             [num_patches, self.model_tester.embed_dim],\n         )\n \n-        if not model_class.__name__ == \"Swinv2Backbone\":\n+        if model_class.__name__ != \"Swinv2Backbone\":\n             reshaped_hidden_states = outputs.reshaped_hidden_states\n             self.assertEqual(len(reshaped_hidden_states), expected_num_layers)\n "
        },
        {
            "sha": "084b03317bae31c325418c912e52d0b8565ae252",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -156,7 +156,7 @@ class UperNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_torchscript = False\n     has_attentions = False\n     test_torch_exportable = True\n-    test_torch_exportable_strictly = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = UperNetModelTester(self)"
        },
        {
            "sha": "10c36a2dd86fab3bbcad937345751c9903e80943",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -144,7 +144,7 @@ class VitMatteModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torch_exportable = True\n-    test_torch_exportable_strictly = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = VitMatteModelTester(self)"
        },
        {
            "sha": "d7782915cffa6bcb0b3bff64f28adaca5ca9a0ff",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -155,7 +155,7 @@ class VitPoseModelTest(ModelTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torch_exportable = True\n-    test_torch_exportable_strictly = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = VitPoseModelTester(self)"
        },
        {
            "sha": "5fcf0e9a2f7f3a06320340a6543d15190d9970cc",
            "filename": "tests/models/zoedepth/test_modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -148,7 +148,7 @@ class ZoeDepthModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_resize_embeddings = False\n     test_head_masking = False\n     # `strict=True/False` are both failing with torch 2.7, see #38677\n-    test_torch_exportable = not get_torch_major_and_minor_version() == \"2.7\"\n+    test_torch_exportable = get_torch_major_and_minor_version() != \"2.7\"\n \n     def setUp(self):\n         self.model_tester = ZoeDepthModelTester(self)"
        },
        {
            "sha": "d32a42b747d0197b4b3878892ec679681322d3aa",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -1193,7 +1193,7 @@ def check_model_type_doc_match():\n     model_docs = [m.stem for m in model_doc_folder.glob(\"*.md\")]\n \n     model_types = list(transformers.models.auto.configuration_auto.MODEL_NAMES_MAPPING.keys())\n-    model_types = [MODEL_TYPE_TO_DOC_MAPPING[m] if m in MODEL_TYPE_TO_DOC_MAPPING else m for m in model_types]\n+    model_types = [MODEL_TYPE_TO_DOC_MAPPING.get(m, m) for m in model_types]\n \n     errors = []\n     for m in model_docs:"
        },
        {
            "sha": "f7dbbec907224e8b549496da4cc38b1f5c2020bd",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -1059,7 +1059,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     runner_not_available = False\n     runner_failed = False\n     # Some jobs don't depend (`needs`) on the job `setup`: in this case, the status of the job `setup` is `skipped`.\n-    setup_failed = False if setup_status in [\"skipped\", \"success\"] else True\n+    setup_failed = setup_status not in [\"skipped\", \"success\"]\n \n     org = \"huggingface\"\n     repo = \"transformers\""
        },
        {
            "sha": "f8f7b8c610a0bebdce7507d08579734d04e5a8ee",
            "filename": "utils/notification_service_doc_tests.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fnotification_service_doc_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Fnotification_service_doc_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_doc_tests.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -371,7 +371,7 @@ def add_path(self, path: str):\n                         file_path, test = line, line\n \n                     job_result[\"failed\"].append(test)\n-                    failure = all_failures[test] if test in all_failures else \"N/A\"\n+                    failure = all_failures.get(test, \"N/A\")\n                     job_result[\"failures\"][test] = failure\n \n     # Save and to be uploaded as artifact"
        },
        {
            "sha": "aae78cabe608b8e898a8085202c1a64f544cbb2c",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e0665a191f73f6b002209c3dfcda478baac6bac/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=1e0665a191f73f6b002209c3dfcda478baac6bac",
            "patch": "@@ -1057,9 +1057,9 @@ def infer_tests_to_run(\n                 test_files_to_run.extend(test_map[f])\n         test_files_to_run = sorted(set(test_files_to_run))\n         # Remove repo utils tests\n-        test_files_to_run = [f for f in test_files_to_run if not f.split(os.path.sep)[1] == \"repo_utils\"]\n+        test_files_to_run = [f for f in test_files_to_run if f.split(os.path.sep)[1] != \"repo_utils\"]\n         # Remove SageMaker tests\n-        test_files_to_run = [f for f in test_files_to_run if not f.split(os.path.sep)[1] == \"sagemaker\"]\n+        test_files_to_run = [f for f in test_files_to_run if f.split(os.path.sep)[1] != \"sagemaker\"]\n         # Make sure we did not end up with a test file that was removed\n         test_files_to_run = [f for f in test_files_to_run if (PATH_TO_REPO / f).exists()]\n "
        }
    ],
    "stats": {
        "total": 526,
        "additions": 229,
        "deletions": 297
    }
}