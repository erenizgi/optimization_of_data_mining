{
    "author": "qubvel",
    "message": "Better pipeline type hints âœ¨ (#38049)\n\n* image-classification\n\n* depth-estimation\n\n* zero-shot-image-classification\n\n* image-feature-extraction\n\n* image-segmentation\n\n* mask-generation\n\n* object-detection\n\n* zero-shot-object-detection\n\n* image-to-image\n\n* image-text-to-text\n\n* image-to-text\n\n* text-classification\n\n* text-generation\n\n* text-to-audio\n\n* text2text_generation\n\n* fixup\n\n* token-classification\n\n* document-qa\n\n* video-classification\n\n* audio-classification\n\n* automatic-speech-recognition\n\n* feature-extraction\n\n* fill-mask\n\n* zero-shot-audio-classification\n\n* Add pipeline function typing\n\n* Add code generator and checker for pipeline types\n\n* Add to makefile\n\n* style\n\n* Add to CI\n\n* Style",
    "sha": "b3b7789cbcce1fa160551d344c03b92c03242c31",
    "files": [
        {
            "sha": "5616355415b4a2e79ae01a2f16b6939b841bb970",
            "filename": ".circleci/config.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/.circleci%2Fconfig.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/.circleci%2Fconfig.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fconfig.yml?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -184,6 +184,7 @@ jobs:\n             - run: python utils/check_dummies.py\n             - run: python utils/check_repo.py\n             - run: python utils/check_inits.py\n+            - run: python utils/check_pipeline_typing.py\n             - run: python utils/check_config_docstrings.py\n             - run: python utils/check_config_attributes.py\n             - run: python utils/check_doctest_list.py"
        },
        {
            "sha": "bfc69d2bba5f988e506eb7bc0f3ef9869b7f2dce",
            "filename": "Makefile",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -40,6 +40,7 @@ repo-consistency:\n \tpython utils/check_dummies.py\n \tpython utils/check_repo.py\n \tpython utils/check_inits.py\n+\tpython utils/check_pipeline_typing.py\n \tpython utils/check_config_docstrings.py\n \tpython utils/check_config_attributes.py\n \tpython utils/check_doctest_list.py\n@@ -81,6 +82,7 @@ fix-copies:\n \tpython utils/check_copies.py --fix_and_overwrite\n \tpython utils/check_modular_conversion.py --fix_and_overwrite\n \tpython utils/check_dummies.py --fix_and_overwrite\n+\tpython utils/check_pipeline_typing.py --fix_and_overwrite\n \tpython utils/check_doctest_list.py --fix_and_overwrite\n \tpython utils/check_docstrings.py --fix_and_overwrite\n "
        },
        {
            "sha": "93a84f3bf61471cc09da5a1b9cbf25805e80b3f8",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 83,
            "deletions": 3,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -564,6 +564,86 @@ def clean_custom_task(task_info):\n     return task_info, None\n \n \n+# <generated-code>\n+# fmt: off\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                       The part of the file below was automatically generated from the code.\n+#           Do NOT edit this part of the file manually as any edits will be overwritten by the generation\n+#           of the file. If any change should be done, please apply the changes to the `pipeline` function\n+#            below and run `python utils/check_pipeline_typing.py --fix_and_overwrite` to update the file.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+\n+from typing import Literal, overload\n+\n+\n+@overload\n+def pipeline(task: Literal[None], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> Pipeline: ...\n+@overload\n+def pipeline(task: Literal[\"audio-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> AudioClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"automatic-speech-recognition\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> AutomaticSpeechRecognitionPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"depth-estimation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> DepthEstimationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"document-question-answering\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> DocumentQuestionAnsweringPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"feature-extraction\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> FeatureExtractionPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"fill-mask\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> FillMaskPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-feature-extraction\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageFeatureExtractionPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-segmentation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageSegmentationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-text-to-text\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageTextToTextPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-to-image\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageToImagePipeline: ...\n+@overload\n+def pipeline(task: Literal[\"image-to-text\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ImageToTextPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"mask-generation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> MaskGenerationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"object-detection\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ObjectDetectionPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"question-answering\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> QuestionAnsweringPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"summarization\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> SummarizationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"table-question-answering\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TableQuestionAnsweringPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"text-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TextClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"text-generation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TextGenerationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"text-to-audio\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TextToAudioPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"text2text-generation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> Text2TextGenerationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"token-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TokenClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"translation\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> TranslationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"video-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> VideoClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"visual-question-answering\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> VisualQuestionAnsweringPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"zero-shot-audio-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ZeroShotAudioClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"zero-shot-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ZeroShotClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"zero-shot-image-classification\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ZeroShotImageClassificationPipeline: ...\n+@overload\n+def pipeline(task: Literal[\"zero-shot-object-detection\"], model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None, config: Optional[Union[str, PretrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None, torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> ZeroShotObjectDetectionPipeline: ...\n+\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                       The part of the file above was automatically generated from the code.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# fmt: on\n+# </generated-code>\n+\n+\n def pipeline(\n     task: Optional[str] = None,\n     model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None,\n@@ -577,12 +657,12 @@ def pipeline(\n     use_fast: bool = True,\n     token: Optional[Union[str, bool]] = None,\n     device: Optional[Union[int, str, \"torch.device\"]] = None,\n-    device_map=None,\n-    torch_dtype=None,\n+    device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = None,\n+    torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n     trust_remote_code: Optional[bool] = None,\n     model_kwargs: Optional[Dict[str, Any]] = None,\n     pipeline_class: Optional[Any] = None,\n-    **kwargs,\n+    **kwargs: Any,\n ) -> Pipeline:\n     \"\"\"\n     Utility factory method to build a [`Pipeline`]."
        },
        {
            "sha": "62813fbb5e1dff3a0a1897cd750ab41e04712da0",
            "filename": "src/transformers/pipelines/audio_classification.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import subprocess\n-from typing import Union\n+from typing import Any, Dict, List, Union\n \n import numpy as np\n import requests\n@@ -27,7 +27,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def ffmpeg_read(bpayload: bytes, sampling_rate: int) -> np.array:\n+def ffmpeg_read(bpayload: bytes, sampling_rate: int) -> np.ndarray:\n     \"\"\"\n     Helper function to read an audio file through ffmpeg.\n     \"\"\"\n@@ -103,11 +103,7 @@ def __init__(self, *args, **kwargs):\n \n         self.check_model_type(MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES)\n \n-    def __call__(\n-        self,\n-        inputs: Union[np.ndarray, bytes, str],\n-        **kwargs,\n-    ):\n+    def __call__(self, inputs: Union[np.ndarray, bytes, str, dict], **kwargs: Any) -> List[Dict[str, Any]]:\n         \"\"\"\n         Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\n         information."
        },
        {
            "sha": "5fa177fe2ba97085e49f3b1c8745e576758075c1",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Dict, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n import numpy as np\n import requests\n@@ -211,11 +211,7 @@ def __init__(\n \n         super().__init__(model, tokenizer, feature_extractor, device=device, torch_dtype=torch_dtype, **kwargs)\n \n-    def __call__(\n-        self,\n-        inputs: Union[np.ndarray, bytes, str],\n-        **kwargs,\n-    ):\n+    def __call__(self, inputs: Union[np.ndarray, bytes, str, dict], **kwargs: Any) -> List[Dict[str, Any]]:\n         \"\"\"\n         Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\n         documentation for more information."
        },
        {
            "sha": "235daf2cc2c79aa5ba8dc1ed556765d9d26426de",
            "filename": "src/transformers/pipelines/depth_estimation.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import List, Union\n+from typing import Any, Dict, List, Union, overload\n \n from ..utils import (\n     add_end_docstrings,\n@@ -52,7 +52,15 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, \"vision\")\n         self.check_model_type(MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES)\n \n-    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n+    @overload\n+    def __call__(self, inputs: Union[str, \"Image.Image\"], **kwargs: Any) -> Dict[str, Any]: ...\n+\n+    @overload\n+    def __call__(self, inputs: List[Union[str, \"Image.Image\"]], **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    def __call__(\n+        self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs: Any\n+    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n         \"\"\"\n         Predict the depth(s) of the image(s) passed as inputs.\n "
        },
        {
            "sha": "2d70a3534df4368918398a6112745b897f4f757a",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n import re\n-from typing import List, Optional, Tuple, Union\n+from typing import Any, Dict, List, Optional, Tuple, Union, overload\n \n import numpy as np\n \n@@ -209,13 +209,28 @@ def _sanitize_parameters(\n \n         return preprocess_params, forward_params, postprocess_params\n \n+    @overload\n     def __call__(\n         self,\n         image: Union[\"Image.Image\", str],\n+        question: str,\n+        word_boxes: Optional[Tuple[str, List[float]]] = None,\n+        **kwargs: Any,\n+    ) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, image: Dict[str, Any], **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, image: List[Dict[str, Any]], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self,\n+        image: Union[\"Image.Image\", str, List[Dict[str, Any]]],\n         question: Optional[str] = None,\n         word_boxes: Optional[Tuple[str, List[float]]] = None,\n-        **kwargs,\n-    ):\n+        **kwargs: Any,\n+    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n         \"\"\"\n         Answer the question(s) given as inputs by using the document(s). A document is defined as an image and an\n         optional list of (word, box) tuples which represent the text in the document. If the `word_boxes` are not"
        },
        {
            "sha": "70971d83b2064707e8cd1d44e68f7ead67aa33bc",
            "filename": "src/transformers/pipelines/feature_extraction.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ffeature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ffeature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ffeature_extraction.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Dict\n+from typing import Any, Dict, List, Union\n \n from ..utils import add_end_docstrings\n from .base import GenericTensor, Pipeline, build_pipeline_init_args\n@@ -73,9 +73,9 @@ def postprocess(self, model_outputs, return_tensors=False):\n         elif self.framework == \"tf\":\n             return model_outputs[0].numpy().tolist()\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(self, *args: Union[str, List[str]], **kwargs: Any) -> Union[Any, List[Any]]:\n         \"\"\"\n-        Extract the features of the input(s).\n+        Extract the features of the input(s) text.\n \n         Args:\n             args (`str` or `List[str]`): One or several texts (or one list of texts) to get the features of."
        },
        {
            "sha": "18c225b1c5afbb277e994355cd33a87e777cba46",
            "filename": "src/transformers/pipelines/fill_mask.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ffill_mask.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Dict\n+from typing import Any, Dict, List, Union, overload\n \n import numpy as np\n \n@@ -245,7 +245,15 @@ def _sanitize_parameters(self, top_k=None, targets=None, tokenizer_kwargs=None):\n             )\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, inputs, **kwargs):\n+    @overload\n+    def __call__(self, inputs: str, **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: List[str], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self, inputs: Union[str, List[str]], **kwargs: Any\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Fill the masked token in the text(s) given as inputs.\n "
        },
        {
            "sha": "bd179a5a66752495975dba674b7fc9b4b8d1d5be",
            "filename": "src/transformers/pipelines/image_classification.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Union\n+from typing import Any, Dict, List, Union, overload\n \n import numpy as np\n \n@@ -122,7 +122,15 @@ def _sanitize_parameters(self, top_k=None, function_to_apply=None, timeout=None)\n             postprocess_params[\"function_to_apply\"] = function_to_apply\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n+    @overload\n+    def __call__(self, inputs: Union[str, \"Image.Image\"], **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: Union[List[str], List[\"Image.Image\"]], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs: Any\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n "
        },
        {
            "sha": "ef8d6e6a00ac4e591e0213ea8f65157c0647ba58",
            "filename": "src/transformers/pipelines/image_feature_extraction.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,10 +1,12 @@\n-from typing import Dict\n+from typing import Any, Dict, List, Union\n \n from ..utils import add_end_docstrings, is_vision_available\n from .base import GenericTensor, Pipeline, build_pipeline_init_args\n \n \n if is_vision_available():\n+    from PIL import Image\n+\n     from ..image_utils import load_image\n \n \n@@ -88,7 +90,7 @@ def postprocess(self, model_outputs, pool=None, return_tensors=False):\n         elif self.framework == \"tf\":\n             return outputs.numpy().tolist()\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(self, *args: Union[str, \"Image.Image\", List[\"Image.Image\"], List[str]], **kwargs: Any) -> List[Any]:\n         \"\"\"\n         Extract the features of the input(s).\n "
        },
        {
            "sha": "ef596e2f5339370340aca3eec386009186c1c2d5",
            "filename": "src/transformers/pipelines/image_segmentation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_segmentation.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Union, overload\n \n import numpy as np\n \n@@ -23,10 +23,6 @@\n logger = logging.get_logger(__name__)\n \n \n-Prediction = Dict[str, Any]\n-Predictions = List[Prediction]\n-\n-\n @add_end_docstrings(build_pipeline_init_args(has_image_processor=True))\n class ImageSegmentationPipeline(Pipeline):\n     \"\"\"\n@@ -94,7 +90,15 @@ def _sanitize_parameters(self, **kwargs):\n \n         return preprocess_kwargs, {}, postprocess_kwargs\n \n-    def __call__(self, inputs=None, **kwargs) -> Union[Predictions, List[Prediction]]:\n+    @overload\n+    def __call__(self, inputs: Union[str, \"Image.Image\"], **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: Union[List[str], List[\"Image.Image\"]], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self, inputs: Union[str, \"Image.Image\", List[str], List[\"Image.Image\"]], **kwargs: Any\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Perform segmentation (detect masks & classes) in the image(s) passed as inputs.\n \n@@ -123,9 +127,8 @@ def __call__(self, inputs=None, **kwargs) -> Union[Predictions, List[Prediction]\n                 the call may block forever.\n \n         Return:\n-            A dictionary or a list of dictionaries containing the result. If the input is a single image, will return a\n-            list of dictionaries, if the input is a list of several images, will return a list of list of dictionaries\n-            corresponding to each image.\n+            If the input is a single image, will return a list of dictionaries, if the input is a list of several images,\n+            will return a list of list of dictionaries corresponding to each image.\n \n             The dictionaries contain the mask, label and score (where applicable) of each detected object and contains\n             the following keys:"
        },
        {
            "sha": "23f914d48c11352ff69455802cce42c593f3d63c",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -15,7 +15,7 @@\n \n import enum\n from collections.abc import Iterable  # pylint: disable=g-importing-member\n-from typing import Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union, overload\n \n from ..generation import GenerationConfig\n from ..processing_utils import ProcessingKwargs, Unpack\n@@ -251,6 +251,22 @@ def _sanitize_parameters(\n             generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n         return preprocess_params, forward_kwargs, postprocess_params\n \n+    @overload\n+    def __call__(\n+        self,\n+        image: Optional[Union[str, \"Image.Image\"]] = None,\n+        text: Optional[str] = None,\n+        **kwargs: Any,\n+    ) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(\n+        self,\n+        image: Optional[Union[List[str], List[\"Image.Image\"]]] = None,\n+        text: Optional[List[str]] = None,\n+        **kwargs: Any,\n+    ) -> List[List[Dict[str, Any]]]: ...\n+\n     def __call__(\n         self,\n         images: Optional[\n@@ -266,7 +282,7 @@ def __call__(\n         ] = None,\n         text: Optional[Union[str, List[str], List[dict]]] = None,\n         **kwargs,\n-    ):\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Generate a text given text and the image(s) passed as inputs.\n "
        },
        {
            "sha": "23fe98ef9e91fef6e6f5b4277e11bf4cab0f23e9",
            "filename": "src/transformers/pipelines/image_to_image.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Union\n+from typing import Any, List, Union, overload\n \n import numpy as np\n \n@@ -84,8 +84,14 @@ def _sanitize_parameters(self, **kwargs):\n \n         return preprocess_params, forward_params, postprocess_params\n \n+    @overload\n+    def __call__(self, images: Union[str, \"Image.Image\"], **kwargs: Any) -> \"Image.Image\": ...\n+\n+    @overload\n+    def __call__(self, images: Union[List[str], List[\"Image.Image\"]], **kwargs: Any) -> List[\"Image.Image\"]: ...\n+\n     def __call__(\n-        self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs\n+        self, images: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs: Any\n     ) -> Union[\"Image.Image\", List[\"Image.Image\"]]:\n         \"\"\"\n         Transform the image(s) passed as inputs."
        },
        {
            "sha": "62942fb96d883d06794760f62f0e393068069b7e",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import List, Union\n+from typing import Any, Dict, List, Union, overload\n \n from ..generation import GenerationConfig\n from ..utils import (\n@@ -111,7 +111,13 @@ def _sanitize_parameters(self, max_new_tokens=None, generate_kwargs=None, prompt\n \n         return preprocess_params, forward_params, {}\n \n-    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):\n+    @overload\n+    def __call__(self, inputs: Union[str, \"Image.Image\"], **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: Union[List[str], List[\"Image.Image\"]], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]], **kwargs):\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n "
        },
        {
            "sha": "5f32b8df8ade665b31710e9a1e1a0400e26e0635",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,5 +1,5 @@\n from collections import defaultdict\n-from typing import Optional\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union, overload\n \n from ..image_utils import load_image\n from ..utils import (\n@@ -16,6 +16,9 @@\n \n     from ..models.auto.modeling_auto import MODEL_FOR_MASK_GENERATION_MAPPING_NAMES\n \n+if TYPE_CHECKING:\n+    from PIL import Image\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -125,12 +128,22 @@ def _sanitize_parameters(self, **kwargs):\n             postprocess_kwargs[\"output_bboxes_mask\"] = kwargs[\"output_bboxes_mask\"]\n         return preprocess_kwargs, forward_params, postprocess_kwargs\n \n-    def __call__(self, image, *args, num_workers=None, batch_size=None, **kwargs):\n+    @overload\n+    def __call__(self, image: Union[str, \"Image.Image\"], *args: Any, **kwargs: Any) -> Dict[str, Any]: ...\n+\n+    @overload\n+    def __call__(\n+        self, image: Union[List[str], List[\"Image.Image\"]], *args: Any, **kwargs: Any\n+    ) -> List[Dict[str, Any]]: ...\n+\n+    def __call__(\n+        self, image: Union[str, \"Image.Image\", List[str], List[\"Image.Image\"]], *args: Any, **kwargs: Any\n+    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n         \"\"\"\n         Generates binary segmentation masks\n \n         Args:\n-            inputs (`np.ndarray` or `bytes` or `str` or `dict`):\n+            image (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n                 Image or list of images.\n             mask_threshold (`float`, *optional*, defaults to 0.0):\n                 Threshold to use when turning the predicted masks into binary values.\n@@ -163,6 +176,8 @@ def __call__(self, image, *args, num_workers=None, batch_size=None, **kwargs):\n                   the \"object\" described by the label and the mask.\n \n         \"\"\"\n+        num_workers = kwargs.pop(\"num_workers\", None)\n+        batch_size = kwargs.pop(\"batch_size\", None)\n         return super().__call__(image, *args, num_workers=num_workers, batch_size=batch_size, **kwargs)\n \n     def preprocess("
        },
        {
            "sha": "3b67ee49a083fcbbda0e8348065b7ca530cf98df",
            "filename": "src/transformers/pipelines/object_detection.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fobject_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fobject_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fobject_detection.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Any, Dict, List, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Union, overload\n \n from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging, requires_backends\n from .base import Pipeline, build_pipeline_init_args\n@@ -16,11 +16,10 @@\n         MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,\n     )\n \n-logger = logging.get_logger(__name__)\n-\n+if TYPE_CHECKING:\n+    from PIL import Image\n \n-Prediction = Dict[str, Any]\n-Predictions = List[Prediction]\n+logger = logging.get_logger(__name__)\n \n \n @add_end_docstrings(build_pipeline_init_args(has_image_processor=True))\n@@ -69,7 +68,15 @@ def _sanitize_parameters(self, **kwargs):\n             postprocess_kwargs[\"threshold\"] = kwargs[\"threshold\"]\n         return preprocess_params, {}, postprocess_kwargs\n \n-    def __call__(self, *args, **kwargs) -> Union[Predictions, List[Prediction]]:\n+    @overload\n+    def __call__(self, image: Union[str, \"Image.Image\"], *args: Any, **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(\n+        self, image: Union[List[str], List[\"Image.Image\"]], *args: Any, **kwargs: Any\n+    ) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(self, *args, **kwargs) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Detect objects (bounding boxes & classes) in the image(s) passed as inputs.\n "
        },
        {
            "sha": "7bd2e47195e7a30be1e5aa492fccd3379b328950",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,5 +1,6 @@\n import enum\n import warnings\n+from typing import Any, Dict, List, Union\n \n from ..generation import GenerationConfig\n from ..tokenization_utils import TruncationStrategy\n@@ -154,7 +155,7 @@ def _parse_and_tokenize(self, *args, truncation):\n             del inputs[\"token_type_ids\"]\n         return inputs\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(self, *args: Union[str, List[str]], **kwargs: Any) -> List[Dict[str, str]]:\n         r\"\"\"\n         Generate the output text(s) using text(s) given as inputs.\n "
        },
        {
            "sha": "a50174e8c954a7478a4b57a0d87ec84669b17fd6",
            "filename": "src/transformers/pipelines/text_classification.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,6 +1,6 @@\n import inspect\n import warnings\n-from typing import Dict\n+from typing import Any, Dict, List, Union\n \n import numpy as np\n \n@@ -120,7 +120,11 @@ def _sanitize_parameters(self, return_all_scores=None, function_to_apply=None, t\n             postprocess_params[\"function_to_apply\"] = function_to_apply\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, inputs, **kwargs):\n+    def __call__(\n+        self,\n+        inputs: Union[str, List[str], Dict[str, str], List[Dict[str, str]]],\n+        **kwargs: Any,\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"\n         Classify the text(s) given as inputs.\n \n@@ -148,7 +152,7 @@ def __call__(self, inputs, **kwargs):\n                 - `\"none\"`: Does not apply any function on the output.\n \n         Return:\n-            A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n+            A list of `dict`: Each result comes as list of dictionaries with the following keys:\n \n             - **label** (`str`) -- The label predicted.\n             - **score** (`float`) -- The corresponding probability."
        },
        {
            "sha": "e9812a26af8e44220f0d9f8979920c3ddcdef546",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,7 +1,7 @@\n import enum\n import itertools\n import types\n-from typing import Dict\n+from typing import Any, Dict, List, overload\n \n from ..generation import GenerationConfig\n from ..utils import ModelOutput, add_end_docstrings, is_tf_available, is_torch_available\n@@ -19,6 +19,8 @@\n \n     from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n \n+ChatType = List[Dict[str, str]]\n+\n \n class ReturnType(enum.Enum):\n     TENSORS = 0\n@@ -231,6 +233,18 @@ def _parse_and_tokenize(self, *args, **kwargs):\n \n         return super()._parse_and_tokenize(*args, **kwargs)\n \n+    @overload\n+    def __call__(self, text_inputs: str, **kwargs: Any) -> List[Dict[str, str]]: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: List[str], **kwargs: Any) -> List[List[Dict[str, str]]]: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: ChatType, **kwargs: Any) -> List[Dict[str, ChatType]]: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: List[ChatType], **kwargs: Any) -> List[List[Dict[str, ChatType]]]: ...\n+\n     def __call__(self, text_inputs, **kwargs):\n         \"\"\"\n         Complete the prompt(s) given as inputs."
        },
        {
            "sha": "e9ca2a74a6dc24efd2c8c5b4be85714f9fead9ad",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.from typing import List, Union\n-from typing import List, Union\n+from typing import Any, Dict, List, Union, overload\n \n from ..generation import GenerationConfig\n from ..utils import is_torch_available\n@@ -173,7 +173,15 @@ def _forward(self, model_inputs, **kwargs):\n \n         return output\n \n-    def __call__(self, text_inputs: Union[str, List[str]], **forward_params):\n+    @overload\n+    def __call__(self, text_inputs: str, **forward_params: Any) -> Dict[str, Any]: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: List[str], **forward_params: Any) -> List[Dict[str, Any]]: ...\n+\n+    def __call__(\n+        self, text_inputs: Union[str, List[str]], **forward_params\n+    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n         \"\"\"\n         Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\n "
        },
        {
            "sha": "e8fded5530c86416d7625e5b3bcedbc1e8bcc577",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,6 +1,6 @@\n import types\n import warnings\n-from typing import List, Optional, Tuple, Union\n+from typing import Any, Dict, List, Optional, Tuple, Union, overload\n \n import numpy as np\n \n@@ -217,7 +217,15 @@ def _sanitize_parameters(\n                     )\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, inputs: Union[str, List[str]], **kwargs):\n+    @overload\n+    def __call__(self, inputs: str, **kwargs: Any) -> List[Dict[str, str]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: List[str], **kwargs: Any) -> List[List[Dict[str, str]]]: ...\n+\n+    def __call__(\n+        self, inputs: Union[str, List[str]], **kwargs: Any\n+    ) -> Union[List[Dict[str, str]], List[List[Dict[str, str]]]]:\n         \"\"\"\n         Classify each token of the text(s) given as inputs.\n "
        },
        {
            "sha": "dffc21fed2d456e9b16187f0ff66055992ff84d1",
            "filename": "src/transformers/pipelines/video_classification.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n import warnings\n from io import BytesIO\n-from typing import List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union, overload\n \n import requests\n \n@@ -77,6 +77,12 @@ def _sanitize_parameters(self, top_k=None, num_frames=None, frame_sampling_rate=\n             postprocess_params[\"function_to_apply\"] = \"softmax\"\n         return preprocess_params, {}, postprocess_params\n \n+    @overload\n+    def __call__(self, inputs: str, **kwargs: Any) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, inputs: List[str], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n     def __call__(self, inputs: Optional[Union[str, List[str]]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the video(s) passed as inputs."
        },
        {
            "sha": "a21e74e20920f70571c35e2dbcec3d3fc607eb0c",
            "filename": "src/transformers/pipelines/zero_shot_audio_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections import UserDict\n-from typing import Union\n+from typing import Any, Dict, List, Union\n \n import numpy as np\n import requests\n@@ -67,7 +67,7 @@ def __init__(self, **kwargs):\n             raise ValueError(f\"The {self.__class__} is only available in PyTorch.\")\n         # No specific FOR_XXX available yet\n \n-    def __call__(self, audios: Union[np.ndarray, bytes, str], **kwargs):\n+    def __call__(self, audios: Union[np.ndarray, bytes, str, dict], **kwargs: Any) -> List[Dict[str, Any]]:\n         \"\"\"\n         Assign labels to the audio(s) passed as inputs.\n "
        },
        {
            "sha": "bae5333735b731e479add3aa4a05de5145038b42",
            "filename": "src/transformers/pipelines/zero_shot_image_classification.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,6 +1,6 @@\n import warnings\n from collections import UserDict\n-from typing import List, Union\n+from typing import Any, Dict, List, Union, overload\n \n from ..utils import (\n     add_end_docstrings,\n@@ -74,7 +74,22 @@ def __init__(self, **kwargs):\n             else MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES\n         )\n \n-    def __call__(self, image: Union[str, List[str], \"Image\", List[\"Image\"]] = None, **kwargs):\n+    @overload\n+    def __call__(\n+        self, image: Union[str, \"Image.Image\"], candidate_labels: List[str], **kwargs: Any\n+    ) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(\n+        self, image: Union[List[str], List[\"Image.Image\"]], candidate_labels: List[str], **kwargs: Any\n+    ) -> List[List[Dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self,\n+        image: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]],\n+        candidate_labels: List[str],\n+        **kwargs: Any,\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Assign labels to the image(s) passed as inputs.\n \n@@ -110,7 +125,7 @@ def __call__(self, image: Union[str, List[str], \"Image\", List[\"Image\"]] = None,\n             image = kwargs.pop(\"images\")\n         if image is None:\n             raise ValueError(\"Cannot call the zero-shot-image-classification pipeline without an images argument!\")\n-        return super().__call__(image, **kwargs)\n+        return super().__call__(image, candidate_labels=candidate_labels, **kwargs)\n \n     def _sanitize_parameters(self, tokenizer_kwargs=None, **kwargs):\n         preprocess_params = {}"
        },
        {
            "sha": "2a92b8d152dc384d098eb2e14965603bbbfa910f",
            "filename": "src/transformers/pipelines/zero_shot_object_detection.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union, overload\n \n from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging, requires_backends\n from .base import ChunkPipeline, build_pipeline_init_args\n@@ -62,12 +62,20 @@ def __init__(self, **kwargs):\n         requires_backends(self, \"vision\")\n         self.check_model_type(MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES)\n \n+    @overload\n+    def __call__(\n+        self, image: Union[str, \"Image.Image\"], candidate_labels: Union[str, List[str]], **kwargs: Any\n+    ) -> List[Dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(self, image: List[Dict[str, Any]], **kwargs: Any) -> List[List[Dict[str, Any]]]: ...\n+\n     def __call__(\n         self,\n         image: Union[str, \"Image.Image\", List[Dict[str, Any]]],\n         candidate_labels: Optional[Union[str, List[str]]] = None,\n-        **kwargs,\n-    ):\n+        **kwargs: Any,\n+    ) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n         \"\"\"\n         Detect objects (bounding boxes & classes) in the image(s) passed as inputs.\n "
        },
        {
            "sha": "16dcf197a52e2256a25ee6c38a5372c6651d0011",
            "filename": "utils/check_pipeline_typing.py",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3b7789cbcce1fa160551d344c03b92c03242c31/utils%2Fcheck_pipeline_typing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3b7789cbcce1fa160551d344c03b92c03242c31/utils%2Fcheck_pipeline_typing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_pipeline_typing.py?ref=b3b7789cbcce1fa160551d344c03b92c03242c31",
            "patch": "@@ -0,0 +1,93 @@\n+import re\n+\n+from transformers.pipelines import SUPPORTED_TASKS, Pipeline\n+\n+\n+HEADER = \"\"\"\n+# fmt: off\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                       The part of the file below was automatically generated from the code.\n+#           Do NOT edit this part of the file manually as any edits will be overwritten by the generation\n+#           of the file. If any change should be done, please apply the changes to the `pipeline` function\n+#            below and run `python utils/check_pipeline_typing.py --fix_and_overwrite` to update the file.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+\n+from typing import Literal, overload\n+\n+\n+\"\"\"\n+\n+FOOTER = \"\"\"\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                       The part of the file above was automatically generated from the code.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# fmt: on\n+\"\"\"\n+\n+TASK_PATTERN = \"task: Optional[str] = None\"\n+\n+\n+def main(pipeline_file_path: str, fix_and_overwrite: bool = False):\n+    with open(pipeline_file_path, \"r\") as file:\n+        content = file.read()\n+\n+    # extract generated code in between <generated-code> and </generated-code>\n+    current_generated_code = re.search(r\"# <generated-code>(.*)# </generated-code>\", content, re.DOTALL).group(1)\n+    content_without_generated_code = content.replace(current_generated_code, \"\")\n+\n+    # extract pipeline signature in between `def pipeline` and `-> Pipeline`\n+    pipeline_signature = re.search(r\"def pipeline(.*) -> Pipeline:\", content_without_generated_code, re.DOTALL).group(\n+        1\n+    )\n+    pipeline_signature = pipeline_signature.replace(\"(\\n    \", \"(\")  # start of the signature\n+    pipeline_signature = pipeline_signature.replace(\",\\n    \", \", \")  # intermediate arguments\n+    pipeline_signature = pipeline_signature.replace(\",\\n)\", \")\")  # end of the signature\n+\n+    # collect and sort available pipelines\n+    pipelines = [(f'\"{task}\"', task_info[\"impl\"]) for task, task_info in SUPPORTED_TASKS.items()]\n+    pipelines = sorted(pipelines, key=lambda x: x[0])\n+    pipelines.insert(0, (None, Pipeline))\n+\n+    # generate new `pipeline` signatures\n+    new_generated_code = \"\"\n+    for task, pipeline_class in pipelines:\n+        if TASK_PATTERN not in pipeline_signature:\n+            raise ValueError(f\"Can't find `{TASK_PATTERN}` in pipeline signature: {pipeline_signature}\")\n+        pipeline_type = pipeline_class if isinstance(pipeline_class, str) else pipeline_class.__name__\n+        new_pipeline_signature = pipeline_signature.replace(TASK_PATTERN, f\"task: Literal[{task}]\")\n+        new_generated_code += f\"@overload\\ndef pipeline{new_pipeline_signature} -> {pipeline_type}: ...\\n\"\n+\n+    new_generated_code = HEADER + new_generated_code + FOOTER\n+    new_generated_code = new_generated_code.rstrip(\"\\n\") + \"\\n\"\n+\n+    if new_generated_code != current_generated_code and fix_and_overwrite:\n+        print(f\"Updating {pipeline_file_path}...\")\n+        wrapped_current_generated_code = \"# <generated-code>\" + current_generated_code + \"# </generated-code>\"\n+        wrapped_new_generated_code = \"# <generated-code>\" + new_generated_code + \"# </generated-code>\"\n+        content = content.replace(wrapped_current_generated_code, wrapped_new_generated_code)\n+\n+        # write content to file\n+        with open(pipeline_file_path, \"w\") as file:\n+            file.write(content)\n+\n+    elif new_generated_code != current_generated_code and not fix_and_overwrite:\n+        message = (\n+            f\"Found inconsistencies in {pipeline_file_path}. \"\n+            \"Run `python utils/check_pipeline_typing.py --fix_and_overwrite` to fix them.\"\n+        )\n+        raise ValueError(message)\n+\n+\n+if __name__ == \"__main__\":\n+    import argparse\n+\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--fix_and_overwrite\", action=\"store_true\", help=\"Whether to fix inconsistencies.\")\n+    parser.add_argument(\n+        \"--pipeline_file_path\",\n+        type=str,\n+        default=\"src/transformers/pipelines/__init__.py\",\n+        help=\"Path to the pipeline file.\",\n+    )\n+    args = parser.parse_args()\n+    main(args.pipeline_file_path, args.fix_and_overwrite)"
        }
    ],
    "stats": {
        "total": 470,
        "additions": 398,
        "deletions": 72
    }
}