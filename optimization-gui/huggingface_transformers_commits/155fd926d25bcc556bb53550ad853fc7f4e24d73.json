{
    "author": "ydshieh",
    "message": "Fix `GptOssModelTest::test_assisted_decoding_matches_greedy_search_1_same` (#40551)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Manuel de Prada Corral <6536835+manueldeprada@users.noreply.github.com>",
    "sha": "155fd926d25bcc556bb53550ad853fc7f4e24d73",
    "files": [
        {
            "sha": "4dd0c9acc8c343d05b06a3be10bbec3054f7ab02",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/155fd926d25bcc556bb53550ad853fc7f4e24d73/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155fd926d25bcc556bb53550ad853fc7f4e24d73/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=155fd926d25bcc556bb53550ad853fc7f4e24d73",
            "patch": "@@ -938,6 +938,10 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         # - assisted_decoding does not support `use_cache = False`\n         # - assisted_decoding does not support `batch_size > 1`\n \n+        # No idea why this cause problem!\n+        if type(self).__name__ not in [\"Gemma3nTextModelTest\"]:\n+            set_model_tester_for_less_flaky_test(self)\n+\n         for model_class in self.all_generative_model_classes:\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n@@ -959,6 +963,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n \n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n+            set_config_for_less_flaky_test(config)\n \n             # force eager attention to support output attentions\n             if self.has_attentions:\n@@ -970,6 +975,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n \n             config.is_decoder = True\n             model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            set_model_for_less_flaky_test(model)\n             config = model.config\n             # Sets assisted generation arguments such that:\n             # a) no EOS is generated, to ensure generation doesn't break early\n@@ -1006,8 +1012,15 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             generation_kwargs.update({\"assistant_model\": assistant_model})\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n \n+            # default values of `has_similar_generate_outputs`\n+            atol, rtol = 1e-5, 1e-5\n+            # `gpt_oss` seems to have larger differences on CPU every other generated tokens, sth. like\n+            # 1e-9, 1e-5, 1e-9, 1e-5. While on GPU, they are all very small 1e-9.\n+            if model.config.model_type == \"gpt_oss\" and torch_device == \"cpu\":\n+                atol, rtol = 1e-4, 1e-4\n+\n             # The two outputs must match and their shape must be as expected\n-            self.assertTrue(has_similar_generate_outputs(output_greedy, output_assisted))\n+            self.assertTrue(has_similar_generate_outputs(output_greedy, output_assisted, atol=atol, rtol=rtol))\n             for output in (output_greedy, output_assisted):\n                 self._check_generate_outputs(output, model.config, use_cache=True)\n \n@@ -5082,7 +5095,7 @@ def has_similar_generate_outputs(output_1, output_2, atol=1e-5, rtol=1e-5) -> bo\n             output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n             output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n             has_matching_scores = torch.allclose(\n-                output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n+                output_1_first_mismatch_scores, output_2_first_mismatch_scores, atol=atol, rtol=rtol\n             )\n             if not has_matching_scores:\n                 break"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 15,
        "deletions": 2
    }
}