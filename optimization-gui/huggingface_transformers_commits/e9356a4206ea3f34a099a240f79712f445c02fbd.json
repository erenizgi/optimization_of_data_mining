{
    "author": "GeLee-Q",
    "message": "Fix qwen2vl float16 inference bug (#33312)\n\n* fix qwen2vl float16 inference bug\r\n\r\n* [run-slow] qwen2_vl",
    "sha": "e9356a4206ea3f34a099a240f79712f445c02fbd",
    "files": [
        {
            "sha": "02716153fddaeed8547aed16076ea8a0a3ea64f6",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9356a4206ea3f34a099a240f79712f445c02fbd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9356a4206ea3f34a099a240f79712f445c02fbd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=e9356a4206ea3f34a099a240f79712f445c02fbd",
            "patch": "@@ -634,6 +634,11 @@ def forward(\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights + causal_mask\n \n+        # Fix precision issues in Qwen2-VL float16 inference\n+        # Replace inf values with zeros in attention weights to prevent NaN propagation\n+        if query_states.dtype == torch.float16:\n+            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n+\n         # upcast attention to fp32\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}