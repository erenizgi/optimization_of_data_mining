{
    "author": "cyyever",
    "message": "[V5] Remove deprecated transformers.onnx (#41214)\n\n* Remove deprecated transformers.onnx\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove onnx docs\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "ca975f1cb880d86dd7d85485c236b8bebad57273",
    "files": [
        {
            "sha": "27aeafab612fa620247ec7ce7fd944d190958c49",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca975f1cb880d86dd7d85485c236b8bebad57273/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca975f1cb880d86dd7d85485c236b8bebad57273/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=ca975f1cb880d86dd7d85485c236b8bebad57273",
            "patch": "@@ -342,8 +342,6 @@\n       title: Models\n     - local: main_classes/text_generation\n       title: Text Generation\n-    - local: main_classes/onnx\n-      title: ONNX\n     - local: main_classes/optimizer_schedules\n       title: Optimization\n     - local: main_classes/output"
        },
        {
            "sha": "5f8869948d2ba950de183bf3bfb0d9e2a173444e",
            "filename": "docs/source/en/main_classes/onnx.md",
            "status": "removed",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fonnx.md?ref=1d1ac078933dc7eb902a4d49b9eb347e5242d1e6",
            "patch": "@@ -1,53 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Exporting ðŸ¤— Transformers models to ONNX\n-\n-ðŸ¤— Transformers provides a `transformers.onnx` package that enables you to\n-convert model checkpoints to an ONNX graph by leveraging configuration objects.\n-\n-See the [guide](../serialization) on exporting ðŸ¤— Transformers models for more\n-details.\n-\n-## ONNX Configurations\n-\n-We provide three abstract classes that you should inherit from, depending on the\n-type of model architecture you wish to export:\n-\n-* Encoder-based models inherit from [`~onnx.config.OnnxConfig`]\n-* Decoder-based models inherit from [`~onnx.config.OnnxConfigWithPast`]\n-* Encoder-decoder models inherit from [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n-\n-### OnnxConfig\n-\n-[[autodoc]] onnx.config.OnnxConfig\n-\n-### OnnxConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxConfigWithPast\n-\n-### OnnxSeq2SeqConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast\n-\n-## ONNX Features\n-\n-Each ONNX configuration is associated with a set of _features_ that enable you\n-to export models for different types of topologies or tasks.\n-\n-### FeaturesManager\n-\n-[[autodoc]] onnx.features.FeaturesManager"
        },
        {
            "sha": "9238f341465c01cc36a399b4cd0c8c25a1ddde87",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca975f1cb880d86dd7d85485c236b8bebad57273/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca975f1cb880d86dd7d85485c236b8bebad57273/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=ca975f1cb880d86dd7d85485c236b8bebad57273",
            "patch": "@@ -125,9 +125,6 @@\n     \"nltk<=3.8.1\",\n     \"num2words\",\n     \"numpy>=1.17\",\n-    \"onnxconverter-common\",\n-    \"onnxruntime-tools>=1.4.2\",\n-    \"onnxruntime>=1.4.0\",\n     \"openai>=1.98.0\",\n     \"opencv-python\",\n     \"optimum-benchmark>=0.3.0\",\n@@ -271,8 +268,6 @@ def run(self):\n \n extras[\"tokenizers\"] = deps_list(\"tokenizers\")\n extras[\"ftfy\"] = deps_list(\"ftfy\")\n-extras[\"onnxruntime\"] = deps_list(\"onnxruntime\", \"onnxruntime-tools\")\n-extras[\"onnx\"] = deps_list(\"onnxconverter-common\") + extras[\"onnxruntime\"]\n extras[\"modelcreation\"] = deps_list(\"cookiecutter\")\n \n extras[\"sagemaker\"] = deps_list(\"sagemaker\")\n@@ -376,7 +371,6 @@ def run(self):\n     + extras[\"ja\"]\n     + extras[\"sklearn\"]\n     + extras[\"modelcreation\"]\n-    + extras[\"onnxruntime\"]\n     + extras[\"num2words\"]\n )\n \n@@ -463,7 +457,6 @@ def run(self):\n extras[\"tests_torch\"] = deps_list()\n extras[\"tests_hub\"] = deps_list()\n extras[\"tests_pipelines_torch\"] = deps_list()\n-extras[\"tests_onnx\"] = deps_list()\n extras[\"tests_examples_torch\"] = deps_list()\n extras[\"tests_custom_tokenizers\"] = deps_list()\n extras[\"tests_exotic_models\"] = deps_list()"
        },
        {
            "sha": "99f022def19faacbba2e2c3af8f68827041820f2",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca975f1cb880d86dd7d85485c236b8bebad57273/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca975f1cb880d86dd7d85485c236b8bebad57273/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=ca975f1cb880d86dd7d85485c236b8bebad57273",
            "patch": "@@ -34,9 +34,6 @@\n     \"nltk\": \"nltk<=3.8.1\",\n     \"num2words\": \"num2words\",\n     \"numpy\": \"numpy>=1.17\",\n-    \"onnxconverter-common\": \"onnxconverter-common\",\n-    \"onnxruntime-tools\": \"onnxruntime-tools>=1.4.2\",\n-    \"onnxruntime\": \"onnxruntime>=1.4.0\",\n     \"openai\": \"openai>=1.98.0\",\n     \"opencv-python\": \"opencv-python\",\n     \"optimum-benchmark\": \"optimum-benchmark>=0.3.0\","
        },
        {
            "sha": "8429699f24a0a29b33bf3c4a9717e638952a9f16",
            "filename": "src/transformers/onnx/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca975f1cb880d86dd7d85485c236b8bebad57273/src%2Ftransformers%2Fonnx%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca975f1cb880d86dd7d85485c236b8bebad57273/src%2Ftransformers%2Fonnx%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2F__init__.py?ref=ca975f1cb880d86dd7d85485c236b8bebad57273",
            "patch": "@@ -25,8 +25,6 @@\n         \"OnnxSeq2SeqConfigWithPast\",\n         \"PatchingSpec\",\n     ],\n-    \"convert\": [\"export\", \"validate_model_outputs\"],\n-    \"features\": [\"FeaturesManager\"],\n     \"utils\": [\"ParameterFormat\", \"compute_serialized_parameters_size\"],\n }\n \n@@ -39,8 +37,6 @@\n         OnnxSeq2SeqConfigWithPast,\n         PatchingSpec,\n     )\n-    from .convert import export, validate_model_outputs\n-    from .features import FeaturesManager\n     from .utils import ParameterFormat, compute_serialized_parameters_size\n \n else:"
        },
        {
            "sha": "db43126fd3fbb0d77eb42fac21740a8b2d1b37b5",
            "filename": "src/transformers/onnx/__main__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 228,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2F__main__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2F__main__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2F__main__.py?ref=1d1ac078933dc7eb902a4d49b9eb347e5242d1e6",
            "patch": "@@ -1,228 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import subprocess\n-import sys\n-import warnings\n-from argparse import ArgumentParser\n-from pathlib import Path\n-\n-from packaging import version\n-\n-from .. import AutoFeatureExtractor, AutoImageProcessor, AutoProcessor, AutoTokenizer\n-from ..utils import logging\n-from ..utils.import_utils import is_optimum_available\n-from .convert import export, validate_model_outputs\n-from .features import FeaturesManager\n-from .utils import get_preprocessor\n-\n-\n-MIN_OPTIMUM_VERSION = \"1.5.0\"\n-\n-ENCODER_DECODER_MODELS = [\"vision-encoder-decoder\"]\n-\n-\n-def export_with_optimum(args):\n-    if is_optimum_available():\n-        from optimum.version import __version__ as optimum_version\n-\n-        parsed_optimum_version = version.parse(optimum_version)\n-        if parsed_optimum_version < version.parse(MIN_OPTIMUM_VERSION):\n-            raise RuntimeError(\n-                f\"transformers.onnx requires optimum >= {MIN_OPTIMUM_VERSION} but {optimum_version} is installed. You \"\n-                \"can upgrade optimum by running: pip install -U optimum[exporters]\"\n-            )\n-    else:\n-        raise RuntimeError(\n-            \"transformers.onnx requires optimum to run, you can install the library by running: pip install \"\n-            \"optimum[exporters]\"\n-        )\n-    cmd_line = [\n-        sys.executable,\n-        \"-m\",\n-        \"optimum.exporters.onnx\",\n-        f\"--model {args.model}\",\n-        f\"--task {args.feature}\",\n-        f\"{args.output}\",\n-    ]\n-    proc = subprocess.Popen(cmd_line, stdout=subprocess.PIPE)\n-    proc.wait()\n-\n-    logger.info(\n-        \"The export was done by optimum.exporters.onnx. We recommend using to use this package directly in future, as \"\n-        \"transformers.onnx is deprecated, and will be removed in v5. You can find more information here: \"\n-        \"https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\"\n-    )\n-\n-\n-def export_with_transformers(args):\n-    args.output = args.output if args.output.is_file() else args.output.joinpath(\"model.onnx\")\n-    if not args.output.parent.exists():\n-        args.output.parent.mkdir(parents=True)\n-\n-    # Allocate the model\n-    model = FeaturesManager.get_model_from_feature(args.feature, args.model, cache_dir=args.cache_dir)\n-\n-    model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=args.feature)\n-    onnx_config = model_onnx_config(model.config)\n-\n-    if model_kind in ENCODER_DECODER_MODELS:\n-        encoder_model = model.get_encoder()\n-        decoder_model = model.get_decoder()\n-\n-        encoder_onnx_config = onnx_config.get_encoder_config(encoder_model.config)\n-        decoder_onnx_config = onnx_config.get_decoder_config(\n-            encoder_model.config, decoder_model.config, feature=args.feature\n-        )\n-\n-        if args.opset is None:\n-            args.opset = max(encoder_onnx_config.default_onnx_opset, decoder_onnx_config.default_onnx_opset)\n-\n-        if args.opset < min(encoder_onnx_config.default_onnx_opset, decoder_onnx_config.default_onnx_opset):\n-            raise ValueError(\n-                f\"Opset {args.opset} is not sufficient to export {model_kind}. At least \"\n-                f\" {min(encoder_onnx_config.default_onnx_opset, decoder_onnx_config.default_onnx_opset)} is required.\"\n-            )\n-\n-        preprocessor = AutoFeatureExtractor.from_pretrained(args.model)\n-\n-        onnx_inputs, onnx_outputs = export(\n-            preprocessor,\n-            encoder_model,\n-            encoder_onnx_config,\n-            args.opset,\n-            args.output.parent.joinpath(\"encoder_model.onnx\"),\n-        )\n-\n-        validate_model_outputs(\n-            encoder_onnx_config,\n-            preprocessor,\n-            encoder_model,\n-            args.output.parent.joinpath(\"encoder_model.onnx\"),\n-            onnx_outputs,\n-            args.atol if args.atol else encoder_onnx_config.atol_for_validation,\n-        )\n-\n-        preprocessor = AutoTokenizer.from_pretrained(args.model)\n-\n-        onnx_inputs, onnx_outputs = export(\n-            preprocessor,\n-            decoder_model,\n-            decoder_onnx_config,\n-            args.opset,\n-            args.output.parent.joinpath(\"decoder_model.onnx\"),\n-        )\n-\n-        validate_model_outputs(\n-            decoder_onnx_config,\n-            preprocessor,\n-            decoder_model,\n-            args.output.parent.joinpath(\"decoder_model.onnx\"),\n-            onnx_outputs,\n-            args.atol if args.atol else decoder_onnx_config.atol_for_validation,\n-        )\n-        logger.info(\n-            f\"All good, model saved at: {args.output.parent.joinpath('encoder_model.onnx').as_posix()},\"\n-            f\" {args.output.parent.joinpath('decoder_model.onnx').as_posix()}\"\n-        )\n-\n-    else:\n-        # Instantiate the appropriate preprocessor\n-        if args.preprocessor == \"auto\":\n-            preprocessor = get_preprocessor(args.model)\n-        elif args.preprocessor == \"tokenizer\":\n-            preprocessor = AutoTokenizer.from_pretrained(args.model)\n-        elif args.preprocessor == \"image_processor\":\n-            preprocessor = AutoImageProcessor.from_pretrained(args.model)\n-        elif args.preprocessor == \"feature_extractor\":\n-            preprocessor = AutoFeatureExtractor.from_pretrained(args.model)\n-        elif args.preprocessor == \"processor\":\n-            preprocessor = AutoProcessor.from_pretrained(args.model)\n-        else:\n-            raise ValueError(f\"Unknown preprocessor type '{args.preprocessor}'\")\n-\n-        # Ensure the requested opset is sufficient\n-        if args.opset is None:\n-            args.opset = onnx_config.default_onnx_opset\n-\n-        if args.opset < onnx_config.default_onnx_opset:\n-            raise ValueError(\n-                f\"Opset {args.opset} is not sufficient to export {model_kind}. \"\n-                f\"At least  {onnx_config.default_onnx_opset} is required.\"\n-            )\n-\n-        onnx_inputs, onnx_outputs = export(\n-            preprocessor,\n-            model,\n-            onnx_config,\n-            args.opset,\n-            args.output,\n-        )\n-\n-        if args.atol is None:\n-            args.atol = onnx_config.atol_for_validation\n-\n-        validate_model_outputs(onnx_config, preprocessor, model, args.output, onnx_outputs, args.atol)\n-        logger.info(f\"All good, model saved at: {args.output.as_posix()}\")\n-        warnings.warn(\n-            \"The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend\"\n-            \" using optimum.exporters.onnx in future. You can find more information here:\"\n-            \" https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\",\n-            FutureWarning,\n-        )\n-\n-\n-def main():\n-    parser = ArgumentParser(\"Hugging Face Transformers ONNX exporter\")\n-    parser.add_argument(\n-        \"-m\", \"--model\", type=str, required=True, help=\"Model ID on huggingface.co or path on disk to load model from.\"\n-    )\n-    parser.add_argument(\n-        \"--feature\",\n-        default=\"default\",\n-        help=\"The type of features to export the model with.\",\n-    )\n-    parser.add_argument(\"--opset\", type=int, default=None, help=\"ONNX opset version to export the model with.\")\n-    parser.add_argument(\n-        \"--atol\", type=float, default=None, help=\"Absolute difference tolerance when validating the model.\"\n-    )\n-    parser.add_argument(\"output\", type=Path, help=\"Path indicating where to store generated ONNX model.\")\n-    parser.add_argument(\"--cache_dir\", type=str, default=None, help=\"Path indicating where to store cache.\")\n-    parser.add_argument(\n-        \"--preprocessor\",\n-        type=str,\n-        choices=[\"auto\", \"tokenizer\", \"feature_extractor\", \"image_processor\", \"processor\"],\n-        default=\"auto\",\n-        help=\"Which type of preprocessor to use. 'auto' tries to automatically detect it.\",\n-    )\n-    parser.add_argument(\n-        \"--export_with_transformers\",\n-        action=\"store_true\",\n-        help=(\n-            \"Whether to use transformers.onnx instead of optimum.exporters.onnx to perform the ONNX export. It can be \"\n-            \"useful when exporting a model supported in transformers but not in optimum, otherwise it is not \"\n-            \"recommended.\"\n-        ),\n-    )\n-\n-    args = parser.parse_args()\n-    if args.export_with_transformers or not is_optimum_available():\n-        export_with_transformers(args)\n-    else:\n-        export_with_optimum(args)\n-\n-\n-if __name__ == \"__main__\":\n-    logger = logging.get_logger(\"transformers.onnx\")  # pylint: disable=invalid-name\n-    logger.setLevel(logging.INFO)\n-    main()"
        },
        {
            "sha": "bcf7fc878890d218f48403c02002b78c2532568e",
            "filename": "src/transformers/onnx/convert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 368,
            "changes": 368,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconvert.py?ref=1d1ac078933dc7eb902a4d49b9eb347e5242d1e6",
            "patch": "@@ -1,368 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import warnings\n-from collections.abc import Iterable\n-from inspect import signature\n-from itertools import chain\n-from pathlib import Path\n-from typing import TYPE_CHECKING, Optional, Union\n-\n-import numpy as np\n-from packaging.version import Version, parse\n-\n-from ..tokenization_utils_base import PreTrainedTokenizerBase\n-from ..utils import (\n-    is_torch_available,\n-    logging,\n-)\n-from .config import OnnxConfig\n-\n-\n-if is_torch_available():\n-    from ..modeling_utils import PreTrainedModel\n-\n-if TYPE_CHECKING:\n-    from ..feature_extraction_utils import FeatureExtractionMixin\n-    from ..processing_utils import ProcessorMixin\n-    from ..tokenization_utils import PreTrainedTokenizer\n-\n-\n-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n-\n-\n-# This is the minimal required version to support some ONNX Runtime features\n-ORT_QUANTIZE_MINIMUM_VERSION = parse(\"1.4.0\")\n-\n-\n-def check_onnxruntime_requirements(minimum_version: Version):\n-    \"\"\"\n-    Check onnxruntime is installed and if the installed version match is recent enough\n-\n-    Raises:\n-        ImportError: If onnxruntime is not installed or too old version is found\n-    \"\"\"\n-    try:\n-        import onnxruntime\n-\n-        # Parse the version of the installed onnxruntime\n-        ort_version = parse(onnxruntime.__version__)\n-\n-        # We require 1.4.0 minimum\n-        if ort_version < ORT_QUANTIZE_MINIMUM_VERSION:\n-            raise ImportError(\n-                f\"We found an older version of onnxruntime ({onnxruntime.__version__}) \"\n-                f\"but we require onnxruntime to be >= {minimum_version} to enable all the conversions options.\\n\"\n-                \"Please update onnxruntime by running `pip install --upgrade onnxruntime`\"\n-            )\n-\n-    except ImportError:\n-        raise ImportError(\n-            \"onnxruntime doesn't seem to be currently installed. \"\n-            \"Please install the onnxruntime by running `pip install onnxruntime`\"\n-            \" and relaunch the conversion.\"\n-        )\n-\n-\n-def export_pytorch(\n-    preprocessor: Union[\"PreTrainedTokenizer\", \"FeatureExtractionMixin\", \"ProcessorMixin\"],\n-    model: \"PreTrainedModel\",\n-    config: OnnxConfig,\n-    opset: int,\n-    output: Path,\n-    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n-    device: str = \"cpu\",\n-) -> tuple[list[str], list[str]]:\n-    \"\"\"\n-    Export a PyTorch model to an ONNX Intermediate Representation (IR)\n-\n-    Args:\n-        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):\n-            The preprocessor used for encoding the data.\n-        model ([`PreTrainedModel`]):\n-            The model to export.\n-        config ([`~onnx.config.OnnxConfig`]):\n-            The ONNX configuration associated with the exported model.\n-        opset (`int`):\n-            The version of the ONNX operator set to use.\n-        output (`Path`):\n-            Directory to store the exported ONNX model.\n-        device (`str`, *optional*, defaults to `cpu`):\n-            The device on which the ONNX model will be exported. Either `cpu` or `cuda`.\n-\n-    Returns:\n-        `tuple[list[str], list[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from\n-        the ONNX configuration.\n-    \"\"\"\n-\n-    if isinstance(preprocessor, PreTrainedTokenizerBase) and tokenizer is not None:\n-        raise ValueError(\"You cannot provide both a tokenizer and a preprocessor to export the model.\")\n-    if tokenizer is not None:\n-        warnings.warn(\n-            \"The `tokenizer` argument is deprecated and will be removed in version 5 of Transformers. Use\"\n-            \" `preprocessor` instead.\",\n-            FutureWarning,\n-        )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n-        preprocessor = tokenizer\n-\n-    if issubclass(type(model), PreTrainedModel):\n-        import torch\n-        from torch.onnx import export as onnx_export\n-\n-        with torch.no_grad():\n-            model.config.return_dict = True\n-            model.eval()\n-\n-            # Check if we need to override certain configuration item\n-            if config.values_override is not None:\n-                logger.info(f\"Overriding {len(config.values_override)} configuration item(s)\")\n-                for override_config_key, override_config_value in config.values_override.items():\n-                    logger.info(f\"\\t- {override_config_key} -> {override_config_value}\")\n-                    setattr(model.config, override_config_key, override_config_value)\n-\n-            # Ensure inputs match\n-            # TODO: Check when exporting QA we provide \"is_pair=True\"\n-            model_inputs = config.generate_dummy_inputs(preprocessor)\n-            device = torch.device(device)\n-            if device.type == \"cuda\" and torch.cuda.is_available():\n-                model.to(device)\n-                model_inputs_device = {}\n-                for k, v in model_inputs.items():\n-                    if isinstance(v, tuple):\n-                        model_inputs_device[k] = tuple(\n-                            x.to(device) if isinstance(x, torch.Tensor) else None for x in v\n-                        )\n-                    elif isinstance(v, list):\n-                        model_inputs_device[k] = [\n-                            tuple(x.to(device) if isinstance(x, torch.Tensor) else None for x in t) for t in v\n-                        ]\n-                    else:\n-                        model_inputs_device[k] = v.to(device)\n-\n-                model_inputs = model_inputs_device\n-\n-            inputs_match, matched_inputs = ensure_model_and_config_inputs_match(model, model_inputs.keys())\n-            onnx_outputs = list(config.outputs.keys())\n-\n-            if not inputs_match:\n-                raise ValueError(\"Model and config inputs doesn't match\")\n-\n-            config.patch_ops()\n-\n-            onnx_export(\n-                model,\n-                (model_inputs,),\n-                f=output.as_posix(),\n-                input_names=list(config.inputs.keys()),\n-                output_names=onnx_outputs,\n-                dynamic_axes=dict(chain(config.inputs.items(), config.outputs.items())),\n-                do_constant_folding=True,\n-                opset_version=opset,\n-            )\n-\n-            config.restore_ops()\n-\n-    return matched_inputs, onnx_outputs\n-\n-\n-def export(\n-    preprocessor: Union[\"PreTrainedTokenizer\", \"FeatureExtractionMixin\", \"ProcessorMixin\"],\n-    model: \"PreTrainedModel\",\n-    config: OnnxConfig,\n-    opset: int,\n-    output: Path,\n-    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n-    device: str = \"cpu\",\n-) -> tuple[list[str], list[str]]:\n-    \"\"\"\n-    Export a Pytorch model to an ONNX Intermediate Representation (IR)\n-\n-    Args:\n-        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):\n-            The preprocessor used for encoding the data.\n-        model ([`PreTrainedModel`):\n-            The model to export.\n-        config ([`~onnx.config.OnnxConfig`]):\n-            The ONNX configuration associated with the exported model.\n-        opset (`int`):\n-            The version of the ONNX operator set to use.\n-        output (`Path`):\n-            Directory to store the exported ONNX model.\n-        device (`str`, *optional*, defaults to `cpu`):\n-            The device on which the ONNX model will be exported. Either `cpu` or `cuda`. Only PyTorch is supported for\n-            export on CUDA devices.\n-\n-    Returns:\n-        `tuple[list[str], list[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from\n-        the ONNX configuration.\n-    \"\"\"\n-    if not is_torch_available():\n-        raise ImportError(\"Cannot convert because PyTorchis not installed. Please install it first.\")\n-\n-    if isinstance(preprocessor, PreTrainedTokenizerBase) and tokenizer is not None:\n-        raise ValueError(\"You cannot provide both a tokenizer and a preprocessor to export the model.\")\n-    if tokenizer is not None:\n-        warnings.warn(\n-            \"The `tokenizer` argument is deprecated and will be removed in version 5 of Transformers. Use\"\n-            \" `preprocessor` instead.\",\n-            FutureWarning,\n-        )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n-        preprocessor = tokenizer\n-\n-    from ..utils import get_torch_version\n-\n-    if not config.is_torch_support_available:\n-        logger.warning(\n-            f\"Unsupported PyTorch version for this model. Minimum required is {config.torch_onnx_minimum_version},\"\n-            f\" got: {get_torch_version()}\"\n-        )\n-\n-    if issubclass(type(model), PreTrainedModel):\n-        return export_pytorch(preprocessor, model, config, opset, output, tokenizer=tokenizer, device=device)\n-\n-\n-def validate_model_outputs(\n-    config: OnnxConfig,\n-    preprocessor: Union[\"PreTrainedTokenizer\", \"FeatureExtractionMixin\", \"ProcessorMixin\"],\n-    reference_model: \"PreTrainedModel\",\n-    onnx_model: Path,\n-    onnx_named_outputs: list[str],\n-    atol: float,\n-    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n-):\n-    from onnxruntime import InferenceSession, SessionOptions\n-\n-    logger.info(\"Validating ONNX model...\")\n-\n-    if isinstance(preprocessor, PreTrainedTokenizerBase) and tokenizer is not None:\n-        raise ValueError(\"You cannot provide both a tokenizer and a preprocessor to validate the model outputs.\")\n-    if tokenizer is not None:\n-        warnings.warn(\n-            \"The `tokenizer` argument is deprecated and will be removed in version 5 of Transformers. Use\"\n-            \" `preprocessor` instead.\",\n-            FutureWarning,\n-        )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n-        preprocessor = tokenizer\n-\n-    # generate inputs with a different batch_size and seq_len that was used for conversion to properly test\n-    # dynamic input shapes.\n-    if issubclass(type(reference_model), PreTrainedModel):\n-        reference_model_inputs = config.generate_dummy_inputs(\n-            preprocessor,\n-            batch_size=config.default_fixed_batch + 1,\n-            seq_length=config.default_fixed_sequence + 1,\n-        )\n-\n-    # Create ONNX Runtime session\n-    options = SessionOptions()\n-    session = InferenceSession(onnx_model.as_posix(), options, providers=[\"CPUExecutionProvider\"])\n-\n-    # Compute outputs from the reference model\n-    if issubclass(type(reference_model), PreTrainedModel):\n-        reference_model.to(\"cpu\")\n-    ref_outputs = reference_model(**reference_model_inputs)\n-    ref_outputs_dict = {}\n-\n-    # We flatten potential collection of outputs (i.e. past_keys) to a flat structure\n-    for name, value in ref_outputs.items():\n-        # Overwriting the output name as \"present\" since it is the name used for the ONNX outputs\n-        # (\"past_key_values\" being taken for the ONNX inputs)\n-        if name == \"past_key_values\":\n-            name = \"present\"\n-        if isinstance(value, (list, tuple)):\n-            value = config.flatten_output_collection_property(name, value)\n-            ref_outputs_dict.update(value)\n-        else:\n-            ref_outputs_dict[name] = value\n-\n-    # Create onnxruntime inputs from the reference model inputs\n-    reference_model_inputs_onnxruntime = config.generate_dummy_inputs_onnxruntime(reference_model_inputs)\n-\n-    # We flatten potential collection of inputs (i.e. past_keys)\n-    onnx_inputs = {}\n-    for name, value in reference_model_inputs_onnxruntime.items():\n-        if isinstance(value, (list, tuple)):\n-            value = config.flatten_output_collection_property(name, value)\n-            onnx_inputs.update({tensor_name: pt_tensor.numpy() for tensor_name, pt_tensor in value.items()})\n-        else:\n-            onnx_inputs[name] = value.numpy()\n-\n-    # Compute outputs from the ONNX model\n-    onnx_outputs = session.run(onnx_named_outputs, onnx_inputs)\n-\n-    # Check we have a subset of the keys into onnx_outputs against ref_outputs\n-    ref_outputs_set, onnx_outputs_set = set(ref_outputs_dict.keys()), set(onnx_named_outputs)\n-    if not onnx_outputs_set.issubset(ref_outputs_set):\n-        logger.info(\n-            f\"\\t-[x] ONNX model output names {onnx_outputs_set} do not match reference model {ref_outputs_set}\"\n-        )\n-\n-        raise ValueError(\n-            \"Outputs doesn't match between reference model and ONNX exported model: \"\n-            f\"{onnx_outputs_set.difference(ref_outputs_set)}\"\n-        )\n-    else:\n-        logger.info(f\"\\t-[âœ“] ONNX model output names match reference model ({onnx_outputs_set})\")\n-\n-    # Check the shape and values match\n-    for name, ort_value in zip(onnx_named_outputs, onnx_outputs):\n-        if is_torch_available() and issubclass(type(reference_model), PreTrainedModel):\n-            ref_value = ref_outputs_dict[name].detach().numpy()\n-        else:\n-            ref_value = ref_outputs_dict[name].numpy()\n-        logger.info(f'\\t- Validating ONNX Model output \"{name}\":')\n-\n-        # Shape\n-        if ort_value.shape != ref_value.shape:\n-            logger.info(f\"\\t\\t-[x] shape {ort_value.shape} doesn't match {ref_value.shape}\")\n-            raise ValueError(\n-                \"Outputs shape doesn't match between reference model and ONNX exported model: \"\n-                f\"Got {ref_value.shape} (reference) and {ort_value.shape} (ONNX)\"\n-            )\n-        else:\n-            logger.info(f\"\\t\\t-[âœ“] {ort_value.shape} matches {ref_value.shape}\")\n-\n-        # Values\n-        if not np.allclose(ref_value, ort_value, atol=atol):\n-            bad_indices = np.logical_not(np.isclose(ref_value, ort_value, atol=atol))\n-            logger.info(f\"\\t\\t-[x] values not close enough (atol: {atol})\")\n-            raise ValueError(\n-                \"Outputs values doesn't match between reference model and ONNX exported model: \"\n-                f\"Got max absolute difference of: {np.amax(np.abs(ref_value - ort_value))} for \"\n-                f\"{ref_value[bad_indices]} vs {ort_value[bad_indices]}\"\n-            )\n-        else:\n-            logger.info(f\"\\t\\t-[âœ“] all values close (atol: {atol})\")\n-\n-\n-def ensure_model_and_config_inputs_match(\n-    model: \"PreTrainedModel\", model_inputs: Iterable[str]\n-) -> tuple[bool, list[str]]:\n-    \"\"\"\n-    :param model_inputs: :param config_inputs: :return:\n-    \"\"\"\n-    forward_parameters = signature(model.forward).parameters\n-    model_inputs_set = set(model_inputs)\n-\n-    # We are fine if config_inputs has more keys than model_inputs\n-    forward_inputs_set = set(forward_parameters.keys())\n-    is_ok = model_inputs_set.issubset(forward_inputs_set)\n-\n-    # Make sure the input order match (VERY IMPORTANT !!!!)\n-    matching_inputs = forward_inputs_set.intersection(model_inputs_set)\n-    ordered_inputs = [parameter for parameter in forward_parameters if parameter in matching_inputs]\n-    return is_ok, ordered_inputs"
        },
        {
            "sha": "1c57c68e8c87d41cc2ee46fd077fd2e05e48ee0d",
            "filename": "src/transformers/onnx/features.py",
            "status": "removed",
            "additions": 0,
            "deletions": 635,
            "changes": 635,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2Ffeatures.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d1ac078933dc7eb902a4d49b9eb347e5242d1e6/src%2Ftransformers%2Fonnx%2Ffeatures.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Ffeatures.py?ref=1d1ac078933dc7eb902a4d49b9eb347e5242d1e6",
            "patch": "@@ -1,635 +0,0 @@\n-from functools import partial, reduce\n-from typing import TYPE_CHECKING, Callable, Optional\n-\n-import transformers\n-\n-from .. import PretrainedConfig, is_torch_available\n-from ..utils import logging\n-from .config import OnnxConfig\n-\n-\n-if TYPE_CHECKING:\n-    from transformers import PreTrainedModel\n-\n-\n-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n-\n-if is_torch_available():\n-    from transformers.models.auto import (\n-        AutoModel,\n-        AutoModelForCausalLM,\n-        AutoModelForImageClassification,\n-        AutoModelForImageSegmentation,\n-        AutoModelForMaskedImageModeling,\n-        AutoModelForMaskedLM,\n-        AutoModelForMultipleChoice,\n-        AutoModelForObjectDetection,\n-        AutoModelForQuestionAnswering,\n-        AutoModelForSemanticSegmentation,\n-        AutoModelForSeq2SeqLM,\n-        AutoModelForSequenceClassification,\n-        AutoModelForSpeechSeq2Seq,\n-        AutoModelForTokenClassification,\n-        AutoModelForVision2Seq,\n-    )\n-else:\n-    logger.warning(\n-        \"The ONNX export features is only supported for PyTorch. You will not be able to export models without it installed.\"\n-    )\n-\n-\n-def supported_features_mapping(\n-    *supported_features: str, onnx_config_cls: Optional[str] = None\n-) -> dict[str, Callable[[PretrainedConfig], OnnxConfig]]:\n-    \"\"\"\n-    Generate the mapping between supported the features and their corresponding OnnxConfig for a given model.\n-\n-    Args:\n-        *supported_features: The names of the supported features.\n-        onnx_config_cls: The OnnxConfig full name corresponding to the model.\n-\n-    Returns:\n-        The dictionary mapping a feature to an OnnxConfig constructor.\n-    \"\"\"\n-    if onnx_config_cls is None:\n-        raise ValueError(\"A OnnxConfig class must be provided\")\n-\n-    config_cls = transformers\n-    for attr_name in onnx_config_cls.split(\".\"):\n-        config_cls = getattr(config_cls, attr_name)\n-    mapping = {}\n-    for feature in supported_features:\n-        if \"-with-past\" in feature:\n-            task = feature.replace(\"-with-past\", \"\")\n-            mapping[feature] = partial(config_cls.with_past, task=task)\n-        else:\n-            mapping[feature] = partial(config_cls.from_model_config, task=feature)\n-\n-    return mapping\n-\n-\n-class FeaturesManager:\n-    _TASKS_TO_AUTOMODELS = {}\n-    if is_torch_available():\n-        _TASKS_TO_AUTOMODELS = {\n-            \"default\": AutoModel,\n-            \"masked-lm\": AutoModelForMaskedLM,\n-            \"causal-lm\": AutoModelForCausalLM,\n-            \"seq2seq-lm\": AutoModelForSeq2SeqLM,\n-            \"sequence-classification\": AutoModelForSequenceClassification,\n-            \"token-classification\": AutoModelForTokenClassification,\n-            \"multiple-choice\": AutoModelForMultipleChoice,\n-            \"object-detection\": AutoModelForObjectDetection,\n-            \"question-answering\": AutoModelForQuestionAnswering,\n-            \"image-classification\": AutoModelForImageClassification,\n-            \"image-segmentation\": AutoModelForImageSegmentation,\n-            \"masked-im\": AutoModelForMaskedImageModeling,\n-            \"semantic-segmentation\": AutoModelForSemanticSegmentation,\n-            \"vision2seq-lm\": AutoModelForVision2Seq,\n-            \"speech2seq-lm\": AutoModelForSpeechSeq2Seq,\n-        }\n-\n-    # Set of model topologies we support associated to the features supported by each topology and the factory\n-    _SUPPORTED_MODEL_TYPE = {\n-        \"albert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.albert.AlbertOnnxConfig\",\n-        ),\n-        \"bart\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            \"sequence-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.bart.BartOnnxConfig\",\n-        ),\n-        # BEiT cannot be used with the masked image modeling autoclass, so this feature is excluded here\n-        \"beit\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.beit.BeitOnnxConfig\"\n-        ),\n-        \"bert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.bert.BertOnnxConfig\",\n-        ),\n-        \"big-bird\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.big_bird.BigBirdOnnxConfig\",\n-        ),\n-        \"bigbird-pegasus\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            \"sequence-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.bigbird_pegasus.BigBirdPegasusOnnxConfig\",\n-        ),\n-        \"blenderbot\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.blenderbot.BlenderbotOnnxConfig\",\n-        ),\n-        \"blenderbot-small\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.blenderbot_small.BlenderbotSmallOnnxConfig\",\n-        ),\n-        \"bloom\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.bloom.BloomOnnxConfig\",\n-        ),\n-        \"camembert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.camembert.CamembertOnnxConfig\",\n-        ),\n-        \"clip\": supported_features_mapping(\n-            \"default\",\n-            onnx_config_cls=\"models.clip.CLIPOnnxConfig\",\n-        ),\n-        \"codegen\": supported_features_mapping(\n-            \"default\",\n-            \"causal-lm\",\n-            onnx_config_cls=\"models.codegen.CodeGenOnnxConfig\",\n-        ),\n-        \"convbert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.convbert.ConvBertOnnxConfig\",\n-        ),\n-        \"convnext\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            onnx_config_cls=\"models.convnext.ConvNextOnnxConfig\",\n-        ),\n-        \"data2vec-text\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.data2vec.Data2VecTextOnnxConfig\",\n-        ),\n-        \"data2vec-vision\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            # ONNX doesn't support `adaptive_avg_pool2d` yet\n-            # \"semantic-segmentation\",\n-            onnx_config_cls=\"models.data2vec.Data2VecVisionOnnxConfig\",\n-        ),\n-        \"deberta\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.deberta.DebertaOnnxConfig\",\n-        ),\n-        \"deberta-v2\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.deberta_v2.DebertaV2OnnxConfig\",\n-        ),\n-        \"deit\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.deit.DeiTOnnxConfig\"\n-        ),\n-        \"detr\": supported_features_mapping(\n-            \"default\",\n-            \"object-detection\",\n-            \"image-segmentation\",\n-            onnx_config_cls=\"models.detr.DetrOnnxConfig\",\n-        ),\n-        \"distilbert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.distilbert.DistilBertOnnxConfig\",\n-        ),\n-        \"electra\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.electra.ElectraOnnxConfig\",\n-        ),\n-        \"flaubert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.flaubert.FlaubertOnnxConfig\",\n-        ),\n-        \"gpt2\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.gpt2.GPT2OnnxConfig\",\n-        ),\n-        \"gptj\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"question-answering\",\n-            \"sequence-classification\",\n-            onnx_config_cls=\"models.gptj.GPTJOnnxConfig\",\n-        ),\n-        \"gpt-neo\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"sequence-classification\",\n-            onnx_config_cls=\"models.gpt_neo.GPTNeoOnnxConfig\",\n-        ),\n-        \"groupvit\": supported_features_mapping(\n-            \"default\",\n-            onnx_config_cls=\"models.groupvit.GroupViTOnnxConfig\",\n-        ),\n-        \"ibert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.ibert.IBertOnnxConfig\",\n-        ),\n-        \"imagegpt\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.imagegpt.ImageGPTOnnxConfig\"\n-        ),\n-        \"layoutlm\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.layoutlm.LayoutLMOnnxConfig\",\n-        ),\n-        \"layoutlmv3\": supported_features_mapping(\n-            \"default\",\n-            \"question-answering\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.layoutlmv3.LayoutLMv3OnnxConfig\",\n-        ),\n-        \"levit\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.levit.LevitOnnxConfig\"\n-        ),\n-        \"longt5\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.longt5.LongT5OnnxConfig\",\n-        ),\n-        \"longformer\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"multiple-choice\",\n-            \"question-answering\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.longformer.LongformerOnnxConfig\",\n-        ),\n-        \"marian\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            onnx_config_cls=\"models.marian.MarianOnnxConfig\",\n-        ),\n-        \"mbart\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"causal-lm\",\n-            \"causal-lm-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            \"sequence-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.mbart.MBartOnnxConfig\",\n-        ),\n-        \"mobilebert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.mobilebert.MobileBertOnnxConfig\",\n-        ),\n-        \"mobilenet-v1\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            onnx_config_cls=\"models.mobilenet_v1.MobileNetV1OnnxConfig\",\n-        ),\n-        \"mobilenet-v2\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            onnx_config_cls=\"models.mobilenet_v2.MobileNetV2OnnxConfig\",\n-        ),\n-        \"mobilevit\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            onnx_config_cls=\"models.mobilevit.MobileViTOnnxConfig\",\n-        ),\n-        \"mt5\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.mt5.MT5OnnxConfig\",\n-        ),\n-        \"m2m-100\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.m2m_100.M2M100OnnxConfig\",\n-        ),\n-        \"owlvit\": supported_features_mapping(\n-            \"default\",\n-            onnx_config_cls=\"models.owlvit.OwlViTOnnxConfig\",\n-        ),\n-        \"perceiver\": supported_features_mapping(\n-            \"image-classification\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            onnx_config_cls=\"models.perceiver.PerceiverOnnxConfig\",\n-        ),\n-        \"poolformer\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.poolformer.PoolFormerOnnxConfig\"\n-        ),\n-        \"rembert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.rembert.RemBertOnnxConfig\",\n-        ),\n-        \"resnet\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            onnx_config_cls=\"models.resnet.ResNetOnnxConfig\",\n-        ),\n-        \"roberta\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.roberta.RobertaOnnxConfig\",\n-        ),\n-        \"roformer\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"token-classification\",\n-            \"multiple-choice\",\n-            \"question-answering\",\n-            \"token-classification\",\n-            onnx_config_cls=\"models.roformer.RoFormerOnnxConfig\",\n-        ),\n-        \"segformer\": supported_features_mapping(\n-            \"default\",\n-            \"image-classification\",\n-            \"semantic-segmentation\",\n-            onnx_config_cls=\"models.segformer.SegformerOnnxConfig\",\n-        ),\n-        \"squeezebert\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.squeezebert.SqueezeBertOnnxConfig\",\n-        ),\n-        \"swin\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.swin.SwinOnnxConfig\"\n-        ),\n-        \"t5\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"seq2seq-lm\",\n-            \"seq2seq-lm-with-past\",\n-            onnx_config_cls=\"models.t5.T5OnnxConfig\",\n-        ),\n-        \"vision-encoder-decoder\": supported_features_mapping(\n-            \"vision2seq-lm\", onnx_config_cls=\"models.vision_encoder_decoder.VisionEncoderDecoderOnnxConfig\"\n-        ),\n-        \"vit\": supported_features_mapping(\n-            \"default\", \"image-classification\", onnx_config_cls=\"models.vit.ViTOnnxConfig\"\n-        ),\n-        \"whisper\": supported_features_mapping(\n-            \"default\",\n-            \"default-with-past\",\n-            \"speech2seq-lm\",\n-            \"speech2seq-lm-with-past\",\n-            onnx_config_cls=\"models.whisper.WhisperOnnxConfig\",\n-        ),\n-        \"xlm\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.xlm.XLMOnnxConfig\",\n-        ),\n-        \"xlm-roberta\": supported_features_mapping(\n-            \"default\",\n-            \"masked-lm\",\n-            \"causal-lm\",\n-            \"sequence-classification\",\n-            \"multiple-choice\",\n-            \"token-classification\",\n-            \"question-answering\",\n-            onnx_config_cls=\"models.xlm_roberta.XLMRobertaOnnxConfig\",\n-        ),\n-        \"yolos\": supported_features_mapping(\n-            \"default\",\n-            \"object-detection\",\n-            onnx_config_cls=\"models.yolos.YolosOnnxConfig\",\n-        ),\n-    }\n-\n-    AVAILABLE_FEATURES = sorted(reduce(lambda s1, s2: s1 | s2, (v.keys() for v in _SUPPORTED_MODEL_TYPE.values())))\n-\n-    @staticmethod\n-    def get_supported_features_for_model_type(\n-        model_type: str, model_name: Optional[str] = None\n-    ) -> dict[str, Callable[[PretrainedConfig], OnnxConfig]]:\n-        \"\"\"\n-        Tries to retrieve the feature -> OnnxConfig constructor map from the model type.\n-\n-        Args:\n-            model_type (`str`):\n-                The model type to retrieve the supported features for.\n-            model_name (`str`, *optional*):\n-                The name attribute of the model object, only used for the exception message.\n-\n-        Returns:\n-            The dictionary mapping each feature to a corresponding OnnxConfig constructor.\n-        \"\"\"\n-        model_type = model_type.lower()\n-        if model_type not in FeaturesManager._SUPPORTED_MODEL_TYPE:\n-            model_type_and_model_name = f\"{model_type} ({model_name})\" if model_name else model_type\n-            raise KeyError(\n-                f\"{model_type_and_model_name} is not supported yet. \"\n-                f\"Only {list(FeaturesManager._SUPPORTED_MODEL_TYPE.keys())} are supported. \"\n-                f\"If you want to support {model_type} please propose a PR or open up an issue.\"\n-            )\n-        return FeaturesManager._SUPPORTED_MODEL_TYPE[model_type]\n-\n-    @staticmethod\n-    def feature_to_task(feature: str) -> str:\n-        return feature.replace(\"-with-past\", \"\")\n-\n-    @staticmethod\n-    def get_model_class_for_feature(feature: str) -> type:\n-        \"\"\"\n-        Attempts to retrieve an AutoModel class from a feature name.\n-\n-        Args:\n-            feature (`str`):\n-                The feature required.\n-\n-        Returns:\n-            The AutoModel class corresponding to the feature.\n-        \"\"\"\n-        task = FeaturesManager.feature_to_task(feature)\n-        task_to_automodel = FeaturesManager._TASKS_TO_AUTOMODELS\n-        if task not in task_to_automodel:\n-            raise KeyError(\n-                f\"Unknown task: {feature}. Possible values are {list(FeaturesManager._TASKS_TO_AUTOMODELS.values())}\"\n-            )\n-\n-        return task_to_automodel[task]\n-\n-    @staticmethod\n-    def get_model_from_feature(feature: str, model: str, cache_dir: Optional[str] = None) -> \"PreTrainedModel\":\n-        \"\"\"\n-        Attempts to retrieve a model from a model's name and the feature to be enabled.\n-\n-        Args:\n-            feature (`str`):\n-                The feature required.\n-            model (`str`):\n-                The name of the model to export.\n-\n-        Returns:\n-            The instance of the model.\n-\n-        \"\"\"\n-        model_class = FeaturesManager.get_model_class_for_feature(feature)\n-        model = model_class.from_pretrained(model, cache_dir=cache_dir)\n-        return model\n-\n-    @staticmethod\n-    def check_supported_model_or_raise(model: \"PreTrainedModel\", feature: str = \"default\") -> tuple[str, Callable]:\n-        \"\"\"\n-        Check whether or not the model has the requested features.\n-\n-        Args:\n-            model: The model to export.\n-            feature: The name of the feature to check if it is available.\n-\n-        Returns:\n-            (str) The type of the model (OnnxConfig) The OnnxConfig instance holding the model export properties.\n-\n-        \"\"\"\n-        model_type = model.config.model_type.replace(\"_\", \"-\")\n-        model_name = getattr(model, \"name\", \"\")\n-        model_features = FeaturesManager.get_supported_features_for_model_type(model_type, model_name=model_name)\n-        if feature not in model_features:\n-            raise ValueError(\n-                f\"{model.config.model_type} doesn't support feature {feature}. Supported values are: {model_features}\"\n-            )\n-\n-        return model.config.model_type, FeaturesManager._SUPPORTED_MODEL_TYPE[model_type][feature]\n-\n-    def get_config(model_type: str, feature: str) -> OnnxConfig:\n-        \"\"\"\n-        Gets the OnnxConfig for a model_type and feature combination.\n-\n-        Args:\n-            model_type (`str`):\n-                The model type to retrieve the config for.\n-            feature (`str`):\n-                The feature to retrieve the config for.\n-\n-        Returns:\n-            `OnnxConfig`: config for the combination\n-        \"\"\"\n-        return FeaturesManager._SUPPORTED_MODEL_TYPE[model_type][feature]"
        },
        {
            "sha": "73557862bf94a90a10fa9da3349abbeba7e1d349",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca975f1cb880d86dd7d85485c236b8bebad57273/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca975f1cb880d86dd7d85485c236b8bebad57273/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=ca975f1cb880d86dd7d85485c236b8bebad57273",
            "patch": "@@ -28,7 +28,6 @@ docs/source/en/main_classes/feature_extractor.md\n docs/source/en/main_classes/image_processor.md\n docs/source/en/main_classes/logging.md\n docs/source/en/main_classes/model.md\n-docs/source/en/main_classes/onnx.md\n docs/source/en/main_classes/optimizer_schedules.md\n docs/source/en/main_classes/output.md\n docs/source/en/main_classes/pipelines.md\n@@ -738,11 +737,7 @@ src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py\n src/transformers/models/yoso/modeling_yoso.py\n src/transformers/models/zamba/configuration_zamba.py\n src/transformers/models/zamba/modeling_zamba.py\n-src/transformers/onnx/__main__.py\n src/transformers/onnx/config.py\n-src/transformers/onnx/convert.py\n-src/transformers/onnx/features.py\n-src/transformers/onnx/utils.py\n src/transformers/optimization.py\n src/transformers/pipelines/audio_classification.py\n src/transformers/pipelines/audio_utils.py\n@@ -815,4 +810,4 @@ src/transformers/utils/peft_utils.py\n src/transformers/utils/quantization_config.py\n src/transformers/utils/sentencepiece_model_pb2.py\n src/transformers/utils/sentencepiece_model_pb2_new.py\n-src/transformers/utils/versions.py\n\\ No newline at end of file\n+src/transformers/utils/versions.py"
        }
    ],
    "stats": {
        "total": 1307,
        "additions": 1,
        "deletions": 1306
    }
}