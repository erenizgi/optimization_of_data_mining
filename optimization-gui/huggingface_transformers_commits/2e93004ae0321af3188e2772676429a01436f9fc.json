{
    "author": "yao-matrix",
    "message": "extend FA2 and other cases to XPU,  (#42536)\n\n* extend FA2 and other cases to XPU, we expect all model cases except CUDAGraph\nspecific, CUDA compute capability specific and FA3 specific can run XPU.\nFor FA3, we are develioping\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update modeling_mimi.py\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "2e93004ae0321af3188e2772676429a01436f9fc",
    "files": [
        {
            "sha": "ff20228794224af083688adfd472fa89337a320f",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -25,7 +25,6 @@\n     require_kernels,\n     require_read_token,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -315,36 +314,47 @@ def test_continuous_batching_parity_gemma_sdpa(self) -> None:\n     # GPT-OSS is not compatible with SDPA because it has an attention sink. TODO: is this fixable?\n \n     # Flash attention test\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_kernels\n     @slow\n     def test_continuous_batching_parity_llama_flash(self) -> None:\n         expected_outputs = Expectations({\n             (\"cuda\", (9, 0)): {\n                 \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5 bolts. The total number of bolts is 4.5 bolts.\",\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5 bolts. The total number of bolts is 4.5 bolts.\",\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|flash_attention_2\", expected_outputs)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_kernels\n     @slow\n     def test_continuous_batching_parity_gemma_flash(self) -> None:\n         expected_outputs = Expectations({\n             (\"cuda\", (9, 0)): {\n                 \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \",\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_0\": \"\\n\\n**$128**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 1\",\n+                \"req_1\":  \"\\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|flash_attention_2\", expected_outputs)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_kernels\n     @slow\n     def test_continuous_batching_parity_qwen_flash(self) -> None:\n-        expected_outputs = {}\n+        expected_outputs = Expectations({\n+            (\"xpu\", None): {\n+                \"req_1\":  \" 3.5 bolts.\\n\\nLet's break it down step by step:\\n\\n- Blue fiber: 2 bolts\\n- White fiber: half of 2 bolts = 1 bolt\\n\\nTotal = \",\n+            },\n+        }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"paged|flash_attention_2\", expected_outputs)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_kernels\n     @slow\n     def test_continuous_batching_parity_gpt_oss_flash(self) -> None:"
        },
        {
            "sha": "d44355c599deec2f6a994bca8c3177c40d6d2e03",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -4,7 +4,7 @@\n from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-from transformers.testing_utils import require_flash_attn, require_torch_gpu, slow\n+from transformers.testing_utils import require_flash_attn, require_torch_accelerator, slow\n \n \n _TEST_PROMPTS = [\n@@ -26,7 +26,7 @@\n \n @slow\n @require_flash_attn\n-@require_torch_gpu\n+@require_torch_accelerator\n class TestBatchGeneration(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):"
        },
        {
            "sha": "f2a77e696afda34658da25394168432733b31e89",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -33,7 +33,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_large_accelerator,\n-    require_torch_large_gpu,\n     run_test_using_subprocess,\n     slow,\n     torch_device,\n@@ -172,16 +171,25 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n \n     @require_read_token\n     @require_flash_attn\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_model_9b_flash_attn(self):\n         # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for gemma2, especially in long context\n         model_id = \"google/gemma-2-9b\"\n-        EXPECTED_TEXTS = [\n-            '<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n-            \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic composed of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\",\n-        ]  # fmt: skip\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (None, None): ['<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n+                               \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic composed of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\",\n+                              ],\n+                (\"xpu\", None): ['<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n+                                \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic consisting of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\",\n+                               ],\n+            }\n+        )\n+        # fmt: on\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             model_id, attn_implementation=\"flash_attention_2\", dtype=\"float16\"\n@@ -192,7 +200,7 @@ def test_model_9b_flash_attn(self):\n         output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @pytest.mark.torch_export_test\n     @slow"
        },
        {
            "sha": "785e543400a83c5df7da4dbfb3469c224b794ea5",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -440,7 +440,7 @@ def test_automodelforcausallm(self):\n             self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_from_config(self):"
        },
        {
            "sha": "d1d247cace2e6e07a2c98b1a5791bbdf3badffa7",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -29,7 +29,7 @@\n     require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -512,7 +512,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\",\n@@ -547,7 +547,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\","
        },
        {
            "sha": "8572dc1e229f1b78ada4d3459ddc92e03d68ce84",
            "filename": "tests/models/glm4v_moe/test_modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -27,7 +27,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     run_first,\n     slow,\n     torch_device,\n@@ -434,7 +434,7 @@ def test_small_model_integration_test_with_video(self):\n \n     @run_first\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Glm4vMoeForConditionalGeneration.from_pretrained(\n             \"zai-org/GLM-4.5V\","
        },
        {
            "sha": "4d8b787dcafcfb07ff5a3ca9e1d13fe7658bff4a",
            "filename": "tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -30,7 +30,7 @@\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -235,7 +235,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n         pass\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @unittest.skip(\n@@ -356,7 +356,7 @@ def test_config_requires_mamba_or_attention_layers(self):\n \n # TODO (@alex-jw-brooks) - update this once the model(s) are out\n @unittest.skip(reason=\"GraniteMoeHybrid models are not yet released\")\n-@require_torch_gpu\n+@require_torch_accelerator\n class GraniteMoeHybridIntegrationTest(unittest.TestCase):\n     @slow\n     def test_model_logits(self):"
        },
        {
            "sha": "4767dfae25569f1f510ff17548953eaf66600a2e",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -36,7 +36,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torch_multi_accelerator,\n     slow,\n     torch_device,\n@@ -645,7 +645,7 @@ def test_integration_test_4bit_batch2(self):\n \n     @pytest.mark.flash_attn_test\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_flash_attn_2_eager_equivalence(self):\n         # Create inputs"
        },
        {
            "sha": "cb77fa242905ec5c6d792e4d33112506ce8466a3",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -33,7 +33,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_vision,\n     slow,\n     torch_device,\n@@ -467,7 +466,7 @@ def test_model_parallelism(self):\n         pass\n \n     # TODO: ydshieh\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     @unittest.skip(reason=\"_update_causal_mask is not implemented yet which fails this test\")\n     def test_sdpa_can_dispatch_on_flash(self):"
        },
        {
            "sha": "85c203bae44f09aad51c583081a06dff69331993",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -25,7 +25,7 @@\n     require_flash_attn,\n     require_large_cpu_ram,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -285,7 +285,7 @@ def _prepare_config_headdim(config, requested_dim):\n         return config\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @mark.flash_attn_test\n     @slow"
        },
        {
            "sha": "96af2264b47e86c60c759cf1836a733bd3bed8aa",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -28,7 +28,7 @@\n     is_torch_available,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -291,7 +291,7 @@ def test_identity_shortcut(self):\n         self.model_tester.create_and_check_model_forward(config, inputs_dict)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()"
        },
        {
            "sha": "8ac641273d171876edbe27191140e2e6b2c445f6",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -27,7 +27,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -344,14 +343,14 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"ModernBert flash attention does not support right padding\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_conversion(self):"
        },
        {
            "sha": "8aa6b5f07e0920007c5bed8dfc5a5ee0b52e9a13",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -40,7 +40,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -282,7 +281,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n@@ -362,7 +361,7 @@ def test_flash_attn_2_inference_equivalence(self):\n                 _ = model_fa(dummy_input, **other_inputs)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n@@ -899,7 +898,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_conversion(self):"
        },
        {
            "sha": "6a5a0729c0fe0d91d5a517318bd7eb5b003e3eb9",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -41,7 +41,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     require_torchaudio,\n     slow,\n     torch_device,\n@@ -291,7 +290,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_flash_attn_2_inference_equivalence\n@@ -373,7 +372,7 @@ def test_flash_attn_2_inference_equivalence(self):\n                 _ = model_fa(dummy_input, **other_inputs)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_flash_attn_2_inference_equivalence_right_padding\n@@ -902,7 +901,7 @@ def test_greedy_generate_stereo_outputs(self):\n         self.model_tester.audio_channels = original_audio_channels\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_conversion(self):"
        },
        {
            "sha": "a4bb23225cde9440333db94051dfb636d5275286",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -19,7 +19,13 @@\n from packaging import version\n \n from transformers.image_utils import load_image\n-from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n@@ -261,7 +267,7 @@ def test_slow_fast_equivalence_batched(self):\n             )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_vision\n     @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):"
        },
        {
            "sha": "0b3af9e1f0723ccd8d99d703641cf5748f485d51",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -37,7 +37,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -831,7 +831,7 @@ def test_small_model_integration_test_w_audio(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "fd8cc883142b68e7a625f584b76fb4f7e2f5fa97",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -35,7 +35,7 @@\n     require_cv2,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -592,7 +592,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n@@ -611,7 +611,8 @@ def test_small_model_integration_test_batch_flashatt2(self):\n \n         expected_decoded_text = Expectations({\n             (\"cuda\", None): \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in\",\n-            (\"rocm\", (9, 4)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in\"\n+            (\"rocm\", (9, 4)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in\",\n+            (\"xpu\", None): \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in\",\n         }).get_expectation()  # fmt: skip\n \n         # Since the test is to generate twice the same text, we just test twice against the expected decoded text\n@@ -621,7 +622,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n@@ -654,6 +655,10 @@ def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n                 'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in',\n                 \"system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\nI am Qwen, a large language model created by Alibaba Cloud. I am designed to answer a wide range of questions and provide information on various topics\",\n             ],\n+            (\"xpu\", None): [\n+                'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and energetic nature, which is evident in',\n+                'system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\niclaim to be a large language model created by Alibaba Cloud. I am called Qwen.',\n+            ],\n         }).get_expectation()  # fmt: skip\n \n         decoded_text = self.processor.batch_decode(output, skip_special_tokens=True)"
        },
        {
            "sha": "2559f1cbee526ea89be1047102c207a59571eaff",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -23,7 +23,7 @@\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     run_first,\n     run_test_using_subprocess,\n     slow,\n@@ -67,7 +67,7 @@ def is_pipeline_test_to_skip(\n         return True\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):"
        },
        {
            "sha": "5852b474fc4f56e336ce03384697d885fe0c6fe4",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -34,7 +34,7 @@\n     backend_empty_cache,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -583,7 +583,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained(\n@@ -611,7 +611,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "e483b17af9b388b3eb88f7e696b648cc1cdbd19b",
            "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -37,7 +37,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -853,7 +853,7 @@ def test_small_model_integration_test_w_audio(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "17f8a049efcb222c9080190391eaacc55b479040",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 20,
            "deletions": 8,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -26,10 +26,11 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -546,7 +547,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n@@ -568,18 +569,29 @@ def test_small_model_integration_test_batch_flashatt2(self):\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n \n-        EXPECTED_DECODED_TEXT = [\n-            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and montane regions\",\n-            \"user\\nWhat kind of dog is this?\\nassistant\\nBased on the image provided, there is no dog present. The animals in the picture are two cats.\\n\\nHere are some observations about the cats in the\"\n-        ]  # fmt: skip\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (None, None): [\"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and montane regions\",\n+                               \"user\\nWhat kind of dog is this?\\nassistant\\nBased on the image provided, there is no dog present. The animals in the picture are two cats.\\n\\nHere are some observations about the cats in the\"\n+                              ],\n+                (\"xpu\", None): [\"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a small wild cat native to the grasslands and steppes\",\n+                                'user\\nWhat kind of dog is this?\\nassistant\\nBased on the image provided, there is no dog present. The animals in the picture are two cats.\\n\\nHere is a description of the scene:\\n-'\n+                              ],\n+            }\n+        )\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+        # fmt: on\n+\n+        DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n+            DECODED_TEXT,\n             EXPECTED_DECODED_TEXT,\n         )\n \n     @slow\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen3VLMoeForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "f45165cbf2009018592732e322c98adcfaeba80b",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -27,7 +27,7 @@\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -90,7 +90,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n             self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attn_2_inference_equivalence(self):\n@@ -599,7 +599,7 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(\"Siglip2 does not support right padding\")\n@@ -743,6 +743,11 @@ def test_inference(self):\n                 [  9.4241,  10.1828,   6.3366], [  2.4371,   3.1062,   4.5530], [-12.3173, -13.7240, -13.4580],\n                 [  1.1502,   1.1716,  -1.9623]\n             ],\n+            (\"xpu\", 3): [\n+                [  1.0195,  -0.0280,  -1.4468], [ -4.5395,  -6.2269,  -1.5667], [  4.1757,   5.0358,   3.5159],\n+                [  9.4264,  10.1879,   6.3353], [  2.4409,   3.1058,   4.5491], [-12.3230, -13.7355, -13.4632],\n+                [  1.1520,   1.1687,  -1.9647]\n+            ],\n         })\n         EXPECTED_LOGITS_PER_TEXT = torch.tensor(expected_logits_per_texts.get_expectation()).to(torch_device)\n         # fmt: on"
        },
        {
            "sha": "5897f08601f2611753de122826f7b54fe573d1ca",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -26,7 +26,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     torch_device,\n )\n \n@@ -1273,7 +1272,7 @@ def test_flex_attention_with_grads(self):\n             _ = model(**dummy_inputs)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     def test_generate_beyond_sliding_window_with_flash_attn(self):\n         config, input_ids, _, attention_mask, _, _ = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "36f64f6419445485195efe447051c148808aa656",
            "filename": "tests/models/video_llama_3/test_modeling_video_llama_3.py",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -40,7 +40,7 @@\n     backend_empty_cache,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n     slow,\n@@ -925,7 +925,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n         self.assertEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = VideoLlama3ForConditionalGeneration.from_pretrained(\n@@ -942,17 +942,26 @@ def test_small_model_integration_test_batch_flashatt2(self):\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n \n-        EXPECTED_DECODED_TEXT = [\n-            'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n-            'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (None, None): ['user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+                               'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+                              ],\n+                (\"xpu\", 3): ['user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+                             'user\\n\\nDescribe the image.\\nassistant\\nThe image depicts a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+                            ],\n+            }\n         )\n+        # fmt: on\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        self.assertEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = VideoLlama3ForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "987c80ba30cd7d8417d4f2ba8076131e09cde586",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -29,7 +29,7 @@\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -349,7 +349,7 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n             check_hidden_states_output(inputs_dict, config, model_class)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()"
        },
        {
            "sha": "a547904f8b9adea7967a69fd787f6d3e97aa5b76",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -26,7 +26,7 @@\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -262,7 +262,7 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @slow\n     @is_flaky()"
        },
        {
            "sha": "59ad60e0552183920f42d9bf7f86ea236a1639f1",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -34,7 +34,7 @@\n     require_flash_attn,\n     require_pyctcdecode,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torchaudio,\n     require_torchcodec,\n     run_test_in_subprocess,\n@@ -1808,7 +1808,7 @@ def run_model(lang):\n             assert run_model(lang) == TRANSCRIPTIONS[lang]\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     def test_inference_ctc_fa2(self):\n         model_fa = Wav2Vec2ForCTC.from_pretrained(\n@@ -1830,7 +1830,7 @@ def test_inference_ctc_fa2(self):\n         self.assertListEqual(predicted_trans, EXPECTED_TRANSCRIPTIONS)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     def test_inference_ctc_fa2_batched(self):\n         model_fa = Wav2Vec2ForCTC.from_pretrained("
        },
        {
            "sha": "1a718f44cb713e1f1add539f5590172433b2c0c4",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -18,7 +18,13 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, is_torch_available, xLSTMConfig\n-from transformers.testing_utils import require_read_token, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -324,7 +330,7 @@ def test_batched_equivalence_without_cache(self):\n             individual_output = tokenizer.batch_decode(individual_gen, skip_special_tokens=True)[0]\n             self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_xlstm_block_train_vs_eval_equivalence(self):\n         # Based on https://github.com/sustcsonglin/flash-linear-attention/issues/63\n         # Credit to zhixuan-lin"
        },
        {
            "sha": "21a05d34332f0226135376772db94e7c441abf75",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e93004ae0321af3188e2772676429a01436f9fc/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2e93004ae0321af3188e2772676429a01436f9fc",
            "patch": "@@ -3365,7 +3365,7 @@ def test_flash_attn_2_fp32_ln(self):\n                     _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @mark.flash_attn_test\n     @pytest.mark.torch_compile_test\n     @slow"
        }
    ],
    "stats": {
        "total": 242,
        "additions": 149,
        "deletions": 93
    }
}