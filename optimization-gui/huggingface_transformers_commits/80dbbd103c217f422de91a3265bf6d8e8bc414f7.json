{
    "author": "gante",
    "message": "ðŸ§¹ remove `generate`-related objects and methods scheduled for removal in v4.48 (#35677)\n\n* remove things scheduled for removal\r\n\r\n* make fixup",
    "sha": "80dbbd103c217f422de91a3265bf6d8e8bc414f7",
    "files": [
        {
            "sha": "39a9c56234942d8048e977667dadb6b15ae4e5b2",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -1377,7 +1377,6 @@\n             \"LogitNormalization\",\n             \"LogitsProcessor\",\n             \"LogitsProcessorList\",\n-            \"LogitsWarper\",\n             \"MaxLengthCriteria\",\n             \"MaxTimeCriteria\",\n             \"MinLengthLogitsProcessor\",\n@@ -6460,7 +6459,6 @@\n             LogitNormalization,\n             LogitsProcessor,\n             LogitsProcessorList,\n-            LogitsWarper,\n             MaxLengthCriteria,\n             MaxTimeCriteria,\n             MinLengthLogitsProcessor,"
        },
        {
            "sha": "e616adbe67980b756f142430680ffe268fc2836b",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -63,17 +63,6 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # TODO: deprecate this function in favor of `cache_position`\n         raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n \n-    # Deprecate in favor of max-cache-shape because we want to be specifc by what we mean with \"max_length\"\n-    # Prev some cache objects didn't have \"max_length\" (SlidingWindowCache or SinkCache) because the cache object technically handles\n-    # infinite amount of tokens. In the codebase what we really need to check is the max capacity of certain cache instances, so\n-    # we change naming to be more explicit\n-    def get_max_length(self) -> Optional[int]:\n-        logger.warning_once(\n-            \"`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. \"\n-            \"Calling `get_max_cache()` will raise error from v4.48\"\n-        )\n-        return self.get_max_cache_shape()\n-\n     def get_max_cache_shape(self) -> Optional[int]:\n         \"\"\"Returns the maximum sequence length (i.e. max capacity) of the cache object\"\"\"\n         raise NotImplementedError(\"Make sure to implement `get_max_cache_shape` in a subclass.\")"
        },
        {
            "sha": "ea39e8a10b824604f0e74e358671e20a1ef84111",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -68,7 +68,6 @@\n         \"LogitNormalization\",\n         \"LogitsProcessor\",\n         \"LogitsProcessorList\",\n-        \"LogitsWarper\",\n         \"MinLengthLogitsProcessor\",\n         \"MinNewTokensLengthLogitsProcessor\",\n         \"MinPLogitsWarper\",\n@@ -89,7 +88,6 @@\n         \"WatermarkLogitsProcessor\",\n     ]\n     _import_structure[\"stopping_criteria\"] = [\n-        \"MaxNewTokensCriteria\",\n         \"MaxLengthCriteria\",\n         \"MaxTimeCriteria\",\n         \"ConfidenceCriteria\",\n@@ -230,7 +228,6 @@\n             LogitNormalization,\n             LogitsProcessor,\n             LogitsProcessorList,\n-            LogitsWarper,\n             MinLengthLogitsProcessor,\n             MinNewTokensLengthLogitsProcessor,\n             MinPLogitsWarper,\n@@ -254,7 +251,6 @@\n             ConfidenceCriteria,\n             EosTokenCriteria,\n             MaxLengthCriteria,\n-            MaxNewTokensCriteria,\n             MaxTimeCriteria,\n             StoppingCriteria,\n             StoppingCriteriaList,"
        },
        {
            "sha": "7351abb1199b2edc69f28025b9839e525062b2c6",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -52,22 +52,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         )\n \n \n-class LogitsWarper:\n-    \"\"\"Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\"\"\"\n-\n-    def __init__(self):\n-        logger.warning_once(\n-            \"`LogitsWarper` is deprecated and will be removed in v4.48. Your class should inherit `LogitsProcessor` \"\n-            \"instead, which has the same properties and interface.\"\n-        )\n-\n-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n class LogitsProcessorList(list):\n     \"\"\"\n     This class can be used to create a list of [`LogitsProcessor`] to subsequently process a `scores` input tensor."
        },
        {
            "sha": "e4814ce4e7c786cba5605d3c225e6a5fbcfb520c",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 31,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -467,28 +467,6 @@ def _fa_peft_dtype_check(self, value):\n         return target_dtype\n \n \n-# TODO Remove in deprecation cycle\n-class GPTNeoXFlashAttention2(GPTNeoXAttention):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        logger.warning_once(\n-            \"The `GPTNeoXFlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\"\n-        )\n-\n-\n-# TODO Remove in deprecation cycle\n-class GPTNeoXSdpaAttention(GPTNeoXAttention):\n-    def __init__(self, config, layer_idx=None):\n-        super().__init__(config, layer_idx=layer_idx)\n-\n-        logger.warning_once(\n-            \"The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\"\n-        )\n-\n-\n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXRotaryEmbedding(nn.Module):\n     def __init__(self, config: GPTNeoXConfig, device=None):\n@@ -600,14 +578,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-GPT_NEOX_ATTENTION_CLASSES = {\n-    \"eager\": GPTNeoXAttention,\n-    \"flash_attention_2\": GPTNeoXFlashAttention2,\n-    \"sdpa\": GPTNeoXSdpaAttention,\n-    \"flex_attention\": GPTNeoXAttention,\n-}\n-\n-\n class GPTNeoXLayer(nn.Module):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n@@ -616,7 +586,7 @@ def __init__(self, config, layer_idx):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\n         self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\n-        self.attention = GPT_NEOX_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.attention = GPTNeoXAttention(config, layer_idx)\n         self.mlp = GPTNeoXMLP(config)\n \n     def forward("
        },
        {
            "sha": "843ff871da52ba4ed2420bfa962ae85a3675330a",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -352,13 +352,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class LogitsWarper(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class MaxLengthCriteria(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e588307690ba6c3c9b51e0ae03503619e184676e",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -70,7 +70,6 @@\n     # Deprecated\n     \"InputExample\",\n     \"InputFeatures\",\n-    \"LogitsWarper\",\n     # Signature is *args/**kwargs\n     \"TFSequenceSummary\",\n     \"TFBertTokenizer\","
        },
        {
            "sha": "d35bf27420cbc72c450f57486b1d66516d37ba88",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/80dbbd103c217f422de91a3265bf6d8e8bc414f7/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80dbbd103c217f422de91a3265bf6d8e8bc414f7/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=80dbbd103c217f422de91a3265bf6d8e8bc414f7",
            "patch": "@@ -946,7 +946,6 @@ def find_all_documented_objects() -> List[str]:\n     \"LineByLineTextDataset\",\n     \"LineByLineWithRefDataset\",\n     \"LineByLineWithSOPTextDataset\",\n-    \"LogitsWarper\",\n     \"NerPipeline\",\n     \"PretrainedBartModel\",\n     \"PretrainedFSMTModel\","
        }
    ],
    "stats": {
        "total": 74,
        "additions": 1,
        "deletions": 73
    }
}