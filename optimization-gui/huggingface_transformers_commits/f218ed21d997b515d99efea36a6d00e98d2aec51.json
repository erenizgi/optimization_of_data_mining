{
    "author": "vasqu",
    "message": "[`Generation`] Fix default overwrite for non-`None` defaults (#42958)\n\n* fix\n\n* test\n\n* fix 2\n\n* should not happen but safety\n\n* fast \"integration\" test",
    "sha": "f218ed21d997b515d99efea36a6d00e98d2aec51",
    "files": [
        {
            "sha": "34a4d0d68f402d1b20196b69c2c757cb2e58eaad",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f218ed21d997b515d99efea36a6d00e98d2aec51/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f218ed21d997b515d99efea36a6d00e98d2aec51/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f218ed21d997b515d99efea36a6d00e98d2aec51",
            "patch": "@@ -1795,6 +1795,20 @@ def _prepare_generation_config(\n         generation_config.update(**self.generation_config.to_dict(), defaults_only=True)\n         generation_config.update(**global_defaults, defaults_only=True)\n \n+        # Due to some values being boolean and not `None`, we need additional logic to overwrite\n+        # them explicitly (`defaults_only=False`) on the condition that it's only a previous\n+        # default value\n+        default_generation_config = GenerationConfig()\n+        generation_config.update(\n+            **{\n+                k: v\n+                for k, v in self.generation_config.to_dict().items()\n+                if isinstance(v, bool)\n+                and hasattr(default_generation_config, k)\n+                and getattr(generation_config, k, None) == getattr(default_generation_config, k)\n+            }\n+        )\n+\n         # Finally, if there are any kwargs, update config with it -> highest priority at the end\n         model_kwargs = generation_config.update(**kwargs)\n "
        },
        {
            "sha": "950c22fff51494b11b0ff2d2e65464bc39701352",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f218ed21d997b515d99efea36a6d00e98d2aec51/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f218ed21d997b515d99efea36a6d00e98d2aec51/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=f218ed21d997b515d99efea36a6d00e98d2aec51",
            "patch": "@@ -4802,6 +4802,25 @@ def test_hub_gen_strategies(self, custom_generate, extra_kwargs):\n         output = model.generate(**generation_kwargs, **model_inputs)\n         self.assertEqual(output.sequences.shape, (1, 9))\n \n+    def test_model_generation_config_can_override_defaults(self):\n+        \"\"\"Sanity check that the model samples, not ignoring the model's generation config\"\"\"\n+        torch.manual_seed(42)  # make it deterministic\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\").eval()\n+        model = model.to(torch_device)\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+        model_inputs = tokenizer([\"Write a poem about the market crashing in summer\"], return_tensors=\"pt\")\n+        model_inputs = model_inputs.to(model.device)\n+\n+        # Overwrite default value or sampling\n+        model.generation_config.do_sample = True\n+        output = model.generate(**model_inputs, max_new_tokens=32)\n+\n+        EXPECTED_TEXT = 'Write a poem about the market crashing in summersong contradictionPr aucitated realiz Comicsflutterąc inventминцій Glad:` Raymond moreover KulturMillteger мартаTEXT CFщая Русбе Świ Sendlink heuresListener Luigiaceae'  # fmt: skip\n+        output = tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertTrue(output == EXPECTED_TEXT)\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 33,
        "deletions": 0
    }
}