{
    "author": "zhanluxianshen",
    "message": "fix docs serving typos. (#37936)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "cc68070d41953d67e577db9541d048c936c58b94",
    "files": [
        {
            "sha": "c240e566a53a8a66a708c9133c721130e4397f09",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc68070d41953d67e577db9541d048c936c58b94/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc68070d41953d67e577db9541d048c936c58b94/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=cc68070d41953d67e577db9541d048c936c58b94",
            "patch": "@@ -51,7 +51,7 @@ By default, vLLM serves the native implementation and if it doesn't exist, it fa\n ```shell\n vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n     --task generate \\\n-    --model-impl transformers \\\n+    --model-impl transformers\n ```\n \n Add the `trust-remote-code` parameter to enable loading a remote code model.\n@@ -60,5 +60,5 @@ Add the `trust-remote-code` parameter to enable loading a remote code model.\n vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n     --task generate \\\n     --model-impl transformers \\\n-    --trust-remote-code \\\n+    --trust-remote-code\n ```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}