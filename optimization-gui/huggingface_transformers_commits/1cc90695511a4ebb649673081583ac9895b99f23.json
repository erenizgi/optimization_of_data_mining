{
    "author": "cyyever",
    "message": "Fix unnecessary single-item container checks (#41279)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "1cc90695511a4ebb649673081583ac9895b99f23",
    "files": [
        {
            "sha": "6cbb138f023fb47873f7c9d833ae3ef9005e51d4",
            "filename": "examples/legacy/pytorch-lightning/run_ner.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -72,12 +72,12 @@ def prepare_data(self):\n                     self.labels,\n                     args.max_seq_length,\n                     self.tokenizer,\n-                    cls_token_at_end=bool(self.config.model_type in [\"xlnet\"]),\n+                    cls_token_at_end=bool(self.config.model_type == \"xlnet\"),\n                     cls_token=self.tokenizer.cls_token,\n-                    cls_token_segment_id=2 if self.config.model_type in [\"xlnet\"] else 0,\n+                    cls_token_segment_id=2 if self.config.model_type == \"xlnet\" else 0,\n                     sep_token=self.tokenizer.sep_token,\n                     sep_token_extra=False,\n-                    pad_on_left=bool(self.config.model_type in [\"xlnet\"]),\n+                    pad_on_left=bool(self.config.model_type == \"xlnet\"),\n                     pad_token=self.tokenizer.pad_token_id,\n                     pad_token_segment_id=self.tokenizer.pad_token_type_id,\n                     pad_token_label_id=self.pad_token_label_id,"
        },
        {
            "sha": "b45a4fab40de1cc12e3e81aa10fabc73d966c55d",
            "filename": "examples/legacy/token-classification/utils_ner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -246,10 +246,10 @@ def __init__(\n                         labels,\n                         max_seq_length,\n                         tokenizer,\n-                        cls_token_at_end=bool(model_type in [\"xlnet\"]),\n+                        cls_token_at_end=bool(model_type == \"xlnet\"),\n                         # xlnet has a cls token at the end\n                         cls_token=tokenizer.cls_token,\n-                        cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n+                        cls_token_segment_id=2 if model_type == \"xlnet\" else 0,\n                         sep_token=tokenizer.sep_token,\n                         sep_token_extra=False,\n                         # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805"
        },
        {
            "sha": "df4bbf1ca60405be357d9cf0cd5103108d8ca8b3",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -587,7 +587,7 @@ def window_function(\n         window = np.hamming(length)\n     elif name in [\"hann\", \"hann_window\"]:\n         window = np.hanning(length)\n-    elif name in [\"povey\"]:\n+    elif name == \"povey\":\n         window = np.power(np.hanning(length), 0.85)\n     else:\n         raise ValueError(f\"Unknown window function '{name}'\")"
        },
        {
            "sha": "97b98f96c2029257585e4f1441b2e4e03b03d14f",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -883,7 +883,7 @@ def _prepare_decoder_input_ids_for_generation(\n             self.config.model_type == \"vision-encoder-decoder\" and \"donut\" in self.config.encoder.model_type.lower()\n         ):\n             pass\n-        elif self.config.model_type in [\"whisper\"]:\n+        elif self.config.model_type == \"whisper\":\n             pass\n         # user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust\n         # decoder_attention_mask if provided)"
        },
        {
            "sha": "d5600050188fcc80101ff0c58ddc95526b1c4ec6",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -329,11 +329,11 @@ def _gguf_parse_value(_value, data_type):\n         _value = int(_value[0])\n     elif data_type in [6, 12]:\n         _value = float(_value[0])\n-    elif data_type in [7]:\n+    elif data_type == 7:\n         _value = bool(_value[0])\n-    elif data_type in [8]:\n+    elif data_type == 8:\n         _value = array(\"B\", list(_value)).tobytes().decode()\n-    elif data_type in [9]:\n+    elif data_type == 9:\n         _value = _gguf_parse_value(_value, array_data_type)\n     return _value\n "
        },
        {
            "sha": "d3e0f57a01c541ea8f0dba3ef74f19f799a92aaa",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -4001,7 +4001,7 @@ def save_pretrained(\n                 if _is_dtensor_available and isinstance(state_dict[tensor], DTensor):\n                     full_tensor = state_dict[tensor].full_tensor()\n                     # to get the correctly ordered tensor we need to repack if packed\n-                    if _get_parameter_tp_plan(tensor, self._tp_plan) in (\"local_packed_rowwise\",):\n+                    if _get_parameter_tp_plan(tensor, self._tp_plan) == \"local_packed_rowwise\":\n                         full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n                     shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly\n                 else:"
        },
        {
            "sha": "474fc48081b5846695bb98cf89610bffb339584f",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -303,7 +303,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -335,7 +335,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "776df308a898ecbc66b169b30a147be5b8084a2e",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -306,7 +306,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -338,7 +338,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "ae5c6bcaa8c34acfc786578fe49dc8441b02193a",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -295,7 +295,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -327,7 +327,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "e338d278577afebb5224a7f242f591f87d597d4c",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -307,7 +307,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -339,7 +339,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "4f9e7413bba6f21a000f1ec919fe008826f8d159",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -864,7 +864,7 @@ def __init__(self, config: DPTConfig):\n         self.config = config\n \n         # postprocessing: only required in case of a non-hierarchical backbone (e.g. ViT, BEiT)\n-        if config.backbone_config is not None and config.backbone_config.model_type in [\"swinv2\"]:\n+        if config.backbone_config is not None and config.backbone_config.model_type == \"swinv2\":\n             self.reassemble_stage = None\n         else:\n             self.reassemble_stage = DPTReassembleStage(config)"
        },
        {
            "sha": "b7bcb920e47acfacadd7a066e773ca2a3d42e2dc",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -516,7 +516,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -548,7 +548,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_image_config_dict` and `image_config` but being different.\n             for key, value in _image_config_dict.items():\n-                if key in image_config and value != image_config[key] and key not in [\"transformers_version\"]:\n+                if key in image_config and value != image_config[key] and key != \"transformers_version\":\n                     # If specified in `image_config_dict`\n                     if key in image_config_dict:\n                         message = (\n@@ -576,11 +576,7 @@ def __init__(\n             # Give a warning if the values exist in both `_multimodal_config_dict` and `multimodal_config` but being\n             # different.\n             for key, value in _multimodal_config_dict.items():\n-                if (\n-                    key in multimodal_config\n-                    and value != multimodal_config[key]\n-                    and key not in [\"transformers_version\"]\n-                ):\n+                if key in multimodal_config and value != multimodal_config[key] and key != \"transformers_version\":\n                     # If specified in `multimodal_config_dict`\n                     if key in multimodal_config_dict:\n                         message = (\n@@ -611,7 +607,7 @@ def __init__(\n                 if (\n                     key in image_codebook_config\n                     and value != image_codebook_config[key]\n-                    and key not in [\"transformers_version\"]\n+                    and key != \"transformers_version\"\n                 ):\n                     # If specified in `image_codebook_config_dict`\n                     if key in image_codebook_config_dict:"
        },
        {
            "sha": "8366b3a08c0b47f282174c29bd4b05cfd9421c09",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -288,7 +288,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -320,7 +320,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "445a83c82ee87bed2d36fddf4a1b60bf7593ea27",
            "filename": "src/transformers/models/imagegpt/convert_imagegpt_original_tf2_to_pytorch.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconvert_imagegpt_original_tf2_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconvert_imagegpt_original_tf2_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconvert_imagegpt_original_tf2_to_pytorch.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -60,15 +60,18 @@ def load_tf_weights_in_imagegpt(model, config, imagegpt_checkpoint_path):\n \n         # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n         # which are not required for using pretrained model\n-        if any(\n-            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n-            for n in name\n-        ) or name[-1] in [\"_step\"]:\n+        if (\n+            any(\n+                n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n+                for n in name\n+            )\n+            or name[-1] == \"_step\"\n+        ):\n             logger.info(\"Skipping {}\".format(\"/\".join(name)))\n             continue\n \n         pointer = model\n-        if name[-1] not in [\"wtet\"]:\n+        if name[-1] != \"wtet\":\n             pointer = getattr(pointer, \"transformer\")\n \n         for m_name in name:"
        },
        {
            "sha": "bcc4a87a62a555e84b23332bdbc075361172321c",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -273,9 +273,7 @@ class Kosmos2_5ModelOutput(ModelOutput):\n     vision_model_output: BaseModelOutputWithPooling = None\n \n     def to_tuple(self) -> tuple[Any]:\n-        return tuple(\n-            (self[k] if k not in [\"vision_model_output\"] else getattr(self, k).to_tuple()) for k in self.keys()\n-        )\n+        return tuple((self[k] if k != \"vision_model_output\" else getattr(self, k).to_tuple()) for k in self.keys())\n \n \n @dataclass\n@@ -333,9 +331,7 @@ class Kosmos2_5ForConditionalGenerationModelOutput(ModelOutput):\n     vision_model_output: BaseModelOutputWithPooling = None\n \n     def to_tuple(self) -> tuple[Any]:\n-        return tuple(\n-            (self[k] if k not in [\"vision_model_output\"] else getattr(self, k).to_tuple()) for k in self.keys()\n-        )\n+        return tuple((self[k] if k != \"vision_model_output\" else getattr(self, k).to_tuple()) for k in self.keys())\n \n \n # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructLayerNorm with Pix2Struct->Kosmos2_5"
        },
        {
            "sha": "e63770a154de3c3eb38770a8926661151c8ac3d8",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -398,7 +398,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n             max_position_embeddings=max_position_embeddings,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n-            tie_word_embeddings=llama_version in [\"3.2\"],\n+            tie_word_embeddings=llama_version == \"3.2\",\n         )\n \n         config.save_pretrained(tmp_model_path)\n@@ -451,7 +451,7 @@ def __init__(self, vocab_file, special_tokens=None, instruct=False, llama_versio\n         # Prevents a null chat_template, which triggers\n         # a parsing warning in the Hub.\n         additional_kwargs = {}\n-        if instruct or llama_version in [\"Guard-3\"]:\n+        if instruct or llama_version == \"Guard-3\":\n             model_id, revision = templates_for_version.get(llama_version, (None, None))\n             if model_id is not None:\n                 from transformers import AutoTokenizer"
        },
        {
            "sha": "4ad1bcde0daa6f26bc3affa0be4f04b81d021345",
            "filename": "src/transformers/models/metaclip_2/configuration_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -277,7 +277,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -309,7 +309,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "ddab1d412c3260fe64cd67347c495b41a58b678d",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -2804,7 +2804,7 @@ class PerceiverAudioPostprocessor(nn.Module):\n     def __init__(self, config: PerceiverConfig, in_channels: int, postproc_type: str = \"patches\") -> None:\n         super().__init__()\n \n-        if postproc_type not in (\"patches\",):  # to be supported: 'conv', 'patches', 'pixels'\n+        if postproc_type != \"patches\":  # to be supported: 'conv', 'patches', 'pixels'\n             raise ValueError(\"Invalid postproc_type!\")\n \n         # Architecture parameters:\n@@ -3137,7 +3137,7 @@ def __init__(\n         super().__init__()\n         self.config = config\n \n-        if prep_type not in (\"patches\",):\n+        if prep_type != \"patches\":\n             raise ValueError(f\"Prep_type {prep_type} is invalid, can only be 'patches'.\")\n \n         if concat_or_add_pos not in [\"concat\", \"add\"]:"
        },
        {
            "sha": "33cee6b37ba57ed4c1010f78646274a09489e33d",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -210,7 +210,7 @@ def _rope_scaling_validation(self):\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_short_factor = self.rope_scaling.get(\"short_factor\", None)\n         rope_scaling_long_factor = self.rope_scaling.get(\"long_factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"longrope\"]:\n+        if rope_scaling_type is None or rope_scaling_type != \"longrope\":\n             raise ValueError(f\"`rope_scaling`'s type field must be one of ['longrope'], got {rope_scaling_type}\")\n         if not (\n             isinstance(rope_scaling_short_factor, list)"
        },
        {
            "sha": "e5e5ca91bfcecb71a52289a6ebc69eacbeac856a",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -452,7 +452,7 @@ def _rope_scaling_validation(self):\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_short_factor = self.rope_scaling.get(\"short_factor\", None)\n         rope_scaling_long_factor = self.rope_scaling.get(\"long_factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"longrope\"]:\n+        if rope_scaling_type is None or rope_scaling_type != \"longrope\":\n             raise ValueError(f\"`rope_scaling`'s type field must be one of ['longrope'], got {rope_scaling_type}\")\n         if not (\n             isinstance(rope_scaling_short_factor, list)"
        },
        {
            "sha": "a008c68d4fdcb44fe9fd79f9241f5331f9d19d71",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -3848,7 +3848,7 @@ def generate(\n                     self.config.talker_config.text_config.vocab_size - 1024,\n                     self.config.talker_config.text_config.vocab_size,\n                 )\n-                if i not in (self.config.talker_config.codec_eos_token_id,)\n+                if i != self.config.talker_config.codec_eos_token_id\n             ]  # Suppress additional special tokens, should not be predicted\n             talker_kwargs = {\n                 \"max_new_tokens\": talker_max_new_tokens,"
        },
        {
            "sha": "a7c147b42fb243bda6f168747cf29960b61dd949",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -2425,7 +2425,7 @@ def generate(\n                     self.config.talker_config.text_config.vocab_size - 1024,\n                     self.config.talker_config.text_config.vocab_size,\n                 )\n-                if i not in (self.config.talker_config.codec_eos_token_id,)\n+                if i != self.config.talker_config.codec_eos_token_id\n             ]  # Suppress additional special tokens, should not be predicted\n             talker_kwargs = {\n                 \"max_new_tokens\": talker_max_new_tokens,"
        },
        {
            "sha": "d4a850cc2f4b54d9ffbce8a5af178adc83e2407a",
            "filename": "src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Ftapas%2Fconvert_tapas_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Ftapas%2Fconvert_tapas_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fconvert_tapas_original_tf_checkpoint_to_pytorch.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -96,7 +96,7 @@ def load_tf_weights_in_tapas(model, config, tf_checkpoint_path):\n                 continue\n         # in case the model is TapasForMaskedLM, we skip the pooler\n         if isinstance(model, TapasForMaskedLM):\n-            if any(n in [\"pooler\"] for n in name):\n+            if any(n == \"pooler\" for n in name):\n                 logger.info(f\"Skipping {'/'.join(name)}\")\n                 continue\n         # if first scope name starts with \"bert\", change it to \"tapas\""
        },
        {
            "sha": "3fa6bb1544a8c86fe8fb43f190b49c0c9c8184ae",
            "filename": "src/transformers/models/x_clip/configuration_x_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -294,7 +294,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n             for key, value in _text_config_dict.items():\n-                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                if key in text_config and value != text_config[key] and key != \"transformers_version\":\n                     # If specified in `text_config_dict`\n                     if key in text_config_dict:\n                         message = (\n@@ -326,7 +326,7 @@ def __init__(\n \n             # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n             for key, value in _vision_config_dict.items():\n-                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                if key in vision_config and value != vision_config[key] and key != \"transformers_version\":\n                     # If specified in `vision_config_dict`\n                     if key in vision_config_dict:\n                         message = ("
        },
        {
            "sha": "a88a444bf928624d7c54c10227c9e674cb3f7aca",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -294,7 +294,7 @@ def __init__(self, config: ZoeDepthConfig):\n         self.config = config\n \n         # postprocessing: only required in case of a non-hierarchical backbone (e.g. ViT, BEiT)\n-        if config.backbone_config is not None and config.backbone_config.model_type in [\"swinv2\"]:\n+        if config.backbone_config is not None and config.backbone_config.model_type == \"swinv2\":\n             self.reassemble_stage = None\n         else:\n             self.reassemble_stage = ZoeDepthReassembleStage(config)"
        },
        {
            "sha": "9d65325083440cbc70102e154de031e641feb13a",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -169,7 +169,7 @@ def inner(items):\n         # input_values, input_pixels, input_ids, ...\n         padded = {}\n         for key in keys:\n-            if key in {\"input_ids\"}:\n+            if key == \"input_ids\":\n                 # ImageGPT uses a feature extractor\n                 if tokenizer is None and feature_extractor is not None:\n                     _padding_value = f_padding_value"
        },
        {
            "sha": "e1ea152d7a0aff05555cfb15cb94217ccfb9f9c8",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -179,7 +179,7 @@ def _sanitize_parameters(\n             generate_kwargs[\"prefix_length\"] = prefix_inputs[\"input_ids\"].shape[-1]\n \n         if handle_long_generation is not None:\n-            if handle_long_generation not in {\"hole\"}:\n+            if handle_long_generation != \"hole\":\n                 raise ValueError(\n                     f\"{handle_long_generation} is not a valid value for `handle_long_generation` parameter expected\"\n                     \" [None, 'hole']\"\n@@ -234,7 +234,7 @@ def _parse_and_tokenize(self, *args, **kwargs):\n         Parse arguments and tokenize\n         \"\"\"\n         # Parse arguments\n-        if self.model.__class__.__name__ in [\"TransfoXLLMHeadModel\"]:\n+        if self.model.__class__.__name__ == \"TransfoXLLMHeadModel\":\n             kwargs.update({\"add_space_before_punct_symbol\": True})\n \n         return super()._parse_and_tokenize(*args, **kwargs)"
        },
        {
            "sha": "58b271934868634684eb1cc983de6bac0be5109e",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -2779,7 +2779,7 @@ def wrapper(*args, **kwargs):\n                             test = test.split(\"::\")[1:]\n                             command[idx] = \"::\".join([f\"{func.__globals__['__file__']}\"] + test)\n                     command = [f\"{sys.executable}\", \"-m\", \"pytest\"] + command\n-                    command = [x for x in command if x not in [\"--no-summary\"]]\n+                    command = [x for x in command if x != \"--no-summary\"]\n                 # Otherwise, simply run the test with no option at all\n                 else:\n                     command = [f\"{sys.executable}\", \"-m\", \"pytest\", f\"{test}\"]\n@@ -4092,7 +4092,7 @@ def use_one_line_repr(obj):\n                     if element_types[0] in [int, float]:\n                         # one-line repr. without width limit\n                         return no_new_line_in_elements\n-                    elif element_types[0] in [str]:\n+                    elif element_types[0] is str:\n                         if len(obj) == 1:\n                             # one single string element --> one-line repr. without width limit\n                             return no_new_line_in_elements"
        },
        {
            "sha": "82357e6f0fe29807d9022b2cfcb8668959ac1dbe",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -1618,7 +1618,7 @@ def post_init(self):\n             if self.hadamard_group_size not in [32, 64, 128]:\n                 raise ValueError(\"Only a `hadamard_group_size` of [32, 64, 128] is supported for 'mxfp4'.\")\n         elif self.forward_dtype == \"nvfp4\":\n-            if self.forward_method not in [\"abs_max\"]:\n+            if self.forward_method != \"abs_max\":\n                 raise ValueError(\"Only 'abs_max' is supported for forward_method for 'nvfp4'.\")\n             if self.hadamard_group_size is None:\n                 self.hadamard_group_size = 16\n@@ -1627,7 +1627,7 @@ def post_init(self):\n         else:\n             raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n \n-        if self.backward_dtype not in [\"bf16\"]:\n+        if self.backward_dtype != \"bf16\":\n             raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n         if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n             raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n@@ -2026,7 +2026,7 @@ def post_init(self):\n         Safety checker that arguments are correct\n         \"\"\"\n         self.activation_scheme = self.activation_scheme.lower()\n-        if self.activation_scheme not in [\"dynamic\"]:\n+        if self.activation_scheme != \"dynamic\":\n             raise ValueError(f\"Activation scheme {self.activation_scheme} not supported\")\n         if len(self.weight_block_size) != 2:\n             raise ValueError(\"weight_block_size must be a tuple of two integers\")"
        },
        {
            "sha": "73aebbfcbf26e988c587cd37c66bdc0440cd8814",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -694,7 +694,7 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n     # can also load with decord, but not cv2/torchvision\n     # both will fail in case of url links\n     video_is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n-    if video_is_url and backend in [\"opencv\"]:\n+    if video_is_url and backend == \"opencv\":\n         raise ValueError(\"If you are trying to load a video from URL, you cannot use 'opencv' as backend\")\n \n     if ("
        },
        {
            "sha": "c721029f0a490e37962d8fc3cd2b5cf21a3b9038",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -287,7 +287,7 @@ def test_forward_signature(self):\n                 \"future_time_features\",\n             ]\n \n-            if model.__class__.__name__ in [\"AutoformerForPrediction\"]:\n+            if model.__class__.__name__ == \"AutoformerForPrediction\":\n                 expected_arg_names.append(\"future_observed_mask\")\n \n             expected_arg_names.extend("
        },
        {
            "sha": "681c1e98bdd8f164b3166f51dc25660d11bbc73b",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -323,7 +323,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n                     dtype=torch.bool,\n                     device=torch_device,\n                 )\n-            elif model_class.__name__ in [\"BrosSpadeEEForTokenClassification\"]:\n+            elif model_class.__name__ == \"BrosSpadeEEForTokenClassification\":\n                 inputs_dict[\"initial_token_labels\"] = torch.zeros(\n                     (self.model_tester.batch_size, self.model_tester.seq_length),\n                     dtype=torch.long,"
        },
        {
            "sha": "f9446e2eaee25d0130efece0886d427dcac49763",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -193,7 +193,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n \n         if return_labels:\n-            if model_class.__name__ in [\"DabDetrForObjectDetection\"]:\n+            if model_class.__name__ == \"DabDetrForObjectDetection\":\n                 labels = []\n                 for i in range(self.model_tester.batch_size):\n                     target = {}"
        },
        {
            "sha": "8ece9b9eebc70fc12d38fd0bfc3a5f17cf29581d",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -549,7 +549,7 @@ def test_post_process_label_fusing(self):\n                     continue\n \n                 # Get number of segments to be fused\n-                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n+                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] == 1]\n                 num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n                 # Expected number of segments after fusing\n                 expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse"
        },
        {
            "sha": "d0f0a0875092d6a8f6ff6c6088f70842ddfe286f",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -537,7 +537,7 @@ def test_post_process_label_fusing(self):\n                     continue\n \n                 # Get number of segments to be fused\n-                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] in {1}]\n+                fuse_targets = [1 for el in el_unfused if el[\"label_id\"] == 1]\n                 num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n                 # Expected number of segments after fusing\n                 expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse"
        },
        {
            "sha": "aea9bc0d0ead086fbb9109e2326913c80c3180ee",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -218,7 +218,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)\n \n         if return_labels:\n-            if model_class in [MaskFormerForInstanceSegmentation]:\n+            if model_class == MaskFormerForInstanceSegmentation:\n                 inputs_dict[\"mask_labels\"] = torch.zeros(\n                     (\n                         self.model_tester.batch_size,"
        },
        {
            "sha": "fcfb8f28e796e837b74b2c9f1d933d14dc75f1bf",
            "filename": "tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -63,7 +63,7 @@ def test_initialization(self):\n                 # The classifier.weight from ModernBertDecoderForSequenceClassification\n                 # is initialized without `initializer_range`, so it's not set to ~0 via the _config_zero_init\n                 if param.requires_grad and not (\n-                    name == \"classifier.weight\" and model_class in [ModernBertDecoderForSequenceClassification]\n+                    name == \"classifier.weight\" and model_class == ModernBertDecoderForSequenceClassification\n                 ):\n                     data = torch.flatten(param.data)\n                     n_elements = torch.numel(data)"
        },
        {
            "sha": "e5b876b426046bc92214acf5a0e64a47f20b8509",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -965,7 +965,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(\n                     reason=\"Llava-like models currently (transformers==4.39.1) requires an attention_mask input\"\n                 )\n-            if config.model_type in [\"paligemma\"]:\n+            if config.model_type == \"paligemma\":\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )"
        },
        {
            "sha": "4d487546c7416f721b651d991f0e1c9aa0e2cad7",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -968,7 +968,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(\n                     reason=\"Llava-like models currently (transformers==4.39.1) requires an attention_mask input\"\n                 )\n-            if config.model_type in [\"paligemma\"]:\n+            if config.model_type == \"paligemma\":\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )"
        },
        {
            "sha": "460f41495d73f8950f35d8d95e35f29a859aa669",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -213,7 +213,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n \n         if return_labels:\n-            if model_class.__name__ in [\"TableTransformerForObjectDetection\"]:\n+            if model_class.__name__ == \"TableTransformerForObjectDetection\":\n                 labels = []\n                 for i in range(self.model_tester.batch_size):\n                     target = {}"
        },
        {
            "sha": "15d221e7a48d6180cc1b6077c9c616ab80a0e80b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -2793,7 +2793,7 @@ def check_device_map_is_respected(self, model, device_map):\n             param_device = device_map[param_name]\n             if param_device in [\"cpu\", \"disk\"]:\n                 self.assertEqual(param.device, torch.device(\"meta\"))\n-            elif param_device in [\"mps\"]:\n+            elif param_device == \"mps\":\n                 self.assertEqual(param.device, torch.device(\"mps\"))\n             else:\n                 # when loaded with device_map, `param_device` are integer values for cuda/xpu/hpu/npu/mlu\n@@ -3532,7 +3532,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            if config.model_type in [\"paligemma\"]:\n+            if config.model_type == \"paligemma\":\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )\n@@ -3560,7 +3560,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 )\n             if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n                 self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n-            if config.model_type in [\"sam\"]:\n+            if config.model_type == \"sam\":\n                 self.skipTest(reason=\"SAM requires an attention_mask input for relative positional embeddings\")\n \n             model = model_class(config)\n@@ -3614,7 +3614,7 @@ def test_sdpa_can_compile_dynamic(self):\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            if config.model_type in [\"dbrx\"]:\n+            if config.model_type == \"dbrx\":\n                 self.skipTest(\n                     \"DBRX (transformers==4.40) requires a modification to support dynamic shapes with compile.\"\n                 )"
        },
        {
            "sha": "266a874b64b54a497f16917ebb71a466aff30c72",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -1764,7 +1764,7 @@ def is_any_loss_nan_or_inf(log_history):\n         self.assertFalse(is_any_loss_nan_or_inf(log_history_filter))\n \n     def test_train_and_eval_dataloaders(self):\n-        if torch_device in [\"cuda\"]:\n+        if torch_device == \"cuda\":\n             n_gpu = max(1, backend_device_count(torch_device))\n         else:\n             # DP is deprecated by PyTorch, accelerators like XPU doesn't support DP"
        },
        {
            "sha": "7b7c3bd4502af0a35954a7d06371888050022e98",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -435,9 +435,9 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         case_allowed = False\n         for attribute in attributes:\n             # Allow if the default value in the configuration class is different from the one in `PretrainedConfig`\n-            if attribute in [\"is_encoder_decoder\"] and default_value is True:\n+            if attribute == \"is_encoder_decoder\" and default_value is True:\n                 case_allowed = True\n-            elif attribute in [\"tie_word_embeddings\"] and default_value is False:\n+            elif attribute == \"tie_word_embeddings\" and default_value is False:\n                 case_allowed = True\n \n             # Allow cases without checking the default value in the configuration class"
        },
        {
            "sha": "57fd337fb7a778e8371d1e5baa3f851bdd88a1f2",
            "filename": "utils/check_model_tester.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Fcheck_model_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Fcheck_model_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_model_tester.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -40,9 +40,9 @@\n                 for k, v in config.to_dict().items():\n                     if isinstance(v, int):\n                         target = None\n-                        if k in [\"vocab_size\"]:\n+                        if k == \"vocab_size\":\n                             target = 100\n-                        elif k in [\"max_position_embeddings\"]:\n+                        elif k == \"max_position_embeddings\":\n                             target = 128\n                         elif k in [\"hidden_size\", \"d_model\"]:\n                             target = 40"
        },
        {
            "sha": "de0b8d52258c4e1e3523bf6d5a16fd202ec57771",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc90695511a4ebb649673081583ac9895b99f23/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=1cc90695511a4ebb649673081583ac9895b99f23",
            "patch": "@@ -409,7 +409,7 @@ def get_diff_for_doctesting(repo: Repo, base_commit: str, commits: list[str]) ->\n             if not diff_obj.b_path.endswith(\".py\") and not diff_obj.b_path.endswith(\".md\"):\n                 continue\n             # We always add new python/md files\n-            if diff_obj.change_type in [\"A\"]:\n+            if diff_obj.change_type == \"A\":\n                 code_diff.append(diff_obj.b_path)\n             # Now for modified files\n             elif diff_obj.change_type in [\"M\", \"R\"]:"
        }
    ],
    "stats": {
        "total": 163,
        "additions": 79,
        "deletions": 84
    }
}