{
    "author": "MekkCyber",
    "message": "Fix require_read_token (#37422)\n\n* nit\n\n* fix\n\n* fix",
    "sha": "9c0c323e12640a46090d2d13d135a1d84321feda",
    "files": [
        {
            "sha": "fd223704cd04447886d9e6e7b51d363eb1cdd166",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c0c323e12640a46090d2d13d135a1d84321feda/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c0c323e12640a46090d2d13d135a1d84321feda/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=9c0c323e12640a46090d2d13d135a1d84321feda",
            "patch": "@@ -156,7 +156,7 @@ class DonutSwinImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "81584fa02e0ef59d1c9e51be9761355dc08e44bc",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c0c323e12640a46090d2d13d135a1d84321feda/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c0c323e12640a46090d2d13d135a1d84321feda/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=9c0c323e12640a46090d2d13d135a1d84321feda",
            "patch": "@@ -57,7 +57,6 @@ class QuarkTest(unittest.TestCase):\n     EXPECTED_RELATIVE_DIFFERENCE = 1.66\n     device_map = None\n \n-    @require_read_token\n     @classmethod\n     def setUpClass(cls):\n         \"\"\"\n@@ -76,15 +75,17 @@ def setUpClass(cls):\n             device_map=cls.device_map,\n         )\n \n+    @require_read_token\n     def test_memory_footprint(self):\n         mem_quantized = self.quantized_model.get_memory_footprint()\n \n         self.assertTrue(self.mem_fp16 / mem_quantized > self.EXPECTED_RELATIVE_DIFFERENCE)\n \n+    @require_read_token\n     def test_device_and_dtype_assignment(self):\n         r\"\"\"\n         Test whether trying to cast (or assigning a device to) a model after quantization will throw an error.\n-        Checks also if other models are casted correctly.\n+        Checks also if other models are casted correctly .\n         \"\"\"\n         # This should work\n         if self.device_map is None:\n@@ -94,6 +95,7 @@ def test_device_and_dtype_assignment(self):\n             # Tries with a `dtype``\n             self.quantized_model.to(torch.float16)\n \n+    @require_read_token\n     def test_original_dtype(self):\n         r\"\"\"\n         A simple test to check if the model succesfully stores the original dtype\n@@ -104,6 +106,7 @@ def test_original_dtype(self):\n \n         self.assertTrue(isinstance(self.quantized_model.model.layers[0].mlp.gate_proj, QParamsLinear))\n \n+    @require_read_token\n     def check_inference_correctness(self, model):\n         r\"\"\"\n         Test the generation quality of the quantized model and see that we are matching the expected output.\n@@ -127,6 +130,7 @@ def check_inference_correctness(self, model):\n         # Get the generation\n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n+    @require_read_token\n     def test_generate_quality(self):\n         \"\"\"\n         Simple test to check the quality of the model by comparing the generated tokens with the expected tokens"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 7,
        "deletions": 3
    }
}