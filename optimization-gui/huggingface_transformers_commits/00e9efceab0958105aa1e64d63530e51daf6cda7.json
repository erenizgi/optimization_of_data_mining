{
    "author": "FightingZhen",
    "message": "[bugfix] fix flash attention 2 unavailable error on Ascend NPU (#39166)\n\n[bugfix] fix flash attention 2 error on Ascend NPU",
    "sha": "00e9efceab0958105aa1e64d63530e51daf6cda7",
    "files": [
        {
            "sha": "24e8765d1f22710a09eea65d5a3e68cd38a53f37",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/00e9efceab0958105aa1e64d63530e51daf6cda7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00e9efceab0958105aa1e64d63530e51daf6cda7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=00e9efceab0958105aa1e64d63530e51daf6cda7",
            "patch": "@@ -103,6 +103,16 @@ def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n     from flash_attn.bert_padding import unpad_input as unpad_input_fa2\n     from flash_attn.layers.rotary import apply_rotary_emb\n \n+    HAS_FA2 = True\n+    FA_VERSION = 2\n+elif is_torch_npu_available():\n+    # patch functions in package `flash-attn` when using flash-attention on Ascend NPU.\n+    from .integrations.npu_flash_attention import npu_apply_rotary_emb as apply_rotary_emb  # noqa: F401\n+    from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_2_func\n+    from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_2_varlen_func\n+    from .integrations.npu_flash_attention import pad_input as pad_input_fa2\n+    from .integrations.npu_flash_attention import unpad_input as unpad_input_fa2\n+\n     HAS_FA2 = True\n     FA_VERSION = 2\n else:\n@@ -136,22 +146,6 @@ def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n     unpad_input = globals()[f\"unpad_input_fa{FA_VERSION}\"]\n     pad_input = globals()[f\"pad_input_fa{FA_VERSION}\"]\n \n-# patch functions in package `flash-attn` when using flash-attention on Ascend NPU.\n-if is_torch_npu_available():\n-    from .integrations.npu_flash_attention import (\n-        npu_apply_rotary_emb as apply_rotary_emb,  # noqa: F401\n-    )\n-    from .integrations.npu_flash_attention import (\n-        npu_flash_attn_func as flash_attn_func,\n-    )\n-    from .integrations.npu_flash_attention import (\n-        npu_flash_attn_varlen_func as flash_attn_varlen_func,\n-    )\n-    from .integrations.npu_flash_attention import (\n-        pad_input,\n-        unpad_input,\n-    )\n-\n \n _flash_supports_window_size = False\n "
        }
    ],
    "stats": {
        "total": 26,
        "additions": 10,
        "deletions": 16
    }
}