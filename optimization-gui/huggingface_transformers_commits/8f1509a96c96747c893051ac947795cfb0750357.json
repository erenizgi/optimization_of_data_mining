{
    "author": "ArthurZucker",
    "message": "Fix more CI tests (#35661)\n\nadd tooslow for the fat ones",
    "sha": "8f1509a96c96747c893051ac947795cfb0750357",
    "files": [
        {
            "sha": "2bbe0d8e5b746cdeba979edb07dce68fb6075122",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f1509a96c96747c893051ac947795cfb0750357/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f1509a96c96747c893051ac947795cfb0750357/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=8f1509a96c96747c893051ac947795cfb0750357",
            "patch": "@@ -28,6 +28,7 @@\n     require_torch,\n     require_torch_gpu,\n     slow,\n+    tooslow,\n     torch_device,\n )\n \n@@ -209,6 +210,7 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n+    @tooslow\n     @require_read_token\n     def test_model_9b_bf16(self):\n         model_id = \"google/gemma-2-9b\"\n@@ -229,6 +231,7 @@ def test_model_9b_bf16(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n+    @tooslow\n     @require_read_token\n     def test_model_9b_fp16(self):\n         model_id = \"google/gemma-2-9b\"\n@@ -250,6 +253,7 @@ def test_model_9b_fp16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_read_token\n+    @tooslow\n     def test_model_9b_pipeline_bf16(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Gemma2 before this PR\n         model_id = \"google/gemma-2-9b\"\n@@ -296,6 +300,7 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n+    @tooslow\n     def test_model_9b_flash_attn(self):\n         # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for gemma2, especially in long context\n         model_id = \"google/gemma-2-9b\"\n@@ -370,6 +375,7 @@ def test_export_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n \n     @require_read_token\n+    @tooslow\n     def test_model_9b_bf16_flex_attention(self):\n         model_id = \"google/gemma-2-9b\"\n         EXPECTED_TEXTS = ["
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}