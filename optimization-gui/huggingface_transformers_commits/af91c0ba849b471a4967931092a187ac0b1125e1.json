{
    "author": "SunMarc",
    "message": "Fp8 dq (#42926)\n\n* fix\n\n* maybe more clearer ?\n\n* style\n\n* style",
    "sha": "af91c0ba849b471a4967931092a187ac0b1125e1",
    "files": [
        {
            "sha": "bac650344d02584e31792d21144c2c76a6905f47",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/af91c0ba849b471a4967931092a187ac0b1125e1/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af91c0ba849b471a4967931092a187ac0b1125e1/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=af91c0ba849b471a4967931092a187ac0b1125e1",
            "patch": "@@ -33,7 +33,7 @@ def validate_environment(self, *args, **kwargs):\n             return\n \n         if not torch.cuda.is_available() and not is_torch_xpu_available():\n-            if self.pre_quantized and not self.quantization_config.dequantize:\n+            if self.pre_quantized:\n                 logger.warning_once(\n                     \"Using FP8 quantized models requires a GPU or XPU, we will default to dequantizing the model to bf16 since no GPU or XPU is available\"\n                 )\n@@ -46,10 +46,12 @@ def validate_environment(self, *args, **kwargs):\n             compute_capability = torch.cuda.get_device_capability()\n             major, minor = compute_capability\n             if (major < 8) or (major == 8 and minor < 9):\n-                raise ValueError(\n+                logger.warning_once(\n                     \"FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100)\"\n-                    f\", actual = `{major}.{minor}`\"\n+                    f\", actual = `{major}.{minor}`. We will default to dequantizing the model to bf16 \"\n                 )\n+                self.quantization_config.dequantize = True\n+                return\n \n         device_map = kwargs.get(\"device_map\")\n         if device_map is None:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 5,
        "deletions": 3
    }
}