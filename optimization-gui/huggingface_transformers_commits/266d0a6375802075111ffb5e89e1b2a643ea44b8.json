{
    "author": "gante",
    "message": "Generate: remove flakyness in `test_generate_from_inputs_embeds_decoder_only` (#33602)\n\nalmost zero is not zero",
    "sha": "266d0a6375802075111ffb5e89e1b2a643ea44b8",
    "files": [
        {
            "sha": "26ece9c25d068f5863b49e84bfbcd1672ff24590",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 12,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/266d0a6375802075111ffb5e89e1b2a643ea44b8/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/266d0a6375802075111ffb5e89e1b2a643ea44b8/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=266d0a6375802075111ffb5e89e1b2a643ea44b8",
            "patch": "@@ -1647,26 +1647,42 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 continue\n \n             # Traditional way of generating text\n-            outputs_from_ids = model.generate(input_ids, max_new_tokens=5)\n-            self.assertEqual(outputs_from_ids.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n+            outputs_from_ids = model.generate(\n+                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n+            )\n+            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs_from_embeds = model.generate(input_ids, inputs_embeds=inputs_embeds, max_new_tokens=5)\n-            self.assertListEqual(outputs_from_ids.tolist(), outputs_from_embeds.tolist())\n+            outputs_from_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=inputs_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n \n-            # But if we pass different inputs_embeds, we should get different outputs\n-            torch.manual_seed(0)\n+            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n+            # same, but the logits will almost surely be different)\n             random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(input_ids, inputs_embeds=random_embeds, max_new_tokens=5)\n-            with self.assertRaises(AssertionError):\n-                self.assertListEqual(outputs_from_rand_embeds.tolist(), outputs_from_embeds.tolist())\n+            outputs_from_rand_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=random_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            for i in range(len(outputs_from_rand_embeds.scores)):\n+                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n \n             # input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same\n-            outputs_from_embeds_wo_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=5)\n+            outputs_from_embeds_wo_ids = model.generate(\n+                inputs_embeds=inputs_embeds, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n+            )\n             self.assertListEqual(\n-                outputs_from_embeds[:, inputs_embeds.shape[1] :].tolist(),\n-                outputs_from_embeds_wo_ids.tolist(),\n+                outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :].tolist(),\n+                outputs_from_embeds_wo_ids.sequences.tolist(),\n             )\n \n     @pytest.mark.generate"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 28,
        "deletions": 12
    }
}