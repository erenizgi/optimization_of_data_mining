{
    "author": "Manalelaidouni",
    "message": "Docs: fix code formatting in torchao docs (#38504)",
    "sha": "1285aec4cc5549ba5b49b52d6fda30be04597c82",
    "files": [
        {
            "sha": "164f6851f3260c39f75f8619027ede795b1ef909",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1285aec4cc5549ba5b49b52d6fda30be04597c82/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1285aec4cc5549ba5b49b52d6fda30be04597c82/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=1285aec4cc5549ba5b49b52d6fda30be04597c82",
            "patch": "@@ -65,13 +65,14 @@ pip install --upgrade torchao transformers\n </hfoption>\n <hfoption id=\"PyTorch Index\">\n Stable Release from the PyTorch index\n+    \n ```bash\n pip install torchao --index-url https://download.pytorch.org/whl/cu126 # options are cpu/cu118/cu126/cu128\n ```\n </hfoption>\n </hfoptions>\n \n-If your torcha version is below 0.10.0, you need to upgrade it, please refer to the [deprecation notice](#deprecation-notice) for more details.\n+If your torchao version is below 0.10.0, you need to upgrade it, please refer to the [deprecation notice](#deprecation-notice) for more details.\n \n ## Quantization examples\n \n@@ -88,6 +89,7 @@ We'll show examples for recommended quantization methods based on hardwares, e.g\n ### H100 GPU\n <hfoptions id=\"examples-H100-GPU\">\n <hfoption id=\"float8-dynamic-and-weight-only\">\n+\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -148,6 +150,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ### A100 GPU\n <hfoptions id=\"examples-A100-GPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n+    \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -215,6 +218,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ### CPU\n <hfoptions id=\"examples-CPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n+    \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -385,13 +389,15 @@ To avoid arbitrary user code execution, torchao sets `weights_only=True` in [tor\n \n <hfoptions id=\"serialization-examples\">\n <hfoption id=\"save-locally\">\n+    \n ```py\n # don't serialize model with Safetensors\n output_dir = \"llama3-8b-int4wo-128\"\n quantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n ```\n </hfoption>\n <hfoption id=\"push-to-huggingface-hub\">\n+    \n ```py\n # don't serialize model with Safetensors\n USER_ID = \"your_huggingface_user_id\""
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}