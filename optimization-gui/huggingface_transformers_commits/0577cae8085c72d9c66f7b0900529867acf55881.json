{
    "author": "ydshieh",
    "message": "Fix device issue for tapas (with `as_tensor`) (#37551)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\n* fix 5\n\n* fix 6\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "0577cae8085c72d9c66f7b0900529867acf55881",
    "files": [
        {
            "sha": "b54d5be1ef8a471bd466422b35030581bd947ca3",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0577cae8085c72d9c66f7b0900529867acf55881/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0577cae8085c72d9c66f7b0900529867acf55881/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=0577cae8085c72d9c66f7b0900529867acf55881",
            "patch": "@@ -1563,7 +1563,7 @@ def __init__(self, indices, num_segments, batch_dims=0):\n                 batch dimensions. Segments in different batch elements are always distinct even if they have the same\n                 index.\n         \"\"\"\n-        self.indices = torch.as_tensor(indices)\n+        self.indices = torch.as_tensor(indices, device=indices.device)\n         self.num_segments = torch.as_tensor(num_segments, device=indices.device)\n         self.batch_dims = batch_dims\n \n@@ -1693,11 +1693,14 @@ def range_index_map(batch_shape, num_segments, name=\"range_index_map\"):\n     Returns:\n         (`IndexMap`): IndexMap of shape batch_shape with elements equal to range(num_segments).\n     \"\"\"\n+    device = num_segments.device if torch.is_tensor(num_segments) else \"cpu\"\n     batch_shape = torch.as_tensor(\n-        batch_shape, dtype=torch.long\n+        batch_shape, dtype=torch.long, device=device\n     )  # create a rank 1 tensor vector containing batch_shape (e.g. [2])\n     assert len(batch_shape.size()) == 1\n-    num_segments = torch.as_tensor(num_segments)  # create a rank 0 tensor (scalar) containing num_segments (e.g. 64)\n+    num_segments = torch.as_tensor(\n+        num_segments, device=device\n+    )  # create a rank 0 tensor (scalar) containing num_segments (e.g. 64)\n     assert len(num_segments.size()) == 0\n \n     indices = torch.arange(\n@@ -1711,7 +1714,7 @@ def range_index_map(batch_shape, num_segments, name=\"range_index_map\"):\n     new_shape = [int(x) for x in new_tensor.tolist()]\n     indices = indices.view(new_shape)\n \n-    multiples = torch.cat([batch_shape, torch.as_tensor([1])], dim=0)\n+    multiples = torch.cat([batch_shape, torch.as_tensor([1], device=device)], dim=0)\n     indices = indices.repeat(multiples.tolist())\n     # equivalent (in Numpy:)\n     # indices = torch.as_tensor(np.tile(indices.numpy(), multiples.tolist()))\n@@ -1752,12 +1755,13 @@ def _segment_reduce(values, index, segment_reduce_fn, name):\n         dim=0, index=flat_index.indices.long(), src=flat_values.float(), reduce=segment_reduce_fn, include_self=False\n     )\n \n+    device = index.num_segments.device\n     # Unflatten the values.\n     new_shape = torch.cat(\n         [\n-            torch.as_tensor(index.batch_shape(), dtype=torch.long),\n-            torch.as_tensor([index.num_segments], dtype=torch.long),\n-            torch.as_tensor(vector_shape, dtype=torch.long),\n+            torch.as_tensor(index.batch_shape(), dtype=torch.long, device=device),\n+            torch.as_tensor([index.num_segments], dtype=torch.long, device=device),\n+            torch.as_tensor(vector_shape, dtype=torch.long, device=device),\n         ],\n         dim=0,\n     )"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 11,
        "deletions": 7
    }
}