{
    "author": "ydshieh",
    "message": "Fix `siglip` flaky `test_eager_matches_sdpa_inference` (#40584)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "c99d43e6ece3984b58801f7c8ecaf27601118f7a",
    "files": [
        {
            "sha": "10ad441ee4d96cff15d73a864fa7077d2490fe28",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c99d43e6ece3984b58801f7c8ecaf27601118f7a/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c99d43e6ece3984b58801f7c8ecaf27601118f7a/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=c99d43e6ece3984b58801f7c8ecaf27601118f7a",
            "patch": "@@ -25,7 +25,6 @@\n \n from transformers import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n from transformers.testing_utils import (\n-    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -97,13 +96,13 @@ def __init__(\n         self,\n         parent,\n         batch_size=12,\n-        image_size=30,\n+        image_size=4,\n         patch_size=2,\n         num_channels=3,\n         is_training=True,\n         hidden_size=64,\n         num_hidden_layers=2,\n-        num_attention_heads=4,\n+        num_attention_heads=2,\n         intermediate_size=37,\n         dropout=0.1,\n         attention_dropout=0.1,\n@@ -255,7 +254,6 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n@@ -273,7 +271,7 @@ def __init__(\n         vocab_size=99,\n         hidden_size=64,\n         num_hidden_layers=2,\n-        num_attention_heads=4,\n+        num_attention_heads=2,\n         intermediate_size=37,\n         dropout=0.1,\n         attention_dropout=0.1,"
        },
        {
            "sha": "417e466db109a7826f753e107dff6343d166717b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c99d43e6ece3984b58801f7c8ecaf27601118f7a/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c99d43e6ece3984b58801f7c8ecaf27601118f7a/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c99d43e6ece3984b58801f7c8ecaf27601118f7a",
            "patch": "@@ -469,10 +469,25 @@ def _test_eager_matches_sdpa_inference(\n                 logits_sdpa = _logits_sdpa\n                 logits_eager = _logits_eager\n \n+            # Avoid test flakiness with bf16!\n+            # bf16 is not good at precision when the magnitude is larger. We have some models like `SiglipVision` with\n+            # this test passing all the time for fp32/fp16 but flaky with bf16. Furthermore, `llama` and `clip` have\n+            # this test passing all the time for bf16: it turns out their outputs are of smaller size (0.1 and 1.0)\n+            # while `siglip` has outputs with maximal values around 3.0/4.0.\n+            outputs_magnitude = float(\n+                (torch.max(logits_sdpa.abs().amax(), logits_eager.abs().amax())).detach().to(\"cpu\")\n+            )\n+            # The choice of `3e-2` in `outputs_magnitude * 1e-2` might not work if a model has even more larger outputs.\n+            # (we can try to analyze the `rtol` more closely element-wise in the future and adjust the `rtol` instead of `atol`).\n+            computed_atol = outputs_magnitude * 3e-2\n+            if dtype == torch.bfloat16:\n+                atol = max(atol, computed_atol)\n+\n             results = [\n                 torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n                 for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n             ]\n+\n             # If 80% batch elements have matched results, it's fine\n             if np.mean(results) < 0.8:\n                 mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 18,
        "deletions": 5
    }
}