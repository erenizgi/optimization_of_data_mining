{
    "author": "eustlb",
    "message": "Add CSM model (#36719)\n\n* draft structure\n\n* depth decoder with forward pre hook\n\n* full model forward draft\n\n* draft update\n\n* depth decoder update\n\n* ConversationalSpeechModelForCausalLM udpates\n\n* add generate\n\n* max length criteria small fix\n\n* udpate\n\n* updates\n\n* generation update\n\n* update in loss compute\n\n* conversion script\n\n* update for correct input embeddings\n\n* handle interleaved rope\n\n* update\n\n* update\n\n* update\n\n* support compile\n\n* update training\n\n* add doc\n\n* update doc\n\n* correct inits\n\n* ConversationalSpeechModel -> Csm\n\n* conf update\n\n* name update\n\n* tests CsmForCausalLMTest\n\n* convert use cached_file\n\n* conf + modeling updates\n\n* generate utils handle third dim shape\n\n* integration test\n\n* modeling + conf updates\n\n* common test handle more than 2 dims\n\n* add nested audio list utils\n\n* processing handle nested audio list\n\n* csm processing draft\n\n* mimi util\n\n* init updates\n\n* modular update\n\n* convert modular\n\n* processing update\n\n* csm tests update\n\n* generate tests handle third dim\n\n* generate utils handle third dim\n\n* propagate _get_initial_cache_position update\n\n* tied_weight_keys update + convert correctly\n\n* fix inputs_embeds\n\n* revert audio nested list\n\n* batch inference update + return audio\n\n* audio_utils update\n\n* processor update\n\n* some more integration tests\n\n* remove old test\n\n* porcessing output labels\n\n* improve\n\n* fix\n\n* update rope values with equivalent ones\n\n* conversion update\n\n* udpate tests\n\n* handle depth decoder generation config\n\n* remove default eos_token_id\n\n* make style\n\n* revert modeling_mimi\n\n* add default generation_config\n\n* remove sdpa since handled by default\n\n* make\n\n* fix conflict\n\n* fix conflicts\n\n* correct naming\n\n* correct imports\n\n* make\n\n* causal -> conditional naming\n\n* causal -> conditional naming\n\n* auto update\n\n* make\n\n* make\n\n* add doc\n\n* test update\n\n* fix weight init\n\n* audio tokens offsets as buffer\n\n* 4d mask in conditional class\n\n* make\n\n* doc update\n\n* fix causal mask\n\n* fix causal mask\n\n* doc update\n\n* doc update\n\n* add processor doc\n\n* update doc\n\n* fix 4d causal mask\n\n* update make_list_of_audio\n\n* do not default to mutable\n\n* remove duplicates\n\n* remove useless reset_parameters\n\n* use GradientCheckpointingLayer\n\n* use can_return_tuple\n\n* formatting\n\n* prepend placeholder in _sample\n\n* torch compile fix\n\n* some more fixies\n\n* convert modular\n\n* fix\n\n* default max_length in convert\n\n* handle depth decoder generation config correctly\n\n* clearer formulation\n\n* handle output_loading_info\n\n* handle softmax warning\n\n* add doc\n\n* propagate _get_initial_cache_position changes\n\n* generation in its own module\n\n* add processor tests\n\n* fix compile witu cuda graphs\n\n* fix compile with cuda graphs\n\n* add csm.md\n\n* include CSM loss\n\n* doc nit\n\n* doc nit\n\n* doc nit\n\n* Update docs/source/en/model_doc/csm.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add save_audio to processor\n\n* Update src/transformers/models/csm/modular_csm.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* doc update\n\n* simplify audio_codes_mask computation\n\n* doc update\n\n* simplify loss computation\n\n* fix static cache test\n\n* fix\n\n* remove comment\n\n* simplify encoded length computation\n\n* use hf-internal-testing\n\n* doc update\n\n* cast to float before numpy\n\n* nit\n\n* mem efficient codebook head\n\n* nit\n\n* cat input values with cutoffs\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
    "files": [
        {
            "sha": "b848976d02c90fcd10353d96c5f3bd4046c0a762",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -825,6 +825,8 @@\n         title: Bark\n       - local: model_doc/clap\n         title: CLAP\n+      - local: model_doc/csm\n+        title: CSM\n       - local: model_doc/dac\n         title: dac\n       - local: model_doc/encodec"
        },
        {
            "sha": "2d916da161f2963cdde52204205745e9a5bd3b0b",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "added",
            "additions": 377,
            "deletions": 0,
            "changes": 377,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,377 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Csm\n+\n+## Overview\n+\n+The Conversational Speech Model (CSM) is the first open-source contextual text-to-speech model [released by Sesame](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice). It is designed to generate natural-sounding speech with or without conversational context. This context typically consists of multi-turn dialogue between speakers, represented as sequences of text and corresponding spoken audio.\n+\n+**Model Architecture:**\n+CSM is composed of two LLaMA-style auto-regressive transformer decoders: a backbone decoder that predicts the first codebook token and a depth decoder that generates the remaining tokens. It uses the pretrained codec model [Mimi](./mimi.md), introduced by Kyutai, to encode speech into discrete codebook tokens and decode them back into audio.\n+\n+The original csm-1b checkpoint is available under the [Sesame](https://huggingface.co/sesame/csm-1b) organization on Hugging Face.\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/csm_architecture.png\"/>\n+</div>\n+\n+## Usage Tips\n+\n+### Without Conversational Context\n+\n+CSM can be used to simply generate speech from a text prompt:\n+\n+```python\n+import torch\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+\n+model_id = \"eustlb/csm-1b\"\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+# load the model and the processor\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n+\n+# prepare the inputs\n+text = \"[0]The past is just a story we tell ourselves.\" # `[0]` for speaker id 0\n+inputs = processor(text, add_special_tokens=True).to(device)\n+\n+# another equivalent way to prepare the inputs\n+conversation = [\n+    {\"role\": \"0\", \"content\": [{\"type\": \"text\", \"text\": \"The past is just a story we tell ourselves.\"}]},\n+]\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+).to(device)\n+\n+# infer the model\n+audio = model.generate(**inputs, output_audio=True)\n+processor.save_audio(audio, \"example_without_context.wav\")\n+```\n+\n+### With Conversational Context\n+\n+CSM can be used to generate speech given a conversation, allowing consistency in the voices and content-aware generation:\n+\n+```python\n+import torch\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from datasets import load_dataset, Audio\n+\n+model_id = \"eustlb/csm-1b\"\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+# load the model and the processor\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n+\n+# prepare the inputs\n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+# ensure the audio is 24kHz\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+conversation = []\n+\n+# 1. context\n+for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+    conversation.append(\n+        {\n+            \"role\": f\"{speaker_id}\",\n+            \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+        }\n+    )\n+\n+# 2. text prompt\n+conversation.append({\"role\": f\"{ds[4]['speaker_id']}\", \"content\": [{\"type\": \"text\", \"text\": ds[4][\"text\"]}]})\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+).to(device)\n+\n+# infer the model\n+audio = model.generate(**inputs, output_audio=True)\n+processor.save_audio(audio, \"example_with_context.wav\")\n+```\n+\n+### Batched Inference\n+\n+CSM supports batched inference!\n+\n+```python\n+import torch\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from datasets import load_dataset, Audio\n+\n+model_id = \"eustlb/csm-1b\"\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+# load the model and the processor\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n+\n+# prepare the inputs \n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+# ensure the audio is 24kHz\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+# here a batch with two prompts\n+conversation = [\n+    [\n+        {\n+            \"role\": f\"{ds[0]['speaker_id']}\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": ds[0][\"text\"]},\n+                {\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n+            ],\n+        },\n+        {\n+            \"role\": f\"{ds[1]['speaker_id']}\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": ds[1][\"text\"]},\n+            ],\n+        },\n+    ],\n+    [\n+        {\n+            \"role\": f\"{ds[0]['speaker_id']}\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": ds[0][\"text\"]},\n+            ],\n+        }\n+    ],\n+]\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+).to(device)\n+\n+audio = model.generate(**inputs, output_audio=True)\n+processor.save_audio(audio, [f\"speech_batch_idx_{i}.wav\" for i in range(len(audio))])\n+```\n+\n+### Making The Model Go Brrr\n+\n+CSM supports full-graph compilation with CUDA graphs!\n+\n+```python\n+import torch\n+import copy\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from datasets import load_dataset\n+\n+model_id = \"eustlb/csm-1b\"\n+device = \"cuda\"\n+\n+# set logs to ensure no recompilation and graph breaks\n+torch._logging.set_logs(graph_breaks=True, recompiles=True, cudagraphs=True)\n+\n+# load the model and the processor\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n+\n+# use static cache, enabling automatically torch compile with fullgraph and reduce-overhead\n+model.generation_config.max_length = 250 # big enough to avoid recompilation\n+model.generation_config.max_new_tokens = None # would take precedence over max_length\n+model.generation_config.cache_implementation = \"static\"\n+model.depth_decoder.generation_config.cache_implementation = \"static\"\n+\n+# generation kwargs\n+gen_kwargs = {\n+    \"do_sample\": False,\n+    \"depth_decoder_do_sample\": False,\n+    \"temperature\": 1.0,\n+    \"depth_decoder_temperature\": 1.0,\n+}\n+\n+# Define a timing decorator\n+class TimerContext:\n+    def __init__(self, name=\"Execution\"):\n+        self.name = name\n+        self.start_event = None\n+        self.end_event = None\n+        \n+    def __enter__(self):\n+        # Use CUDA events for more accurate GPU timing\n+        self.start_event = torch.cuda.Event(enable_timing=True)\n+        self.end_event = torch.cuda.Event(enable_timing=True)\n+        self.start_event.record()\n+        return self\n+\n+    def __exit__(self, *args):\n+        self.end_event.record()\n+        torch.cuda.synchronize()\n+        elapsed_time = self.start_event.elapsed_time(self.end_event) / 1000.0\n+        print(f\"{self.name} time: {elapsed_time:.4f} seconds\")\n+\n+# prepare the inputs \n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+\n+conversation = [\n+    {\n+        \"role\": f\"{ds[0]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[0][\"text\"]},\n+            {\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n+        ],\n+    },\n+    {\n+        \"role\": f\"{ds[1]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[1][\"text\"]},\n+            {\"type\": \"audio\", \"path\": ds[1][\"audio\"][\"array\"]},\n+        ],\n+    },\n+    {\n+        \"role\": f\"{ds[2]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[2][\"text\"]},\n+        ],\n+    },\n+]\n+\n+padded_inputs_1 = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+).to(device)\n+\n+print(\"\\n\" + \"=\"*50)\n+print(\"First generation - compiling and recording CUDA graphs...\")\n+with TimerContext(\"First generation\"):\n+    _ = model.generate(**padded_inputs_1, **gen_kwargs)\n+print(\"=\"*50)\n+\n+print(\"\\n\" + \"=\"*50)\n+print(\"Second generation - fast !!!\")\n+with TimerContext(\"Second generation\"):\n+    _ = model.generate(**padded_inputs_1, **gen_kwargs)\n+print(\"=\"*50)\n+\n+# now with different inputs\n+conversation = [\n+    {\n+        \"role\": f\"{ds[0]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[2][\"text\"]},\n+            {\"type\": \"audio\", \"path\": ds[2][\"audio\"][\"array\"]},\n+        ],\n+    },\n+    {\n+        \"role\": f\"{ds[1]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[3][\"text\"]},\n+            {\"type\": \"audio\", \"path\": ds[3][\"audio\"][\"array\"]},\n+        ],\n+    },\n+    {\n+        \"role\": f\"{ds[2]['speaker_id']}\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": ds[4][\"text\"]},\n+        ],\n+    },\n+]\n+padded_inputs_2 = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+).to(device)\n+\n+print(\"\\n\" + \"=\"*50)\n+print(\"Generation with other inputs!\")\n+with TimerContext(\"Generation with different inputs\"):\n+    _ = model.generate(**padded_inputs_2, **gen_kwargs)\n+print(\"=\"*50)\n+```\n+\n+### Training\n+\n+CSM Transformers integration supports training!\n+\n+```python\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from datasets import load_dataset, Audio\n+\n+model_id = \"eustlb/csm-1b\"\n+device = \"cuda\"\n+\n+# load the model and the processor\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n+model.train()\n+\n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+# ensure the audio is 24kHz\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+conversation = []\n+\n+# context\n+for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+    conversation.append(\n+        {\n+            \"role\": f\"{speaker_id}\",\n+            \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+        }\n+    )\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    return_dict=True,\n+    output_labels=True,\n+).to(device)\n+\n+out = model(**inputs)\n+out.loss.backward()\n+```\n+\n+This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb).\n+The original code can be found [here](https://github.com/SesameAILabs/csm).\n+\n+\n+## CsmConfig\n+\n+[[autodoc]] CsmConfig\n+\n+## CsmDepthDecoderConfig\n+\n+[[autodoc]] CsmDepthDecoderConfig\n+\n+## CsmProcessor\n+\n+[[autodoc]] CsmProcessor\n+    - __call__\n+\n+## CsmForConditionalGeneration\n+\n+[[autodoc]] CsmForConditionalGeneration\n+    - forward\n+    - generate\n+\n+## CsmDepthDecoderForCausalLM\n+\n+[[autodoc]] CsmDepthDecoderForCausalLM\n+\n+## CsmDepthDecoderModel\n+\n+[[autodoc]] CsmDepthDecoderModel\n+\n+## CsmBackboneModel\n+\n+[[autodoc]] CsmBackboneModel"
        },
        {
            "sha": "e980d4cbef5d5346e081277b1687af2cd19423da",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -24,7 +24,12 @@\n import numpy as np\n import requests\n \n-from .utils import is_librosa_available, requires_backends\n+from .utils import (\n+    is_librosa_available,\n+    is_numpy_array,\n+    is_torch_tensor,\n+    requires_backends,\n+)\n \n \n if is_librosa_available():\n@@ -69,6 +74,36 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n ]\n \n \n+def is_valid_audio(audio):\n+    return is_numpy_array(audio) or is_torch_tensor(audio)\n+\n+\n+def is_valid_list_of_audio(audio):\n+    return audio and all(is_valid_audio(audio_i) for audio_i in audio)\n+\n+\n+def make_list_of_audio(\n+    audio: Union[list[AudioInput], AudioInput],\n+) -> AudioInput:\n+    \"\"\"\n+    Ensure that the output is a list of audio.\n+    Args:\n+        audio (`Union[List[AudioInput], AudioInput]`):\n+            The input audio.\n+    Returns:\n+        list: A list of audio.\n+    \"\"\"\n+    # If it's a list of audios, it's already in the right format\n+    if isinstance(audio, (list, tuple)) and is_valid_list_of_audio(audio):\n+        return audio\n+\n+    # If it's a single audio, convert it to a list of\n+    if is_valid_audio(audio):\n+        return [audio]\n+\n+    raise ValueError(\"Invalid input type. Must be a single audio or a list of audio\")\n+\n+\n def hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n     \"\"\"\n     Convert frequency from hertz to mels."
        },
        {
            "sha": "6e47b041abcec66abf2936e5b1632901e79c97c0",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -73,7 +73,7 @@ def __init__(self, max_length: int, max_position_embeddings: Optional[int] = Non\n \n     @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n-        cur_len = input_ids.shape[-1]\n+        cur_len = input_ids.shape[1]\n         is_done = cur_len >= self.max_length\n         if self.max_position_embeddings is not None and not is_done and cur_len >= self.max_position_embeddings:\n             logger.warning_once("
        },
        {
            "sha": "3c7e83369a94540ad9ce41f2a36f269401817842",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -563,7 +563,7 @@ def prepare_inputs_for_generation(\n             if model_inputs[\"inputs_embeds\"] is not None:\n                 batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n             else:\n-                batch_size, sequence_length = model_inputs[input_ids_key].shape\n+                batch_size, sequence_length = model_inputs[input_ids_key].shape[:2]\n \n             # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n             # the 4D causal mask exists, it should be present in the base model (XXXModel class) or in its decoder.\n@@ -1708,7 +1708,7 @@ def _prepare_generation_config(\n \n         return generation_config, model_kwargs\n \n-    def _get_initial_cache_position(self, input_ids, model_kwargs):\n+    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         \"\"\"Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length\"\"\"\n         # `torch.compile`-friendly `torch.arange` from a shape -- the lines below are equivalent to `torch.arange`\n         if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n@@ -1718,7 +1718,7 @@ def _get_initial_cache_position(self, input_ids, model_kwargs):\n                 torch.ones_like(model_kwargs[\"decoder_inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n             )\n         else:\n-            cache_position = torch.ones_like(input_ids[0, :], dtype=torch.int64).cumsum(0) - 1\n+            cache_position = torch.ones(seq_length, dtype=torch.int64, device=device).cumsum(0) - 1\n \n         past_length = 0\n         if model_kwargs.get(\"past_key_values\") is not None:\n@@ -2332,7 +2332,7 @@ def generate(\n             streamer.put(input_ids.cpu())\n \n         # 6. Prepare `max_length` depending on other stopping criteria.\n-        input_ids_length = input_ids.shape[-1]\n+        input_ids_length = input_ids.shape[1]\n         has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n         has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n         generation_config = self._prepare_generated_length(\n@@ -2805,9 +2805,9 @@ def _dola_decoding(\n         decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n \n         # keep track of which sequences are already finished\n-        batch_size = input_ids.shape[0]\n+        batch_size, cur_length = input_ids.shape[:2]\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_length, input_ids.device, model_kwargs)\n \n         this_peer_finished = False\n \n@@ -3016,9 +3016,9 @@ def _contrastive_search(\n             )\n \n         # keep track of which sequences are already finished\n-        batch_size = input_ids.shape[0]\n+        batch_size, cur_len = input_ids.shape[:2]\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         # Create cosine_matrix_mask based on the attention_mask\n         cosine_matrix_mask = torch.ones_like(input_ids, dtype=torch.long)\n@@ -3428,10 +3428,10 @@ def _sample(\n             )\n \n         # keep track of which sequences are already finished\n-        batch_size, cur_len = input_ids.shape\n+        batch_size, cur_len = input_ids.shape[:2]\n         this_peer_finished = False\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         model_forward = self.__call__\n         compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n@@ -3834,7 +3834,7 @@ def _beam_search(\n         num_beams = generation_config.num_beams\n         num_return_sequences = generation_config.num_return_sequences\n \n-        batch_size_unflattened, cur_len = input_ids.shape\n+        batch_size_unflattened, cur_len = input_ids.shape[:2]\n         batch_size = batch_size_unflattened // num_beams\n         # TODO (joao): standardize special cases\n         if self.__class__.__name__ == \"MoshiDepthDecoder\":\n@@ -3857,7 +3857,7 @@ def _beam_search(\n             dim=0,\n         ).to(input_ids.device)\n \n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         # (joao) feature lost in the refactor. Probably won't implement, hurts readability with minimal gains (there\n         # are newer low-memory alternatives like the offloaded cache)\n@@ -4156,7 +4156,7 @@ def _group_beam_search(\n         device = input_ids.device\n \n         batch_beam_size, cur_len = input_ids.shape\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         if return_dict_in_generate and output_scores:\n             beam_indices = [tuple(() for _ in range(num_sub_beams * batch_size)) for _ in range(num_beam_groups)]\n@@ -4190,7 +4190,7 @@ def _group_beam_search(\n \n         this_peer_finished = False\n \n-        decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n+        decoder_prompt_len = input_ids.shape[1]  # record the prompt length of decoder\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             # predicted tokens in cur_len step\n             current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n@@ -4444,8 +4444,8 @@ def _constrained_beam_search(\n         batch_size = len(constrained_beam_scorer._beam_hyps)\n         num_beams = constrained_beam_scorer.num_beams\n \n-        batch_beam_size, cur_len = input_ids.shape\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        batch_beam_size, cur_len = input_ids.shape[:2]\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         if num_beams * batch_size != batch_beam_size:\n             raise ValueError(\n@@ -4477,7 +4477,7 @@ def _constrained_beam_search(\n \n         this_peer_finished = False\n \n-        decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n+        decoder_prompt_len = input_ids.shape[1]  # record the prompt length of decoder\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n \n@@ -4698,14 +4698,14 @@ def _assisted_decoding(\n             )\n \n         # keep track of which sequences are already finished\n-        batch_size = input_ids.shape[0]\n+        batch_size, cur_len = input_ids.shape[:2]\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n         this_peer_finished = False\n         is_first_iteration = True  # to preserve the same API in the output as other generation methods\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            cur_len = input_ids.shape[-1]\n+            cur_len = input_ids.shape[1]\n \n             #  1. Fetch candidate sequences from a `CandidateGenerator` and move to the correct device\n             candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n@@ -4795,7 +4795,7 @@ def _assisted_decoding(\n             input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n             if streamer is not None:\n                 streamer.put(valid_tokens.cpu())\n-            new_cur_len = input_ids.shape[-1]\n+            new_cur_len = input_ids.shape[1]\n \n             # 4.2. Discard past key values relative to unused assistant tokens\n             new_cache_size = new_cur_len - 1"
        },
        {
            "sha": "aad42d3fd528499986d29cc7310386006ffec8a1",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -158,4 +158,5 @@ def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"DFineForObjectDetection\": DFineForObjectDetectionLoss,\n+    \"CsmForConditionalGeneration\": ForCausalLMLoss,\n }"
        },
        {
            "sha": "8d713b482ba92f5f654d6147bbc5502fb2a52492",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -68,6 +68,7 @@\n     from .convnextv2 import *\n     from .cpm import *\n     from .cpmant import *\n+    from .csm import *\n     from .ctrl import *\n     from .cvt import *\n     from .d_fine import *"
        },
        {
            "sha": "01c58a5062925deecd31238b312a6cc92d05382a",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -80,6 +80,7 @@\n         (\"convnext\", \"ConvNextConfig\"),\n         (\"convnextv2\", \"ConvNextV2Config\"),\n         (\"cpmant\", \"CpmAntConfig\"),\n+        (\"csm\", \"CsmConfig\"),\n         (\"ctrl\", \"CTRLConfig\"),\n         (\"cvt\", \"CvtConfig\"),\n         (\"d_fine\", \"DFineConfig\"),\n@@ -437,6 +438,7 @@\n         (\"convnextv2\", \"ConvNeXTV2\"),\n         (\"cpm\", \"CPM\"),\n         (\"cpmant\", \"CPM-Ant\"),\n+        (\"csm\", \"CSM\"),\n         (\"ctrl\", \"CTRL\"),\n         (\"cvt\", \"CvT\"),\n         (\"d_fine\", \"D-FINE\"),"
        },
        {
            "sha": "b196a7718f7bce6faecb8664de1d98a2b6d62b7f",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -78,6 +78,7 @@\n         (\"convnext\", \"ConvNextModel\"),\n         (\"convnextv2\", \"ConvNextV2Model\"),\n         (\"cpmant\", \"CpmAntModel\"),\n+        (\"csm\", \"CsmForConditionalGeneration\"),\n         (\"ctrl\", \"CTRLModel\"),\n         (\"cvt\", \"CvtModel\"),\n         (\"d_fine\", \"DFineModel\"),\n@@ -1446,6 +1447,7 @@\n     [\n         # Model for Text-To-Waveform mapping\n         (\"bark\", \"BarkModel\"),\n+        (\"csm\", \"CsmForConditionalGeneration\"),\n         (\"fastspeech2_conformer\", \"FastSpeech2ConformerWithHifiGan\"),\n         (\"musicgen\", \"MusicgenForConditionalGeneration\"),\n         (\"musicgen_melody\", \"MusicgenMelodyForConditionalGeneration\"),"
        },
        {
            "sha": "59468442b52eb71fbcb984c28bb465cec2be91e5",
            "filename": "src/transformers/models/csm/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2F__init__.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_csm import *\n+    from .modeling_csm import *\n+    from .processing_csm import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e6d6d2e27c6a7b1d561740010dd3ba75f2255249",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "added",
            "additions": 440,
            "deletions": 0,
            "changes": 440,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,440 @@\n+# coding=utf-8\n+# Copyright 2025 Sesame and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import logging\n+from ..auto.configuration_auto import AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class CsmDepthDecoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`CsmDepthDecoderModel`]. It is used to instantiate an CSM depth decoder\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n+    a similar configuration to that of the csm-1b.\n+\n+    e.g. [eustlb/csm-1b](https://huggingface.co/eustlb/csm-1b)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        num_codebooks (`int`, *optional*, defaults to 32):\n+            Number of codebooks used in the underlying codec model responsible for tokenizing the audio.\n+        backbone_hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations of the backbone model used with this depth decoder.\n+        vocab_size (`int`, *optional*, defaults to 2051):\n+            Vocabulary size of the CsmDepthDecoder model. Defines the number of different audio tokens that can be represented by each codebook.\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 4):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 2):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 33):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 2050):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*):\n+            End of stream token id.\n+        rope_theta (`float`, *optional*, defaults to 500000):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n+\n+    ```python\n+    >>> from transformers import CsmDepthDecoder, CsmDepthDecoderConfig\n+\n+    >>> # Initializing a CsmDepthDecoder\n+    >>> configuration = CsmDepthDecoderConfig()\n+    >>> model = CsmDepthDecoderModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"csm_depth_decoder_model\"\n+    base_config_key = \"depth_decoder_config\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        num_codebooks=32,\n+        backbone_hidden_size=2048,\n+        vocab_size=2051,\n+        hidden_size=1024,\n+        intermediate_size=8192,\n+        num_hidden_layers=4,\n+        num_attention_heads=8,\n+        num_key_value_heads=2,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=33,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=None,\n+        eos_token_id=None,\n+        rope_theta=500000,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        if kwargs.pop(\"tie_word_embeddings\", False):\n+            raise ValueError(\"`tie_word_embeddings=True` is not supported for CsmDepthDecoderConfig\")\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=False,\n+            **kwargs,\n+        )\n+        self.num_codebooks = num_codebooks\n+        self.vocab_size = vocab_size\n+        self.backbone_hidden_size = backbone_hidden_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+\n+class CsmConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`CsmForConditionalGeneration`]. It is used to instantiate an CSM\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the csm-1b.\n+\n+    e.g. [eustlb/csm-1b](https://huggingface.co/eustlb/csm-1b)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        num_codebooks (`int`, *optional*, defaults to 32):\n+            Number of codebooks used in the underlying codec model responsible for tokenizing the audio.\n+        vocab_size (`int`, *optional*, defaults to 2051):\n+            Vocabulary size of the Csm model. Defines the number of different audio tokens that can be represented by each codebook.\n+        text_vocab_size (`int`, *optional*, defaults to 128256):\n+            Vocabulary size of the text input for the Csm model. Defines the number of different text tokens that can be represented.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations of the backbone model.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the MLP representations of the backbone model.\n+        num_hidden_layers (`int`, *optional*, defaults to 16):\n+            Number of hidden layers in the backbone model Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the backbone model Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf).\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the backbone model Transformer decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 128002):\n+            Padding token id.\n+        codebook_pad_token_id (`int`, *optional*, defaults to 2050):\n+            Padding token id for codebook tokens.\n+        codebook_eos_token_id (`int`, *optional*, defaults to 0):\n+            End of stream token id for codebook tokens.\n+        bos_token_id (`int`, *optional*, defaults to 128000):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*):\n+            End of stream token id.\n+        audio_token_id (`int`, *optional*, defaults to 128002):\n+            Audio token id in the text input.\n+        audio_eos_token_id (`int`, *optional*, defaults to 128003):\n+            End of stream token id for audio in the text input.\n+        rope_theta (`float`, *optional*, defaults to 500000):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*, defaults to `{'factor': 32.0, 'high_freq_factor': 0.5, 'low_freq_factor': 0.125, 'original_max_position_embeddings': 1024, 'rope_type': 'llama3'}`):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n+        tie_codebooks_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie the codebook tokens embeddings of the backbone model to the codebook tokens embeddings of the depth decoder.\n+        depth_decoder_config (`CsmDepthDecoderConfig`, *optional*):\n+            Configuration for the depth decoder.\n+        codec_config (`PretrainedConfig`, *optional*):\n+            Configuration for the codec.\n+\n+    ```python\n+    >>> from transformers import CsmForConditionalGeneration, CsmConfig\n+\n+    >>> # Initializing a CsmConfig\n+    >>> configuration = CsmConfig()\n+\n+    >>> # Initializing a model\n+    >>> model = CsmForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"csm\"\n+    base_config_key = \"csm_config\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    sub_configs = {\n+        \"codec_config\": AutoConfig,\n+        \"depth_decoder_config\": CsmDepthDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        num_codebooks=32,\n+        vocab_size=2051,\n+        text_vocab_size=128256,\n+        hidden_size=2048,\n+        intermediate_size=8192,\n+        num_hidden_layers=16,\n+        num_attention_heads=32,\n+        num_key_value_heads=8,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=128002,\n+        codebook_pad_token_id=2050,\n+        codebook_eos_token_id=0,\n+        bos_token_id=128000,\n+        eos_token_id=None,\n+        audio_token_id=128002,\n+        audio_eos_token_id=128003,\n+        rope_theta=500000,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        head_dim=None,\n+        tie_codebooks_embeddings=True,\n+        depth_decoder_config=None,\n+        codec_config=None,\n+        **kwargs,\n+    ):\n+        if kwargs.pop(\"tie_word_embeddings\", False):\n+            raise ValueError(\"`tie_word_embeddings=True` is not supported for CsmConfig\")\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=False,\n+            **kwargs,\n+        )\n+\n+        if depth_decoder_config is None:\n+            self.depth_decoder_config = CsmDepthDecoderConfig()\n+            logger.info(\"depth_decoder_config is None, using default depth decoder config.\")\n+        elif isinstance(depth_decoder_config, dict):\n+            self.depth_decoder_config = CsmDepthDecoderConfig(**depth_decoder_config)\n+        elif isinstance(depth_decoder_config, CsmDepthDecoderConfig):\n+            self.depth_decoder_config = depth_decoder_config\n+\n+        if codec_config is None:\n+            self.codec_config = AutoConfig.for_model(\"mimi\")\n+            logger.info(\"codec_config is None, using default audio encoder config.\")\n+        elif isinstance(codec_config, dict):\n+            self.codec_config = AutoConfig.for_model(**codec_config)\n+        elif isinstance(codec_config, PretrainedConfig):\n+            self.codec_config = codec_config\n+\n+        self.text_vocab_size = text_vocab_size\n+        self.num_codebooks = num_codebooks\n+        self.audio_token_id = audio_token_id\n+        self.audio_eos_token_id = audio_eos_token_id\n+        self.codebook_pad_token_id = codebook_pad_token_id\n+        self.codebook_eos_token_id = codebook_eos_token_id\n+        self.tie_codebooks_embeddings = tie_codebooks_embeddings\n+\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+\n+__all__ = [\n+    \"CsmDepthDecoderConfig\",\n+    \"CsmConfig\",\n+]"
        },
        {
            "sha": "dc84e2cf3daf8908cd804feaa0e6dd0587edfccd",
            "filename": "src/transformers/models/csm/convert_csm.py",
            "status": "added",
            "additions": 339,
            "deletions": 0,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,339 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import os\n+import re\n+\n+import torch\n+from tokenizers.processors import TemplateProcessing\n+\n+from transformers import (\n+    AutoFeatureExtractor,\n+    AutoTokenizer,\n+    CsmConfig,\n+    CsmDepthDecoderConfig,\n+    CsmForConditionalGeneration,\n+    CsmProcessor,\n+    MimiModel,\n+)\n+from transformers.utils.hub import cached_file\n+\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"backbone\\.layers\\.(\\d+)\":                r\"backbone_model.layers.\\1\",\n+    r\"decoder\\.layers\\.(\\d+)\":            r\"depth_decoder.model.layers.\\1\",\n+\n+    r\"attn\":                                                  r\"self_attn\",\n+    r\"output_proj\":                                              r\"o_proj\",\n+    r\"w1\":                                                    r\"gate_proj\",\n+    r\"w2\":                                                    r\"down_proj\",\n+    r\"w3\":                                                      r\"up_proj\",\n+\n+    r\"text_embeddings\":   r\"embed_text_tokens\",\n+    r\"audio_embeddings\": r\"backbone_model.embed_tokens.embed_audio_tokens\",\n+\n+    r\"codebook0_head\":                                          r\"lm_head\",\n+    r\"audio_head\":                  r\"depth_decoder.codebooks_head.weight\",\n+    r\"projection\":          r\"depth_decoder.model.inputs_embeds_projector\",\n+\n+    r\"sa_norm.scale\":                            r\"input_layernorm.weight\",\n+    r\"mlp_norm.scale\":                  r\"post_attention_layernorm.weight\",\n+    r\"decoder.norm.scale\":              r\"depth_decoder.model.norm.weight\",\n+    r\"backbone.norm.scale\":                  r\"backbone_model.norm.weight\",\n+}\n+# fmt: on\n+\n+\n+def permute_for_rope(input_tensor, n_heads, dim1, dim2):\n+    \"\"\"\n+    When you go from the complex ROPE formulation to sin and cos one, you need\n+    to permute the query and key weights (to avoid doing it on the fly)\n+    \"\"\"\n+    input_tensor = input_tensor.reshape(dim1, dim2)\n+    input_tensor = input_tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    input_tensor = input_tensor.transpose(1, 2).reshape(dim1, dim2)\n+    return input_tensor\n+\n+\n+def convert_key(key, mapping):\n+    for pattern, replacement in mapping.items():\n+        key = re.sub(pattern, replacement, key)\n+    return key\n+\n+\n+def write_model(\n+    input_path_or_repo,\n+    model_name,\n+    codec_model_path_or_repo,\n+    output_dir,\n+    safe_serialization=True,\n+):\n+    print(\"Converting the model.\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    codec_model = MimiModel.from_pretrained(codec_model_path_or_repo)\n+    codec_model.config._attn_implementation_autoset = False\n+\n+    # prepare rope scaling args: the model uses originally\n+    # 1 - for the depth decoder\n+    # rope_theta=500000,\n+    # rope_scaling={\n+    # \t\"factor\": 32.0,\n+    # \t\"high_freq_factor\": 4.0,\n+    # \t\"low_freq_factor\": 1.0,\n+    # \t\"original_max_position_embeddings\": 8192,\n+    # \t\"rope_type\": \"llama3\",\n+    # },\n+    # 2 - for the backbone\n+    # rope_theta=500000,\n+    # rope_scaling={\n+    # \t\"factor\": 32.0,\n+    # \t\"high_freq_factor\": 4.0,\n+    # \t\"low_freq_factor\": 1.0,\n+    # \t\"original_max_position_embeddings\": 8192,\n+    # \t\"rope_type\": \"llama3\",\n+    # },\n+    #\n+    # Yet we want to use max_position_embeddings=32, resp. 2048\n+    # This will throw warning as we would have original_max_position_embeddings >= max_position_embeddings\n+    # Therefore, we convert values to equivalent ones\n+\n+    depth_decoder_config = CsmDepthDecoderConfig(\n+        rope_scaling={\n+            \"factor\": 32.0,\n+            \"high_freq_factor\": 0.0078125,\n+            \"low_freq_factor\": 0.001953125,\n+            \"original_max_position_embeddings\": 16,\n+            \"rope_type\": \"llama3\",\n+        },\n+    )\n+\n+    config = CsmConfig(\n+        codec_config=codec_model.config,\n+        depth_decoder_config=depth_decoder_config,\n+        rope_scaling={\n+            \"factor\": 32.0,\n+            \"high_freq_factor\": 0.5,\n+            \"low_freq_factor\": 0.125,\n+            \"original_max_position_embeddings\": 1024,\n+            \"rope_type\": \"llama3\",\n+        },\n+    )\n+\n+    params = {\n+        \"backbone\": {\n+            \"num_attention_heads\": config.num_attention_heads,\n+            \"num_key_value_heads\": config.num_key_value_heads,\n+            \"dim_per_head\": config.head_dim,\n+            \"key_value_dim\": config.head_dim * config.num_key_value_heads,\n+            \"dim\": config.hidden_size,\n+        },\n+        \"depth_decoder\": {\n+            \"num_attention_heads\": config.depth_decoder_config.num_attention_heads,\n+            \"num_key_value_heads\": config.depth_decoder_config.num_key_value_heads,\n+            \"dim_per_head\": config.depth_decoder_config.head_dim,\n+            \"key_value_dim\": config.depth_decoder_config.head_dim * config.depth_decoder_config.num_key_value_heads,\n+            \"dim\": config.depth_decoder_config.hidden_size,\n+        },\n+    }\n+\n+    model_path = cached_file(\n+        input_path_or_repo,\n+        model_name,\n+    )\n+    print(f\"Fetching all parameters from the checkpoint at {model_path}...\")\n+    loaded = torch.load(model_path, map_location=\"cpu\")\n+\n+    print(\"Converting model...\")\n+    state_dict = {}\n+\n+    # -----------------------\n+    # convert parameter names\n+    # -----------------------\n+\n+    # Add codec_model. prefix to every key in the codec model state dict\n+    codec_state_dict = {f\"codec_model.{k}\": v for k, v in codec_model.state_dict().items()}\n+    state_dict.update(codec_state_dict)\n+\n+    for key, value in loaded.items():\n+        new_key = convert_key(key, ORIGINAL_TO_CONVERTED_KEY_MAPPING)\n+        current_parameter = value\n+\n+        # Post-process the current_parameter.\n+        if re.search(\"(k|q)_proj.weight\", new_key):\n+            params_keys = \"backbone\" if \"backbone\" in new_key else \"depth_decoder\"\n+            if \"q_proj\" in new_key:\n+                num_heads = params[params_keys][\"num_attention_heads\"]\n+                dim_per_head = params[params_keys][\"dim_per_head\"]\n+                param_dim = params[params_keys][\"dim\"]\n+                dim = params[params_keys][\"dim\"]\n+            else:\n+                num_heads = params[params_keys][\"num_key_value_heads\"]\n+                dim_per_head = params[params_keys][\"dim_per_head\"]\n+                param_dim = params[params_keys][\"key_value_dim\"]\n+                dim = params[params_keys][\"dim\"]\n+\n+            current_parameter = permute_for_rope(value, num_heads, param_dim, dim)\n+            state_dict[new_key] = current_parameter.reshape(num_heads * dim_per_head, dim)\n+\n+        state_dict[new_key] = current_parameter\n+\n+    # add the depth decoder embed audio tokens weights, latter tied to the backbone embed audio tokens weights\n+    state_dict[\"depth_decoder.model.embed_tokens.weight\"] = state_dict[\n+        \"backbone_model.embed_tokens.embed_audio_tokens.weight\"\n+    ].clone()\n+    del loaded\n+    gc.collect()\n+\n+    # -------------------------\n+    # load the weights and save\n+    # -------------------------\n+\n+    print(\"Loading the checkpoint in a Csm model.\")\n+    with torch.device(\"meta\"):\n+        model = CsmForConditionalGeneration(config)\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    # default generation config\n+    model.generation_config._from_model_config = False\n+    model.generation_config.max_new_tokens = 125\n+    model.generation_config.do_sample = True\n+    model.generation_config.top_k = 50\n+    model.generation_config.temperature = 0.9\n+    model.generation_config.depth_decoder_do_sample = True\n+    model.generation_config.depth_decoder_top_k = 50\n+    model.generation_config.depth_decoder_temperature = 0.9\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    CsmForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n+\n+def write_tokenizer(output_dir):\n+    # from https://github.com/SesameAILabs/csm/blob/2d720827843b653c4d67bb4445b1c0a4f59e646f/generator.py#L22-L36\n+    def load_llama3_tokenizer():\n+        \"\"\"\n+        https://github.com/huggingface/transformers/issues/22794#issuecomment-2092623992\n+        \"\"\"\n+        tokenizer_name = \"meta-llama/Llama-3.2-1B\"\n+        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n+        bos = tokenizer.bos_token\n+        eos = tokenizer.eos_token\n+        tokenizer._tokenizer.post_processor = TemplateProcessing(\n+            single=f\"{bos}:0 $A:0 {eos}:0\",\n+            pair=f\"{bos}:0 $A:0 {eos}:0 {bos}:1 $B:1 {eos}:1\",\n+            special_tokens=[(f\"{bos}\", tokenizer.bos_token_id), (f\"{eos}\", tokenizer.eos_token_id)],\n+        )\n+\n+        return tokenizer\n+\n+    tokenizer = load_llama3_tokenizer()\n+    tokenizer.pad_token = tokenizer.eos_token\n+    tokenizer.save_pretrained(output_dir)\n+\n+    # manually modify in tokenizer_config.json\n+    # \"128002\": {\n+    #     \"content\": \"<|AUDIO|>\",\n+    #     ...\n+    # }\n+    # \"128003\": {\n+    #     \"content\": \"<|audio_eos|>\",\n+    #     ...\n+    # }\n+    print(\n+        \"Tokenizer saved successfully. Please manually modify in tokenizer_config.json AND tokenizer.json as follows: \"\n+    )\n+    print(\"\"\"\n+    # \"128002\": {\n+    #     \"content\": \"<|AUDIO|>\",\n+    #     ...\n+    # }\n+    # \"128003\": {\n+    #     \"content\": \"<|audio_eos|>\",\n+    #     ...\n+    # }\n+    \"\"\")\n+\n+\n+def write_processor(output_dir, codec_model_path_or_repo):\n+    chat_template = \"\\n{%- for message in messages %}\\n    {#-- Validate role is a stringified integer --#}\\n    {%- if not message['role'] is string or not message['role'].isdigit() %}\\n        {{- raise_exception(\\\"The role must be an integer or a stringified integer (e.g. '0') designating the speaker id\\\") }}\\n    {%- endif %}\\n\\n    {#-- Validate content is a list --#}\\n    {%- set content = message['content'] %}\\n    {%- if content is not iterable or content is string %}\\n        {{- raise_exception(\\\"The content must be a list\\\") }}\\n    {%- endif %}\\n\\n    {#-- Collect content types --#}\\n    {%- set content_types = content | map(attribute='type') | list %}\\n    {%- set is_last = loop.last %}\\n\\n    {#-- Last message validation --#}\\n    {%- if is_last %}\\n        {%- if 'text' not in content_types %}\\n            {{- raise_exception(\\\"The last message must include one item of type 'text'\\\") }}\\n        {%- elif (content_types | select('equalto', 'text') | list | length > 1) or (content_types | select('equalto', 'audio') | list | length > 1) %}\\n            {{- raise_exception(\\\"At most two items are allowed in the last message: one 'text' and one 'audio'\\\") }}\\n        {%- endif %}\\n\\n    {#-- All other messages validation --#}\\n    {%- else %}\\n        {%- if content_types | select('equalto', 'text') | list | length != 1\\n              or content_types | select('equalto', 'audio') | list | length != 1 %}\\n            {{- raise_exception(\\\"Each message (except the last) must contain exactly one 'text' and one 'audio' item\\\") }}\\n        {%- elif content_types | reject('in', ['text', 'audio']) | list | length > 0 %}\\n            {{- raise_exception(\\\"Only 'text' and 'audio' types are allowed in content\\\") }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{%- for message in messages %}\\n    {{- bos_token }}\\n    {{- '[' + message['role'] + ']' }}\\n    {{- message['content'][0]['text'] }}\\n    {{- eos_token }}\\n    {%- if message['content']|length > 1 %}\\n        {{- '<|AUDIO|><|audio_eos|>' }}\\n    {%- endif %}\\n{%- endfor %}\\n\"\n+    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n+    feature_extractor = AutoFeatureExtractor.from_pretrained(codec_model_path_or_repo)\n+\n+    processor = CsmProcessor(\n+        tokenizer=tokenizer,\n+        feature_extractor=feature_extractor,\n+        chat_template=chat_template,\n+    )\n+\n+    processor.save_pretrained(output_dir)\n+    print(\"Processor saved successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert Csm weights to HuggingFace format\")\n+    parser.add_argument(\n+        \"--input_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing Csm weights\",\n+    )\n+    parser.add_argument(\n+        \"--model_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the model in input_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--codec_model_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing the codec model\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    args = parser.parse_args()\n+\n+    write_model(\n+        args.input_path_or_repo,\n+        args.model_name,\n+        args.codec_model_path_or_repo,\n+        output_dir=args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    write_tokenizer(args.output_dir)\n+\n+    write_processor(args.output_dir, args.codec_model_path_or_repo)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "b1c2cd920d75f58b74f053639bc340b60d4bd6fa",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "added",
            "additions": 491,
            "deletions": 0,
            "changes": 491,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,491 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+from dataclasses import dataclass\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...generation import (\n+    GenerateDecoderOnlyOutput,\n+    GenerationConfig,\n+    GenerationMixin,\n+    GenerationMode,\n+)\n+from ...generation.logits_process import LogitsProcessorList\n+from ...generation.stopping_criteria import MaxLengthCriteria, StoppingCriteriaList\n+from ...generation.utils import GenerateNonBeamOutput\n+from ...utils import logging\n+\n+\n+if TYPE_CHECKING:\n+    from ...generation.streamers import BaseStreamer\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@dataclass\n+class CsmGenerateOutput(GenerateDecoderOnlyOutput):\n+    \"\"\"\n+    Outputs of CsmForConditionalGeneration.generate.\n+\n+    Args:\n+        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n+            if all batches finished early due to the `eos_token_id`.\n+        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n+            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n+            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n+        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n+        audio (`list(torch.FloatTensor)` of length `batch_size`):\n+            The generated audio.\n+    \"\"\"\n+\n+    audio: Optional[List[torch.Tensor]] = None\n+\n+\n+class CsmGenerationMixin(GenerationMixin):\n+    def _get_stopping_criteria(\n+        self,\n+        *args,\n+        **kwargs,\n+    ) -> StoppingCriteriaList:\n+        criteria = super()._get_stopping_criteria(*args, **kwargs)\n+\n+        kept_criteria = StoppingCriteriaList()\n+        for criterion in criteria:\n+            if not isinstance(criterion, MaxLengthCriteria):\n+                logger.warning(\n+                    f\"Csm does not support {criterion.__class__.__name__} stopping criteria, it will be ignored.\"\n+                )\n+            else:\n+                kept_criteria.append(criterion)\n+        return kept_criteria\n+\n+    def _prepare_generation_config(\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Dict\n+    ) -> Tuple[GenerationConfig, Dict]:\n+        \"\"\"\n+        This method overrides [~generation.utils.GenerationMixin._prepare_generation_config].\n+        It ensures that the depth decoder generation config is initialized and that passed args as depth_decoder_* are properly handled.\n+        \"\"\"\n+        # extract depth decoder kwargs and remove them from the main kwargs\n+        depth_decoder_kwargs = {\n+            k[len(\"depth_decoder_\") :]: v for k, v in kwargs.items() if k.startswith(\"depth_decoder_\")\n+        }\n+\n+        # remove the depth decoder keys from the original kwargs\n+        kwargs = {k: v for k, v in kwargs.items() if not k.startswith(\"depth_decoder_\")}\n+\n+        # initialize the generation config\n+        generation_config, model_kwargs = super()._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n+        self.depth_decoder.generation_config.update(**depth_decoder_kwargs)\n+\n+        # ensure the depth decoder generation config is valid\n+        depth_decoder_min_new_tokens = getattr(self.depth_decoder.generation_config, \"min_new_tokens\") or (\n+            self.config.num_codebooks - 1\n+        )\n+        depth_decoder_max_new_tokens = getattr(self.depth_decoder.generation_config, \"max_new_tokens\") or (\n+            self.config.num_codebooks - 1\n+        )\n+\n+        if {depth_decoder_min_new_tokens, depth_decoder_max_new_tokens} != {self.config.num_codebooks - 1}:\n+            raise ValueError(\n+                f\"depth_decoder_generation_config's min_new_tokens ({depth_decoder_min_new_tokens}) and max_new_tokens ({depth_decoder_max_new_tokens}) must be equal to self.config.num_codebooks - 1 ({self.config.num_codebooks - 1})\"\n+            )\n+        elif self.depth_decoder.generation_config.return_dict_in_generate:\n+            logger.warning(\n+                \"depth_decoder_generation_config.return_dict_in_generate is set to True, but this will be ignored as the depth decoder model does not return a dictionary in generate\"\n+            )\n+            self.depth_decoder.generation_config.return_dict_in_generate = False\n+\n+        self.depth_decoder.generation_config.min_new_tokens = depth_decoder_min_new_tokens\n+        self.depth_decoder.generation_config.max_new_tokens = depth_decoder_max_new_tokens\n+\n+        # Monkey patch the get_generation_mode method to support CSM model\n+        original_get_generation_mode = generation_config.get_generation_mode\n+\n+        def patched_get_generation_mode(assistant_model=None):\n+            generation_mode = original_get_generation_mode(assistant_model)\n+            if generation_mode not in [GenerationMode.GREEDY_SEARCH, GenerationMode.SAMPLE]:\n+                raise ValueError(\n+                    f\"Generation mode {generation_mode} is not supported for CSM model. Please set generation parameters to use greedy or sampling generation.\"\n+                )\n+\n+            return generation_mode\n+\n+        generation_config.get_generation_mode = patched_get_generation_mode\n+\n+        return generation_config, model_kwargs\n+\n+    def _sample(\n+        self,\n+        input_ids: torch.LongTensor,\n+        logits_processor: LogitsProcessorList,\n+        stopping_criteria: StoppingCriteriaList,\n+        generation_config: GenerationConfig,\n+        synced_gpus: bool,\n+        streamer: Optional[\"BaseStreamer\"],\n+        **model_kwargs,\n+    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n+        \"\"\"\n+        This method overrides [~generation.utils.GenerationMixin._sample].\n+        To ease maintenance, modifications are marked with the comment \"Csm specific\".\n+\n+        Indeed, Csm model requires a custom generation sampling step:\n+        1. Infer the backbone model to sample the first codebook token\n+        2. Call generate on the depth decoder with the first codebook token as input_ids to sample the next codebook tokens\n+        3. Use these generated codebook tokens as input_ids to sample the next first codebook token using the backbone model\n+        4. Repeat until stopping criteria is met\n+\n+        Csm supports two stopping criterias:\n+        - stop when the generated sequence is at max_length\n+        - stop when all the generated codebook tokens are the codebook_eos_token_id\n+        \"\"\"\n+        # init values\n+        # *************** Csm specific ***************\n+        pad_token_id = self.config.codebook_pad_token_id\n+        has_eos_stopping_criteria = generation_config._eos_token_tensor is not None\n+        # ============================================\n+        output_attentions = generation_config.output_attentions\n+        output_hidden_states = generation_config.output_hidden_states\n+        output_scores = generation_config.output_scores\n+        output_logits = generation_config.output_logits\n+        return_dict_in_generate = generation_config.return_dict_in_generate\n+        do_sample = generation_config.do_sample\n+\n+        # init attention / hidden states / scores tuples\n+        scores = () if (return_dict_in_generate and output_scores) else None\n+        raw_logits = () if (return_dict_in_generate and output_logits) else None\n+        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n+        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n+\n+        # keep track of which sequences are already finished\n+        batch_size, cur_len = input_ids.shape[:2]\n+        this_peer_finished = False\n+        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n+        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n+\n+        # *************** Csm specific ***************\n+        if input_ids.ndim == 2 and model_kwargs.get(\"inputs_embeds\") is None:\n+            # in the case where the passed input_ids correspond to text tokens, i.e. don't have a third dimension for codebook ids,\n+            # we need to remove the input length to the MaxLengthCriteria stopping criteria has such input are not returned\n+            for criterion in stopping_criteria:\n+                if isinstance(criterion, MaxLengthCriteria):\n+                    criterion.max_length -= cur_len\n+        # ============================================\n+\n+        model_forward = self.__call__\n+        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n+        if compile_forward:\n+            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+            model_forward = self.get_compiled_call(generation_config.compile_config)\n+\n+        is_prefill = True\n+        while self._has_unfinished_sequences(\n+            this_peer_finished,\n+            synced_gpus,\n+            device=input_ids.device,\n+        ):\n+            # prepare model inputs\n+            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+\n+            # prepare variable output controls (note: some models won't accept all output controls)\n+            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n+            # *************** Csm specific ***************\n+            model_inputs.update({\"output_hidden_states\": True})\n+            # ============================================\n+\n+            if is_prefill:\n+                outputs = self(**model_inputs, return_dict=True)\n+                is_prefill = False\n+            else:\n+                outputs = model_forward(**model_inputs, return_dict=True)\n+\n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+            )\n+            if synced_gpus and this_peer_finished:\n+                continue\n+\n+            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n+            # (the clone itself is always small)\n+            next_token_logits = outputs.logits[:, -1, :].clone().float()\n+            next_token_logits = next_token_logits.to(input_ids.device)\n+\n+            # pre-process distribution\n+            next_token_scores = logits_processor(input_ids, next_token_logits)\n+\n+            # Store scores, attentions and hidden_states when required\n+            if return_dict_in_generate:\n+                if output_scores:\n+                    scores += (next_token_scores,)\n+                if output_logits:\n+                    raw_logits += (next_token_logits,)\n+                if output_attentions:\n+                    decoder_attentions += (outputs.attentions,)\n+\n+                if output_hidden_states:\n+                    decoder_hidden_states += (outputs.hidden_states,)\n+\n+            # token selection\n+            if do_sample:\n+                probs = nn.functional.softmax(next_token_scores, dim=-1)\n+                # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\n+                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n+            else:\n+                next_tokens = torch.argmax(next_token_scores, dim=-1)\n+\n+            # *************** Csm specific ***************\n+            # infer the depth decoder\n+            first_codebook_ids = next_tokens[:, None]\n+            # adds place holder in position 0 that will be replaced by the backbone_last_hidden_state\n+            depth_decoder_input_ids = nn.functional.pad(first_codebook_ids, (1, 0), value=0)\n+            backbone_last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n+\n+            depth_decoder_outputs = self.depth_decoder.generate(\n+                input_ids=depth_decoder_input_ids, backbone_last_hidden_state=backbone_last_hidden_state.clone()\n+            )\n+            codebook_ids = (\n+                depth_decoder_outputs\n+                if isinstance(depth_decoder_outputs, torch.Tensor)\n+                else depth_decoder_outputs.sequences\n+            )\n+            # remove the place holder in position 0\n+            codebook_ids = codebook_ids[:, 1:]\n+            next_tokens = codebook_ids\n+\n+            # finished sentences should have their next token be a padding token\n+            if has_eos_stopping_criteria:\n+                next_tokens = next_tokens * unfinished_sequences.unsqueeze(-1) + pad_token_id * (\n+                    1 - unfinished_sequences.unsqueeze(-1)\n+                )\n+\n+            # update generated ids, model inputs, and length for next step\n+            if input_ids.ndim == 2:\n+                input_ids = next_tokens[:, None, :]\n+            else:\n+                input_ids = torch.cat([input_ids, next_tokens[:, None, :]], dim=1)\n+            # ============================================\n+\n+            if streamer is not None:\n+                streamer.put(next_tokens.cpu())\n+\n+            # *************** Csm specific ***************\n+            # for the eos stopping criteria, is it expected that the eos token is the same for each codebook !!!!\n+            unfinished_sequences = unfinished_sequences & ~(\n+                input_ids[:, -1, :-1] == self.config.codebook_eos_token_id\n+            ).all(-1)\n+            # ============================================\n+            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n+            this_peer_finished = unfinished_sequences.max() == 0\n+            cur_len += 1\n+\n+            # This is needed to properly delete outputs.logits which may be very large for first iteration\n+            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n+            del outputs\n+\n+            # *************** Csm specific ***************\n+            del depth_decoder_outputs\n+            # ============================================\n+\n+        if streamer is not None:\n+            streamer.end()\n+\n+        if return_dict_in_generate:\n+            return GenerateDecoderOnlyOutput(\n+                sequences=input_ids,\n+                scores=scores,\n+                logits=raw_logits,\n+                attentions=decoder_attentions,\n+                hidden_states=decoder_hidden_states,\n+                past_key_values=model_kwargs.get(\"past_key_values\"),\n+            )\n+        else:\n+            return input_ids\n+\n+    def generate(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        input_values: Optional[torch.Tensor] = None,\n+        input_values_cutoffs: Optional[torch.Tensor] = None,\n+        generation_config: Optional[GenerationConfig] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        stopping_criteria: Optional[StoppingCriteriaList] = None,\n+        synced_gpus: Optional[bool] = None,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n+        output_audio: Optional[bool] = False,\n+        **kwargs,\n+    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n+        r\"\"\"\n+        This method overrides [`~generation.utils.GenerationMixin.generate`] to match the specifics of the Csm model.\n+        Indeed, Csm model requires a custom generation sampling step:\n+        1. Infer the backbone model to sample the first codebook token\n+        2. Call generate on the depth decoder with the first codebook token as `input_ids` to sample the next codebook tokens\n+        3. Use these generated codebook tokens as `input_ids` to sample the next first codebook token using the backbone model\n+        4. Repeat until stopping criteria is met\n+\n+        <Tip warning={true}>\n+\n+        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n+        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n+        parameters to generate(), e.g. `.generate(inputs, do_sample=True)`.\n+        </Tip>\n+\n+        Parameters:\n+            inputs_ids (`torch.Tensor` of shape (batch_size, seq_length), *optional*):\n+                The sequence used as a prompt for the backbone model.\n+            input_values (`torch.Tensor` of shape (batch_size, channels, max_concatenated_audio_length), *optional*):\n+                The batched audio input values, where each batch entry contains the concatenation of all audio segments for that entry.\n+                These values will be encoded into codebook tokens using the codec model and merged with the text input ids provided in `input_ids`.\n+            input_values_cutoffs (`torch.Tensor` of shape (batch_size, max_num_audio), *optional*):\n+                Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n+                If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n+                where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n+                the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n+            generation_config ([`~generation.GenerationConfig`], *optional*):\n+                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n+                passed to generate matching the attributes of `generation_config` will override them. If\n+                `generation_config` is not provided, the default will be used, which has the following loading\n+                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n+                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n+                default values, whose documentation should be checked to parameterize generation.\n+            logits_processor (`LogitsProcessorList`, *optional*):\n+                Custom logits processors that complement the default logits processors built from arguments and\n+                generation config. If a logit processor is passed that is already created with the arguments or a\n+                generation config an error is thrown. This feature is intended for advanced users.\n+            stopping_criteria (`StoppingCriteriaList`, *optional*):\n+                Custom stopping criteria that complements the default stopping criteria built from arguments and a\n+                generation config. If a stopping criteria is passed that is already created with the arguments or a\n+                generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n+                sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n+                intended for advanced users.\n+            synced_gpus (`bool`, *optional*):\n+                Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n+                to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n+                deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n+            streamer (`BaseStreamer`, *optional*):\n+                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n+                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n+            output_audio (`bool`, *optional*):\n+                Whether to return the generated audio.\n+            kwargs (`Dict[str, Any]`, *optional*):\n+                Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n+                forwarded to the `forward` function of the model. Depth decoder specific kwargs should be prefixed with *depth_decoder_*.\n+\n+        Return:\n+            [`CsmGenerateOutput`] or `torch.LongTensor` or `List[torch.FloatTensor]`: A [`CsmGenerateOutput`]\n+            (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`) or a `torch.LongTensor` when `output_audio=False`\n+            or a `List[torch.FloatTensor]` otherwise.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import CsmProcessor, CsmForConditionalGeneration\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"eustlb/csm-1b\"\n+        >>> torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        >>> # ensure the audio is 24kHz\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+\n+        >>> conversation = []\n+        >>> # prepare a conversation with text and corresponding audio\n+        >>> for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+        ...     conversation.append(\n+        ...         {\n+        ...             \"role\": f\"{speaker_id}\",\n+        ...             \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+        ...         }\n+        ...     )\n+\n+        >>> # text prompt\n+        >>> conversation.append({\"role\": f\"{ds[4]['speaker_id']}\", \"content\": [{\"type\": \"text\", \"text\": ds[4][\"text\"]}]})\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     conversation,\n+        ...     tokenize=True,\n+        ...     return_dict=True,\n+        ... ).to(torch_device)\n+\n+        >>> model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        >>> audio = model.generate(**inputs, output_audio=True)\n+        >>> processor.save_audio(audio, \"output.wav\")\n+        ```\n+        \"\"\"\n+        generate_output = super().generate(\n+            input_ids=input_ids,\n+            input_values=input_values,\n+            input_values_cutoffs=input_values_cutoffs,\n+            generation_config=generation_config,\n+            logits_processor=logits_processor,\n+            stopping_criteria=stopping_criteria,\n+            synced_gpus=synced_gpus,\n+            streamer=streamer,\n+            **kwargs,\n+        )\n+\n+        generate_returned_dict = not isinstance(generate_output, torch.Tensor)\n+        audio = None\n+        if output_audio:\n+            generated_audio_codes = generate_output.sequences if generate_returned_dict else generate_output\n+\n+            # infer the codec model\n+            audio = []\n+            with torch.no_grad():\n+                # =======================================\n+                # TODO: @eustlb, this should be batched !!!\n+                # but requires making sure batched inference of the codec model works as intended\n+                for audio_codes_batch in generated_audio_codes:\n+                    eos_idxs = (audio_codes_batch == self.config.codebook_eos_token_id).all(dim=-1).nonzero()\n+                    if eos_idxs.numel() != 0:\n+                        cutoff_idx = eos_idxs.min()\n+                    else:\n+                        cutoff_idx = audio_codes_batch.shape[1]\n+\n+                    audio_codes_batch = audio_codes_batch[:cutoff_idx]\n+                    codec_decode_output = self.codec_model.decode(audio_codes_batch.transpose(0, 1).unsqueeze(0))\n+                    audio.append(codec_decode_output.audio_values[0, 0])\n+                # =======================================\n+\n+        if generate_returned_dict:\n+            return CsmGenerateOutput(audio=audio, **generate_output)\n+        elif output_audio:\n+            return audio\n+        else:\n+            return generate_output"
        },
        {
            "sha": "03cbc07df4d099347b03670f60480121cf3e933e",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "added",
            "additions": 1710,
            "deletions": 0,
            "changes": 1710,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,1710 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/csm/modular_csm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_csm.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Sesame and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..auto import AutoModel\n+from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n+from .generation_csm import CsmGenerationMixin\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+_CONFIG_FOR_DOC = \"CsmConfig\"\n+\n+\n+@dataclass\n+class CsmOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for the model autoregressive outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction) of the depth decoder model.\n+        depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n+        depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+        backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction) of the backbone model.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    depth_decoder_loss: Optional[torch.FloatTensor] = None\n+    depth_decoder_logits: torch.FloatTensor = None\n+    depth_decoder_past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    depth_decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    depth_decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    backbone_loss: Optional[torch.FloatTensor] = None\n+\n+\n+START_DOCSTRING_BASE = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`{config_class}`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+CSM_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmConfig\"))\n+\n+\n+@add_start_docstrings(\n+    \"The bare Csm Model outputting raw hidden-states without any specific head on top.\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmPreTrainedModel(PreTrainedModel):\n+    config_class = CsmConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"CsmDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # does not because of Mimi codec model\n+    # _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, CsmCodebooksHead):\n+            num_codebooks = module.num_codebooks\n+            for i in range(num_codebooks - 1):\n+                module.weight.data[i].normal_(mean=0.0, std=std)\n+        elif isinstance(module, CsmRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class CsmRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        CsmRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class CsmRotaryEmbedding(nn.Module):\n+    def __init__(self, config: CsmConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class CsmMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class CsmAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: CsmConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class CsmDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: CsmConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = CsmAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = CsmMLP(config)\n+        self.input_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+CSM_DEPTH_DECODER_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmDepthDecoderConfig\"))\n+\n+\n+INPUTS_DOCSTRING_BASE = r\"\"\"\n+    Args:\n+        {input_ids_docstring}\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+DEPTH_DECODER_INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\"\"\"\n+\n+\n+CSM_DEPTH_DECODER_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(\n+    INPUTS_DOCSTRING_BASE.format(input_ids_docstring=DEPTH_DECODER_INPUT_IDS_DOCSTRING)\n+)\n+\n+\n+@add_start_docstrings(\n+    \"The bare CsmDepthDecoderModel outputting raw hidden-states without any specific head on top.\",\n+    CSM_DEPTH_DECODER_START_DOCSTRING,\n+)\n+class CsmDepthDecoderModel(CsmPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n+\n+    Args:\n+        config: CsmDepthDecoderConfig\n+    \"\"\"\n+\n+    config_class = CsmDepthDecoderConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.embed_tokens = nn.Embedding((config.num_codebooks * config.vocab_size), config.backbone_hidden_size)\n+        self.layers = nn.ModuleList(\n+            [CsmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = CsmRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        self.inputs_embeds_projector = nn.Linear(config.backbone_hidden_size, config.hidden_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n+            The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n+            is provided in the `input_ids` argument.\n+        \"\"\"\n+        if position_ids is not None and not torch.compiler.is_compiling():\n+            logger.warning_once(\n+                \"Custom `position_ids` were provided but will be ignored. CSM depth decoder automatically determines position_ids \"\n+                \"from `cache_position` and as it requires them to be identical across the batch, the provided position_ids will be ignored.\"\n+            )\n+            position_ids = None\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds.\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            inputs_seq_length = inputs_embeds.shape[1] if inputs_embeds is not None else input_ids.shape[1]\n+            device = inputs_embeds.device if inputs_embeds is not None else input_ids.device\n+            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_seq_length, device=device)\n+\n+        if inputs_embeds is None:\n+            codebook_idxs = torch.clamp(cache_position - 1, min=0)\n+            offset = codebook_idxs * self.vocab_size\n+            inputs_embeds = self.embed_tokens(input_ids + offset)\n+\n+            input_ids_are_first_codebook = cache_position[0] == 0\n+            if backbone_last_hidden_state is not None:\n+                inputs_embeds[:, 0] = backbone_last_hidden_state\n+            else:\n+                if not torch.compiler.is_compiling() and input_ids_are_first_codebook:\n+                    logger.warning(\n+                        \"When the first codebook token is provided, `backbone_last_hidden_state` should also be provided for correct inference.\"\n+                    )\n+\n+        inputs_embeds = self.inputs_embeds_projector(inputs_embeds)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_ids = cache_position.unsqueeze(0)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+class CsmCodebooksHead(nn.Module):\n+    def __init__(self, hidden_size, num_codebooks, vocab_size):\n+        super().__init__()\n+        self.num_codebooks = num_codebooks\n+        self.weight = nn.Parameter(torch.empty(self.num_codebooks - 1, hidden_size, vocab_size))\n+\n+    def forward(self, hidden_states, cache_position=None):\n+        if cache_position is None:\n+            seq_length = hidden_states.shape[1]\n+            codebook_weight = self.weight[torch.arange(seq_length)]\n+        else:\n+            codebook_idxs = cache_position - 1\n+            codebook_weight = self.weight[codebook_idxs]\n+\n+        hidden_states = [\n+            nn.functional.linear(hidden_states[:, codebook_idx, :], codebook_weight[codebook_idx].T)\n+            for codebook_idx in range(codebook_weight.shape[0])\n+        ]\n+        hidden_states = torch.stack(hidden_states, dim=1)\n+\n+        return hidden_states\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The CsmDepthDecoder Model transformer, with a [`CsmCodebooksHead`] on top,\n+    which can be seen a position-specific language modeling head, allowing to use a different linear layer for each codebook\n+    (e.g. position 0 is the first codebook and uses the first codebook head, etc.)\n+    \"\"\",\n+    CSM_DEPTH_DECODER_START_DOCSTRING,\n+)\n+class CsmDepthDecoderForCausalLM(CsmPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = None\n+    _tp_plan = None\n+    _pp_plan = None\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = CsmDepthDecoderModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.codebooks_head = CsmCodebooksHead(config.hidden_size, config.num_codebooks, config.vocab_size)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, CsmDepthDecoderForCausalLM\n+\n+        >>> model = CsmDepthDecoderForCausalLM.from_pretrained(\"meta-csm_depth_decoder/CsmDepthDecoder-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-csm_depth_decoder/CsmDepthDecoder-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\n+            backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n+                The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n+                is provided in the `input_ids` argument.\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            backbone_last_hidden_state=backbone_last_hidden_state,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        if isinstance(logits_to_keep, int):\n+            if logits_to_keep == 0:\n+                # skip idx 0 logits since it's for the concatenated backbone last hidden state\n+                slice_indices = slice(1, None)\n+            else:\n+                slice_indices = slice(-logits_to_keep, None)\n+        else:\n+            slice_indices = logits_to_keep\n+\n+        logits = self.codebooks_head(\n+            hidden_states[:, slice_indices, :], cache_position[slice_indices] if cache_position is not None else None\n+        )\n+        logits = logits.contiguous()\n+\n+        loss = None\n+        if labels is not None:\n+            shift_labels = labels[..., 1:].contiguous()\n+            loss = self.loss_function(\n+                logits=logits, labels=None, vocab_size=self.config.vocab_size, shift_labels=shift_labels, **kwargs\n+            )\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        past_key_values: Optional[Cache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n+        )\n+\n+        is_first_generation_step = model_inputs[\"cache_position\"][0] == 0\n+        if not is_first_generation_step:\n+            model_inputs.pop(\"backbone_last_hidden_state\")\n+\n+        # csm depth decoder does not use position_ids\n+        model_inputs.pop(\"position_ids\")\n+\n+        return model_inputs\n+\n+\n+class CsmBackboneModelEmbeddings(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.embed_audio_tokens = nn.Embedding((config.num_codebooks * config.vocab_size), config.hidden_size)\n+        self.register_buffer(\n+            \"audio_tokens_offsets\", torch.arange(config.num_codebooks) * config.vocab_size, persistent=False\n+        )\n+\n+    def forward(self, input_ids):\n+        input_embeds = self.embed_audio_tokens(input_ids + self.audio_tokens_offsets)\n+        input_embeds = input_embeds.sum(dim=2)\n+        return input_embeds\n+\n+\n+INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n+\n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\"\"\"\n+\n+\n+CSM_BACKBONE_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n+\n+\n+@add_start_docstrings(\n+    \"The bare CsmBackboneModel Model outputting raw hidden-states without any specific head on top.\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmBackboneModel(CsmPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n+\n+    Args:\n+        config: CsmBackboneModelConfig\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.embed_tokens = CsmBackboneModelEmbeddings(config)\n+        self.layers = nn.ModuleList(\n+            [CsmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = CsmRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_BACKBONE_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+CSM_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The Csm model consists of two llama-like auto-regressive transformer models: a backbone model that predicts the first codebook token and a depth decoder that predicts the other codebook tokens.\n+    \"\"\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n+    _tied_weights_keys = [\n+        \"backbone_model.embed_tokens.embed_audio_tokens.weight\",\n+        \"depth_decoder.model.embed_tokens.weight\",\n+    ]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.embed_text_tokens = nn.Embedding(config.text_vocab_size, config.hidden_size)\n+        self.backbone_model = CsmBackboneModel._from_config(config)\n+        self.depth_decoder = CsmDepthDecoderForCausalLM._from_config(config.depth_decoder_config)\n+        self.codec_model = AutoModel.from_config(config.codec_config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.backbone_model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.backbone_model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def _tie_weights(self):\n+        if self.config.tie_codebooks_embeddings:\n+            self._tie_or_clone_weights(\n+                self.backbone_model.embed_tokens.embed_audio_tokens,\n+                self.depth_decoder.model.embed_tokens,\n+            )\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        if kwargs.get(\"output_loading_info\", False):\n+            model, loading_info = super().from_pretrained(*args, **kwargs)\n+        else:\n+            model = super().from_pretrained(*args, **kwargs)\n+\n+        # copy depth decoder generation conf attr to the depth decoder generation config\n+        prefix = \"depth_decoder_\"\n+        prefix_len = len(prefix)\n+        depth_decoder_attrs = {\n+            attr[prefix_len:]: value\n+            for attr, value in vars(model.generation_config).items()\n+            if attr.startswith(prefix)\n+        }\n+\n+        vars(model.depth_decoder.generation_config).update({\"_from_model_config\": False, **depth_decoder_attrs})\n+\n+        # remove the depth decoder generation conf attr from the model generation config\n+        for attr in depth_decoder_attrs:\n+            delattr(model.generation_config, prefix + attr)\n+\n+        if \"output_loading_info\" in kwargs:\n+            return model, loading_info\n+        else:\n+            return model\n+\n+    def save_pretrained(self, *args, **kwargs):\n+        # copy the depth decoder generation config attributes to the model generation config\n+        prefix = \"depth_decoder_\"\n+        depth_decoder_attrs = self.depth_decoder.generation_config.to_diff_dict()\n+        depth_decoder_attrs.pop(\"transformers_version\", None)\n+        for attr, value in depth_decoder_attrs.items():\n+            setattr(self.generation_config, prefix + attr, value)\n+\n+        super().save_pretrained(*args, **kwargs)\n+\n+    def _merge_input_ids_with_input_values(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        input_values: Optional[torch.Tensor] = None,\n+        input_values_cutoffs: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+    ) -> Optional[torch.Tensor]:\n+        \"\"\"\n+        Merges the input_ids and input_values to produce a single inputs_embeds tensor:\n+        1 - Infers the codec model on the input_values to retreive codebook token.\n+        2 - Embeds codebook tokens and places them at the correct positions in the inputs_embeds tensor.\n+        3 - If labels are provided, expands them to match codebook dimensions and position the target codebook tokens in the inputs_embeds tensor.\n+\n+        Args:\n+            input_ids (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+                The input ids to embed.\n+            input_values (`torch.Tensor` of shape `(batch_size, channels, audio_sequence_length)`):\n+                The audio input values to embed.\n+            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`):\n+                The cutoffs of the audio input values relative to its batch index, padded with -1 when no audio.\n+        \"\"\"\n+        inputs_embeds = self.embed_text_tokens(input_ids)\n+\n+        if input_values is not None:\n+            # infer input_values_mask\n+            input_values_cutoffs = nn.functional.pad(input_values_cutoffs, (1, 0))\n+            audio_lengths = input_values_cutoffs[input_values_cutoffs >= 0].diff()\n+            audio_lengths = audio_lengths[audio_lengths > 0]\n+            input_values_mask = torch.arange(input_values_cutoffs.max(), device=input_values.device).expand(\n+                len(audio_lengths), -1\n+            )\n+            input_values_mask = input_values_mask < audio_lengths.unsqueeze(1)\n+\n+            # =======================================\n+            # TODO: @eustlb, this should be batched !!!\n+            # but requires making sure batched inference of the codec model works as intended\n+            audio_tokens_list = []\n+            for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n+                batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n+                for i in range(batch_input_values_cutoffs.shape[0] - 1):\n+                    start_idx = batch_input_values_cutoffs[i]\n+                    end_idx = batch_input_values_cutoffs[i + 1]\n+                    audio_batch = batch_input_values[..., start_idx:end_idx]\n+                    codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n+                    codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n+                    audio_tokens_list.append(codebook_ids[0])\n+\n+            max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n+            batched_audio_token_ids = torch.stack(\n+                [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n+            )\n+            audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n+            # =======================================\n+            audio_token_id = self.config.audio_token_id\n+            audio_token_mask = input_ids == audio_token_id\n+\n+            audio_embeds = self.backbone_model.embed_tokens(batched_audio_token_ids)\n+            inputs_embeds[audio_token_mask] = audio_embeds[audio_codes_mask]\n+\n+            # same for the audio eos token\n+            audio_eos_frame_ids = (\n+                torch.ones((1, 1, self.config.num_codebooks), device=input_ids.device, dtype=torch.long)\n+                * self.config.codebook_eos_token_id\n+            )\n+            audio_eos_embeds = self.backbone_model.embed_tokens(audio_eos_frame_ids).squeeze(1)\n+\n+            audio_eos_token_mask = input_ids == self.config.audio_eos_token_id\n+            inputs_embeds[audio_eos_token_mask] = audio_eos_embeds.repeat(audio_eos_token_mask.sum(), 1)\n+\n+            # if the labels are provided, we need to expand the labels to (batch_size, seq_length, num_codebooks)\n+            if labels is not None:\n+                labels_expanded = labels.unsqueeze(-1).repeat(1, 1, self.config.num_codebooks)\n+                labels_expanded[audio_token_mask] = batched_audio_token_ids[audio_codes_mask]\n+                # mask depth decoder\n+                depth_decoder_ignore_frames_idxs = (labels == -101).nonzero(as_tuple=True)\n+                labels_expanded[depth_decoder_ignore_frames_idxs[0], depth_decoder_ignore_frames_idxs[1], 1:] = -100\n+                labels = labels_expanded\n+\n+        return {\"inputs_embeds\": inputs_embeds, \"labels\": labels}\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        past_key_values: Optional[Cache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        if input_ids is not None and input_ids.ndim == 2 and model_inputs.get(\"inputs_embeds\") is None:\n+            merged_inputs = self._merge_input_ids_with_input_values(\n+                input_ids=input_ids,\n+                input_values=kwargs.get(\"input_values\"),\n+                input_values_cutoffs=kwargs.get(\"input_values_cutoffs\"),\n+                labels=kwargs.get(\"labels\"),\n+            )\n+            model_inputs.update(\n+                {\"inputs_embeds\": merged_inputs[\"inputs_embeds\"], \"labels\": merged_inputs[\"labels\"], \"input_ids\": None}\n+            )\n+\n+        return model_inputs\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CsmOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        input_values: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        input_values_cutoffs: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CsmOutputWithPast]:\n+        r\"\"\"\n+            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n+                Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n+                If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n+                where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n+                the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n+\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n+                Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+                - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n+                - `-100` will be ignored in the loss computation\n+                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+\n+                Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                Kept for compatibility. Does not support another value than:\n+                1. `0`, which is equivalent to keeping all logits, used in the training regime\n+                2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import CsmForConditionalGeneration, AutoProcessor\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"eustlb/csm-1b\"\n+        >>> torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        >>> # ensure the audio is 24kHz\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+\n+        >>> conversation = []\n+        >>> # prepare a conversation with text and corresponding audio\n+        >>> for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+        ...     conversation.append(\n+        ...         {\n+        ...             \"role\": f\"{speaker_id}\",\n+        ...             \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+        ...         }\n+        ...     )\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     conversation,\n+        ...     tokenize=True,\n+        ...     return_dict=True,\n+        ...     output_labels=True,\n+        ... ).to(torch_device)\n+\n+        >>> model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        >>> output = model(**inputs)\n+        >>> output.loss.backward()\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if input_ids is not None and input_ids.ndim == 2:\n+            merged_inputs = self._merge_input_ids_with_input_values(\n+                input_ids, input_values, input_values_cutoffs, labels\n+            )\n+            inputs_embeds = merged_inputs[\"inputs_embeds\"]\n+            labels = merged_inputs[\"labels\"]\n+            input_ids = None\n+\n+        backbone_outputs = self.backbone_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        backbone_hidden_states = backbone_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        backbone_logits = self.lm_head(backbone_hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        backbone_loss = None\n+        depth_decoder_loss = None\n+        depth_decoder_outputs = None\n+        if labels is not None:\n+            # select first codebook as labels for the backbone model\n+            backbone_labels = labels[:, :, 0]\n+            backbone_loss = self.loss_function(\n+                logits=backbone_logits, labels=backbone_labels, vocab_size=self.config.vocab_size, **kwargs\n+            )\n+\n+            # for the depth decoder, we need to select the frames to train on\n+            # those are frames where the label is not uniformly `ignore_index` along the codebook dimension\n+            train_mask = ~(labels[:, :, 1:] == -100).all(dim=-1)\n+            depth_decoder_input_ids = labels[train_mask][..., : self.config.num_codebooks - 1]\n+            # add place holder in position 0 that will be replaced by the backbone_last_hidden_state\n+            depth_decoder_input_ids = nn.functional.pad(depth_decoder_input_ids, (1, 0), value=0)\n+\n+            train_idxs = train_mask.nonzero(as_tuple=True)\n+            backbone_last_hidden_states = backbone_hidden_states[train_idxs[0], train_idxs[1] - 1, :]\n+            depth_decoder_labels = labels[train_mask]\n+\n+            depth_decoder_outputs = self.depth_decoder(\n+                input_ids=depth_decoder_input_ids,\n+                backbone_last_hidden_state=backbone_last_hidden_states,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=True,\n+                labels=depth_decoder_labels,\n+            )\n+\n+            depth_decoder_loss = depth_decoder_outputs.loss\n+            loss = backbone_loss + depth_decoder_loss\n+\n+        return CsmOutputWithPast(\n+            loss=loss,\n+            backbone_loss=backbone_loss,\n+            depth_decoder_loss=depth_decoder_loss,\n+            logits=backbone_logits,\n+            past_key_values=backbone_outputs.past_key_values,\n+            hidden_states=backbone_outputs.hidden_states,\n+            attentions=backbone_outputs.attentions,\n+            depth_decoder_logits=depth_decoder_outputs.logits if depth_decoder_outputs is not None else None,\n+            depth_decoder_past_key_values=depth_decoder_outputs.past_key_values\n+            if depth_decoder_outputs is not None\n+            else None,\n+            depth_decoder_hidden_states=depth_decoder_outputs.hidden_states\n+            if depth_decoder_outputs is not None\n+            else None,\n+            depth_decoder_attentions=depth_decoder_outputs.attentions if depth_decoder_outputs is not None else None,\n+        )\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+__all__ = [\n+    \"CsmPreTrainedModel\",\n+    \"CsmBackboneModel\",\n+    \"CsmDepthDecoderModel\",\n+    \"CsmDepthDecoderForCausalLM\",\n+    \"CsmForConditionalGeneration\",\n+]"
        },
        {
            "sha": "ed3d57103480cc72b76d733387ae2c6f4d172036",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "added",
            "additions": 1042,
            "deletions": 0,
            "changes": 1042,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,1042 @@\n+# coding=utf-8\n+# Copyright 2025 Sesame and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..auto import AutoModel\n+from ..llama.modeling_llama import (\n+    KwargsForCausalLM,\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaMLP,\n+    LlamaModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+)\n+from .configuration_csm import (\n+    CsmConfig,\n+    CsmDepthDecoderConfig,\n+)\n+from .generation_csm import CsmGenerationMixin\n+\n+\n+logger = logging.get_logger(__name__)\n+_CONFIG_FOR_DOC = \"CsmConfig\"\n+\n+\n+@dataclass\n+class CsmOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for the model autoregressive outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction) of the depth decoder model.\n+        depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n+        depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+        backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction) of the backbone model.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    depth_decoder_loss: Optional[torch.FloatTensor] = None\n+    depth_decoder_logits: torch.FloatTensor = None\n+    depth_decoder_past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    depth_decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    depth_decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    backbone_loss: Optional[torch.FloatTensor] = None\n+\n+\n+START_DOCSTRING_BASE = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`{config_class}`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+CSM_DEPTH_DECODER_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmDepthDecoderConfig\"))\n+\n+\n+CSM_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmConfig\"))\n+\n+\n+@add_start_docstrings(\n+    \"The bare Csm Model outputting raw hidden-states without any specific head on top.\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmPreTrainedModel(PreTrainedModel):\n+    config_class = CsmConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"CsmDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    # does not because of Mimi codec model\n+    # _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, CsmCodebooksHead):\n+            num_codebooks = module.num_codebooks\n+            for i in range(num_codebooks - 1):\n+                module.weight.data[i].normal_(mean=0.0, std=std)\n+        elif isinstance(module, CsmRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+INPUTS_DOCSTRING_BASE = r\"\"\"\n+    Args:\n+        {input_ids_docstring}\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+DEPTH_DECODER_INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\"\"\"\n+\n+\n+INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n+\n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\"\"\"\n+\n+\n+CSM_DEPTH_DECODER_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(\n+    INPUTS_DOCSTRING_BASE.format(input_ids_docstring=DEPTH_DECODER_INPUT_IDS_DOCSTRING)\n+)\n+\n+\n+CSM_BACKBONE_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n+\n+\n+# manually specify names for correct naming when converting from modualr\n+class CsmRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class CsmRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class CsmMLP(LlamaMLP):\n+    pass\n+\n+\n+class CsmAttention(LlamaAttention):\n+    pass\n+\n+\n+class CsmDecoderLayer(LlamaDecoderLayer):\n+    pass\n+\n+\n+@add_start_docstrings(\n+    \"The bare CsmDepthDecoderModel outputting raw hidden-states without any specific head on top.\",\n+    CSM_DEPTH_DECODER_START_DOCSTRING,\n+)\n+class CsmDepthDecoderModel(LlamaModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n+\n+    Args:\n+        config: CsmDepthDecoderConfig\n+    \"\"\"\n+\n+    config_class = CsmDepthDecoderConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.embed_tokens = nn.Embedding((config.num_codebooks * config.vocab_size), config.backbone_hidden_size)\n+        self.inputs_embeds_projector = nn.Linear(config.backbone_hidden_size, config.hidden_size, bias=False)\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n+            The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n+            is provided in the `input_ids` argument.\n+        \"\"\"\n+        if position_ids is not None and not torch.compiler.is_compiling():\n+            logger.warning_once(\n+                \"Custom `position_ids` were provided but will be ignored. CSM depth decoder automatically determines position_ids \"\n+                \"from `cache_position` and as it requires them to be identical across the batch, the provided position_ids will be ignored.\"\n+            )\n+            position_ids = None\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds.\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            inputs_seq_length = inputs_embeds.shape[1] if inputs_embeds is not None else input_ids.shape[1]\n+            device = inputs_embeds.device if inputs_embeds is not None else input_ids.device\n+            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_seq_length, device=device)\n+\n+        if inputs_embeds is None:\n+            codebook_idxs = torch.clamp(cache_position - 1, min=0)\n+            offset = codebook_idxs * self.vocab_size\n+            inputs_embeds = self.embed_tokens(input_ids + offset)\n+\n+            input_ids_are_first_codebook = cache_position[0] == 0\n+            if backbone_last_hidden_state is not None:\n+                inputs_embeds[:, 0] = backbone_last_hidden_state\n+            else:\n+                if not torch.compiler.is_compiling() and input_ids_are_first_codebook:\n+                    logger.warning(\n+                        \"When the first codebook token is provided, `backbone_last_hidden_state` should also be provided for correct inference.\"\n+                    )\n+\n+        inputs_embeds = self.inputs_embeds_projector(inputs_embeds)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_ids = cache_position.unsqueeze(0)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class CsmCodebooksHead(nn.Module):\n+    def __init__(self, hidden_size, num_codebooks, vocab_size):\n+        super().__init__()\n+        self.num_codebooks = num_codebooks\n+        self.weight = nn.Parameter(torch.empty(self.num_codebooks - 1, hidden_size, vocab_size))\n+\n+    def forward(self, hidden_states, cache_position=None):\n+        if cache_position is None:\n+            seq_length = hidden_states.shape[1]\n+            codebook_weight = self.weight[torch.arange(seq_length)]\n+        else:\n+            codebook_idxs = cache_position - 1\n+            codebook_weight = self.weight[codebook_idxs]\n+\n+        hidden_states = [\n+            nn.functional.linear(hidden_states[:, codebook_idx, :], codebook_weight[codebook_idx].T)\n+            for codebook_idx in range(codebook_weight.shape[0])\n+        ]\n+        hidden_states = torch.stack(hidden_states, dim=1)\n+\n+        return hidden_states\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The CsmDepthDecoder Model transformer, with a [`CsmCodebooksHead`] on top,\n+    which can be seen a position-specific language modeling head, allowing to use a different linear layer for each codebook\n+    (e.g. position 0 is the first codebook and uses the first codebook head, etc.)\n+    \"\"\",\n+    CSM_DEPTH_DECODER_START_DOCSTRING,\n+)\n+class CsmDepthDecoderForCausalLM(LlamaForCausalLM, GenerationMixin):\n+    _tied_weights_keys = None\n+    _tp_plan = None\n+    _pp_plan = None\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.lm_head\n+        self.codebooks_head = CsmCodebooksHead(config.hidden_size, config.num_codebooks, config.vocab_size)\n+        self.model = CsmDepthDecoderModel(config)\n+\n+    def get_output_embeddings(self):\n+        raise AttributeError(\"Not needed for Csm\")\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        raise AttributeError(\"Not needed for Csm\")\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        past_key_values: Optional[Cache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n+        )\n+\n+        is_first_generation_step = model_inputs[\"cache_position\"][0] == 0\n+        if not is_first_generation_step:\n+            model_inputs.pop(\"backbone_last_hidden_state\")\n+\n+        # csm depth decoder does not use position_ids\n+        model_inputs.pop(\"position_ids\")\n+\n+        return model_inputs\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        backbone_last_hidden_state: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n+            The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n+            is provided in the `input_ids` argument.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            backbone_last_hidden_state=backbone_last_hidden_state,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        if isinstance(logits_to_keep, int):\n+            if logits_to_keep == 0:\n+                # skip idx 0 logits since it's for the concatenated backbone last hidden state\n+                slice_indices = slice(1, None)\n+            else:\n+                slice_indices = slice(-logits_to_keep, None)\n+        else:\n+            slice_indices = logits_to_keep\n+\n+        logits = self.codebooks_head(\n+            hidden_states[:, slice_indices, :], cache_position[slice_indices] if cache_position is not None else None\n+        )\n+        logits = logits.contiguous()\n+\n+        loss = None\n+        if labels is not None:\n+            shift_labels = labels[..., 1:].contiguous()\n+            loss = self.loss_function(\n+                logits=logits, labels=None, vocab_size=self.config.vocab_size, shift_labels=shift_labels, **kwargs\n+            )\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CsmBackboneModelEmbeddings(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.embed_audio_tokens = nn.Embedding((config.num_codebooks * config.vocab_size), config.hidden_size)\n+        self.register_buffer(\n+            \"audio_tokens_offsets\", torch.arange(config.num_codebooks) * config.vocab_size, persistent=False\n+        )\n+\n+    def forward(self, input_ids):\n+        input_embeds = self.embed_audio_tokens(input_ids + self.audio_tokens_offsets)\n+        input_embeds = input_embeds.sum(dim=2)\n+        return input_embeds\n+\n+\n+@add_start_docstrings(\n+    \"The bare CsmBackboneModel Model outputting raw hidden-states without any specific head on top.\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmBackboneModel(LlamaModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n+\n+    Args:\n+        config: CsmBackboneModelConfig\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.embed_tokens = CsmBackboneModelEmbeddings(config)\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_BACKBONE_INPUTS_DOCSTRING)\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+CSM_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The Csm model consists of two llama-like auto-regressive transformer models: a backbone model that predicts the first codebook token and a depth decoder that predicts the other codebook tokens.\n+    \"\"\",\n+    CSM_START_DOCSTRING,\n+)\n+class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n+    _tied_weights_keys = [\n+        \"backbone_model.embed_tokens.embed_audio_tokens.weight\",\n+        \"depth_decoder.model.embed_tokens.weight\",\n+    ]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.embed_text_tokens = nn.Embedding(config.text_vocab_size, config.hidden_size)\n+        self.backbone_model = CsmBackboneModel._from_config(config)\n+        self.depth_decoder = CsmDepthDecoderForCausalLM._from_config(config.depth_decoder_config)\n+        self.codec_model = AutoModel.from_config(config.codec_config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.backbone_model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.backbone_model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def _tie_weights(self):\n+        if self.config.tie_codebooks_embeddings:\n+            self._tie_or_clone_weights(\n+                self.backbone_model.embed_tokens.embed_audio_tokens,\n+                self.depth_decoder.model.embed_tokens,\n+            )\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        if kwargs.get(\"output_loading_info\", False):\n+            model, loading_info = super().from_pretrained(*args, **kwargs)\n+        else:\n+            model = super().from_pretrained(*args, **kwargs)\n+\n+        # copy depth decoder generation conf attr to the depth decoder generation config\n+        prefix = \"depth_decoder_\"\n+        prefix_len = len(prefix)\n+        depth_decoder_attrs = {\n+            attr[prefix_len:]: value\n+            for attr, value in vars(model.generation_config).items()\n+            if attr.startswith(prefix)\n+        }\n+\n+        vars(model.depth_decoder.generation_config).update({\"_from_model_config\": False, **depth_decoder_attrs})\n+\n+        # remove the depth decoder generation conf attr from the model generation config\n+        for attr in depth_decoder_attrs:\n+            delattr(model.generation_config, prefix + attr)\n+\n+        if \"output_loading_info\" in kwargs:\n+            return model, loading_info\n+        else:\n+            return model\n+\n+    def save_pretrained(self, *args, **kwargs):\n+        # copy the depth decoder generation config attributes to the model generation config\n+        prefix = \"depth_decoder_\"\n+        depth_decoder_attrs = self.depth_decoder.generation_config.to_diff_dict()\n+        depth_decoder_attrs.pop(\"transformers_version\", None)\n+        for attr, value in depth_decoder_attrs.items():\n+            setattr(self.generation_config, prefix + attr, value)\n+\n+        super().save_pretrained(*args, **kwargs)\n+\n+    def _merge_input_ids_with_input_values(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        input_values: Optional[torch.Tensor] = None,\n+        input_values_cutoffs: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+    ) -> Optional[torch.Tensor]:\n+        \"\"\"\n+        Merges the input_ids and input_values to produce a single inputs_embeds tensor:\n+        1 - Infers the codec model on the input_values to retreive codebook token.\n+        2 - Embeds codebook tokens and places them at the correct positions in the inputs_embeds tensor.\n+        3 - If labels are provided, expands them to match codebook dimensions and position the target codebook tokens in the inputs_embeds tensor.\n+\n+        Args:\n+            input_ids (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n+                The input ids to embed.\n+            input_values (`torch.Tensor` of shape `(batch_size, channels, audio_sequence_length)`):\n+                The audio input values to embed.\n+            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`):\n+                The cutoffs of the audio input values relative to its batch index, padded with -1 when no audio.\n+        \"\"\"\n+        inputs_embeds = self.embed_text_tokens(input_ids)\n+\n+        if input_values is not None:\n+            # infer input_values_mask\n+            input_values_cutoffs = nn.functional.pad(input_values_cutoffs, (1, 0))\n+            audio_lengths = input_values_cutoffs[input_values_cutoffs >= 0].diff()\n+            audio_lengths = audio_lengths[audio_lengths > 0]\n+            input_values_mask = torch.arange(input_values_cutoffs.max(), device=input_values.device).expand(\n+                len(audio_lengths), -1\n+            )\n+            input_values_mask = input_values_mask < audio_lengths.unsqueeze(1)\n+\n+            # =======================================\n+            # TODO: @eustlb, this should be batched !!!\n+            # but requires making sure batched inference of the codec model works as intended\n+            audio_tokens_list = []\n+            for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n+                batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n+                for i in range(batch_input_values_cutoffs.shape[0] - 1):\n+                    start_idx = batch_input_values_cutoffs[i]\n+                    end_idx = batch_input_values_cutoffs[i + 1]\n+                    audio_batch = batch_input_values[..., start_idx:end_idx]\n+                    codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n+                    codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n+                    audio_tokens_list.append(codebook_ids[0])\n+\n+            max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n+            batched_audio_token_ids = torch.stack(\n+                [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n+            )\n+            audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n+            # =======================================\n+            audio_token_id = self.config.audio_token_id\n+            audio_token_mask = input_ids == audio_token_id\n+\n+            audio_embeds = self.backbone_model.embed_tokens(batched_audio_token_ids)\n+            inputs_embeds[audio_token_mask] = audio_embeds[audio_codes_mask]\n+\n+            # same for the audio eos token\n+            audio_eos_frame_ids = (\n+                torch.ones((1, 1, self.config.num_codebooks), device=input_ids.device, dtype=torch.long)\n+                * self.config.codebook_eos_token_id\n+            )\n+            audio_eos_embeds = self.backbone_model.embed_tokens(audio_eos_frame_ids).squeeze(1)\n+\n+            audio_eos_token_mask = input_ids == self.config.audio_eos_token_id\n+            inputs_embeds[audio_eos_token_mask] = audio_eos_embeds.repeat(audio_eos_token_mask.sum(), 1)\n+\n+            # if the labels are provided, we need to expand the labels to (batch_size, seq_length, num_codebooks)\n+            if labels is not None:\n+                labels_expanded = labels.unsqueeze(-1).repeat(1, 1, self.config.num_codebooks)\n+                labels_expanded[audio_token_mask] = batched_audio_token_ids[audio_codes_mask]\n+                # mask depth decoder\n+                depth_decoder_ignore_frames_idxs = (labels == -101).nonzero(as_tuple=True)\n+                labels_expanded[depth_decoder_ignore_frames_idxs[0], depth_decoder_ignore_frames_idxs[1], 1:] = -100\n+                labels = labels_expanded\n+\n+        return {\"inputs_embeds\": inputs_embeds, \"labels\": labels}\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        past_key_values: Optional[Cache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        if input_ids is not None and input_ids.ndim == 2 and model_inputs.get(\"inputs_embeds\") is None:\n+            merged_inputs = self._merge_input_ids_with_input_values(\n+                input_ids=input_ids,\n+                input_values=kwargs.get(\"input_values\"),\n+                input_values_cutoffs=kwargs.get(\"input_values_cutoffs\"),\n+                labels=kwargs.get(\"labels\"),\n+            )\n+            model_inputs.update(\n+                {\"inputs_embeds\": merged_inputs[\"inputs_embeds\"], \"labels\": merged_inputs[\"labels\"], \"input_ids\": None}\n+            )\n+\n+        return model_inputs\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(CSM_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CsmOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        input_values: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        input_values_cutoffs: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CsmOutputWithPast]:\n+        r\"\"\"\n+            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n+                Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n+                If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n+                where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n+                the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n+\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n+                Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+                - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n+                - `-100` will be ignored in the loss computation\n+                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+\n+                Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                Kept for compatibility. Does not support another value than:\n+                1. `0`, which is equivalent to keeping all logits, used in the training regime\n+                2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import CsmForConditionalGeneration, AutoProcessor\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"eustlb/csm-1b\"\n+        >>> torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        >>> # ensure the audio is 24kHz\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+\n+        >>> conversation = []\n+        >>> # prepare a conversation with text and corresponding audio\n+        >>> for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+        ...     conversation.append(\n+        ...         {\n+        ...             \"role\": f\"{speaker_id}\",\n+        ...             \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+        ...         }\n+        ...     )\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     conversation,\n+        ...     tokenize=True,\n+        ...     return_dict=True,\n+        ...     output_labels=True,\n+        ... ).to(torch_device)\n+\n+        >>> model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        >>> output = model(**inputs)\n+        >>> output.loss.backward()\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if input_ids is not None and input_ids.ndim == 2:\n+            merged_inputs = self._merge_input_ids_with_input_values(\n+                input_ids, input_values, input_values_cutoffs, labels\n+            )\n+            inputs_embeds = merged_inputs[\"inputs_embeds\"]\n+            labels = merged_inputs[\"labels\"]\n+            input_ids = None\n+\n+        backbone_outputs = self.backbone_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        backbone_hidden_states = backbone_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        backbone_logits = self.lm_head(backbone_hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        backbone_loss = None\n+        depth_decoder_loss = None\n+        depth_decoder_outputs = None\n+        if labels is not None:\n+            # select first codebook as labels for the backbone model\n+            backbone_labels = labels[:, :, 0]\n+            backbone_loss = self.loss_function(\n+                logits=backbone_logits, labels=backbone_labels, vocab_size=self.config.vocab_size, **kwargs\n+            )\n+\n+            # for the depth decoder, we need to select the frames to train on\n+            # those are frames where the label is not uniformly `ignore_index` along the codebook dimension\n+            train_mask = ~(labels[:, :, 1:] == -100).all(dim=-1)\n+            depth_decoder_input_ids = labels[train_mask][..., : self.config.num_codebooks - 1]\n+            # add place holder in position 0 that will be replaced by the backbone_last_hidden_state\n+            depth_decoder_input_ids = nn.functional.pad(depth_decoder_input_ids, (1, 0), value=0)\n+\n+            train_idxs = train_mask.nonzero(as_tuple=True)\n+            backbone_last_hidden_states = backbone_hidden_states[train_idxs[0], train_idxs[1] - 1, :]\n+            depth_decoder_labels = labels[train_mask]\n+\n+            depth_decoder_outputs = self.depth_decoder(\n+                input_ids=depth_decoder_input_ids,\n+                backbone_last_hidden_state=backbone_last_hidden_states,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=True,\n+                labels=depth_decoder_labels,\n+            )\n+\n+            depth_decoder_loss = depth_decoder_outputs.loss\n+            loss = backbone_loss + depth_decoder_loss\n+\n+        return CsmOutputWithPast(\n+            loss=loss,\n+            backbone_loss=backbone_loss,\n+            depth_decoder_loss=depth_decoder_loss,\n+            logits=backbone_logits,\n+            past_key_values=backbone_outputs.past_key_values,\n+            hidden_states=backbone_outputs.hidden_states,\n+            attentions=backbone_outputs.attentions,\n+            depth_decoder_logits=depth_decoder_outputs.logits if depth_decoder_outputs is not None else None,\n+            depth_decoder_past_key_values=depth_decoder_outputs.past_key_values\n+            if depth_decoder_outputs is not None\n+            else None,\n+            depth_decoder_hidden_states=depth_decoder_outputs.hidden_states\n+            if depth_decoder_outputs is not None\n+            else None,\n+            depth_decoder_attentions=depth_decoder_outputs.attentions if depth_decoder_outputs is not None else None,\n+        )\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+__all__ = [\n+    \"CsmPreTrainedModel\",\n+    \"CsmBackboneModel\",\n+    \"CsmDepthDecoderModel\",\n+    \"CsmDepthDecoderForCausalLM\",\n+    \"CsmForConditionalGeneration\",\n+]"
        },
        {
            "sha": "486c5eda4c76f7d1a58cf782b11892f1cd9dffeb",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "added",
            "additions": 364,
            "deletions": 0,
            "changes": 364,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,364 @@\n+# coding=utf-8\n+# Copyright 2025 Sesame and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Union\n+\n+import numpy as np\n+\n+from ...utils import is_soundfile_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_soundfile_available():\n+    import soundfile as sf\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import AudioKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+\n+\n+class CsmAudioKwargs(AudioKwargs, total=False):\n+    encoded_length_kwargs: Optional[Dict[str, Any]]\n+\n+\n+class CsmProcessorKwargs(ProcessingKwargs, total=False):\n+    audio_kwargs: CsmAudioKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+            \"padding_side\": \"left\",\n+            \"add_special_tokens\": False,\n+        },\n+        \"audio_kwargs\": {\n+            \"encoded_length_kwargs\": {\n+                \"kernel_sizes\": [7, 3, 1, 8, 3, 1, 10, 3, 1, 12, 3, 1, 16, 3, 4],\n+                \"strides\": [1, 1, 1, 4, 1, 1, 5, 1, 1, 6, 1, 1, 8, 1, 2],\n+                \"dilations\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n+                \"use_causal_conv\": True,\n+            },\n+            \"sampling_rate\": 24000,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class CsmProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Csm processor which wraps [`EncodecFeatureExtractor`] and\n+    [`PretrainedTokenizerFast`] into a single processor that inherits both the audio feature extraction and\n+    tokenizer functionalities. See the [`~CsmProcessor.__call__`] for more\n+    information.\n+    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n+        ```python\n+        from transformers import CsmProcessor\n+        from datasets import load_dataset\n+\n+        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        audio = ds[0][\"audio\"][\"array\"]\n+\n+        processor = CsmProcessor.from_pretrained(\"eustlb/csm-1b\")\n+\n+        processor(\n+            text=[\"<|begin_of_text|>[0]What are you working on?<|end_of_text|><|AUDIO|><|audio_eos|><|begin_of_text|>[1]I'm figuring out my budget.<|end_of_text|>\"],\n+            audio=audio,\n+            text_kwargs = {\"padding\": False},\n+            audio_kwargs = {\"sampling_rate\": 16000},\n+            common_kwargs = {\"return_tensors\": \"pt\"},\n+        )\n+        # this should error out because EncodecFeatureExtractor expects a 24kHz audio :)\n+        ```\n+\n+    Args:\n+        feature_extractor ([`EncodecFeatureExtractor`]):\n+            The feature extractor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+\n+    \"\"\"\n+\n+    attributes = [\"feature_extractor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\"]\n+    feature_extractor_class = \"EncodecFeatureExtractor\"\n+    tokenizer_class = \"PreTrainedTokenizerFast\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        tokenizer,\n+        chat_template=None,\n+    ):\n+        if not hasattr(tokenizer, \"audio_token\"):\n+            self.audio_token = \"<|AUDIO|>\"\n+            self.audio_token_id = tokenizer.convert_tokens_to_ids(self.audio_token)\n+        else:\n+            self.audio_token = tokenizer.audio_token\n+            self.audio_token_id = tokenizer.audio_token_id\n+\n+        if not hasattr(tokenizer, \"audio_eos_token\"):\n+            self.audio_eos_token = \"<|audio_eos|>\"\n+            self.audio_eos_token_id = tokenizer.convert_tokens_to_ids(self.audio_eos_token)\n+        else:\n+            self.audio_eos_token = tokenizer.audio_eos_token\n+            self.audio_eos_token_id = tokenizer.audio_eos_token_id\n+\n+        super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n+\n+    @staticmethod\n+    def _get_encoded_length(audio_length, kernel_sizes=None, strides=None, dilations=None, use_causal_conv=None):\n+        \"\"\"\n+        Compute the length of the encoded audio sequence.\n+\n+        Args:\n+            audio_length (int): The length of the audio sequence.\n+            kernel_sizes (List[int]): The kernel sizes for the convolutional layers.\n+            strides (List[int]): The strides for the convolutional layers.\n+            use_causal_conv (bool): Whether to use causal convolutions.\n+        \"\"\"\n+        cur_length = audio_length\n+\n+        if kernel_sizes is None or strides is None or dilations is None or use_causal_conv is None:\n+            return cur_length\n+\n+        for kernel_size, stride, dilation in zip(kernel_sizes, strides, dilations):\n+            effective_kernel_size = (kernel_size - 1) * dilation + 1\n+            padding_total = kernel_size - stride\n+            padding_right = padding_total // 2\n+            padding_left = padding_total - padding_right\n+\n+            n_frames = (cur_length - effective_kernel_size + padding_total) / stride + 1\n+            n_frames = math.ceil(n_frames) - 1\n+            ideal_length = n_frames * stride + kernel_size - padding_total\n+            extra_padding = ideal_length - cur_length\n+\n+            if use_causal_conv:\n+                padding_left = padding_total\n+                padding_right = extra_padding\n+            else:\n+                padding_left = padding_left\n+                padding_right = padding_right + extra_padding\n+\n+            cur_length = cur_length + padding_left + padding_right\n+            cur_length = (cur_length - dilation * (kernel_size - 1) - 1) // stride + 1\n+\n+        return cur_length\n+\n+    def save_audio(\n+        self,\n+        audio: AudioInput,\n+        saving_path: Union[str, Path, List[Union[str, Path]]],\n+        **kwargs: Unpack[CsmProcessorKwargs],\n+    ):\n+        # TODO: @eustlb, this should be in AudioProcessor\n+        if not is_soundfile_available():\n+            raise ImportError(\"Please install `soundfile` to save audio files.\")\n+\n+        # ensure correct audio input\n+        audio = make_list_of_audio(audio)\n+\n+        # ensure correct saving path\n+        if isinstance(saving_path, (str, Path)):\n+            saving_path = [saving_path]\n+        elif not (isinstance(saving_path, (list, tuple)) and all(isinstance(p, (str, Path)) for p in saving_path)):\n+            raise ValueError(\"Invalid input path. Please provide a string, or a list of strings\")\n+\n+        if len(audio) != len(saving_path):\n+            raise ValueError(\"The number of audio and saving paths must be the same\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            CsmProcessorKwargs,\n+            **kwargs,\n+        )\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        sampling_rate = audio_kwargs[\"sampling_rate\"]\n+\n+        for audio_value, p in zip(audio, saving_path):\n+            if isinstance(audio_value, torch.Tensor):\n+                audio_value = audio_value.cpu().float().numpy()\n+            sf.write(p, audio_value, sampling_rate)\n+\n+    def __call__(\n+        self,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]],\n+        audio: Optional[AudioInput] = None,\n+        output_labels: Optional[bool] = False,\n+        depth_decoder_labels_ratio: Optional[float] = 1.0,\n+        **kwargs: Unpack[CsmProcessorKwargs],\n+    ):\n+        r\"\"\"\n+        Main method to prepare text(s) and audio to be fed as input to the model. This method forwards the `text`\n+        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode\n+        the text. To prepare the audio, this method forwards the `audio` arguments to\n+        EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`]. Please refer\n+        to the docstring of the above two methods for more information.\n+\n+        Args:\n+            audio (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The audio or batch of audio to be prepared. Each audio can be a NumPy array or PyTorch\n+                tensor.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            output_labels (bool, *optional*, default=False):\n+                Whether to return labels for training. Indices will be in `[config.audio_token_id, -100, -101]`.\n+                - `config.audio_token_id` indicates an audio frame (considering sequence length elements as frames)\n+                - `-100` will be ignored in the loss computation\n+                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+            depth_decoder_labels_ratio (float, *optional*, default=1.0):\n+                The ratio of audio frames to keep for the depth decoder labels.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **input_values** -- List of audio values to be fed to a model. Returned when `audio` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **labels** -- List of labels for the audio frames. Returned when `output_labels=True`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            CsmProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        common_kwargs = output_kwargs[\"common_kwargs\"]\n+\n+        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+        n_audio_in_text = [t.count(self.audio_token) for t in text]\n+\n+        n_audio = 0\n+        if audio is not None:\n+            audio = make_list_of_audio(audio)\n+            n_audio = len(audio)\n+\n+        if sum(n_audio_in_text) > 0 and n_audio != sum(n_audio_in_text):\n+            if audio is None:\n+                raise ValueError(\"No audio were provided, but there are audio tokens in the prompt\")\n+            else:\n+                raise ValueError(\n+                    f\"The number of audio tokens in each text ({n_audio_in_text}) should be the same as the \"\n+                    f\"number of provided audios ({n_audio}).\"\n+                )\n+\n+        if audio is not None:\n+            encoded_length_kwargs = audio_kwargs.pop(\"encoded_length_kwargs\", {})\n+            num_audio_tokens_list = [\n+                self._get_encoded_length(audio_array.shape[-1], **encoded_length_kwargs) for audio_array in audio\n+            ]\n+            num_audio_tokens_list_copy = num_audio_tokens_list.copy()\n+\n+            # expand the text to repeat the audio token for the corresponding number of frames\n+            expanded_text = []\n+            for sample in text:\n+                replace_str = []\n+                while self.audio_token in sample:\n+                    num_audio_tokens = num_audio_tokens_list_copy.pop(0)\n+                    expanded_audio_token = self.audio_token * num_audio_tokens\n+\n+                    replace_str.append(expanded_audio_token)\n+                    sample = sample.replace(self.audio_token, \"<placeholder>\", 1)\n+\n+                while \"<placeholder>\" in sample:\n+                    sample = sample.replace(\"<placeholder>\", replace_str.pop(0), 1)\n+                expanded_text.append(sample)\n+\n+            text = expanded_text\n+\n+        encoding = self.tokenizer(text, **text_kwargs)\n+        data = {}\n+        data.update(encoding)\n+\n+        if audio is not None:\n+            audio_kwargs.pop(\"return_attention_mask\", None)  # not supported by the feature extractor\n+\n+            concatenated_audio, input_values_cutoffs = [], []\n+            offset = 0\n+            for n_audio in n_audio_in_text:\n+                if n_audio == 0:\n+                    concatenated_audio.append(np.zeros(0))\n+                    input_values_cutoffs.append(torch.tensor([-1]))\n+                else:\n+                    concatenated_audio.append(\n+                        np.concatenate(\n+                            [\n+                                el.cpu().numpy() if isinstance(el, torch.Tensor) else el\n+                                for el in audio[offset : offset + n_audio]\n+                            ],\n+                            axis=-1,\n+                        )\n+                    )\n+                    input_values_cutoffs.append(\n+                        torch.tensor([el.shape[-1] for el in audio[offset : offset + n_audio]]).cumsum(dim=-1)\n+                    )\n+                    offset += n_audio\n+\n+            audio_inputs = self.feature_extractor(concatenated_audio, **audio_kwargs)\n+            audio_inputs.pop(\"padding_mask\", None)  # not applicable here\n+            data.update(audio_inputs)\n+\n+            # pad and stack the audio cut idxs\n+            max_len = max(cut_idxs.shape[-1] for cut_idxs in input_values_cutoffs)\n+            input_values_cutoffs = [\n+                torch.nn.functional.pad(cut_idxs, (0, max_len - cut_idxs.shape[-1]), value=-1)\n+                for cut_idxs in input_values_cutoffs\n+            ]\n+            data[\"input_values_cutoffs\"] = torch.stack(input_values_cutoffs, dim=0)\n+\n+        if output_labels:\n+            audio_frame_idxs = (data[\"input_ids\"] == self.audio_token_id).nonzero()\n+            n_audio_frames = audio_frame_idxs.shape[0]\n+\n+            if depth_decoder_labels_ratio <= 1.0:\n+                rand_idxs = torch.randperm(n_audio_frames)[: int(n_audio_frames * (1 - depth_decoder_labels_ratio))]\n+                skip_frames_idxs = audio_frame_idxs[rand_idxs]\n+            else:\n+                skip_frames_idxs = audio_frame_idxs\n+\n+            labels = torch.where(data[\"input_ids\"] == self.audio_token_id, data[\"input_ids\"], -100)\n+            labels[skip_frames_idxs[:, 0], skip_frames_idxs[:, 1]] = -101\n+\n+            data[\"labels\"] = labels\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"CsmProcessor\"]"
        },
        {
            "sha": "1ba8536839d4bbe1e60762540551babccf6de5c4",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -1111,7 +1111,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_\n         )\n         return model_inputs\n \n-    def _get_initial_cache_position(self, input_ids, model_kwargs):\n+    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         \"\"\"\n         Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length.\n         Since gpt bigcode is special, the method is overridden here, other models use it from `generation.utils.py`.\n@@ -1125,8 +1125,8 @@ def _get_initial_cache_position(self, input_ids, model_kwargs):\n         if \"inputs_embeds\" in model_kwargs:\n             cur_len = model_kwargs[\"inputs_embeds\"].shape[1]\n         else:\n-            cur_len = input_ids.shape[-1]\n-        model_kwargs[\"cache_position\"] = torch.arange(past_length, cur_len, device=input_ids.device)\n+            cur_len = seq_length\n+        model_kwargs[\"cache_position\"] = torch.arange(past_length, cur_len, device=device)\n         return model_kwargs\n \n     @add_start_docstrings_to_model_forward(GPT_BIGCODE_INPUTS_DOCSTRING)"
        },
        {
            "sha": "853e371c89e2cd0e282f1f1b2ad171a97c58d588",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -1563,7 +1563,7 @@ def generate(\n \n         inputs_embeds = self.get_input_embeddings()(input_tokens)\n \n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(seq_len, device, model_kwargs)\n \n         if model_kwargs.get(\"past_key_values\", None) is None:\n             # Prepare cache if not provided."
        },
        {
            "sha": "50cb0021bff4e7ad70114f20fe993501598993a5",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -1378,7 +1378,7 @@ def generate(\n \n         inputs_embeds = self.get_input_embeddings()(input_tokens)\n \n-        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = self._get_initial_cache_position(seq_len, device, model_kwargs)\n \n         if model_kwargs.get(\"past_key_values\", None) is None:\n             # Prepare cache if not provided."
        },
        {
            "sha": "bfe6698c555fdf3f8abf878163c0a8e5db9d6794",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -216,6 +216,32 @@ def _pad1d(hidden_states: torch.Tensor, paddings: Tuple[int, int], mode: str = \"\n         end = padded.shape[-1] - extra_pad\n         return padded[..., :end]\n \n+    def _get_output_length(self, input_length: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Return the length of the output of the MimiConv1d.\n+        \"\"\"\n+        # padding size\n+        n_frames = (input_length - self.kernel_size + self.padding_total) / self.stride + 1\n+        n_frames = torch.ceil(n_frames).to(torch.int64) - 1\n+        ideal_length = n_frames * self.stride + self.kernel_size - self.padding_total\n+        extra_padding = ideal_length - input_length\n+\n+        if self.causal:\n+            padding_left = self.padding_total\n+            padding_right = extra_padding\n+        else:\n+            padding_left = self.padding_left\n+            padding_right = self.padding_right + extra_padding\n+\n+        # padding\n+        input_length = input_length + padding_left + padding_right\n+\n+        # conv\n+        output_lenght = (\n+            input_length + 2 * self.conv.padding[0] - self.conv.dilation[0] * (self.conv.kernel_size[0] - 1) - 1\n+        ) // self.conv.stride[0] + 1\n+        return output_lenght\n+\n     def forward(self, hidden_states):\n         extra_padding = self._get_extra_padding_for_conv1d(hidden_states)\n \n@@ -331,21 +357,28 @@ def __init__(self, config: MimiConfig):\n         model = [MimiConv1d(config, config.audio_channels, config.num_filters, config.kernel_size)]\n         scaling = 1\n \n+        # keep track of MimiConv1d submodule layer names for easy encoded length computation\n+        mimiconv1d_layer_names = [\"layers.0\"]\n+\n         # Downsample to raw audio scale\n         for ratio in reversed(config.upsampling_ratios):\n             current_scale = scaling * config.num_filters\n             # Add residual layers\n             for j in range(config.num_residual_layers):\n+                mimiconv1d_layer_names.extend([f\"layers.{len(model)}.block.1\", f\"layers.{len(model)}.block.3\"])\n                 model += [MimiResnetBlock(config, current_scale, [config.dilation_growth_rate**j, 1])]\n             # Add downsampling layers\n             model += [nn.ELU()]\n+            mimiconv1d_layer_names.append(f\"layers.{len(model)}\")\n             model += [MimiConv1d(config, current_scale, current_scale * 2, kernel_size=ratio * 2, stride=ratio)]\n             scaling *= 2\n \n         model += [nn.ELU()]\n+        mimiconv1d_layer_names.append(f\"layers.{len(model)}\")\n         model += [MimiConv1d(config, scaling * config.num_filters, config.hidden_size, config.last_kernel_size)]\n \n         self.layers = nn.ModuleList(model)\n+        self._mimiconv1d_layer_names = mimiconv1d_layer_names\n \n     # Copied from transformers.models.encodec.modeling_encodec.EncodecEncoder.forward\n     def forward(self, hidden_states):\n@@ -1567,6 +1600,38 @@ def _encode_frame(\n         codes = codes.transpose(0, 1)\n         return codes, past_key_values\n \n+    def get_encoded_length(self, input_length: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Return the number of frames of the encoded audio waveform.\n+        \"\"\"\n+        output_length = input_length\n+\n+        # encoder\n+        for layer_name in self.encoder._mimiconv1d_layer_names:\n+            output_length = self.encoder.get_submodule(layer_name)._get_output_length(output_length)\n+\n+        # downsample\n+        output_length = self.downsample._get_output_length(output_length)\n+\n+        return output_length\n+\n+    def get_audio_codes_mask(self, padding_mask: torch.Tensor, padding_side: str = \"right\"):\n+        \"\"\"\n+        Get the mask for the audio codes from the original padding mask.\n+        \"\"\"\n+        encoded_lengths = self.get_encoded_length(padding_mask.sum(dim=-1))\n+\n+        audio_codes_mask = torch.arange(encoded_lengths.max(), device=encoded_lengths.device).expand(\n+            len(encoded_lengths), -1\n+        )\n+        audio_codes_mask = audio_codes_mask < encoded_lengths.unsqueeze(1)\n+        audio_codes_mask = audio_codes_mask.to(padding_mask.device)\n+\n+        if padding_side == \"right\":\n+            return audio_codes_mask\n+        else:\n+            return audio_codes_mask.flip(dims=[-1])\n+\n     def encode(\n         self,\n         input_values: torch.Tensor,"
        },
        {
            "sha": "9737e1437e8263341e33f40cbec2da1ef9d8ba27",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -3084,10 +3084,10 @@ def forward(\n             thinker_reply_part=thinker_reply_part,\n         )\n \n-    def _get_initial_cache_position(self, input_ids, model_kwargs):\n+    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         # Talker needs to calculate cache_position with input_ids, so pop inputs_embeds temporarily\n         inputs_embeds = model_kwargs.pop(\"inputs_embeds\")\n-        model_kwargs = super()._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = super()._get_initial_cache_position(seq_length, device, model_kwargs)\n         model_kwargs[\"inputs_embeds\"] = inputs_embeds\n         return model_kwargs\n "
        },
        {
            "sha": "9b5c4167646608a1db8a360a007b465c33f31bcf",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -2771,10 +2771,10 @@ def forward(\n             thinker_reply_part=thinker_reply_part,\n         )\n \n-    def _get_initial_cache_position(self, input_ids, model_kwargs):\n+    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         # Talker needs to calculate cache_position with input_ids, so pop inputs_embeds temporarily\n         inputs_embeds = model_kwargs.pop(\"inputs_embeds\")\n-        model_kwargs = super()._get_initial_cache_position(input_ids, model_kwargs)\n+        model_kwargs = super()._get_initial_cache_position(seq_length, device, model_kwargs)\n         model_kwargs[\"inputs_embeds\"] = inputs_embeds\n         return model_kwargs\n "
        },
        {
            "sha": "d07dad02051d5b62e0165e3062e0be9caa65db0e",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -1058,7 +1058,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n             # update defaults with arguments from tokenizer init\n             for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n                 # init with tokenizer init kwargs if necessary\n-                if modality_key in tokenizer_init_kwargs:\n+                if tokenizer_init_kwargs is not None and modality_key in tokenizer_init_kwargs:\n                     value = (\n                         getattr(self.tokenizer, modality_key)\n                         if hasattr(self.tokenizer, modality_key)"
        },
        {
            "sha": "20edc1c89737cbd26e84af9280dcafb063d4a8d9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 52,
            "deletions": 50,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -501,9 +501,9 @@ def test_greedy_generate(self):\n             output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs(self):\n@@ -525,13 +525,13 @@ def test_greedy_generate_dict_outputs(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -565,10 +565,10 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n \n             self._check_generate_outputs(output_generate, model.config, use_cache=True)\n@@ -582,9 +582,9 @@ def test_sample_generate(self):\n             output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_sample_generate_dict_output(self):\n@@ -607,13 +607,13 @@ def test_sample_generate_dict_output(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -632,9 +632,9 @@ def test_beam_search_generate(self):\n             output_generate = self._beam_search_generate(model=model, inputs_dict=inputs_dict, beam_kwargs=beam_kwargs)\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_beam_search_generate_dict_output(self):\n@@ -657,13 +657,13 @@ def test_beam_search_generate_dict_output(self):\n                 use_cache=False,\n             )\n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -706,10 +706,10 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n \n             self._check_generate_outputs(\n@@ -759,9 +759,9 @@ def test_beam_sample_generate(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_beam_sample_generate_dict_output(self):\n@@ -786,13 +786,13 @@ def test_beam_sample_generate_dict_output(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -840,9 +840,9 @@ def test_group_beam_search_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n             # check `group_beam_search` for higher than 1 `num_return_sequences`\n             num_return_sequences = 2\n@@ -853,9 +853,9 @@ def test_group_beam_search_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_group_beam_search_generate_dict_output(self):\n@@ -878,13 +878,13 @@ def test_group_beam_search_generate_dict_output(self):\n                 use_cache=False,\n             )\n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -923,9 +923,9 @@ def test_constrained_beam_search_generate(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n@@ -947,9 +947,9 @@ def test_constrained_beam_search_generate(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n@@ -987,13 +987,13 @@ def test_constrained_beam_search_generate_dict_output(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n@@ -1031,9 +1031,9 @@ def test_contrastive_generate(self):\n                 use_cache=True,  # Enable cache\n             )\n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n+                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n \n     @pytest.mark.generate\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n@@ -1067,10 +1067,10 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             )\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n \n             self._check_generate_outputs(output_generate, model.config, use_cache=True)\n@@ -1499,7 +1499,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n                 position_ids.masked_fill_(attention_mask == 0, 1)\n                 model_kwargs[\"position_ids\"] = position_ids\n             if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                cache_position = torch.arange(input_ids.shape[1], device=torch_device)\n                 model_kwargs[\"cache_position\"] = cache_position\n             return model_kwargs\n \n@@ -1525,10 +1525,12 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             pad_token_id = (\n                 config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n             )\n-            pad_size = (input_ids.shape[0], 32)\n+            pad_size = (input_ids.shape[0], 32, *input_ids.shape[2:])\n             padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n             padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+            padded_attention_mask = torch.cat(\n+                (torch.zeros(pad_size[:2], dtype=input_ids.dtype, device=torch_device), attention_mask), dim=1\n+            )\n             model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n@@ -1587,7 +1589,7 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n                         else text_config.num_attention_heads\n                     )\n                     encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n-                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape\n+                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n                     # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n                     # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n                     default_cross_attention_shape = (\n@@ -1606,7 +1608,7 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n                         for _ in range(num_decoder_layers)\n                     ]\n                 else:\n-                    batch_size, seq_length = inputs[\"input_ids\"].shape\n+                    batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n                     default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n                     all_cache_shapes = [\n                         [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n@@ -1727,7 +1729,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 \"min_new_tokens\": 5,  # generate exactly 5 tokens\n             }\n             outputs_from_ids = model.generate(input_ids, **generation_kwargs, **inputs_dict)\n-            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n+            self.assertEqual(outputs_from_ids.sequences.shape[:2], (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output).\n             # The output of the two calls should be the same.\n@@ -2262,11 +2264,11 @@ def test_generate_compilation_all_outputs(self):\n                 self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n             if model.config.is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n             else:\n                 self.assertTrue(\n-                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n                 )\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n \n@@ -2408,7 +2410,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n         config = config.text_config if hasattr(config, \"text_config\") else config\n \n         generated_length = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - prompt_length\n+            output.sequences.shape[1] - 1 if config.is_encoder_decoder else output.sequences.shape[1] - prompt_length\n         )\n         decoder_past_key_values = getattr(output, \"past_key_values\", None)\n         if config.is_encoder_decoder and isinstance(decoder_past_key_values, EncoderDecoderCache):\n@@ -2441,7 +2443,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n                     batch_size=internal_batch_size,\n                     attentions=output.decoder_attentions,\n                     prompt_length=1,  # the BOS token\n-                    output_length=output.sequences.shape[-1],\n+                    output_length=output.sequences.shape[1],\n                     config=config,\n                     decoder_past_key_values=decoder_past_key_values,\n                 )\n@@ -2450,7 +2452,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n                     batch_size=internal_batch_size,\n                     attentions=output.attentions,\n                     prompt_length=prompt_length,\n-                    output_length=output.sequences.shape[-1],\n+                    output_length=output.sequences.shape[1],\n                     config=config,\n                     decoder_past_key_values=decoder_past_key_values,\n                 )\n@@ -2469,7 +2471,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n                 batch_size=internal_batch_size,\n                 hidden_states=output.decoder_hidden_states,\n                 prompt_length=1,  # the BOS token\n-                output_length=output.sequences.shape[-1],\n+                output_length=output.sequences.shape[1],\n                 config=config,\n                 use_cache=use_cache,\n             )\n@@ -2478,7 +2480,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n                 batch_size=internal_batch_size,\n                 hidden_states=output.hidden_states,\n                 prompt_length=prompt_length,\n-                output_length=output.sequences.shape[-1],\n+                output_length=output.sequences.shape[1],\n                 config=config,\n                 use_cache=use_cache,\n             )\n@@ -2506,7 +2508,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n         )\n         if has_standard_cache:\n             if use_cache:\n-                cache_length = output.sequences.shape[-1] - 1\n+                cache_length = output.sequences.shape[1] - 1\n                 self._check_past_key_values_for_generate(\n                     batch_size=internal_batch_size,\n                     decoder_past_key_values=decoder_past_key_values,"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/csm/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2F__init__.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba"
        },
        {
            "sha": "93423598ba8381c78639d3200241f74173cf2236",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "added",
            "additions": 693,
            "deletions": 0,
            "changes": 693,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,693 @@\n+# coding=utf-8\n+# Copyright 2024, The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch ConversationalSpeechModel model.\"\"\"\n+\n+import collections\n+import copy\n+import re\n+import unittest\n+\n+import pytest\n+from parameterized import parameterized\n+\n+from transformers import (\n+    AutoProcessor,\n+    CsmConfig,\n+    CsmForConditionalGeneration,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_datasets_available\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    ids_tensor,\n+)\n+\n+\n+if is_datasets_available():\n+    from datasets import load_dataset\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers.pytorch_utils import id_tensor_storage\n+\n+\n+class CsmModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        batch_size=3,\n+        seq_length=7,\n+        is_training=True,\n+        depth_decoder_config={\n+            \"num_codebooks\": 10,\n+            \"backbone_hidden_size\": 64,\n+            \"vocab_size\": 6,\n+            \"hidden_size\": 64,\n+            \"intermediate_size\": 128,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"max_position_embeddings\": 10,\n+        },\n+        codec_config={\n+            \"model_type\": \"mimi\",\n+            \"audio_channels\": 1,\n+            \"chunk_in_sec\": None,\n+            \"hidden_size\": 32,\n+            \"num_filters\": 8,\n+            \"num_residual_layers\": 1,\n+            \"upsampling_ratios\": [8, 4],\n+            \"codebook_size\": 64,\n+            \"vector_quantization_hidden_dimension\": 64,\n+            \"upsample_groups\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"sliding_window\": 4,\n+            \"codebook_dim\": 64,\n+            \"use_cache\": False,\n+        },\n+        config={\n+            \"num_codebooks\": 10,\n+            \"vocab_size\": 6,\n+            \"text_vocab_size\": 99,\n+            \"hidden_size\": 64,\n+            \"intermediate_size\": 64,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"max_position_embeddings\": 10,\n+            \"bos_token_id\": 1,\n+            \"pad_token_id\": 2,\n+            \"eos_token_id\": 3,\n+            \"codebook_pad_token_id\": 2,\n+            \"codebook_eos_token_id\": 3,\n+        },\n+    ):\n+        self.parent = parent\n+        self.is_training = is_training\n+        self.ignore_index = ignore_index\n+        self.depth_decoder_config = depth_decoder_config\n+        self.codec_config = codec_config\n+        self.config = config\n+        self.seq_length = seq_length\n+        self.batch_size = batch_size\n+\n+        self.num_hidden_layers = config[\"num_hidden_layers\"]\n+        self.vocab_size = config[\"vocab_size\"]\n+        self.hidden_size = config[\"hidden_size\"]\n+        self.num_attention_heads = config[\"num_attention_heads\"]\n+        self.pad_token_id = config[\"pad_token_id\"]\n+\n+    def get_config(self):\n+        return CsmConfig(\n+            depth_decoder_config=self.depth_decoder_config,\n+            codec_config=self.codec_config,\n+            **self.config,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        input_ids = ids_tensor([self.batch_size, self.seq_length, config.num_codebooks], config.vocab_size - 1) + 1\n+        attention_mask = input_ids[..., -1].ne(1).to(torch_device)\n+        return config, input_ids, attention_mask\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_ids, attention_mask = self.prepare_config_and_inputs()\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+        return config, inputs_dict\n+\n+\n+class CsmForConditionalGenerationTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (CsmForConditionalGeneration,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_headmasking = False\n+    test_resize_embeddings = False\n+    test_resize_embeddings_untied = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = CsmModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=CsmConfig)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        \"\"\"\n+        Overrides [ModelTesterMixin._prepare_for_class] to handle third input_ids dimension.\n+        \"\"\"\n+        inputs_dict = copy.deepcopy(inputs_dict)\n+\n+        if return_labels:\n+            inputs_dict[\"labels\"] = torch.zeros(\n+                (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.seq_length,\n+                    self.model_tester.config[\"num_codebooks\"],\n+                ),\n+                dtype=torch.long,\n+                device=torch_device,\n+            )\n+\n+        return inputs_dict\n+\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n+        \"\"\"\n+        Overrides [GenerationTesterMixin._get_logits_processor_kwargs] to restrict to top_k, top_p, and temperature sampling.\n+        \"\"\"\n+        logits_processor_kwargs = {}\n+        if do_sample:\n+            logits_processor_kwargs.update(\n+                {\n+                    \"top_k\": 10,\n+                    \"top_p\": 0.7,\n+                    \"temperature\": 0.7,\n+                }\n+            )\n+\n+        return logits_processor_kwargs\n+\n+    def test_initialization(self):\n+        \"\"\"\n+        Overrides [ModelTesterMixin.test_initialization] because of specificities of Mimi codec model.\n+        See https://github.com/huggingface/transformers/blob/1077603410cd73ba71d64a522033574d66d64b55/tests/models/mimi/test_modeling_mimi.py#L384-L397\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                uniform_init_parms = [\"conv\", \"input_proj\", \"output_proj\"]\n+                if param.requires_grad:\n+                    if any(x in name for x in uniform_init_parms):\n+                        self.assertTrue(\n+                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+    def _check_similar_generate_outputs(self, output_1, output_2, atol=1e-5, rtol=1e-5):\n+        \"\"\"\n+        Overrides [GenerationTesterMixin._check_similar_generate_outputs] to handle third input_ids dimension.\n+        Here we only look a the first codebook (index 0 on last dimension of the generated sequences) since returned scores\n+        are for this token.\n+        \"\"\"\n+        # scores doesn't include data regarding decoder input tokens\n+        decoder_input_length = output_1.sequences.shape[1] - len(output_1.scores)\n+        output_matches = output_1.sequences[..., 0] == output_2.sequences[..., 0]\n+        has_matching_outputs = output_matches.all()\n+        has_matching_scores = None\n+        if not has_matching_outputs:\n+            for batch_idx in range(output_1.sequences.shape[0]):\n+                batch_matches = output_matches[batch_idx]\n+                if batch_matches.all():\n+                    continue\n+                first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n+                first_mismatch_idx -= decoder_input_length\n+                output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n+                output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n+                has_matching_scores = torch.allclose(\n+                    output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n+                )\n+                if not has_matching_scores:\n+                    break\n+        self.assertTrue(has_matching_outputs or has_matching_scores)\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support assisted decoding.\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support assisted decoding.\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support Dola decoding.\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_beam_sample_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_beam_search_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_beam_sample_generate_dict_output(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support group beam search.\")\n+    def test_group_beam_search_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support group beam search.\")\n+    def test_group_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support constrained beam search.\")\n+    def test_constrained_beam_search_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support constrained beam search.\")\n+    def test_constrained_beam_search_generate_dict_output(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support contrastive search.\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support prompt lookup decoding.\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support prompt lookup decoding.\")\n+    def test_prompt_lookup_decoding_stops_at_eos(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"CSM has custom embedding approach (text and audio embeddings).\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"CSM has custom embedding approach (text and audio embeddings).\")\n+    def test_tie_model_weights(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_generate_from_inputs_embeds_1_beam_search(self, _, num_beams):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"CSM does not support beam search.\")\n+    def test_model_parallel_beam_search(self):\n+        pass\n+\n+    def test_tied_weights_keys(self):\n+        \"\"\"\n+        Overrides [ModelTesterMixin.test_tied_weights_keys] to not test for text config (not applicable to CSM).\n+        \"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model_tied = model_class(config)\n+\n+            ptrs = collections.defaultdict(list)\n+            for name, tensor in model_tied.state_dict().items():\n+                ptrs[id_tensor_storage(tensor)].append(name)\n+\n+            # These are all the pointers of shared tensors.\n+            tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n+\n+            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n+            # Detect we get a hit for each key\n+            for key in tied_weight_keys:\n+                is_tied_key = any(re.search(key, p) for group in tied_params for p in group)\n+                self.assertTrue(is_tied_key, f\"{key} is not a tied weight key for {model_class}.\")\n+\n+            # Removed tied weights found from tied params -> there should only be one left after\n+            for key in tied_weight_keys:\n+                for i in range(len(tied_params)):\n+                    tied_params[i] = [p for p in tied_params[i] if re.search(key, p) is None]\n+\n+            tied_params = [group for group in tied_params if len(group) > 1]\n+            self.assertListEqual(\n+                tied_params,\n+                [],\n+                f\"Missing `_tied_weights_keys` for {model_class}: add all of {tied_params} except one.\",\n+            )\n+\n+    def _get_custom_4d_mask_test_data(self):\n+        \"\"\"\n+        Overrides [ModelTesterMixin._get_custom_4d_mask_test_data] to handle third input_ids dimension.\n+        \"\"\"\n+        # Sequence in which all but the last token is the same\n+        input_ids = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 5]], device=torch_device, dtype=torch.int64)\n+        input_ids = input_ids.unsqueeze(-1).expand(-1, -1, self.model_tester.config[\"num_codebooks\"])\n+        position_ids = torch.tensor([[0, 1, 2, 3]] * 3, device=torch_device, dtype=torch.int64)\n+\n+        # Combining common prefix with the unique ending tokens:\n+        input_ids_shared_prefix = torch.cat([input_ids[0][:-1], input_ids[:, -1]]).unsqueeze(0)\n+\n+        # Creating a 4D mask where each of the last 3 tokens do not attend to each other.\n+        mask_shared_prefix = torch.tensor(\n+            [\n+                [\n+                    [\n+                        [1, 0, 0, 0, 0, 0],\n+                        [1, 1, 0, 0, 0, 0],\n+                        [1, 1, 1, 0, 0, 0],\n+                        [1, 1, 1, 1, 0, 0],\n+                        [1, 1, 1, 0, 1, 0],\n+                        [1, 1, 1, 0, 0, 1],\n+                    ]\n+                ]\n+            ],\n+        )\n+        # inverting the attention mask\n+        mask_dtype = torch.float32\n+        min_dtype = torch.finfo(mask_dtype).min\n+        mask_shared_prefix = (mask_shared_prefix.eq(0.0)).to(dtype=mask_dtype, device=torch_device) * min_dtype\n+\n+        # Creating a position_ids tensor. note the repeating figures in the end.\n+        position_ids_shared_prefix = torch.tensor([[0, 1, 2, 3, 3, 3]], device=torch_device, dtype=torch.int64)\n+\n+        return input_ids, position_ids, input_ids_shared_prefix, mask_shared_prefix, position_ids_shared_prefix\n+\n+\n+class CsmForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        # TODO: @eustlb, update with correct sesame's repo\n+        self.model_checkpoint = \"eustlb/csm-1b\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def _load_conversation(self):\n+        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        ds = ds.filter(lambda x: x[\"conversation_id\"] == 0)\n+        ds = ds.sort(\"turn_id\")\n+        return ds[0]\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_1b_model_integration_generate(self):\n+        \"\"\"\n+        Tests the generated tokens match the ones from the original model implementation.\n+        Such tokens are to be retreived using https://gist.github.com/eustlb/d25577a357ddcf8f4a8cd0d00baca551, which is a script that infers the original model.\n+        \"\"\"\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        prompt = \"<|begin_of_text|>[0]What are you working on?<|end_of_text|><|AUDIO|><|audio_eos|><|begin_of_text|>[1]I'm figuring out my budget.<|end_of_text|>\"\n+\n+        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        audio = ds[0][\"audio\"][\"array\"]\n+        inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\").to(torch_device)\n+\n+        model = CsmForConditionalGeneration.from_pretrained(self.model_checkpoint, device_map=torch_device)\n+        output_tokens = model.generate(**inputs, do_sample=False, depth_decoder_do_sample=False)\n+\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS = torch.tensor([[\n+            [1140, 10, 37, 1180, 1100, 1319, 601, 1482, 1918, 1739, 372, 856, 674, 1, 854, 459, 1843, 1191, 347, 349, 1087, 846, 759, 1690, 947, 1280, 580, 1909, 1192, 487, 1302, 1601],\n+            [1494, 1412, 1824, 1852, 150, 928, 91, 326, 623, 1632, 1163, 1221, 1949, 999, 1779, 248, 693, 1149, 1423, 1503, 598, 80, 223, 1798, 251, 385, 1391, 1692, 1228, 1631, 1101, 866],\n+            [778, 645, 830, 1812, 524, 1704, 1805, 1289, 74, 1069, 243, 1622, 1755, 1281, 1397, 620, 1962, 1995, 253, 1124, 1007, 518, 89, 559, 1304, 1482, 523, 1747, 1979, 1003, 1707, 1578],\n+            [1356, 481, 642, 989, 287, 1819, 171, 1115, 824, 1253, 1488, 1074, 1019, 342, 279, 513, 1275, 1364, 893, 2007, 553, 407, 882, 1170, 1586, 485, 762, 559, 100, 542, 911, 1460],\n+            [1860, 593, 1944, 404, 575, 545, 862, 830, 1002, 125, 2010, 268, 1779, 804, 811, 809, 255, 373, 387, 1756, 259, 822, 1191, 700, 1686, 390, 1676, 844, 2006, 286, 1376, 719],\n+            [1165, 1047, 848, 212, 1018, 1470, 93, 1709, 1487, 1691, 1190, 275, 1278, 2018, 121, 1023, 485, 463, 39, 1825, 1936, 1817, 569, 209, 1553, 1599, 1137, 769, 968, 558, 1957, 265],\n+            [902, 1608, 719, 850, 371, 1920, 75, 1917, 2005, 1238, 562, 1743, 713, 95, 1107, 1463, 696, 840, 8, 487, 1950, 1171, 1004, 1516, 1130, 303, 1866, 1728, 2046, 238, 265, 153],\n+            [1932, 839, 334, 1167, 134, 2025, 40, 505, 1244, 1238, 1840, 800, 697, 72, 216, 486, 940, 1312, 510, 361, 549, 583, 1364, 844, 397, 1181, 1779, 962, 457, 1782, 1316, 465],\n+            [31, 1558, 1048, 404, 354, 7, 827, 414, 1082, 807, 243, 1517, 801, 1364, 99, 1276, 1655, 1488, 1313, 464, 828, 1612, 774, 1558, 745, 1496, 960, 1874, 995, 1943, 255, 213],\n+            [355, 1270, 413, 1519, 1659, 1904, 690, 552, 1279, 1821, 2022, 458, 1779, 2003, 604, 832, 661, 1295, 305, 1701, 173, 869, 230, 539, 1188, 669, 117, 692, 250, 388, 1995, 294],\n+            [629, 199, 1899, 1123, 1070, 344, 578, 1795, 1451, 1257, 168, 1410, 1120, 1270, 316, 983, 1245, 1870, 165, 471, 966, 1337, 308, 1118, 746, 67, 1767, 1480, 1517, 1585, 871, 1110],\n+            [1281, 1173, 784, 404, 368, 403, 580, 526, 853, 1692, 792, 895, 1286, 573, 1368, 896, 931, 1958, 1912, 644, 583, 1706, 1176, 1262, 1637, 315, 524, 1629, 795, 1211, 915, 533],\n+            [9, 1783, 621, 1954, 1212, 993, 197, 977, 1662, 1340, 618, 1997, 1689, 1001, 74, 1765, 1865, 797, 1219, 1609, 671, 1491, 950, 1849, 1301, 2031, 875, 323, 203, 1063, 1490, 1538],\n+            [1944, 1578, 1256, 1169, 790, 1444, 1382, 1616, 1100, 1264, 214, 1646, 488, 573, 1333, 285, 1954, 74, 1333, 674, 1303, 266, 622, 1290, 402, 109, 1331, 1666, 1347, 780, 106, 605],\n+            [221, 161, 1322, 1, 565, 1507, 1403, 1091, 1557, 932, 1664, 1165, 1828, 1647, 2008, 1616, 648, 1113, 1870, 22, 734, 1458, 1940, 1756, 1689, 925, 1318, 1095, 985, 473, 604, 1974],\n+            [1178, 597, 1804, 747, 1383, 360, 1497, 406, 1053, 1023, 1901, 56, 1221, 628, 75, 1729, 575, 1681, 840, 410, 650, 794, 1171, 1889, 187, 54, 1364, 1390, 505, 1285, 1814, 90],\n+            [1432, 1221, 1800, 1873, 1255, 627, 41, 9, 630, 896, 1469, 1195, 1098, 145, 442, 1460, 13, 57, 2039, 1015, 149, 461, 1084, 1288, 1099, 910, 63, 157, 906, 111, 1394, 460],\n+            [1352, 593, 307, 780, 1614, 1675, 1491, 1253, 723, 1793, 1032, 1486, 1805, 1904, 777, 398, 1791, 951, 770, 499, 1858, 244, 1372, 1514, 1858, 1200, 69, 181, 673, 1144, 1938, 1191],\n+            [905, 403, 1626, 1529, 581, 1443, 976, 754, 1561, 1370, 1048, 253, 194, 1271, 853, 959, 1532, 30, 286, 1594, 1255, 1135, 1410, 1699, 1423, 2002, 260, 69, 941, 1640, 895, 722],\n+            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n+        ]])\n+        # fmt: on\n+\n+        torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_1b_model_integration_generate_no_audio(self):\n+        \"\"\"\n+        Tests the generated tokens match the ones from the original model implementation.\n+        Such tokens are to be retreived using https://gist.github.com/eustlb/aed822f765e928b9612e01b0d8836d69, which is a script that infers the original model.\n+        \"\"\"\n+\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+\n+        conversation = [\n+            {\"role\": \"0\", \"content\": [{\"type\": \"text\", \"text\": \"The past is just a story we tell ourselves.\"}]},\n+        ]\n+\n+        inputs = processor.apply_chat_template(conversation, tokenize=True, return_dict=True).to(torch_device)\n+\n+        model = CsmForConditionalGeneration.from_pretrained(self.model_checkpoint, device_map=torch_device)\n+        output_tokens = model.generate(**inputs, do_sample=False, depth_decoder_do_sample=False)\n+\n+        print(output_tokens)\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS = torch.tensor([[\n+            [1656, 629, 723, 1785, 206, 1873, 1059, 1190, 1833, 240, 618, 350, 156, 109, 2010, 452, 435, 1764, 77, 654, 1133, 908, 1095, 74, 804, 494, 1760, 1343, 1312, 1464, 1657, 324],\n+            [366, 1532, 1945, 21, 145, 1428, 1417, 1987, 1793, 1444, 356, 1491, 849, 333, 788, 426, 1423, 1004, 414, 1823, 1169, 257, 1892, 696, 1572, 998, 1098, 523, 390, 1977, 546, 1692],\n+            [1343, 1382, 1288, 1744, 1685, 1154, 1837, 1156, 1680, 1641, 1479, 1548, 632, 824, 694, 2010, 671, 1251, 1822, 343, 638, 1372, 696, 1272, 144, 125, 1332, 579, 936, 77, 159, 357],\n+            [456, 1534, 349, 274, 1956, 1502, 1268, 1038, 1911, 523, 1360, 1159, 761, 293, 718, 1143, 63, 705, 168, 550, 413, 1372, 1771, 787, 631, 693, 784, 1789, 2039, 1131, 1601, 918],\n+            [456, 829, 2026, 1108, 1649, 207, 1308, 1440, 1192, 1394, 426, 546, 590, 36, 1682, 1827, 1387, 1425, 1909, 1500, 1438, 1297, 5, 888, 948, 1745, 1304, 1364, 1692, 131, 300, 1908],\n+            [2027, 1431, 1037, 1789, 1296, 1264, 1331, 1787, 1235, 1902, 1161, 1591, 590, 561, 1633, 1218, 510, 148, 1962, 118, 212, 608, 565, 1869, 583, 598, 532, 658, 1416, 9, 1172, 493],\n+            [1215, 460, 1722, 317, 1423, 716, 1589, 1177, 1927, 1860, 1756, 1552, 1674, 643, 74, 1256, 587, 1742, 771, 2028, 469, 1070, 1683, 1614, 699, 494, 2020, 139, 1365, 1171, 171, 904],\n+            [1615, 339, 323, 317, 469, 714, 104, 2015, 1407, 278, 468, 77, 2007, 650, 1630, 269, 168, 934, 1544, 58, 1487, 1373, 705, 874, 1252, 2031, 1995, 254, 1334, 1171, 1911, 1607],\n+            [1259, 693, 666, 1700, 1115, 607, 982, 769, 1106, 1500, 101, 88, 1698, 1864, 1358, 1594, 192, 153, 1868, 1654, 604, 1948, 526, 778, 172, 1664, 1966, 99, 1334, 1030, 1349, 1209],\n+            [1211, 579, 1369, 492, 1725, 203, 1125, 778, 701, 1982, 1420, 155, 736, 1145, 2018, 609, 658, 561, 1147, 923, 1794, 1753, 116, 1374, 612, 956, 1587, 392, 1062, 2047, 901, 1931],\n+            [460, 1093, 1346, 1917, 1223, 470, 271, 390, 547, 112, 143, 1633, 1030, 643, 96, 1759, 920, 1959, 75, 1280, 1630, 999, 333, 853, 1110, 1291, 1911, 57, 171, 1658, 1704, 1508],\n+            [908, 500, 393, 184, 1437, 482, 2008, 1834, 356, 1435, 1550, 1407, 1236, 109, 1167, 452, 1141, 934, 207, 957, 660, 670, 28, 1066, 1252, 1932, 669, 906, 1904, 1820, 2043, 881],\n+            [1599, 1031, 1474, 336, 1540, 571, 437, 1440, 1616, 1365, 1412, 1246, 400, 405, 1776, 96, 296, 38, 1597, 466, 1630, 1256, 1940, 887, 1769, 294, 285, 842, 1756, 1619, 451, 1529],\n+            [1615, 339, 1722, 525, 942, 105, 1365, 670, 785, 1316, 465, 1860, 438, 968, 547, 1938, 1816, 1429, 1065, 1942, 660, 1446, 1093, 1066, 931, 121, 688, 1033, 1178, 754, 1783, 94],\n+            [912, 1354, 598, 254, 341, 1980, 1166, 585, 1302, 473, 554, 242, 174, 2030, 2011, 325, 978, 1690, 258, 396, 1831, 1768, 1291, 1699, 2001, 433, 1414, 2012, 1045, 511, 533, 1104],\n+            [80, 1791, 1062, 1136, 391, 568, 1651, 101, 959, 2043, 1683, 760, 794, 181, 570, 540, 1599, 20, 1017, 973, 1654, 396, 586, 778, 2044, 1664, 1911, 929, 66, 897, 510, 643],\n+            [1161, 1093, 161, 1296, 589, 54, 906, 981, 1927, 605, 516, 1731, 1461, 1204, 1902, 920, 1488, 177, 805, 1402, 610, 1446, 1154, 1067, 2025, 645, 762, 1715, 415, 1658, 1713, 1607],\n+            [374, 1444, 1577, 792, 1450, 628, 604, 1729, 322, 514, 1725, 540, 1070, 575, 653, 800, 250, 187, 569, 349, 354, 1573, 176, 793, 897, 359, 536, 276, 1224, 23, 145, 1287],\n+            [1184, 415, 1644, 1737, 1788, 385, 784, 1861, 1172, 1118, 367, 1156, 234, 1946, 1742, 981, 828, 1798, 1821, 361, 1148, 670, 518, 1288, 761, 1050, 1642, 1006, 1747, 840, 1599, 720],\n+            [1141, 1731, 1670, 1542, 1347, 1907, 683, 753, 1347, 68, 2031, 153, 556, 719, 736, 1759, 1131, 1073, 1747, 1730, 1487, 1137, 1869, 1624, 699, 1900, 748, 49, 1312, 735, 726, 1268],\n+            [1141, 1383, 405, 1033, 490, 488, 1102, 471, 713, 1630, 447, 703, 1495, 1001, 1855, 354, 456, 411, 786, 853, 168, 407, 116, 699, 605, 128, 532, 1076, 208, 447, 1448, 1071],\n+            [345, 1013, 948, 1728, 1837, 337, 930, 1226, 1643, 1729, 983, 1688, 2009, 435, 1358, 721, 42, 1779, 1332, 1077, 1873, 128, 1327, 125, 1226, 1704, 705, 1459, 1449, 862, 155, 1870],\n+            [336, 904, 684, 184, 1542, 714, 1752, 1180, 1373, 1816, 504, 1716, 1066, 1086, 1212, 530, 1413, 1278, 75, 1347, 82, 1623, 1307, 1717, 1861, 494, 888, 1589, 670, 1999, 905, 1430],\n+            [578, 554, 14, 523, 1016, 300, 1589, 1017, 356, 1583, 1654, 414, 449, 376, 1413, 58, 706, 963, 388, 1626, 131, 352, 1024, 1054, 2025, 1561, 77, 1589, 1486, 431, 1249, 1508],\n+            [184, 2043, 169, 1673, 580, 162, 1752, 397, 1119, 2009, 697, 150, 1475, 157, 1523, 1402, 575, 86, 1373, 1230, 1564, 1308, 626, 1093, 1603, 1446, 1390, 1543, 1778, 1142, 1357, 1831],\n+            [1484, 1987, 932, 1728, 1504, 1618, 291, 1865, 1151, 460, 1792, 141, 234, 2043, 829, 513, 435, 791, 1037, 1541, 65, 424, 1589, 1711, 312, 1306, 212, 686, 673, 984, 1914, 1549],\n+            [513, 1536, 1844, 1319, 572, 1069, 121, 735, 1949, 1211, 1362, 1027, 105, 1379, 315, 1782, 706, 1658, 1510, 1989, 1443, 1690, 822, 1614, 1194, 1460, 992, 2040, 1178, 1474, 1110, 1326],\n+            [1858, 194, 1594, 1935, 1622, 1892, 1577, 137, 1907, 2015, 757, 414, 1823, 836, 496, 530, 1385, 1503, 1065, 1554, 664, 525, 1031, 433, 69, 466, 1016, 1846, 1609, 1658, 911, 94],\n+            [1134, 1744, 323, 691, 1837, 347, 1871, 172, 811, 91, 1883, 436, 1912, 23, 1336, 1684, 519, 1612, 1219, 1402, 728, 1953, 1658, 641, 27, 1340, 436, 139, 2008, 1030, 159, 324],\n+            [1270, 1536, 1639, 414, 1387, 1170, 1067, 1701, 1414, 505, 1122, 36, 1731, 350, 1552, 1214, 1444, 30, 107, 172, 480, 1858, 655, 168, 1107, 691, 1272, 797, 1656, 548, 1407, 1375],\n+            [1270, 286, 1371, 1552, 1622, 1739, 1348, 2018, 345, 1537, 1941, 2024, 1423, 740, 284, 513, 91, 1228, 2015, 385, 992, 39, 813, 803, 2025, 497, 663, 462, 1609, 334, 927, 1470],\n+            [1718, 994, 265, 1421, 1622, 1098, 845, 1868, 832, 459, 447, 619, 1970, 929, 513, 63, 1448, 1509, 1219, 1942, 285, 1373, 1259, 1004, 11, 1040, 1984, 57, 188, 1687, 1475, 805],\n+            [1157, 832, 480, 1225, 1019, 347, 326, 999, 125, 1542, 118, 1383, 1343, 1077, 1821, 1602, 1978, 1642, 618, 808, 692, 1953, 1353, 963, 619, 1291, 1016, 1458, 1995, 1688, 1872, 1718],\n+            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n+        ]])\n+        # fmt: on\n+\n+        torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_1b_model_integration_generate_multiple_audio(self):\n+        \"\"\"\n+        Test the generated tokens match the ones from the original model implementation.\n+        Such tokens are to be retreived using https://gist.github.com/eustlb/0c94de002e1325abb61d32217f74c0f8, which is a script that infers the original model.\n+        \"\"\"\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+\n+        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        conversation = []\n+\n+        # context\n+        for text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\n+            conversation.append(\n+                {\n+                    \"role\": f\"{speaker_id}\",\n+                    \"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n+                }\n+            )\n+\n+        # text prompt\n+        conversation.append({\"role\": f\"{ds[4]['speaker_id']}\", \"content\": [{\"type\": \"text\", \"text\": ds[4][\"text\"]}]})\n+\n+        inputs = processor.apply_chat_template(\n+            conversation,\n+            tokenize=True,\n+            return_dict=True,\n+        ).to(torch_device)\n+\n+        model = CsmForConditionalGeneration.from_pretrained(self.model_checkpoint, device_map=torch_device)\n+        output_tokens = model.generate(**inputs, do_sample=False, depth_decoder_do_sample=False)\n+\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS = torch.tensor([[\n+            [420, 1189, 1311, 318, 359, 694, 1550, 1044, 1614, 1437, 1978, 537, 554, 1681, 147, 1225, 422, 1357, 1681, 1619, 165, 641, 1132, 1975, 1568, 406, 756, 503, 1673, 1428, 762, 781],\n+            [1848, 1412, 957, 1656, 871, 540, 1999, 175, 711, 1383, 1814, 104, 742, 1285, 733, 1251, 1165, 1915, 1392, 645, 1804, 913, 1772, 632, 376, 1507, 1132, 725, 716, 1121, 1769, 1509],\n+            [429, 1138, 895, 1018, 1099, 257, 1395, 1015, 576, 1599, 497, 19, 1858, 1437, 282, 357, 1143, 828, 1481, 70, 985, 551, 935, 278, 1102, 1453, 1902, 755, 526, 498, 1441, 1733],\n+            [546, 343, 1547, 879, 2039, 692, 1999, 1150, 1969, 1866, 1178, 199, 1913, 1738, 1530, 1728, 1193, 74, 695, 612, 1095, 1597, 1381, 683, 1385, 2045, 1069, 865, 438, 70, 1437, 318],\n+            [1741, 1621, 733, 1580, 1006, 1790, 1031, 1563, 569, 1822, 1229, 854, 142, 1554, 792, 741, 147, 552, 731, 772, 908, 831, 1291, 1819, 296, 290, 1871, 100, 1904, 1420, 1903, 1653],\n+            [1264, 1576, 963, 12, 1403, 453, 259, 1359, 1270, 466, 1744, 1579, 1081, 1691, 1495, 1293, 110, 1020, 2042, 189, 1358, 955, 784, 1317, 2, 1794, 388, 376, 327, 511, 866, 1308],\n+            [1407, 1412, 1665, 1683, 284, 874, 1859, 326, 1491, 1343, 777, 695, 1424, 396, 274, 202, 178, 747, 470, 1805, 1414, 2000, 127, 1884, 531, 215, 1322, 1098, 1674, 1227, 1092, 204],\n+            [584, 637, 1665, 1683, 1136, 1201, 212, 310, 1441, 1619, 190, 1611, 1629, 2011, 1754, 1587, 413, 1287, 1251, 1382, 1904, 444, 1665, 1047, 1982, 1169, 1200, 809, 117, 327, 958, 1877],\n+            [471, 1469, 1679, 1184, 343, 974, 1442, 897, 1888, 1468, 1092, 1398, 1714, 963, 1577, 1797, 766, 565, 403, 920, 1806, 466, 1193, 446, 825, 775, 1886, 1095, 159, 1085, 858, 504],\n+            [28, 1511, 1510, 1580, 447, 1934, 1031, 1439, 202, 1435, 474, 1731, 724, 1080, 1121, 421, 625, 1410, 95, 605, 815, 1825, 127, 785, 900, 1673, 178, 1242, 2033, 1230, 350, 139],\n+            [20, 1215, 253, 955, 871, 1689, 1986, 24, 1648, 423, 562, 1937, 1146, 26, 1266, 346, 188, 318, 179, 1164, 1100, 1978, 478, 1192, 715, 392, 1837, 425, 1492, 766, 1651, 822],\n+            [1879, 1401, 1444, 723, 1754, 732, 1307, 702, 1768, 2013, 1284, 577, 1287, 1532, 647, 189, 903, 587, 800, 152, 898, 182, 2016, 639, 1074, 1220, 1934, 264, 250, 745, 1652, 536],\n+            [1874, 1526, 232, 1580, 1980, 988, 1623, 341, 1768, 956, 1430, 1667, 1687, 1289, 826, 1378, 173, 1466, 479, 835, 1786, 1671, 328, 131, 815, 871, 379, 1329, 440, 1117, 392, 272],\n+            [1762, 426, 1350, 1590, 314, 190, 1514, 344, 1926, 822, 534, 523, 703, 36, 379, 494, 464, 1886, 1555, 1318, 1654, 1469, 1976, 304, 218, 655, 1826, 958, 502, 326, 1898, 861],\n+            [1577, 386, 503, 1492, 698, 405, 1031, 349, 1804, 2012, 1450, 996, 1140, 26, 449, 33, 1917, 354, 702, 1255, 1942, 1184, 864, 2045, 514, 744, 466, 54, 37, 486, 362, 525],\n+            [1109, 1920, 445, 1719, 1670, 1220, 745, 40, 171, 1921, 999, 104, 489, 1911, 883, 306, 649, 1751, 762, 1183, 1085, 1112, 1912, 2035, 1940, 1129, 1592, 1276, 1570, 1236, 738, 209],\n+            [1837, 990, 1063, 318, 1398, 1838, 1678, 906, 754, 802, 562, 353, 1389, 207, 1319, 1188, 2013, 1079, 888, 1706, 1042, 657, 482, 953, 94, 2007, 871, 485, 1596, 275, 410, 1855],\n+            [872, 974, 1344, 1798, 655, 805, 1604, 1913, 455, 615, 1827, 966, 1330, 1826, 1285, 359, 544, 221, 1538, 1658, 374, 1352, 1714, 1925, 235, 65, 350, 931, 1009, 1164, 218, 736],\n+            [1547, 617, 1622, 740, 655, 265, 1324, 1265, 1449, 482, 1037, 105, 1128, 701, 1866, 1674, 1999, 1302, 985, 1942, 663, 449, 1881, 698, 805, 1446, 1742, 1192, 1623, 605, 948, 2],\n+            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n+        ]])\n+        # fmt: on\n+\n+        torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_1b_model_integration_generate_batched(self):\n+        \"\"\"\n+        Test the generated tokens match the ones from the original model implementation.\n+        Such tokens are to be retreived using https://gist.github.com/eustlb/bcc532b53161bc31da3d66cb07ae193f, which is a script that infers the original model.\n+        \"\"\"\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+\n+        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+        conversation = [\n+            [\n+                {\n+                    \"role\": f\"{ds[0]['speaker_id']}\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": ds[0][\"text\"]},\n+                        {\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n+                    ],\n+                },\n+                {\n+                    \"role\": f\"{ds[1]['speaker_id']}\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": ds[1][\"text\"]},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": f\"{ds[0]['speaker_id']}\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": ds[0][\"text\"]},\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            conversation,\n+            tokenize=True,\n+            return_dict=True,\n+        ).to(torch_device)\n+\n+        model = CsmForConditionalGeneration.from_pretrained(self.model_checkpoint, device_map=torch_device)\n+        output_tokens = model.generate(**inputs, do_sample=False, depth_decoder_do_sample=False)\n+\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS = torch.tensor([\n+            [\n+                [1140, 10, 37, 1180, 1100, 1319, 601, 1482, 1918, 1739, 372, 856, 674, 1, 854, 459, 1843, 1191, 347, 349, 1087, 846, 759, 1690, 947, 1280, 580, 1909, 1192, 487, 1302, 1601],\n+                [1494, 1412, 1824, 1852, 150, 928, 91, 326, 623, 1632, 1163, 1221, 1949, 999, 1779, 248, 693, 1149, 1423, 1503, 1656, 80, 1947, 1666, 933, 1950, 1544, 1577, 1612, 1791, 1883, 765],\n+                [778, 645, 830, 1051, 524, 1704, 1805, 1438, 211, 906, 691, 814, 1798, 1642, 1042, 284, 1906, 1513, 520, 137, 1052, 1548, 423, 1564, 330, 873, 1381, 188, 317, 1503, 1707, 1744],\n+                [1416, 864, 242, 1653, 604, 1577, 202, 1808, 926, 1867, 204, 134, 1096, 1765, 496, 1680, 268, 1796, 2024, 1989, 583, 183, 952, 105, 765, 1534, 669, 895, 2008, 11, 1199, 195],\n+                [1356, 796, 25, 1580, 15, 344, 1730, 99, 1330, 315, 955, 1964, 1731, 543, 1159, 1860, 671, 732, 63, 382, 143, 395, 1749, 1421, 1640, 1340, 650, 100, 171, 1346, 41, 806],\n+                [1860, 1835, 823, 388, 254, 1734, 1135, 324, 1508, 983, 937, 1703, 1541, 875, 1319, 799, 1259, 1175, 1295, 807, 261, 760, 1916, 1606, 1616, 1894, 1605, 441, 387, 167, 2016, 222],\n+                [1165, 919, 1318, 54, 1727, 1766, 777, 1128, 623, 353, 1840, 241, 977, 424, 1055, 898, 395, 655, 1695, 1084, 1346, 616, 1028, 1927, 603, 858, 758, 1539, 0, 1655, 1853, 1661],\n+                [902, 1746, 1318, 298, 1982, 1184, 775, 328, 1676, 871, 133, 1374, 1927, 1984, 698, 1037, 100, 1884, 1596, 429, 1794, 2046, 105, 2037, 1767, 178, 176, 1293, 1893, 1780, 1832, 1382],\n+                [1932, 714, 1084, 1167, 624, 509, 1213, 651, 1000, 1686, 1537, 555, 461, 623, 1433, 1089, 1212, 1628, 834, 1111, 943, 1816, 1947, 1063, 354, 1843, 1741, 2015, 404, 928, 1488, 168],\n+                [1437, 314, 1356, 404, 1274, 2016, 998, 1350, 155, 553, 368, 1501, 1431, 1563, 1105, 1353, 535, 908, 1305, 1214, 1656, 65, 1469, 1517, 480, 252, 1289, 696, 302, 632, 246, 72],\n+                [724, 848, 1140, 927, 1669, 296, 447, 1708, 1898, 685, 1041, 1685, 708, 1510, 1623, 876, 11, 99, 43, 586, 1705, 1753, 1477, 1191, 583, 1249, 1613, 992, 1319, 677, 418, 668],\n+                [925, 54, 1810, 674, 1306, 848, 573, 1772, 105, 301, 1753, 989, 440, 1057, 823, 1313, 1663, 750, 1477, 102, 1437, 1114, 399, 1440, 319, 118, 1827, 295, 1429, 139, 1594, 55],\n+                [629, 149, 784, 838, 984, 604, 685, 1229, 1432, 859, 1526, 1336, 1949, 281, 988, 1260, 52, 6, 1216, 1542, 1426, 1938, 253, 280, 1319, 794, 901, 843, 615, 437, 814, 20],\n+                [1281, 502, 1237, 404, 625, 1444, 397, 1999, 2016, 1686, 533, 1785, 1152, 1245, 579, 1906, 1204, 549, 1334, 536, 1351, 1979, 208, 111, 2011, 751, 677, 1948, 1772, 1525, 2038, 419],\n+                [9, 490, 869, 2026, 1928, 1489, 587, 549, 1241, 460, 1458, 1636, 924, 222, 1246, 480, 706, 398, 75, 1717, 604, 1446, 333, 237, 805, 1446, 421, 1343, 78, 1260, 1872, 1116],\n+                [1944, 755, 375, 332, 1464, 828, 1273, 579, 1457, 353, 1510, 1910, 1609, 705, 400, 1666, 227, 1544, 1270, 136, 1857, 1975, 1762, 2006, 1102, 221, 1965, 151, 2041, 198, 1830, 287],\n+                [221, 502, 440, 247, 181, 1912, 42, 357, 1883, 596, 919, 953, 1774, 772, 915, 188, 438, 1226, 544, 1313, 726, 1298, 85, 677, 566, 1581, 30, 341, 878, 1732, 591, 1446],\n+                [1178, 1690, 320, 1746, 1798, 685, 1941, 666, 832, 623, 1907, 128, 337, 1779, 824, 923, 1041, 287, 1165, 437, 1803, 1222, 870, 646, 358, 220, 2009, 735, 468, 1908, 1349, 1603],\n+                [1432, 1286, 540, 1687, 1741, 951, 299, 1233, 1061, 1128, 985, 953, 1917, 198, 2031, 1559, 1096, 1455, 780, 437, 163, 1268, 649, 1029, 1081, 1518, 304, 1638, 814, 364, 140, 1385],\n+                [905, 463, 1739, 1063, 351, 936, 1652, 101, 1323, 1731, 298, 1193, 266, 1554, 1837, 1659, 409, 1739, 1012, 725, 851, 1909, 213, 1918, 1759, 1561, 1250, 970, 1571, 352, 911, 195],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n+            ],\n+            [\n+                [1375, 203, 265, 164, 200, 1867, 976, 924, 1972, 1637, 1048, 271, 1912, 1430, 853, 1942, 260, 1642, 400, 57, 1376, 1626, 1821, 1163, 619, 777, 1076, 951, 389, 1820, 84, 1417],\n+                [914, 527, 286, 968, 305, 1314, 805, 1703, 87, 559, 1980, 1124, 1726, 36, 1139, 618, 1628, 519, 1943, 781, 400, 1265, 438, 113, 87, 856, 465, 162, 1099, 352, 1141, 274],\n+                [1408, 6, 126, 2009, 90, 996, 934, 134, 1857, 126, 602, 876, 1092, 1962, 1205, 828, 707, 1063, 393, 1533, 123, 1086, 1749, 1324, 1, 1763, 1707, 1191, 34, 1323, 1017, 1787],\n+                [1000, 683, 1630, 703, 1574, 587, 25, 1049, 213, 1270, 1641, 1072, 1892, 1634, 1603, 90, 867, 2037, 1021, 715, 206, 507, 1138, 959, 1822, 1785, 280, 1100, 1660, 251, 1903, 988],\n+                [1657, 1981, 246, 1048, 1952, 451, 305, 423, 2000, 416, 756, 1748, 7, 748, 1866, 1795, 1682, 1832, 338, 212, 1685, 518, 154, 1407, 416, 765, 776, 25, 55, 458, 612, 262],\n+                [1034, 564, 667, 1474, 1212, 350, 712, 941, 1151, 1182, 1280, 640, 924, 1722, 1816, 458, 226, 359, 1518, 102, 1203, 459, 676, 1788, 1110, 393, 1974, 1721, 795, 1459, 798, 1723],\n+                [742, 1616, 119, 653, 441, 679, 246, 1432, 486, 1615, 1191, 500, 650, 223, 687, 1765, 1875, 963, 1385, 863, 151, 1771, 458, 1170, 737, 1932, 785, 1954, 1067, 16, 1986, 2029],\n+                [1437, 1078, 1767, 1452, 1392, 45, 2010, 1664, 245, 2015, 1416, 1055, 457, 985, 740, 1594, 1562, 1838, 258, 1431, 701, 604, 1813, 352, 792, 632, 21, 895, 70, 609, 850, 1599],\n+                [983, 1961, 54, 135, 846, 711, 473, 1630, 1373, 1094, 251, 525, 632, 1014, 1594, 1594, 1752, 398, 1266, 1357, 942, 1680, 191, 874, 483, 1291, 381, 1873, 1964, 1278, 1477, 122],\n+                [1663, 1969, 1887, 113, 145, 251, 1133, 156, 245, 1641, 209, 1322, 2037, 836, 539, 667, 940, 797, 1758, 1357, 191, 1137, 587, 1699, 27, 701, 395, 99, 1682, 876, 762, 839],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n+            ]\n+        ])\n+        # fmt: on\n+\n+        torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)"
        },
        {
            "sha": "da96381246095b576d3dd0476db801c1a64b9cbd",
            "filename": "tests/models/csm/test_processor_csm.py",
            "status": "added",
            "additions": 140,
            "deletions": 0,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -0,0 +1,140 @@\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import shutil\n+import tempfile\n+import unittest\n+\n+import jinja2\n+import numpy as np\n+\n+from transformers import CsmProcessor\n+from transformers.testing_utils import require_torch\n+from transformers.utils import is_torch_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_torch\n+class CsmProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = CsmProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        # TODO: @eustlb, change for hf-internal-testing/csm-1b\n+        cls.checkpoint = \"eustlb/csm-1b\"\n+        processor = CsmProcessor.from_pretrained(cls.checkpoint)\n+        cls.audio_token = processor.audio_token\n+        cls.audio_token_id = processor.audio_token_id\n+        cls.pad_token_id = processor.tokenizer.pad_token_id\n+        cls.bos_token_id = processor.tokenizer.bos_token_id\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": \"\\n{%- for message in messages %}\\n    {#-- Validate role is a stringified integer --#}\\n    {%- if not message['role'] is string or not message['role'].isdigit() %}\\n        {{- raise_exception(\\\"The role must be an integer or a stringified integer (e.g. '0') designating the speaker id\\\") }}\\n    {%- endif %}\\n\\n    {#-- Validate content is a list --#}\\n    {%- set content = message['content'] %}\\n    {%- if content is not iterable or content is string %}\\n        {{- raise_exception(\\\"The content must be a list\\\") }}\\n    {%- endif %}\\n\\n    {#-- Collect content types --#}\\n    {%- set content_types = content | map(attribute='type') | list %}\\n    {%- set is_last = loop.last %}\\n\\n    {#-- Last message validation --#}\\n    {%- if is_last %}\\n        {%- if 'text' not in content_types %}\\n            {{- raise_exception(\\\"The last message must include one item of type 'text'\\\") }}\\n        {%- elif (content_types | select('equalto', 'text') | list | length > 1) or (content_types | select('equalto', 'audio') | list | length > 1) %}\\n            {{- raise_exception(\\\"At most two items are allowed in the last message: one 'text' and one 'audio'\\\") }}\\n        {%- endif %}\\n\\n    {#-- All other messages validation --#}\\n    {%- else %}\\n        {%- if content_types | select('equalto', 'text') | list | length != 1\\n              or content_types | select('equalto', 'audio') | list | length != 1 %}\\n            {{- raise_exception(\\\"Each message (except the last) must contain exactly one 'text' and one 'audio' item\\\") }}\\n        {%- elif content_types | reject('in', ['text', 'audio']) | list | length > 0 %}\\n            {{- raise_exception(\\\"Only 'text' and 'audio' types are allowed in content\\\") }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{%- for message in messages %}\\n    {{- bos_token }}\\n    {{- '[' + message['role'] + ']' }}\\n    {{- message['content'][0]['text'] }}\\n    {{- eos_token }}\\n    {%- if message['content']|length > 1 %}\\n        {{- '<|AUDIO|><|audio_eos|>' }}\\n    {%- endif %}\\n{%- endfor %}\\n\"}  # fmt: skip\n+\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n+    def test_apply_chat_template(self):\n+        # Message contains content which a mix of lists with images and image urls and string\n+        messages = [\n+            {\n+                \"role\": \"0\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a test sentence 0.\"},\n+                    {\"type\": \"audio\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"1\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a test sentence 1.\"},\n+                    {\"type\": \"audio\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"0\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a prompt.\"},\n+                ],\n+            },\n+        ]\n+        processor = CsmProcessor.from_pretrained(self.tmpdirname)\n+        rendered = processor.apply_chat_template(messages, tokenize=False)\n+\n+        expected_rendered = (\n+            \"<|begin_of_text|>[0]This is a test sentence 0.<|end_of_text|>\"\n+            \"<|AUDIO|><|audio_eos|>\"\n+            \"<|begin_of_text|>[1]This is a test sentence 1.<|end_of_text|>\"\n+            \"<|AUDIO|><|audio_eos|>\"\n+            \"<|begin_of_text|>[0]This is a prompt.<|end_of_text|>\"\n+        )\n+        self.assertEqual(rendered, expected_rendered)\n+\n+        messages = [\n+            {\n+                \"role\": \"0\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a test sentence.\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"1\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a test sentence.\"},\n+                ],\n+            },\n+        ]\n+\n+        # this should raise an error because the CSM processor requires audio content in the messages expect the last one\n+        with self.assertRaises(jinja2.exceptions.TemplateError):\n+            input_ids = processor.apply_chat_template(messages, tokenize=False)\n+\n+        # now let's very that it expands audio tokens correctly\n+        messages = [\n+            {\n+                \"role\": \"0\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"This is a test sentence.\"},\n+                    {\"type\": \"audio\", \"audio\": np.zeros(4096)},\n+                ],\n+            },\n+        ]\n+\n+        input_ids = processor.apply_chat_template(messages, tokenize=True)\n+\n+        # 4096 audio input values should give 3 audio tokens\n+        expected_ids = torch.tensor(\n+            [[128000, 58, 15, 60, 2028, 374, 264, 1296, 11914, 13, 128001, 128002, 128002, 128002, 128003]]\n+        )\n+        torch.testing.assert_close(input_ids, expected_ids)"
        },
        {
            "sha": "974bfb7b5a79e3b00c2ec17b1272b5a66e5ee82c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -4350,8 +4350,8 @@ def test_custom_4d_attention_mask(self):\n             out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n \n             # comparing softmax-normalized logits:\n-            normalized_0 = F.softmax(out_last_tokens)\n-            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            normalized_0 = F.softmax(out_last_tokens, dim=-1)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens, dim=-1)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n     @slow\n@@ -4403,7 +4403,7 @@ def test_forward_with_logits_to_keep(self):\n                 self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n \n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-            batch_size, sequence_length = inputs[\"input_ids\"].shape\n+            batch_size, sequence_length = inputs[\"input_ids\"].shape[:2]\n             vocab_size = config.get_text_config().vocab_size\n             model = model_class(config).to(device=torch_device).eval()\n             # some models have labels but `logits_to_keep` should not be used in train mode"
        },
        {
            "sha": "960a73e734b9398d03159b250d0e4923c95b2728",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/798f948e88fd0b93fc515ec6b96e0503b78ad6ba/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=798f948e88fd0b93fc515ec6b96e0503b78ad6ba",
            "patch": "@@ -159,6 +159,9 @@\n         \"InternVLVisionModel\",  # Building part of bigger (tested) model\n         \"JanusVisionModel\",  # Building part of bigger (tested) model\n         \"TimesFmModel\",  # Building part of bigger (tested) model\n+        \"CsmDepthDecoderForCausalLM\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n+        \"CsmDepthDecoderModel\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n+        \"CsmBackboneModel\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n     ]\n )\n \n@@ -368,6 +371,10 @@\n     \"Qwen2_5OmniToken2WavModel\",  # Building part of a bigger model\n     \"Qwen2_5OmniToken2WavBigVGANModel\",  # Building part of a bigger model\n     \"Qwen2_5OmniToken2WavDiTModel\",  # Building part of a bigger model\n+    \"CsmBackboneModel\",  # Building part of a bigger model\n+    \"CsmDepthDecoderModel\",  # Building part of a bigger model\n+    \"CsmDepthDecoderForCausalLM\",  # Building part of a bigger model\n+    \"CsmForConditionalGeneration\",  # Building part of a bigger model\n ]\n \n # DO NOT edit this list!"
        }
    ],
    "stats": {
        "total": 5913,
        "additions": 5827,
        "deletions": 86
    }
}