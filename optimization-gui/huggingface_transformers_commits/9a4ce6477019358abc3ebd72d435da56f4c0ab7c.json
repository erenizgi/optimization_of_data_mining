{
    "author": "molbap",
    "message": ":red_circle: Update CLIP vision attention to new attention interface (#37498)\n\n* update attention interface\n\n* fix test\n\n* propagate attention changes\n\n* revert weird changes\n\n* fix modular\n\n* what?\n\n* ruff is mocking me\n\n* ruff being ruff\n\n* simplify test suite + fix FA2\n\n* fixup tests  + propagate FA2 fixes\n\n* add Copied From where relevant\n\n* fix conflict between copies and modular\n\n* recover FA2 training for CLIP + handle quantization\n\n* don't ditch the warning\n\n* tiny import fix\n\n* code review (FA2 support, copied from)\n\n* fix style\n\n* modularity\n\n* wrong copies\n\n* future-proofing for TP\n\n* mlcd inherits from CLIP",
    "sha": "9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
    "files": [
        {
            "sha": "9c8be7ea8079a34276a9513ca1ee4eaa6287e2fb",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 73,
            "deletions": 70,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import Any, Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -30,9 +30,15 @@\n     BaseModelOutputWithPoolingAndCrossAttentions,\n     BaseModelOutputWithPoolingAndProjection,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, add_start_docstrings_to_model_forward, logging, replace_return_docstrings, torch_int\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n \n@@ -721,7 +727,29 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->AltCLIP\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class AltCLIPAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -738,15 +766,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -756,74 +782,51 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n+            self.is_causal = causal_attention_mask is not None\n \n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->AltCLIP"
        },
        {
            "sha": "4136aabf1dcfeadb05981bdf2517565227a98bbe",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 65,
            "deletions": 237,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -15,19 +15,16 @@\n \"\"\"PyTorch CLIP model.\"\"\"\n \n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n@@ -41,10 +38,6 @@\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n # General docstring\n@@ -297,10 +290,34 @@ def forward(\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    output_attentions: bool = True,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    if not output_attentions:\n+        attn_weights = None\n+    return attn_output, attn_weights\n+\n+\n class CLIPAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -313,15 +330,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -331,242 +346,55 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            self.is_causal = causal_attention_mask is not None\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n-\n-\n-class CLIPFlashAttention2(CLIPAttention):\n-    \"\"\"\n-    CLIPAttention flash attention module. This module inherits from `CLIPAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        output_attentions = False\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            is_causal=causal_attention_mask is not None,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            output_attentions=output_attentions,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class CLIPSdpaAttention(CLIPAttention):\n-    \"\"\"\n-    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `CLIPAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from CLIPAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n-                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n-                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                causal_attention_mask=causal_attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n-        if attention_mask is not None and causal_attention_mask is not None:\n-            attn_mask = attention_mask + causal_attention_mask\n-        elif causal_attention_mask is not None:\n-            attn_mask = causal_attention_mask\n-        else:\n-            attn_mask = attention_mask\n-\n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=attn_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            scale=self.scale,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n-CLIP_ATTENTION_CLASSES = {\n-    \"eager\": CLIPAttention,\n-    \"sdpa\": CLIPSdpaAttention,\n-    \"flash_attention_2\": CLIPFlashAttention2,\n-}\n-\n-\n class CLIPMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -583,10 +411,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class CLIPEncoderLayer(nn.Module):\n-    def __init__(self, config: CLIPConfig):\n+    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = CLIP_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = CLIPAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = CLIPMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -952,7 +780,7 @@ def forward(\n \n         # expand attention_mask\n         if attention_mask is not None and not self._use_flash_attention_2:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n         encoder_outputs: BaseModelOutput = self.encoder("
        },
        {
            "sha": "d626914d72f0f3710b447522ff5a3b03046e9412",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 69,
            "deletions": 71,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -17,7 +17,7 @@\n import copy\n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -26,7 +26,7 @@\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -264,11 +264,33 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->CLIPSeg\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class CLIPSegAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Union[CLIPSegVisionConfig, CLIPSegTextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -281,15 +303,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -299,74 +319,52 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+            self.is_causal = causal_attention_mask is not None\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n+        if not output_attentions:\n+            attn_weights = None\n \n-        return attn_output, attn_weights_reshaped\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->CLIPSeg"
        },
        {
            "sha": "320cf17126cc48595755007a1cc31a54d5676cd3",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -303,7 +303,6 @@ def __init__(self, config):\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.use_attention_bias)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    # Copied from transformers.models.clip.modeling_clip.CLIPAttention._shape\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "047b3a1fc43e5cb813dfd83eed5b32ec880666bb",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 67,
            "deletions": 71,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -17,15 +17,14 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...file_utils import ModelOutput\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import (\n@@ -34,9 +33,10 @@\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithPast,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n+    ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     logging,\n@@ -698,7 +698,6 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         return embeddings\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPMLP\n class GitVisionMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -714,7 +713,29 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->GitVision\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class GitVisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -731,15 +752,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -749,74 +768,51 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n+            self.is_causal = causal_attention_mask is not None\n \n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->GitVision"
        },
        {
            "sha": "5b2ef5ae3ee3568ee47e8196b185c50c35cf525c",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 73,
            "deletions": 72,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -16,15 +16,19 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...utils import ModelOutput, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...utils import (\n+    ModelOutput,\n+    logging,\n+)\n from .configuration_idefics import IdeficsVisionConfig\n \n \n@@ -160,11 +164,33 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->IdeficsVision\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class IdeficsVisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: IdeficsVisionConfig):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -177,15 +203,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -195,74 +219,51 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+            self.is_causal = causal_attention_mask is not None\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->IdeficsVision"
        },
        {
            "sha": "e557293ee3700c54e29a5a375d76f7a0f6bf2091",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 67,
            "deletions": 70,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import Any, Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -31,7 +31,7 @@\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithCrossAttentions,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -466,7 +466,29 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         return embeddings\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->Kosmos2Vision\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class Kosmos2VisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -483,15 +505,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -501,74 +521,51 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n+            self.is_causal = causal_attention_mask is not None\n \n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Kosmos2Vision"
        },
        {
            "sha": "851cd01a1ae3fe5e85bf5f3f86838b1afcc8a0b9",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -172,25 +172,6 @@ def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n         return embeddings\n \n \n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -217,6 +198,25 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n def apply_rotary_pos_emb_vision(\n     q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n@@ -253,16 +253,13 @@ def __init__(self, config: MLCDVisionConfig):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.num_key_value_groups = config.num_key_value_groups\n-        self.is_causal = False\n-\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n     def forward(\n         self,"
        },
        {
            "sha": "a6a36559610766e741b1079fc9ebbe11dd6757f2",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 70,
            "deletions": 74,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -14,9 +14,9 @@\n # limitations under the License.\n \"\"\"PyTorch X-CLIP model.\"\"\"\n \n-from copy import copy\n+import copy\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -25,7 +25,7 @@\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -223,7 +223,29 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->XCLIP\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n class XCLIPAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -240,15 +262,13 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -258,77 +278,54 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n-\n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation != \"flash_attention_2\":\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+            self.is_causal = causal_attention_mask is not None\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n+        if not output_attentions:\n+            attn_weights = None\n \n-        return attn_output, attn_weights_reshaped\n+        return attn_output, attn_weights\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->XCLIP\n class XCLIPMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1330,8 +1327,7 @@ def __init__(self, config: XCLIPConfig):\n \n         self.prompts_visual_layernorm = nn.LayerNorm(self.vision_embed_dim, eps=config.vision_config.layer_norm_eps)\n         self.prompts_visual_projection = nn.Parameter(torch.randn(self.vision_embed_dim, self.projection_dim))\n-\n-        mit_config = copy(vision_config)\n+        mit_config = copy.copy(vision_config)\n         mit_config.hidden_size = vision_config.mit_hidden_size\n         mit_config.intermediate_size = vision_config.mit_intermediate_size\n         mit_config.num_hidden_layers = vision_config.mit_num_hidden_layers"
        },
        {
            "sha": "739ae1f5903d34f11ce3c1197c8d66b80a7dbb49",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 32,
            "deletions": 210,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a4ce6477019358abc3ebd72d435da56f4c0ab7c/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=9a4ce6477019358abc3ebd72d435da56f4c0ab7c",
            "patch": "@@ -17,7 +17,6 @@\n import os\n import tempfile\n import unittest\n-from typing import Optional\n \n import numpy as np\n import requests\n@@ -36,14 +35,12 @@\n )\n from transformers.utils import (\n     is_torch_available,\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n-    is_torch_sdpa_available,\n     is_vision_available,\n )\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n     _config_zero_init,\n     floats_tensor,\n@@ -67,11 +64,6 @@\n         CLIPVisionModelWithProjection,\n     )\n \n-\n-if is_torch_sdpa_available():\n-    from torch.nn.attention import SDPBackend, sdpa_kernel\n-\n-\n if is_vision_available():\n     from PIL import Image\n \n@@ -170,6 +162,11 @@ def prepare_config_and_inputs_for_common(self):\n         inputs_dict = {\"pixel_values\": pixel_values}\n         return config, inputs_dict\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n \n class CLIPModelTesterMixin(ModelTesterMixin):\n     \"\"\"\n@@ -178,6 +175,7 @@ class CLIPModelTesterMixin(ModelTesterMixin):\n     different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n     \"\"\"\n \n+    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -186,8 +184,8 @@ def test_sdpa_can_dispatch_composite_models(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n-                # Load the model with SDPA\n-                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                # Load the model with SDPA (it is the default, but we explicit it for clarity)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n                 # Load model with eager attention\n@@ -197,180 +195,17 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n-            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n-\n-            # `None` as it is the requested one which will be assigned to each sub-config\n-            # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n-            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n-                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+            if hasattr(model_sdpa, \"vision_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+\n+            if hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n \n             self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n             self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n-            for name, submodule in model_eager.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-            has_sdpa = False\n-            for name, submodule in model_sdpa.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    has_sdpa = True\n-                    break\n-            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-    def test_eager_matches_sdpa_inference(\n-        self,\n-        torch_dtype: str,\n-        use_attention_mask_options: tuple[Optional[str], ...] = (None, \"left\", \"right\"),\n-        logit_keys: tuple[str, ...] = (\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n-    ):\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Convert to torch dtype\n-        dtypes = {\n-            \"float16\": torch.float16,\n-            \"bfloat16\": torch.bfloat16,\n-            \"float32\": torch.float32,\n-        }\n-        torch_dtype = dtypes[torch_dtype]\n-\n-        atols = {\n-            torch.float32: 1e-5,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-        rtols = {\n-            torch.float32: 1e-4,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-\n-        atol = atols[torch_dtype]\n-        rtol = rtols[torch_dtype]\n-\n-        def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n-            return f\"{msg} {current_case}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                # Load the model with SDPA\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                # Load model with eager attention\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-            # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n-            # but it would be nicer to have an efficient way to use parameterized.expand\n-            cases = [\n-                (use_mask, output_attentions, sdpa_backend, batch_size)\n-                for use_mask in use_attention_mask_options\n-                for output_attentions in [True, False]\n-                for sdpa_backend in [\n-                    [SDPBackend.MATH],\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                ]\n-                for batch_size in [1, 5]\n-            ]\n-            fail_cases = []\n-\n-            for use_mask, output_attentions, sdpa_backend, batch_size in cases:\n-                processed_inputs = inputs_dict.copy()\n-\n-                # convert to torch_dtype\n-                if \"pixel_values\" in processed_inputs:\n-                    processed_inputs[\"pixel_values\"] = processed_inputs[\"pixel_values\"].to(torch_dtype)\n-\n-                # slice for different batch sizes\n-                for key in [\"pixel_values\", \"input_ids\", \"attention_mask\"]:\n-                    if key in processed_inputs:\n-                        processed_inputs[key] = processed_inputs[key][:batch_size]\n-\n-                # set attention mask with left padding\n-                if not use_mask:\n-                    processed_inputs.pop(\"attention_mask\", None)\n-                elif use_mask == \"left\":\n-                    dummy_attention_mask = processed_inputs[\"attention_mask\"]\n-                    dummy_attention_mask[:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-                elif use_mask == \"right\":\n-                    dummy_attention_mask = processed_inputs[\"attention_mask\"]\n-                    dummy_attention_mask[:] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-                else:\n-                    raise ValueError(f\"Invalid value for use_mask={use_mask}\")\n-\n-                processed_inputs[\"output_attentions\"] = output_attentions\n-                processed_inputs[\"output_hidden_states\"] = True\n-\n-                current_case = f\"use_mask={use_mask}, batch_size={batch_size}, sdpa_backend={sdpa_backend}\"\n-\n-                prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-\n-                with torch.no_grad():\n-                    try:\n-                        with sdpa_kernel(sdpa_backend):\n-                            outputs_eager = model_eager(**prepared_inputs)\n-                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-                    except Exception as e:\n-                        fail_cases.append(f\"{current_case}: {e}\")\n-                        continue\n-\n-                keys = set(logit_keys) & set(outputs_eager.keys())\n-                self.assertTrue(\n-                    keys, f\"Keys {logit_keys} not found in outputs. Available keys: {outputs_eager.keys()}\"\n-                )\n-\n-                for key in keys:\n-                    try:\n-                        eager_logits = outputs_eager[key]\n-                        sdpa_logits = outputs_sdpa[key]\n-                    except KeyError:\n-                        raise KeyError(f\"Key {key} not found in outputs. Available keys: {outputs_eager.keys()}\")\n-\n-                    if \"hidden_state\" in key and use_mask == \"left\":\n-                        eager_logits = eager_logits[:, 1:]\n-                        sdpa_logits = sdpa_logits[:, 1:]\n-                    elif \"hidden_state\" in key and use_mask == \"right\":\n-                        eager_logits = eager_logits[:, :-1]\n-                        sdpa_logits = sdpa_logits[:, :-1]\n-\n-                    is_close = torch.allclose(eager_logits, sdpa_logits, atol=atol, rtol=rtol)\n-                    if not is_close:\n-                        fail_cases.append(get_mean_reldiff(key, current_case, sdpa_logits, eager_logits, atol, rtol))\n-\n-            self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n \n @require_torch\n class CLIPVisionModelTest(CLIPModelTesterMixin, unittest.TestCase):\n@@ -458,16 +293,12 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertIsNotNone(model)\n         self.assertTrue(hasattr(model, \"visual_projection\"))\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n-    @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"last_hidden_state\", \"pooler_output\", \"image_embeds\"),\n-            use_attention_mask_options=(None,),\n-        )\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n@@ -632,16 +463,13 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertIsNotNone(model)\n         self.assertTrue(hasattr(model, \"text_projection\"))\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n     @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"last_hidden_state\", \"pooler_output\", \"text_embeds\"),\n-            use_attention_mask_options=(None, \"right\"),  # \"left\" is not supported for text model\n-        )\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n@@ -860,16 +688,13 @@ def test_model_from_pretrained(self):\n         model = CLIPModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n     @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"logits_per_image\", \"logits_per_text\"),\n-            use_attention_mask_options=(None, \"right\"),  # \"left\" is not supported for text model\n-        )\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n@@ -1033,16 +858,13 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_initialization(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n     @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"logits\",),\n-            use_attention_mask_options=(None,),\n-        )\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n@@ -1062,7 +884,7 @@ class CLIPModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference(self):\n         model_name = \"openai/clip-vit-base-patch32\"\n-        model = CLIPModel.from_pretrained(model_name).to(torch_device)\n+        model = CLIPModel.from_pretrained(model_name, attn_implementation=\"sdpa\").to(torch_device)\n         processor = CLIPProcessor.from_pretrained(model_name)\n \n         image = prepare_img()\n@@ -1122,5 +944,5 @@ def test_inference_interpolate_pos_encoding(self):\n         ).to(torch_device)\n \n         torch.testing.assert_close(\n-            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=6e-3, atol=4e-4\n         )"
        }
    ],
    "stats": {
        "total": 1435,
        "additions": 536,
        "deletions": 899
    }
}