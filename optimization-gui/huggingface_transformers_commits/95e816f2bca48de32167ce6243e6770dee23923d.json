{
    "author": "gante",
    "message": "Cohere: update RoPE structure (#33408)",
    "sha": "95e816f2bca48de32167ce6243e6770dee23923d",
    "files": [
        {
            "sha": "3c1237e51137898f455bb113512dd60fcda8090a",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -20,6 +20,7 @@\n \"\"\"Cohere model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -79,6 +80,43 @@ class CohereConfig(PretrainedConfig):\n             Whether to tie weight embeddings\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -121,6 +159,7 @@ def __init__(\n         eos_token_id=255001,\n         tie_word_embeddings=True,\n         rope_theta=10000.0,\n+        rope_scaling=None,\n         attention_bias=False,\n         attention_dropout=0.0,\n         use_qk_norm=False,\n@@ -144,10 +183,14 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_qk_norm = use_qk_norm\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_config_validation(self)\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "ae84a9ec2d1a4364bb4a614c15db7075053cf474",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 131,
            "deletions": 39,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -37,6 +37,7 @@\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -135,35 +136,97 @@ def forward(self, hidden_states):\n \n \n class CohereRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n+    # Note: the forward pass of this RoPE is slightly different from Llama's, resulting in different `sin`/`cos` for\n+    # the same parameterization. The differences are highlighted with a comment.\n+\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[CohereConfig] = None,\n+    ):\n         super().__init__()\n-        self.scaling_factor = scaling_factor\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`CohereRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n     def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.repeat_interleave(freqs, 2, dim=-1)\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # This line differs from Llama's implementation\n             cos = emb.cos()\n             sin = emb.sin()\n-        return cos, sin\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def rotate_half(x):\n-    # Split and rotate\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n     x1 = x[..., ::2]\n     x2 = x[..., 1::2]\n     rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n@@ -272,17 +335,10 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-        self._init_rope()\n \n-    # Ignore copy\n-    def _init_rope(self):\n-        self.rotary_emb = CohereRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n+        self.rotary_emb = CohereRotaryEmbedding(config=self.config)\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -292,6 +348,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -310,7 +367,16 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -350,8 +416,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Cohere\n-# TODO(joao): add me back asap :)\n+# Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Cohere\n class CohereFlashAttention2(CohereAttention):\n     \"\"\"\n     Cohere flash attention module. This module inherits from `CohereAttention` as the weights of the module stays\n@@ -377,6 +442,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -402,7 +468,16 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -418,7 +493,6 @@ def forward(\n \n         dropout_rate = self.attention_dropout if self.training else 0.0\n \n-        # Ignore copy\n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in the correct dtype just to be sure everything works as expected.\n@@ -465,16 +539,13 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention Llama->Cohere\n-# TODO(joao): add me back asap :)\n class CohereSdpaAttention(CohereAttention):\n     \"\"\"\n     Cohere attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n     `CohereAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n     SDPA API.\n     \"\"\"\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -484,6 +555,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -517,7 +589,16 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -587,6 +668,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -601,6 +683,11 @@ def forward(\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n         \"\"\"\n         residual = hidden_states\n \n@@ -615,6 +702,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n \n         # Fully Connected\n@@ -755,8 +843,7 @@ def _init_weights(self, module):\n     \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n     COHERE_START_DOCSTRING,\n )\n-# copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Cohere\n-# TODO(joao): add me back asap :)\n+# Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Cohere, LLAMA->COHERE\n class CohereModel(CoherePreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CohereDecoderLayer`]\n@@ -776,6 +863,7 @@ def __init__(self, config: CohereConfig):\n             [CohereDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+        self.rotary_emb = CohereRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -787,14 +875,13 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    # Ignore copy\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -823,30 +910,33 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        past_seen_tokens = 0\n         return_legacy_cache = False\n         if (\n             use_cache and not isinstance(past_key_values, Cache) and not self.training\n         ):  # kept for BC (non `Cache` `past_key_values` inputs)\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            logger.warning_once(\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+            )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n-\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n-\n-        # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -866,6 +956,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -876,6 +967,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "43bac44ba1be20ab3641dc9de3e632a4c15e677a",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -1066,7 +1066,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "b14e0a4b3d8ca5fee31cdd1b04b32f1331ac3200",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -862,7 +862,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "876f5ed2a7c8dad4a4ff2b12eb25b719aa97d7a2",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -839,7 +839,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "a3667e065345643d8f92d4ce927d2bae3079ef32",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -192,7 +192,7 @@ def __init__(\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         # Validate the correctness of rotary position embeddings parameters\n-        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)"
        },
        {
            "sha": "c7017832b9324c821cc0b3cc31c7d952252c7dac",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -951,7 +951,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "ffe16b272033016e735e74852d9e37a1de68aad6",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -767,7 +767,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             return_legacy_cache = True\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "c7062e75b1085c93a978f6c73e528ecb9d8fa38c",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -1023,7 +1023,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "b4bda8e2db5251324f5c31ee0e696ac15ce48138",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -873,7 +873,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "a33338365312db10e9228a42d1d7711c8a239c29",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -1012,7 +1012,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "ccaa2c7fd29aae2e1050cdfe50cba579010d4df5",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -690,7 +690,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "648d1653a3b5036ba343664622f62ca4d9a490c2",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -981,7 +981,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "ec395679ae6207846349600f91bfe04b6509cbe6",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -1008,7 +1008,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "d0ea8ef0e376e0d0e2300de69f6fabf7e024c00e",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -920,7 +920,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "6f483e50cde0657d18ae38a13b553fa0c7f1f44f",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -1084,7 +1084,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "d91c0832ed33da84425f0ecd87abc0658aa41d88",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -965,7 +965,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        },
        {
            "sha": "0be37c4e1fb91cdd12e4a480e6cc79b3bb0be745",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95e816f2bca48de32167ce6243e6770dee23923d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=95e816f2bca48de32167ce6243e6770dee23923d",
            "patch": "@@ -894,7 +894,7 @@ def forward(\n             use_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n                 \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n "
        }
    ],
    "stats": {
        "total": 245,
        "additions": 190,
        "deletions": 55
    }
}