{
    "author": "winglian",
    "message": "allow registration of custom checkpoint conversion mappings (#42634)\n\n* allow registration of custom checkpoint conversion mappings\n\n* add tests\n\n* chore: lint\n\n* move tests to test_core_model_loading.py\n\n* fixup\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "45d8168e45553816cc13dcd3925b1328a5f2fdcf",
    "files": [
        {
            "sha": "0fa1e279a9734e25e230e1248f4ec82d4731971c",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/45d8168e45553816cc13dcd3925b1328a5f2fdcf/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45d8168e45553816cc13dcd3925b1328a5f2fdcf/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=45d8168e45553816cc13dcd3925b1328a5f2fdcf",
            "patch": "@@ -186,10 +186,22 @@ def _build_checkpoint_conversion_mapping():\n \n def get_checkpoint_conversion_mapping(model_type):\n     global _checkpoint_conversion_mapping_cache\n-    _checkpoint_conversion_mapping_cache = _build_checkpoint_conversion_mapping()\n+    if _checkpoint_conversion_mapping_cache is None:\n+        _checkpoint_conversion_mapping_cache = _build_checkpoint_conversion_mapping()\n     return deepcopy(_checkpoint_conversion_mapping_cache.get(model_type))\n \n \n+def register_checkpoint_conversion_mapping(\n+    model_type: str, mapping: list[WeightConverter | WeightRenaming], overwrite: bool = False\n+) -> None:\n+    global _checkpoint_conversion_mapping_cache\n+    if _checkpoint_conversion_mapping_cache is None:\n+        _checkpoint_conversion_mapping_cache = _build_checkpoint_conversion_mapping()\n+    if model_type in _checkpoint_conversion_mapping_cache and not overwrite:\n+        raise ValueError(f\"Model type {model_type} already exists in the checkpoint conversion mapping.\")\n+    _checkpoint_conversion_mapping_cache[model_type] = mapping\n+\n+\n # DO NOT MODIFY, KEPT FOR BC ONLY\n VLMS = [\n     \"aria\","
        },
        {
            "sha": "5fc4369fe7022f7e434b3b4d8fe87f61a910e934",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/45d8168e45553816cc13dcd3925b1328a5f2fdcf/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45d8168e45553816cc13dcd3925b1328a5f2fdcf/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=45d8168e45553816cc13dcd3925b1328a5f2fdcf",
            "patch": "@@ -18,6 +18,7 @@\n import torch.nn as nn\n \n from transformers import PretrainedConfig\n+from transformers.conversion_mapping import get_checkpoint_conversion_mapping, register_checkpoint_conversion_mapping\n from transformers.core_model_loading import (\n     Chunk,\n     Concatenate,\n@@ -505,5 +506,43 @@ def __init__(self):\n         torch.testing.assert_close(dequantized_q, expected_q, rtol=1e-2, atol=1e-2)\n \n \n+class TestConversionMapping(unittest.TestCase):\n+    def test_register_checkpoint_conversion_mapping(self):\n+        register_checkpoint_conversion_mapping(\n+            \"foobar\",\n+            [\n+                WeightRenaming(\".block_sparse_moe.gate\", \".mlp.gate\"),\n+            ],\n+        )\n+        self.assertEqual(len(get_checkpoint_conversion_mapping(\"foobar\")), 1)\n+\n+    def test_register_checkpoint_conversion_mapping_overwrites(self):\n+        register_checkpoint_conversion_mapping(\n+            \"foobarbaz\",\n+            [\n+                WeightRenaming(\".block_sparse_moe.gate\", \".mlp.gate\"),\n+            ],\n+        )\n+        with self.assertRaises(ValueError):\n+            register_checkpoint_conversion_mapping(\n+                \"foobarbaz\",\n+                [\n+                    WeightRenaming(\".block_sparse_moe.foo\", \".mlp.foo\"),\n+                    WeightRenaming(\".block_sparse_moe.bar\", \".mlp.bar\"),\n+                ],\n+            )\n+\n+        register_checkpoint_conversion_mapping(\n+            \"foobarbaz\",\n+            [\n+                WeightRenaming(\".block_sparse_moe.foo\", \".mlp.foo\"),\n+                WeightRenaming(\".block_sparse_moe.bar\", \".mlp.bar\"),\n+            ],\n+            overwrite=True,\n+        )\n+\n+        self.assertEqual(len(get_checkpoint_conversion_mapping(\"foobarbaz\")), 2)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 52,
        "deletions": 1
    }
}