{
    "author": "kaitolucifer",
    "message": "Fix Tensor + Embedding error in some cases when using SiglipVisionModel (#33994)\n\nFix Tensor + Embedding error in some cases\r\n\r\nCo-authored-by: kaitolucifer <kaito.o@ghelia.com>",
    "sha": "e782e95e3465b66a377c0a7fe95f8f10cd87459d",
    "files": [
        {
            "sha": "176a26d74f00d3aa179cd17efe2aaab52442b27d",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e782e95e3465b66a377c0a7fe95f8f10cd87459d/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e782e95e3465b66a377c0a7fe95f8f10cd87459d/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=e782e95e3465b66a377c0a7fe95f8f10cd87459d",
            "patch": "@@ -283,7 +283,7 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embedding\n+            return self.position_embedding(self.position_ids)\n \n         patch_pos_embed = self.position_embedding\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}