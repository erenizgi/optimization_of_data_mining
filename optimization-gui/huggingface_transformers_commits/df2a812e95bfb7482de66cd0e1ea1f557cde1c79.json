{
    "author": "MekkCyber",
    "message": "Fix expected output for ggml test (#35686)\n\nfix expected output",
    "sha": "df2a812e95bfb7482de66cd0e1ea1f557cde1c79",
    "files": [
        {
            "sha": "0b70b4c3d853bd3a2b6fc994b0dd1786e51b4aed",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/df2a812e95bfb7482de66cd0e1ea1f557cde1c79/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df2a812e95bfb7482de66cd0e1ea1f557cde1c79/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=df2a812e95bfb7482de66cd0e1ea1f557cde1c79",
            "patch": "@@ -630,9 +630,9 @@ def test_falcon7b_q2_k(self):\n         )\n \n         text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"].to(torch_device)\n-        out = model.generate(text, max_new_tokens=10)\n+        out = model.generate(text, max_new_tokens=16)\n \n-        EXPECTED_TEXT = \"Hello All,\\nI am new to this forum.\"\n+        EXPECTED_TEXT = 'Hello,\\nI am trying to use the \"get_post_meta\"'\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n     def test_falcon7b_weights_conversion_fp16(self):"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}