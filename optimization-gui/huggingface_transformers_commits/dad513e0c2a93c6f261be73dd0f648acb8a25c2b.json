{
    "author": "gante",
    "message": "[generate] remove cache v4.47 deprecations (#36212)",
    "sha": "dad513e0c2a93c6f261be73dd0f648acb8a25c2b",
    "files": [
        {
            "sha": "07d4654c35aa3682bd8356e444f86159b4373f2d",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 20,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=dad513e0c2a93c6f261be73dd0f648acb8a25c2b",
            "patch": "@@ -363,8 +363,7 @@ class DynamicCache(Cache):\n         ```\n     \"\"\"\n \n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n+    def __init__(self) -> None:\n         super().__init__()\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n         self.key_cache: List[torch.Tensor] = []\n@@ -466,10 +465,7 @@ def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n         return legacy_cache\n \n     @classmethod\n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def from_legacy_cache(\n-        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, num_hidden_layers: int = None\n-    ) -> \"DynamicCache\":\n+    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`. Used for\n         backward compatibility.\"\"\"\n         cache = cls()\n@@ -495,10 +491,7 @@ def crop(self, max_length: int):\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def batch_split(\n-        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n-    ) -> List[\"DynamicCache\"]:\n+    def batch_split(self, full_batch_size: int, split_size: int) -> List[\"DynamicCache\"]:\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         out = []\n@@ -511,8 +504,7 @@ def batch_split(\n         return out\n \n     @classmethod\n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int = None) -> \"DynamicCache\":\n+    def from_batch_splits(cls, splits: List[\"DynamicCache\"]) -> \"DynamicCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n         cache = cls()\n@@ -1527,10 +1519,7 @@ def crop(self, maximum_length: int):\n         self.check_dynamic_cache(self.crop.__name__)\n         self.self_attention_cache.crop(maximum_length)\n \n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def batch_split(\n-        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n-    ) -> \"List[EncoderDecoderCache]\":\n+    def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDecoderCache]\":\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         self.check_dynamic_cache(self.batch_split.__name__)\n@@ -1543,10 +1532,7 @@ def batch_split(\n         return out\n \n     @classmethod\n-    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n-    def from_batch_splits(\n-        cls, splits: List[\"EncoderDecoderCache\"], num_hidden_layers: int = None\n-    ) -> \"EncoderDecoderCache\":\n+    def from_batch_splits(cls, splits: List[\"EncoderDecoderCache\"]) -> \"EncoderDecoderCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n         self_attention_cache = DynamicCache()"
        },
        {
            "sha": "9760b37dea3cd5a6d46a51f3aafee1403a6037e1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=dad513e0c2a93c6f261be73dd0f648acb8a25c2b",
            "patch": "@@ -4520,7 +4520,7 @@ def _ranking_fast(\n     return selected_idx\n \n \n-def _split(data, full_batch_size: int, num_hidden_layers: int, split_size: int = None):\n+def _split(data, full_batch_size: int, split_size: int = None):\n     \"\"\"\n     Takes care of three cases:\n     1. data is a tensor: e.g. last_hidden_state, pooler_output etc. split them on the batch_size dim\n@@ -4538,7 +4538,7 @@ def _split(data, full_batch_size: int, num_hidden_layers: int, split_size: int =\n     elif isinstance(data, DynamicCache) or (\n         isinstance(data, EncoderDecoderCache) and isinstance(data.self_attention_cache, DynamicCache)\n     ):\n-        return data.batch_split(full_batch_size, split_size, num_hidden_layers)\n+        return data.batch_split(full_batch_size, split_size)\n     elif isinstance(data, tuple):\n         # If the elements of the tuple are also tuples (e.g., past_key_values in our earlier example)\n         if isinstance(data[0], tuple):\n@@ -4591,11 +4591,9 @@ def _split_model_inputs(\n     keys_to_ignore = [\"cache_position\", \"encoder_outputs\", \"logits_to_keep\"]\n     non_bool_keys = [k for k in keys if not isinstance(model_input[k], bool) and k not in keys_to_ignore]\n \n-    num_hidden_layers = config.get_text_config().num_hidden_layers\n-\n     # we split the tensors and tuples of tensors\n     data_split_list = [\n-        {k: _split(model_input[k], full_batch_size, num_hidden_layers, split_size)[i] for k in non_bool_keys}\n+        {k: _split(model_input[k], full_batch_size, split_size)[i] for k in non_bool_keys}\n         for i in range(full_batch_size // split_size)\n     ]\n     # bool values are the same and replicated for each split\n@@ -4632,7 +4630,6 @@ def stack_model_outputs(model_outputs: List[ModelOutput], config: PretrainedConf\n \n     # Infer the class from the first object in the list\n     model_output_cls = type(model_outputs[0])\n-    num_hidden_layers = config.get_text_config().num_hidden_layers\n \n     # Ensure all objects are of the same type\n     if not all(isinstance(obj, model_output_cls) for obj in model_outputs):\n@@ -4649,9 +4646,9 @@ def _concat(data):\n             return torch.cat(data, dim=0)\n         # New cache format\n         elif isinstance(data[0], DynamicCache):\n-            return DynamicCache.from_batch_splits(data, num_hidden_layers=num_hidden_layers)\n+            return DynamicCache.from_batch_splits(data)\n         elif isinstance(data[0], EncoderDecoderCache):\n-            return EncoderDecoderCache.from_batch_splits(data, num_hidden_layers=num_hidden_layers)\n+            return EncoderDecoderCache.from_batch_splits(data)\n         elif isinstance(data[0], tuple):\n             # If the elements of the tuple are also tuples (e.g., past_key_values in our earlier example)\n             if isinstance(data[0][0], tuple):"
        },
        {
            "sha": "b3dc1eba6826c05df2bb4c80216bc35ed3ea17d2",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=dad513e0c2a93c6f261be73dd0f648acb8a25c2b",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import PhimoeConfig, StaticCache, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    is_flaky,\n     require_torch,\n     slow,\n     torch_device,\n@@ -449,6 +450,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n \n     @parameterized.expand([(\"longrope\",)])\n+    @is_flaky()  # TODO (joao): unify rope tests in the mixin\n     def test_model_rope_scaling_short_long_factor(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         n_factors = config.hidden_size // config.num_key_value_heads // 2"
        },
        {
            "sha": "dcb0816a0d0a2a2a1f07942c61eb789d5f98bd71",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad513e0c2a93c6f261be73dd0f648acb8a25c2b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=dad513e0c2a93c6f261be73dd0f648acb8a25c2b",
            "patch": "@@ -27,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -347,6 +348,10 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_compile_fullgraph(self):\n         pass\n \n+    @is_flaky()  # TODO (joao/raushan): Investigate why this test is flaky on this model\n+    def test_prompt_lookup_decoding_matches_greedy_search(self):\n+        super().test_prompt_lookup_decoding_matches_greedy_search()\n+\n \n @require_torch\n class Qwen2_5_VLIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 18,
        "deletions": 28
    }
}