{
    "author": "gabe-l-hart",
    "message": "feat(granitemoe*): Remove logits upcast when computing loss (#42753)\n\n* feat: Remove logits upcast when computing loss\n\nWhen the CausalLM loss is used, the upcast is done in the loss function\nutils, so this is redundant.\n\nhttps://github.com/huggingface/transformers/issues/42709\nBranch: GraniteOptionalUpcast-42709\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\n\n* chore: make fix-copies\n\nhttps://github.com/huggingface/transformers/issues/42709\nBranch: GraniteOptionalUpcast-42709\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\n\n---------\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>",
    "sha": "0af2381f7bfae0569b876dd40eefda1503da927f",
    "files": [
        {
            "sha": "7a8b29446115df64634b57facd94c1b761fca81a",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=0af2381f7bfae0569b876dd40eefda1503da927f",
            "patch": "@@ -714,8 +714,6 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n             # Flatten the tokens\n             loss = self.loss_function(\n                 logits,"
        },
        {
            "sha": "1696d9890c82e676eed27c0860cd6cd3074092a5",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=0af2381f7bfae0569b876dd40eefda1503da927f",
            "patch": "@@ -295,8 +295,6 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n             # Flatten the tokens\n             loss = self.loss_function(\n                 logits,"
        },
        {
            "sha": "2116a9811667f775b798e64bc31a0e1de0fcf8b2",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=0af2381f7bfae0569b876dd40eefda1503da927f",
            "patch": "@@ -1510,8 +1510,6 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n             # Flatten the tokens\n             loss = self.loss_function(\n                 logits,"
        },
        {
            "sha": "b62a7ca99a39238bad3c79c5d415b05b72ff5e6d",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0af2381f7bfae0569b876dd40eefda1503da927f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=0af2381f7bfae0569b876dd40eefda1503da927f",
            "patch": "@@ -785,8 +785,6 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n             # Flatten the tokens\n             loss = self.loss_function(\n                 logits,"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 0,
        "deletions": 8
    }
}