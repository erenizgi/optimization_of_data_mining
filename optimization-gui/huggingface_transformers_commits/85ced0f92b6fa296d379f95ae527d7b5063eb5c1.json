{
    "author": "SunMarc",
    "message": "Update replace_with_ for quants methods to not use recursion (#42711)\n\n* Fix replace\n\n* fix bnb\n\n* fix\n\n* style\n\n* fix\n\n* fix\n\n* styke\n\n* fix\n\n* style\n\n* Apply suggestions from code review\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
    "files": [
        {
            "sha": "e70e8ae35b817168ed5c7c5af96f38e58cbc5910",
            "filename": "src/transformers/integrations/aqlm.py",
            "status": "modified",
            "additions": 38,
            "deletions": 66,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faqlm.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -13,88 +13,60 @@\n # limitations under the License.\n \"AQLM (Additive Quantization of Language Model) integration file\"\n \n-from ..utils import ACCELERATE_MIN_VERSION, is_accelerate_available, is_aqlm_available, is_torch_available\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_torch_available, logging\n \n \n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n if is_torch_available():\n     import torch.nn as nn\n \n+logger = logging.get_logger(__name__)\n \n-def replace_with_aqlm_linear(\n-    model,\n-    quantization_config=None,\n-    linear_weights_not_to_quantize=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n-):\n+\n+def replace_with_aqlm_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with AQLM quantized layers.\n-    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successful or not.\n \n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n-        quantization_config (`AqlmConfig`):\n-            The quantization config object that contains the quantization parameters.\n-        linear_weights_not_to_quantize (`list[str]`, *optional*):\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n             A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n             converted.\n-        current_key_name (`list`, *optional*):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n+        quantization_config (`AqlmConfig`):\n+            The quantization config object that contains the quantization parameters.\n     \"\"\"\n-    if not is_aqlm_available():\n-        raise ValueError(\"AQLM is not available. Please install it with `pip install aqlm[cpu,gpu]`\")\n-\n-    if not is_accelerate_available():\n-        raise ValueError(\n-            f\"AQLM requires Accelerate to be installed: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n-        )\n-\n-    if linear_weights_not_to_quantize is None:\n-        linear_weights_not_to_quantize = []\n-\n-    from accelerate import init_empty_weights\n     from aqlm import QuantizedLinear\n \n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if isinstance(module, nn.Linear):\n-            # Check if the current key is not in the `linear_weights_not_to_quantize`\n-            if \".\".join(current_key_name) + \".weight\" not in linear_weights_not_to_quantize:\n-                with init_empty_weights():\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                new_module = QuantizedLinear(\n+                    module.in_features,\n+                    module.out_features,\n+                    bias=module.bias is not None,\n+                    in_group_size=quantization_config.in_group_size,\n+                    out_group_size=quantization_config.out_group_size,\n+                    num_codebooks=quantization_config.num_codebooks,\n+                    nbits_per_codebook=quantization_config.nbits_per_codebook,\n+                )\n+                new_module.source_cls = type(module)\n+                new_module.requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n \n-                    model._modules[name] = QuantizedLinear(\n-                        in_features,\n-                        out_features,\n-                        bias=module.bias is not None,\n-                        in_group_size=quantization_config.in_group_size,\n-                        out_group_size=quantization_config.out_group_size,\n-                        num_codebooks=quantization_config.num_codebooks,\n-                        nbits_per_codebook=quantization_config.nbits_per_codebook,\n-                    )\n-                    has_been_replaced = True\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n \n-                    # Store the module class in case we need to transpose the weight later\n-                    model._modules[name].source_cls = type(module)\n-                    # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_aqlm_linear(\n-                module,\n-                quantization_config=quantization_config,\n-                linear_weights_not_to_quantize=linear_weights_not_to_quantize,\n-                current_key_name=current_key_name,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    return model"
        },
        {
            "sha": "48b89c5bdae8a3958fbd824be441e228b7faa81e",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 45,
            "deletions": 100,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -1,3 +1,4 @@\n+from ..quantizers.quantizers_utils import should_convert_module\n from ..utils import is_accelerate_available, is_torch_available, logging\n \n \n@@ -314,113 +315,57 @@ def forward(self, input):\n         return output\n \n \n-def _replace_with_bitnet_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    has_been_replaced=False,\n-    pre_quantized=False,\n-):\n+def replace_with_bitnet_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n     \"\"\"\n-    Private method that wraps the recursion for module replacement.\n+    Public method that replaces the linear layers of the given model with bitnet quantized layers.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n-    \"\"\"\n-\n-    if current_key_name is None:\n-        current_key_name = []\n-\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        # Check if the current key is not in the `modules_to_not_convert`\n-        if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n-            with init_empty_weights():\n-                if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-                    if quantization_config and quantization_config.linear_class == \"autobitlinear\":\n-                        model._modules[name] = AutoBitLinear(\n-                            in_features=in_features,\n-                            out_features=out_features,\n-                            bias=module.bias is not None,\n-                            device=module.weight.device,\n-                            dtype=module.weight.dtype,\n-                            online_quant=(quantization_config.quantization_mode == \"online\"),\n-                            use_rms_norm=quantization_config.use_rms_norm,\n-                            rms_norm_eps=quantization_config.rms_norm_eps,\n-                        )\n-                        if quantization_config.quantization_mode == \"offline\":\n-                            model._modules[name].requires_grad_(False)\n-                    else:\n-                        model._modules[name] = BitLinear(\n-                            in_features=in_features,\n-                            out_features=out_features,\n-                            bias=module.bias is not None,\n-                            device=module.weight.device,\n-                            dtype=module.weight.dtype,\n-                            use_rms_norm=quantization_config.use_rms_norm if quantization_config else False,\n-                            rms_norm_eps=quantization_config.rms_norm_eps if quantization_config else 1e-6,\n-                        )\n-                        model._modules[name].requires_grad_(False)\n-                    has_been_replaced = True\n-\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_bitnet_linear(\n-                module,\n-                modules_to_not_convert=modules_to_not_convert,\n-                current_key_name=current_key_name,\n-                quantization_config=quantization_config,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n-\n-def replace_with_bitnet_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    pre_quantized=False,\n-):\n-    \"\"\"\n-    A helper function to replace all `torch.nn.Linear` modules by `BitLinear158` modules`.\n-\n-    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n-    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n-    CPU/GPU memory is required to run this function. Each weight will be quantized along the channel.\n-\n-    Parameters:\n+    Args:\n         model (`torch.nn.Module`):\n-            Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n-            Names of the modules to not convert in `BitLinear`. In practice we keep the `lm_head` in full precision\n-            for numerical stability reasons.\n-        current_key_name (`list[`str`]`, *optional*):\n-            An array to track the current key of the recursion. This is used to check whether the current key (part of\n-            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n-            `disk`).\n+            The model to convert, can be any `torch.nn.Module` instance.\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n+            converted.\n+        quantization_config (`BitNetConfig`):\n+            The quantization config object that contains the quantization parameters.\n     \"\"\"\n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n-    if quantization_config and quantization_config.modules_to_not_convert is not None:\n-        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n-    modules_to_not_convert = list(set(modules_to_not_convert))\n-    model, has_been_replaced = _replace_with_bitnet_linear(\n-        model,\n-        modules_to_not_convert,\n-        current_key_name,\n-        quantization_config,\n-        pre_quantized=pre_quantized,\n-    )\n+\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                if quantization_config and quantization_config.linear_class == \"autobitlinear\":\n+                    new_module = AutoBitLinear(\n+                        in_features=module.in_features,\n+                        out_features=module.out_features,\n+                        bias=module.bias is not None,\n+                        device=module.weight.device,\n+                        dtype=module.weight.dtype,\n+                        online_quant=(quantization_config.quantization_mode == \"online\"),\n+                        use_rms_norm=quantization_config.use_rms_norm,\n+                        rms_norm_eps=quantization_config.rms_norm_eps,\n+                    )\n+                    if quantization_config.quantization_mode == \"offline\":\n+                        new_module.requires_grad_(False)\n+                else:\n+                    new_module = BitLinear(\n+                        in_features=module.in_features,\n+                        out_features=module.out_features,\n+                        bias=module.bias is not None,\n+                        device=module.weight.device,\n+                        dtype=module.weight.dtype,\n+                        use_rms_norm=quantization_config.use_rms_norm if quantization_config else False,\n+                        rms_norm_eps=quantization_config.rms_norm_eps if quantization_config else 1e-6,\n+                    )\n+                    new_module.requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n \n     if not has_been_replaced:\n         logger.warning(\n-            \"You are loading your model using bitnet but no linear modules were found in your model.\"\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n             \" Please double check your model architecture, or submit an issue on github if you think this is\"\n             \" a bug.\"\n         )"
        },
        {
            "sha": "ae5d509da2341a3f23dae4c046f2fd06cf9bd37d",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 78,
            "deletions": 192,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -1,8 +1,7 @@\n import inspect\n-from inspect import signature\n \n from ..core_model_loading import ConversionOps\n-from ..quantizers.quantizers_utils import get_module_from_name\n+from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n from ..utils import (\n     get_available_devices,\n     is_accelerate_available,\n@@ -156,136 +155,77 @@ def convert(\n         return {key_weight: new_value}\n \n \n-def _replace_with_bnb_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n+def replace_with_bnb_linear(\n+    model: torch.nn.Module,\n+    modules_to_not_convert: list[str] | None = None,\n     quantization_config=None,\n-    has_been_replaced=False,\n     pre_quantized=False,\n ):\n     \"\"\"\n-    Private method that wraps the recursion for module replacement.\n+    A helper function to replace all `torch.nn.Linear` modules by bnb modules from the `bitsandbytes` library.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n+    Args:\n+        model (`torch.nn.Module`):\n+            The model to convert, can be any `torch.nn.Module` instance.\n+        modules_to_not_convert (`list[str]`, defaults to `None`):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n+            converted.\n+        quantization_config (`BitsAndBytesConfig`):\n+            The quantization config object that contains the quantization parameters.\n+        pre_quantized (`book`, defaults to `False`):\n+            Whether the model is pre-quantized or not\n     \"\"\"\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if (isinstance(module, (nn.Linear, Conv1D))) and name not in modules_to_not_convert:\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(\n-                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n-            ):\n-                with init_empty_weights():\n-                    if isinstance(module, Conv1D):\n-                        in_features, out_features = module.weight.shape\n-                    else:\n-                        in_features = module.in_features\n-                        out_features = module.out_features\n-\n-                    if quantization_config.quantization_method() == \"llm_int8\":\n-                        new_module = bnb.nn.Linear8bitLt(\n-                            in_features,\n-                            out_features,\n-                            module.bias is not None,\n-                            has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n-                            threshold=quantization_config.llm_int8_threshold,\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        new_module = None\n+        with init_empty_weights():\n+            if isinstance(module, (nn.Linear, Conv1D)):\n+                if isinstance(module, Conv1D):\n+                    in_features, out_features = module.weight.shape\n+                else:\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+                if quantization_config.quantization_method() == \"llm_int8\":\n+                    new_module = bnb.nn.Linear8bitLt(\n+                        in_features,\n+                        out_features,\n+                        module.bias is not None,\n+                        has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n+                        threshold=quantization_config.llm_int8_threshold,\n+                    )\n+                    if pre_quantized:\n+                        # this is kind of an edge case when supporting both loading and quantization ...\n+                        # we need to set the right dtype as we cast the checkpoint with the dtype of the meta model\n+                        new_module.weight.data = new_module.weight.data.to(dtype=torch.int8)\n+                else:\n+                    new_module = bnb.nn.Linear4bit(\n+                        in_features,\n+                        out_features,\n+                        module.bias is not None,\n+                        quantization_config.bnb_4bit_compute_dtype,\n+                        compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n+                        quant_type=quantization_config.bnb_4bit_quant_type,\n+                        quant_storage=quantization_config.bnb_4bit_quant_storage,\n+                    )\n+                    if pre_quantized:\n+                        # same here\n+                        new_module.weight.data = new_module.weight.data.to(\n+                            dtype=quantization_config.bnb_4bit_quant_storage\n                         )\n-                        if pre_quantized:\n-                            new_module.weight.data = new_module.weight.data.to(dtype=torch.int8)\n-                        model._modules[name] = new_module\n-                        has_been_replaced = True\n-                    else:\n-                        if (\n-                            quantization_config.llm_int8_skip_modules is not None\n-                            and name in quantization_config.llm_int8_skip_modules\n-                        ):\n-                            pass\n-                        else:\n-                            extra_kwargs = (\n-                                {\"quant_storage\": quantization_config.bnb_4bit_quant_storage}\n-                                if \"quant_storage\" in list(signature(bnb.nn.Linear4bit).parameters)\n-                                else {}\n-                            )\n-                            new_module = bnb.nn.Linear4bit(\n-                                in_features,\n-                                out_features,\n-                                module.bias is not None,\n-                                quantization_config.bnb_4bit_compute_dtype,\n-                                compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n-                                quant_type=quantization_config.bnb_4bit_quant_type,\n-                                **extra_kwargs,\n-                            )\n-                            if pre_quantized:\n-                                # this is kind of an edge case when supporting both loading and quantization ...\n-                                # we need to set the right dtype as we cast the checkpoint with the dtype of the meta model\n-                                new_module.weight.data = new_module.weight.data.to(\n-                                    dtype=quantization_config.bnb_4bit_quant_storage\n-                                )\n-                            model._modules[name] = new_module\n-                            has_been_replaced = True\n+                if new_module is not None:\n                     # Store the module class in case we need to transpose the weight later\n-                    model._modules[name].source_cls = type(module)\n+                    new_module.source_cls = type(module)\n                     # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_bnb_linear(\n-                module,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-                pre_quantized=pre_quantized,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n-\n-def replace_with_bnb_linear(\n-    model, modules_to_not_convert=None, current_key_name=None, quantization_config=None, pre_quantized=False\n-):\n-    \"\"\"\n-    A helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules from the `bitsandbytes`\n-    library. This will enable running your models using mixed int8 precision as described by the paper `LLM.int8():\n-    8-bit Matrix Multiplication for Transformers at Scale`. Make sure `bitsandbytes` compiled with the correct CUDA\n-    version of your hardware is installed before running this function. `pip install -i https://test.pypi.org/simple/\n-    bitsandbytes`\n-\n-    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n-    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n-    CPU/GPU memory is required to run this function. Int8 mixed-precision matrix decomposition works by separating a\n-    matrix multiplication into two streams: (1) and systematic feature outlier stream matrix multiplied in fp16\n-    (0.01%), (2) a regular stream of int8 matrix multiplication (99.9%). With this method, int8 inference with no\n-    predictive degradation is possible for very large models (>=176B parameters).\n-\n-    Parameters:\n-        model (`torch.nn.Module`):\n-            Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n-            Names of the modules to not convert in `Linear8bitLt`. In practice we keep the `lm_head` in full precision\n-            for numerical stability reasons.\n-        current_key_name (`list[`str`]`, *optional*):\n-            An array to track the current key of the recursion. This is used to check whether the current key (part of\n-            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n-            `disk`).\n-        quantization_config ('transformers.utils.quantization_config.BitsAndBytesConfig'):\n-            To configure and manage settings related to quantization, a technique used to compress neural network models\n-            by reducing the precision of the weights and activations, thus making models more efficient in terms of both\n-            storage and computation.\n-    \"\"\"\n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n-    model, has_been_replaced = _replace_with_bnb_linear(\n-        model, modules_to_not_convert, current_key_name, quantization_config, pre_quantized=pre_quantized\n-    )\n+                    new_module.requires_grad_(False)\n+                    model.set_submodule(module_name, new_module)\n+                    has_been_replaced = True\n \n     if not has_been_replaced:\n         logger.warning(\n-            \"You are loading your model in 8bit or 4bit but no linear modules were found in your model.\"\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n             \" Please double check your model architecture, or submit an issue on github if you think this is\"\n             \" a bug.\"\n         )\n@@ -343,104 +283,50 @@ def _create_accelerate_new_hook(old_hook):\n     return new_hook\n \n \n-def _dequantize_and_replace(\n+def dequantize_and_replace(\n     model,\n-    dtype,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n     quantization_config=None,\n-    has_been_replaced=False,\n ):\n     \"\"\"\n     Converts a quantized model into its dequantized original version. The newly converted model will have\n     some performance drop compared to the original model before quantization - use it only for specific usecases\n     such as QLoRA adapters merging.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n+    Returns the converted model.\n     \"\"\"\n     quant_method = quantization_config.quantization_method()\n \n     target_cls = bnb.nn.Linear8bitLt if quant_method == \"llm_int8\" else bnb.nn.Linear4bit\n \n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if isinstance(module, target_cls) and name not in modules_to_not_convert:\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            current_key_name_str = \".\".join(current_key_name)\n-\n-            if not any(\n-                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n-            ):\n+    for module_name, module in model.named_modules():\n+        if isinstance(module, target_cls):\n+            with init_empty_weights():\n                 bias = getattr(module, \"bias\", None)\n-\n-                device = module.weight.device\n-                with init_empty_weights():\n-                    new_module = torch.nn.Linear(module.in_features, module.out_features, bias=bias is not None)\n-\n-                if quant_method == \"llm_int8\":\n-                    state = module.state\n-                else:\n-                    state = None\n-\n-                new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, dtype, state))\n-\n-                if bias is not None:\n-                    new_module.bias = bias\n-\n-                # Create a new hook and attach it in case we use accelerate\n-                if hasattr(module, \"_hf_hook\"):\n-                    old_hook = module._hf_hook\n-                    new_hook = _create_accelerate_new_hook(old_hook)\n-\n-                    remove_hook_from_module(module)\n-                    add_hook_to_module(new_module, new_hook)\n-\n-                new_module.to(device)\n-                model._modules[name] = new_module\n-                has_been_replaced = True\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _dequantize_and_replace(\n-                module,\n-                dtype,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n-\n-def dequantize_and_replace(\n-    model,\n-    modules_to_not_convert=None,\n-    quantization_config=None,\n-):\n-    model, has_been_replaced = _dequantize_and_replace(\n-        model,\n-        model.dtype,\n-        modules_to_not_convert=modules_to_not_convert,\n-        quantization_config=quantization_config,\n-    )\n+                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=bias is not None)\n+            state = module.state if quant_method == \"llm_int8\" else None\n+            new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, model.dtype, state))\n+            if bias is not None:\n+                new_module.bias = bias\n+            if hasattr(module, \"_hf_hook\"):\n+                old_hook = module._hf_hook\n+                new_hook = _create_accelerate_new_hook(old_hook)\n+                remove_hook_from_module(module)\n+                add_hook_to_module(new_module, new_hook)\n+            new_module.to(module.weight.device)\n+            model.set_submodule(module_name, new_module)\n+            has_been_replaced = True\n \n     if not has_been_replaced:\n         logger.warning(\n             \"For some reason the model has not been properly dequantized. You might see unexpected behavior.\"\n         )\n-\n     return model\n \n \n def validate_bnb_backend_availability(raise_exception=False):\n     \"\"\"\n     Validates if the available devices are supported by bitsandbytes, optionally raising an exception if not.\n     \"\"\"\n-    import bitsandbytes as bnb\n-\n     bnb_supported_devices = getattr(bnb, \"supported_torch_devices\", set())\n     available_devices = set(get_available_devices())\n "
        },
        {
            "sha": "67b4acb7157495cd18047839a04d79028eb8fa95",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -86,25 +86,16 @@ def forward(self, input):\n         return output\n \n \n-def replace_with_eetq_linear(\n-    model: torch.nn.Module, modules_to_not_convert: list[str] | None = None, pre_quantized=False\n-):\n+def replace_with_eetq_linear(model, modules_to_not_convert: list[str] | None = None, pre_quantized=False):\n     \"\"\"\n     A helper function to replace all `torch.nn.Linear` modules by `EetqLinear` modules.\n-    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `modules_to_not_convert` that should\n-    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n-    CPU/GPU memory is required to run this function. Each weight will be quantized along the channel.\n \n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n         modules_to_not_convert (`list[`str`]`, *optional*, defaults to `None`):\n             Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`list[`str`]`, *optional*):\n-            An array to track the current key of the recursion. This is used to check whether the current key (part of\n-            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n-            `disk`).\n     \"\"\"\n     from kernels import get_kernel\n "
        },
        {
            "sha": "42836ceee93f9660a7db29dab04853d60e0be53c",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -216,26 +216,21 @@ def forward(self, hidden_states):\n \n \n def replace_with_fbgemm_fp8_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    quantization_config=None,\n-    pre_quantized=False,\n-    tp_plan=None,\n+    model, modules_to_not_convert: list[str] | None = None, quantization_config=None, pre_quantized=False, tp_plan=None\n ):\n     \"\"\"\n     A helper function to replace all `torch.nn.Linear` modules by `FbgemmFp8Linear` modules.\n     This will enable running your models using high performance fp8 kernel from FBGEMM library.\n \n-    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n-    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n-    CPU/GPU memory is required to run this function. Each weight will be quantized along the channel.\n-\n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n-            Names of the modules to not convert in `FP8Linear`. In practice we keep the `lm_head` in full precision\n-            for numerical stability reasons.\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `None`):\n+            Names of the modules to not convert. In practice we keep the `lm_head` in full precision for numerical stability reasons.\n+        quantization_config (`FbgemmFp8Config`):\n+            The quantization config object that contains the quantization parameters.\n+        pre_quantized (`book`, defaults to `False`):\n+            Whether the model is pre-quantized or not\n     \"\"\"\n \n     has_been_replaced = False"
        },
        {
            "sha": "bbcf5eef4246ccd5b0901220db44357a8841ab5f",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -589,12 +589,22 @@ def linear(self, input: torch.Tensor, weight: torch.Tensor, weight_scale_inv: to\n \n \n def replace_with_fp8_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    quantization_config=None,\n-    pre_quantized=False,\n+    model, modules_to_not_convert: list[str] | None = None, quantization_config=None, pre_quantized=False\n ):\n-    \"\"\"Helper function to replace model layers with FP8 versions.\"\"\"\n+    \"\"\"\n+    A helper function to replace all `torch.nn.Linear` modules by `FP8Linear` modules.\n+\n+    Parameters:\n+        model (`torch.nn.Module`):\n+            Input model or `torch.nn.Module` as the function is run recursively.\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `None`):\n+            Names of the modules to not convert. In practice we keep the `lm_head` in full precision for numerical stability reasons.\n+        quantization_config (`FbgemmFp8Config`):\n+            The quantization config object that contains the quantization parameters.\n+        pre_quantized (`book`, defaults to `False`):\n+            Whether the model is pre-quantized or not\n+    \"\"\"\n+\n     if quantization_config.dequantize:\n         return model\n "
        },
        {
            "sha": "3751be8cc0fb58200fa91fc405d31230ef1148d2",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 40,
            "deletions": 62,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -15,17 +15,16 @@\n \n from math import sqrt\n \n-from ..utils import (\n-    is_flute_available,\n-    is_hadamard_available,\n-    is_torch_available,\n-)\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_flute_available, is_hadamard_available, is_torch_available, logging\n \n \n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n if is_torch_available():\n     import torch\n-    from torch import nn\n-\n+    import torch.nn as nn\n \n if is_flute_available():\n     from flute.integrations.higgs import prepare_data_transposed\n@@ -34,6 +33,8 @@\n if is_hadamard_available():\n     from fast_hadamard_transform import hadamard_transform\n \n+logger = logging.get_logger(__name__)\n+\n \n def pad_to_block(tensor, dims, had_block_size, value=0):\n     pad_dims = [0 for _ in range(2 * len(tensor.shape))]\n@@ -549,70 +550,47 @@ def forward(self, x):\n         )\n \n \n-def replace_with_higgs_linear(\n-    model,\n-    quantization_config=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n-    modules_to_not_convert=None,\n-):\n+def replace_with_higgs_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n     \"\"\"\n-    Public method that recursively replaces the Linear layers of the given model with HIGGS quantized layers.\n-    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successful or not.\n+    Public method that replaces the Linear layers of the given model with HIGGS quantized layers.\n \n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n+            converted.\n         quantization_config (`HiggsConfig`):\n             The quantization config object that contains the quantization parameters.\n-        current_key_name (`list`, *optional*):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n     \"\"\"\n \n-    from accelerate import init_empty_weights\n-\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if isinstance(module, nn.Linear):\n-            # Check if the current key is not in the `quantization_config.modules_to_not_convert`\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(current_key_name_str.endswith(key) for key in modules_to_not_convert):\n-                with init_empty_weights():\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-\n-                    model._modules[name] = HiggsLinear(\n-                        in_features,\n-                        out_features,\n-                        bias=module.bias is not None,\n-                        num_bits=quantization_config.bits,\n-                        hadamard_size=quantization_config.hadamard_size,\n-                        group_size=quantization_config.group_size,\n-                    )\n-                    has_been_replaced = True\n-\n-                    # Store the module class in case we need to transpose the weight later\n-                    model._modules[name].source_cls = type(module)\n-                    # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_higgs_linear(\n-                module,\n-                quantization_config=quantization_config,\n-                current_key_name=current_key_name,\n-                has_been_replaced=has_been_replaced,\n-                modules_to_not_convert=modules_to_not_convert,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                new_module = HiggsLinear(\n+                    module.in_features,\n+                    module.out_features,\n+                    bias=module.bias is not None,\n+                    num_bits=quantization_config.bits,\n+                    hadamard_size=quantization_config.hadamard_size,\n+                    group_size=quantization_config.group_size,\n+                )\n+                new_module.source_cls = type(module)\n+                new_module.requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n+\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n+    return model\n \n \n def dequantize_higgs(model, current_key_name=None):"
        },
        {
            "sha": "e4f5b16c934843c2a2c45490db53f5e4a454ac44",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -594,7 +594,19 @@ def swizzle_mxfp4_convertops(blocks, scales, module, proj, target_device, triton\n     )\n \n \n-def replace_with_mxfp4_linear(model, modules_to_not_convert=None, quantization_config=None):\n+def replace_with_mxfp4_linear(model, quantization_config=None, modules_to_not_convert: list[str] | None = None):\n+    \"\"\"\n+    Public method that replaces the expert layers of the given model with mxfp4 quantized layers.\n+\n+    Args:\n+        model (`torch.nn.Module`):\n+            The model to convert, can be any `torch.nn.Module` instance.\n+        quantization_config (`Mxfp4Config`, defaults to `None`):\n+            The quantization config object that contains the quantization parameters.\n+        modules_to_not_convert (`list`, *optional*, defaults to `None`):\n+            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\n+            converted.\n+    \"\"\"\n     if quantization_config.dequantize:\n         return model\n "
        },
        {
            "sha": "753397448a611ef00bec4df9164bdf9d6b7e45ef",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -58,7 +58,7 @@ def convert(\n def replace_with_quanto_layers(\n     model,\n     quantization_config=None,\n-    modules_to_not_convert=None,\n+    modules_to_not_convert: list[str] | None = None,\n ):\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with Quanto quantized layers."
        },
        {
            "sha": "e21efa2625b87c1e99c43493ea88e3dd618c7994",
            "filename": "src/transformers/integrations/spqr.py",
            "status": "modified",
            "additions": 44,
            "deletions": 90,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fspqr.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -13,110 +13,64 @@\n # limitations under the License.\n \"SpQR (Sparse-Quantized Representation) integration file\"\n \n-from ..utils import is_accelerate_available, is_spqr_available, is_torch_available\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_spqr_available, is_torch_available, logging\n \n \n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n if is_torch_available():\n     import torch.nn as nn\n \n+logger = logging.get_logger(__name__)\n+\n \n-def replace_with_spqr_linear(\n-    model,\n-    quantization_config=None,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n-):\n+def replace_with_spqr_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n     \"\"\"\n-    Public method that recursively replaces the Linear layers of the given model with SpQR quantized layers.\n-    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successful or not.\n+    Public method that replaces the Linear layers of the given model with SPQR quantized layers.\n \n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n-        quantization_config (`SpQRConfig`):\n-            The quantization config object that contains the quantization parameters.\n-        modules_to_not_convert (`list[str]`, *optional*):\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n             A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n             converted.\n-        current_key_name (`list`, *optional*):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n+        quantization_config (`SpQRConfig`):\n+            The quantization config object that contains the quantization parameters.\n     \"\"\"\n-    if modules_to_not_convert is None:\n-        modules_to_not_convert = []\n-\n-    if is_accelerate_available():\n-        from accelerate import init_empty_weights\n     if is_spqr_available():\n         from spqr_quant import QuantizedLinear\n \n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-\n-        if isinstance(module, nn.Linear):\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            if \".\".join(current_key_name) + \".weight\" not in modules_to_not_convert:\n-                with init_empty_weights():\n-                    tensor_name = \".\".join(current_key_name)\n-\n-                    shapes = quantization_config.shapes\n-                    shapes_keys = shapes.keys()\n-\n-                    shapes_valid = (\n-                        f\"{tensor_name}.dense_weights.shape\" in shapes_keys\n-                        and f\"{tensor_name}.row_offsets.shape\" in shapes_keys\n-                        and f\"{tensor_name}.col_vals.shape\" in shapes_keys\n-                        and f\"{tensor_name}.in_perm.shape\" in shapes_keys\n-                    )\n-\n-                    if not shapes_valid:\n-                        raise ValueError(\n-                            f\"The SpQR quantization config does not contain the shape \"\n-                            f\"configuration for {tensor_name}. This indicates that the \"\n-                            f\"configuration is either invalid or corrupted.\"\n-                        )\n-\n-                    dense_weights_shape = shapes[f\"{tensor_name}.dense_weights.shape\"]\n-                    row_offsets_shape = shapes[f\"{tensor_name}.row_offsets.shape\"]\n-                    col_vals_shape = shapes[f\"{tensor_name}.col_vals.shape\"]\n-                    in_perm_shape = shapes[f\"{tensor_name}.in_perm.shape\"]\n-\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-\n-                    model._modules[name] = QuantizedLinear.create_placehodler(\n-                        rows=out_features,\n-                        cols=in_features,\n-                        bits=quantization_config.bits,\n-                        beta1=quantization_config.beta1,\n-                        beta2=quantization_config.beta2,\n-                        dense_weights_shape=dense_weights_shape,\n-                        row_offsets_shape=row_offsets_shape,\n-                        col_vals_shape=col_vals_shape,\n-                        in_perm_shape=in_perm_shape,\n-                    )\n-                    has_been_replaced = True\n-\n-                    # Store the module class in case we need to transpose the weight later\n-                    model._modules[name].source_cls = type(module)\n-                    # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-            else:\n-                pass\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_spqr_linear(\n-                module,\n-                quantization_config=quantization_config,\n-                modules_to_not_convert=modules_to_not_convert,\n-                current_key_name=current_key_name,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    has_been_replaced = False\n+    # we need this to correctly materialize the weights during quantization\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                shapes = quantization_config.shapes\n+\n+                new_module = QuantizedLinear.create_placehodler(\n+                    rows=module.out_features,\n+                    cols=module.in_features,\n+                    bits=quantization_config.bits,\n+                    beta1=quantization_config.beta1,\n+                    beta2=quantization_config.beta2,\n+                    dense_weights_shape=shapes[f\"{module_name}.dense_weights.shape\"],\n+                    row_offsets_shape=shapes[f\"{module_name}.row_offsets.shape\"],\n+                    col_vals_shape=shapes[f\"{module_name}.col_vals.shape\"],\n+                    in_perm_shape=shapes[f\"{module_name}.in_perm.shape\"],\n+                )\n+                # Force requires grad to False to avoid unexpected errors\n+                model._modules[module_name].requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n+                has_been_replaced = True\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n+\n+    return model"
        },
        {
            "sha": "546ad7b3ce858f1d8e266132dc1ec542572912a7",
            "filename": "src/transformers/integrations/vptq.py",
            "status": "modified",
            "additions": 42,
            "deletions": 59,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fvptq.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -13,64 +13,49 @@\n # limitations under the License.\n \"VPTQ (Vector Post-Training Quantization) integration file\"\n \n-import torch.nn as nn\n-from accelerate import init_empty_weights\n-from vptq import VQuantLinear\n+from ..quantizers.quantizers_utils import should_convert_module\n+from ..utils import is_accelerate_available, is_torch_available, logging\n \n \n-def replace_with_vptq_linear(\n-    model,\n-    quantization_config=None,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    has_been_replaced=False,\n-):\n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n+if is_torch_available():\n+    import torch.nn as nn\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def replace_with_vptq_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n     \"\"\"\n-    Public method that recursively replaces the Linear layers of the given model with VPTQ quantized layers.\n-    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successful or not.\n+    Public method that replaces the Linear layers of the given model with SPQR quantized layers.\n \n     Args:\n         model (`torch.nn.Module`):\n             The model to convert, can be any `torch.nn.Module` instance.\n+        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n+            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n+            converted.\n         quantization_config (`VptqConfig`):\n             The quantization config object that contains the quantization parameters.\n-        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n-            Names of the modules to not convert in `VQuantLinear`. In practice we keep the `lm_head` in full precision\n-            for numerical stability reasons.\n-        current_key_name (`list`, *optional*):\n-            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n-        has_been_replaced (`bool`, *optional*):\n-            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n-            should not be passed by the user.\n     \"\"\"\n+    from vptq import VQuantLinear\n \n-    modules_to_not_convert = modules_to_not_convert if modules_to_not_convert else [\"lm_head\"]\n-\n-    for name, module in model.named_children():\n-        if current_key_name is None:\n-            current_key_name = []\n-        current_key_name.append(name)\n-        layer_name = \".\".join(current_key_name)\n-        shared_layer_config = quantization_config.shared_layer_config\n-        config_for_layers = quantization_config.config_for_layers\n-\n-        if (\n-            isinstance(module, nn.Linear)\n-            and layer_name not in modules_to_not_convert\n-            and ((layer_name in config_for_layers) or (current_key_name[-1] in shared_layer_config))\n-        ):\n-            layer_params = config_for_layers.get(layer_name, None) or shared_layer_config.get(\n-                current_key_name[-1], None\n-            )\n+    has_been_replaced = False\n+    shared_layer_config = quantization_config.shared_layer_config\n+    config_for_layers = quantization_config.config_for_layers\n \n-            with init_empty_weights():\n-                in_features = module.in_features\n-                out_features = module.out_features\n-\n-                model._modules[name] = VQuantLinear(\n-                    in_features,\n-                    out_features,\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+        with init_empty_weights():\n+            if isinstance(module, nn.Linear):\n+                layer_params = config_for_layers.get(module_name, None) or shared_layer_config.get(\n+                    module_name.rsplit(\".\")[1], None\n+                )\n+                new_module = VQuantLinear(\n+                    module.in_features,\n+                    module.out_features,\n                     vector_lens=layer_params[\"vector_lens\"],\n                     num_centroids=layer_params[\"num_centroids\"],\n                     num_res_centroids=layer_params[\"num_res_centroids\"],\n@@ -84,18 +69,16 @@ def replace_with_vptq_linear(\n                     enable_proxy_error=False,\n                     bias=module.bias is not None,\n                 )\n+                # Force requires grad to False to avoid unexpected errors\n+                model._modules[module_name].requires_grad_(False)\n+                model.set_submodule(module_name, new_module)\n                 has_been_replaced = True\n \n-                # Force requires grad to False to avoid unexpected errors\n-                model._modules[name].requires_grad_(False)\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = replace_with_vptq_linear(\n-                module,\n-                quantization_config=quantization_config,\n-                modules_to_not_convert=modules_to_not_convert,\n-                current_key_name=current_key_name,\n-                has_been_replaced=has_been_replaced,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n+\n+    return model"
        },
        {
            "sha": "eb66b25998984227ce5a6bd0bd2aa8a99ded7c15",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -195,9 +195,7 @@ def is_trainable(self) -> bool:\n     def _dequantize(self, model):\n         from ..integrations import dequantize_and_replace\n \n-        model = dequantize_and_replace(\n-            model, self.modules_to_not_convert, quantization_config=self.quantization_config\n-        )\n+        model = dequantize_and_replace(model, quantization_config=self.quantization_config)\n         return model\n \n     def get_quantize_ops(self):"
        },
        {
            "sha": "6a07e67f786a1d22924b5dda443b6c3757516f82",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85ced0f92b6fa296d379f95ae527d7b5063eb5c1/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=85ced0f92b6fa296d379f95ae527d7b5063eb5c1",
            "patch": "@@ -164,9 +164,7 @@ def is_trainable(self) -> bool:\n     def _dequantize(self, model):\n         from ..integrations import dequantize_and_replace\n \n-        model = dequantize_and_replace(\n-            model, self.modules_to_not_convert, quantization_config=self.quantization_config\n-        )\n+        model = dequantize_and_replace(model, quantization_config=self.quantization_config)\n         return model\n \n     def get_quantize_ops(self):"
        }
    ],
    "stats": {
        "total": 930,
        "additions": 326,
        "deletions": 604
    }
}