{
    "author": "yao-matrix",
    "message": "fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8 (#39069)\n\n* fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* qwen3\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* quanto\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* models\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* idefics2\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
    "files": [
        {
            "sha": "747963aa50e1e9e23a8cfbcad4ee8172360998b0",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -30,6 +30,7 @@\n )\n from transformers.models.idefics3 import Idefics3VisionConfig\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     require_bitsandbytes,\n     require_torch,\n@@ -483,23 +484,26 @@ def test_batched_generation(self):\n             device=model.device, dtype=model.dtype\n         )\n \n-        EXPECTED_OUTPUT = {\n-            \"cpu\": [\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a grassy hill. The alpaca has\",\n-            ],  # cpu output\n-            \"cuda\": [\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a patch of ground with some dry grass. The\",\n-            ],  # cuda output\n-            \"xpu\": [\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n-                \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a grassy hill. The alpaca has\",\n-            ],  # xpu output\n-        }\n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"cpu\", None): [\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a grassy hill. The alpaca has\",\n+                ],\n+                (\"cuda\", None): [\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a patch of ground with some dry grass. The\",\n+                ],\n+                (\"xpu\", 3): [\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n <image>\\n USER: What's the difference of two images?\\n ASSISTANT:<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The first image features a cute, light-colored puppy sitting on a paved surface with\",\n+                    \"<|im_start|>user\\n<fim_prefix><fim_suffix> <image>\\n USER: Describe the image.\\n ASSISTANT:<|im_end|>\\n <|im_start|>assistant\\n The image shows a young alpaca standing on a patch of ground with some dry grass. The\",\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n         generate_ids = model.generate(**inputs, max_new_tokens=20)\n         outputs = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-        self.assertListEqual(outputs, EXPECTED_OUTPUT[model.device.type])\n+        self.assertListEqual(outputs, EXPECTED_OUTPUT)\n \n     def test_tokenizer_integration(self):\n         model_id = \"rhymes-ai/Aria\""
        },
        {
            "sha": "5cde1f216ec9cef9ab790cf729b977c3b6ec7ef1",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -422,7 +422,7 @@ def test_small_model_integration_generate_text_only(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n                 # 4-bit\n                 (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n                 (\"cuda\", 8): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n@@ -434,6 +434,7 @@ def test_small_model_integration_generate_text_only(self):\n \n     @slow\n     @require_torch_accelerator\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_generate_chat_template(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         model = self.get_model()\n@@ -458,7 +459,7 @@ def test_small_model_integration_generate_chat_template(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,\",\n+                (\"xpu\", 3): 'The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,',\n                 # 4-bit\n                 (\"cuda\", 7): 'The image depicts two cats comfortably resting on a pink blanket spread across a sofa. The cats,',\n                 (\"cuda\", 8): 'The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,',"
        },
        {
            "sha": "d0796468c3995a858138615c83ceb33a01fc7951",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -823,6 +823,7 @@ def test_gpt2_sample(self):\n                 (\"rocm\", None): 'Today is a nice day and we can do this again.\"\\n\\nDana said that she will',\n                 (\"rocm\", (9, 5)): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n                 (\"cuda\", None): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n+                (\"xpu\", 3): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n             }\n         )  # fmt: skip\n         EXPECTED_OUTPUT = expected_outputs.get_expectation()"
        },
        {
            "sha": "6ce19ddfadeb1ea9a9672599fc13ce497c4df6f9",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -624,6 +624,7 @@ def test_integration_test_4bit(self):\n \n         expected_generated_texts = Expectations(\n             {\n+                (\"xpu\", 3): \"In this image, we see the Statue of Liberty, the Hudson River,\",\n                 (\"cuda\", None): \"In this image, we see the Statue of Liberty, the Hudson River,\",\n                 (\"rocm\", (9, 5)): \"In this image, we see the Statue of Liberty, the New York City\",\n             }"
        },
        {
            "sha": "f482f0a06803178137508c4a7bbf2612cab60c42",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -389,16 +389,15 @@ def test_small_model_integration_test(self):\n \n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): 'user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different model or method. The models are color-coded and labeled with their respective names. The axes are labeled with terms such as \"VQA,\" \"GQA,\" \"MQA,\" \"VQAv2,\" \"MM-Vet,\" \"LLaVA-Bench,\" \"LLaVA-1',\n                 (\"cuda\", 7): 'user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different model or method. The models are color-coded and labeled with their respective names. The axes are labeled with terms such as \"VQA,\" \"GQA,\" \"MQA,\" \"VQAv2,\" \"MM-Vet,\" \"LLaVA-Bench,\" \"LLaVA-1',\n                 (\"cuda\", 8): 'user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different model or method. The models are color-coded and labeled with their respective names. The axes are labeled with terms such as \"VQA,\" \"GQA,\" \"MQA,\" \"VIZ,\" \"TextVQA,\" \"SQA-IMG,\" and \"MQE.\" The radar chart shows',\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+        DECODED_TEXT = self.processor.decode(output[0], skip_special_tokens=True)\n \n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        self.assertEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "94ceb0e4a70b2b845d49921724a2cd579ccca015",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -194,6 +194,7 @@ def test_small_model_logits_batched(self):\n         # fmt: off\n         EXPECTED_LOGITS_LEFT_UNPADDED = Expectations(\n             {\n+                (\"xpu\", 3): torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2295, 0.6055], [0.2676, -0.7070, 0.2461]]).to(torch_device),\n                 (\"cuda\", 7): torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2275, 0.6054], [0.2656, -0.7070, 0.2460]]).to(torch_device),\n                 (\"cuda\", 8): torch.Tensor([[0.2207, 0.5234, -0.3828], [0.8203, -0.2285, 0.6055], [0.2656, -0.7109, 0.2451]]).to(torch_device),\n                 (\"rocm\", 9): torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2285, 0.6055], [0.2637, -0.7109, 0.2451]]).to(torch_device),\n@@ -203,6 +204,7 @@ def test_small_model_logits_batched(self):\n \n         EXPECTED_LOGITS_RIGHT_UNPADDED = Expectations(\n             {\n+                (\"xpu\", 3): torch.Tensor([[0.2178, 0.1270, -0.1641], [-0.3496, 0.2988, -1.0312], [0.0693, 0.7930, 0.8008]]).to(torch_device),\n                 (\"cuda\", 7): torch.Tensor([[0.2167, 0.1269, -0.1640], [-0.3496, 0.2988, -1.0312], [0.0688, 0.7929, 0.8007]]).to(torch_device),\n                 (\"cuda\", 8): torch.Tensor([[0.2178, 0.1270, -0.1621], [-0.3496, 0.3008, -1.0312], [0.0693, 0.7930, 0.7969]]).to(torch_device),\n                 (\"rocm\", 9): torch.Tensor([[0.2197, 0.1250, -0.1611], [-0.3516, 0.3008, -1.0312], [0.0684, 0.7930, 0.8008]]).to(torch_device),"
        },
        {
            "sha": "72669fd390f6e964a6b07f07b16d586e4efc6d1b",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -28,6 +28,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     require_flash_attn,\n     require_torch,\n@@ -482,15 +483,23 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n+        DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets'\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): [\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+                ],\n+                (\"cuda\", None): [\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+                    'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets',\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        self.assertEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_flash_attn"
        },
        {
            "sha": "5f961ac79e013568aea842886c9691f18acc5c38",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -207,6 +207,7 @@ def test_model_600m_long_prompt_sdpa(self):\n     def test_speculative_generation(self):\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n+                (\"xpu\", 3): \"My favourite condiment is 100% peanut butter. I love it so much that I can't help but use it\",\n                 (\"cuda\", 7): \"My favourite condiment is 100% natural. It's a little spicy and a little sweet, but it's the\",\n                 (\"cuda\", 8): \"My favourite condiment is 100% peanut butter. I love it so much that I can't help but use it\",\n             }"
        },
        {
            "sha": "a4e0b4786978dc3336794c6d1f93e13232f2c29a",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=0106a50a6bcf6eb0d4ef28dfda68e8becc3531e3",
            "patch": "@@ -223,7 +223,9 @@ def test_serialization_bin(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             with self.assertRaises(ValueError) as e:\n                 self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            self.assertIn(\"The model is quantized with quanto and is not serializable\", str(e.exception))\n+            self.assertIn(\n+                \"The model is quantized with QuantizationMethod.QUANTO and is not serializable\", str(e.exception)\n+            )\n             # TODO: replace by the following when it works\n             # quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n             #     tmpdirname, torch_dtype=torch.float32, device_map=\"cpu\"\n@@ -237,7 +239,9 @@ def test_serialization_safetensors(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             with self.assertRaises(ValueError) as e:\n                 self.quantized_model.save_pretrained(tmpdirname)\n-            self.assertIn(\"The model is quantized with quanto and is not serializable\", str(e.exception))\n+            self.assertIn(\n+                \"The model is quantized with QuantizationMethod.QUANTO and is not serializable\", str(e.exception)\n+            )\n             # quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n             #     tmpdirname, torch_dtype=torch.float32, device_map=\"cpu\"\n             # )"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 53,
        "deletions": 31
    }
}