{
    "author": "vasqu",
    "message": "[`CI`] Fix post merge ernie 4.5 (#39561)\n\nfix repo consistency",
    "sha": "049a674e681181c2616fec4124086ec0ec55ed2d",
    "files": [
        {
            "sha": "0d98583e1b448a1c314b129b6391f01c3917a5aa",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/049a674e681181c2616fec4124086ec0ec55ed2d/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049a674e681181c2616fec4124086ec0ec55ed2d/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=049a674e681181c2616fec4124086ec0ec55ed2d",
            "patch": "@@ -350,12 +350,6 @@ def __init__(self, config: Ernie4_5Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -432,18 +426,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "630162b7dc5c17f4e8f866d8442b91d9c3793390",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/049a674e681181c2616fec4124086ec0ec55ed2d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049a674e681181c2616fec4124086ec0ec55ed2d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=049a674e681181c2616fec4124086ec0ec55ed2d",
            "patch": "@@ -516,12 +516,6 @@ def __init__(self, config: Ernie4_5_MoEConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -687,18 +681,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 0,
        "deletions": 36
    }
}