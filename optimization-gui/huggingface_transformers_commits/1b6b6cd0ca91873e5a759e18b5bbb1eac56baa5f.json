{
    "author": "Cyrilvallez",
    "message": "Fix tied_weights for vlms (#42523)\n\n* one fix\n\n* attempt mistral3\n\n* empty dict\n\n* that was olmoe's problem\n\n* current CI status?\n\n* actual CI status\n\n* simplify\n\n* hmm?\n\n* bird\n\n* force tie word embeddings to false\n\n* specifics of FSMT\n\n* wrong reference?\n\n* finalize\n\n* fixup\n\n* weird mamba error\n\n* fix tied weights\n\n* hack musicgen\n\n* tie_encoder_decoder workaround\n\n* revert unwanted changes\n\n* hardcode llava onevision\n\n* more\n\n* revert\n\n* fix\n\n* modular\n\n* modular\n\n---------\n\nCo-authored-by: Pablo Montalvo <pablo.montalvo.leroux@gmail.com>\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
    "files": [
        {
            "sha": "a399f6d8f00d94959b2a991b389ddaccc69a0368",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
            "patch": "@@ -1069,7 +1069,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n @auto_docstring\n class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.embed_tokens.weight\"}\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     _keep_in_fp32_modules_strict = [\"codec_model\"]"
        },
        {
            "sha": "31abf2408d12234ce3e929ca566c74eab42c0150",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
            "patch": "@@ -251,6 +251,7 @@ def __init__(self, config):\n \n \n class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.embed_tokens.weight\"}\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n     output_modalities = (\"audio\", \"text\")\n "
        },
        {
            "sha": "32ea6e7a14afd33a029c9bf7548bcac23521111b",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
            "patch": "@@ -54,8 +54,6 @@ class LlavaNextVideoConfig(PreTrainedConfig):\n         image_grid_pinpoints (`List`, *optional*, defaults to `[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]`):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`.\n-        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-            Whether the model's input and output word embeddings should be tied.\n         video_token_index (`int`, *optional*, defaults to 32000):\n             The video token index to encode the image prompt.\n         spatial_pool_mode (`str`, *optional*, defaults to `\"average\"`):\n@@ -103,7 +101,6 @@ def __init__(\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-2,\n         image_grid_pinpoints=None,\n-        tie_word_embeddings=False,\n         video_token_index=32000,\n         spatial_pool_mode=\"average\",\n         spatial_pool_stride=2,\n@@ -160,7 +157,13 @@ def __init__(\n \n         self.text_config = text_config\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(**kwargs)\n+\n+        # Due to a mismatch at model addition-time, the `tie_word_embeddings` was saved in the text config, even\n+        # though it concerns the main model, while it was set to False by default in the main model... So we hardcode a fix here\n+        if not self.tie_word_embeddings and self.text_config.tie_word_embeddings:\n+            self.tie_word_embeddings = True\n+            self.text_config.tie_word_embeddings = False\n \n \n __all__ = [\"LlavaNextVideoConfig\"]"
        },
        {
            "sha": "61b4b5fdf9205260259524c2a73486a661a6704c",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
            "patch": "@@ -72,8 +72,6 @@ class LlavaNextVideoConfig(PreTrainedConfig):\n         image_grid_pinpoints (`List`, *optional*, defaults to `[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]`):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`.\n-        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-            Whether the model's input and output word embeddings should be tied.\n         video_token_index (`int`, *optional*, defaults to 32000):\n             The video token index to encode the image prompt.\n         spatial_pool_mode (`str`, *optional*, defaults to `\"average\"`):\n@@ -121,7 +119,6 @@ def __init__(\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-2,\n         image_grid_pinpoints=None,\n-        tie_word_embeddings=False,\n         video_token_index=32000,\n         spatial_pool_mode=\"average\",\n         spatial_pool_stride=2,\n@@ -178,7 +175,13 @@ def __init__(\n \n         self.text_config = text_config\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(**kwargs)\n+\n+        # Due to a mismatch at model addition-time, the `tie_word_embeddings` was saved in the text config, even\n+        # though it concerns the main model, while it was set to False by default in the main model... So we hardcode a fix here\n+        if not self.tie_word_embeddings and self.text_config.tie_word_embeddings:\n+            self.tie_word_embeddings = True\n+            self.text_config.tie_word_embeddings = False\n \n \n class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):"
        },
        {
            "sha": "b72c2235ed7ecb58da82ce78ace1bf4623116aee",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=1b6b6cd0ca91873e5a759e18b5bbb1eac56baa5f",
            "patch": "@@ -58,8 +58,6 @@ class LlavaOnevisionConfig(PreTrainedConfig):\n         image_grid_pinpoints (`List`, *optional*):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`.\n-        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-            Whether the model's input and output word embeddings should be tied.\n         multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use bias in the multimodal projector.\n \n@@ -102,7 +100,6 @@ def __init__(\n         vision_feature_layer=-1,\n         vision_aspect_ratio=\"anyres_max_9\",\n         image_grid_pinpoints=None,\n-        tie_word_embeddings=False,\n         multimodal_projector_bias=True,\n         **kwargs,\n     ):\n@@ -188,7 +185,13 @@ def __init__(\n \n         self.text_config = text_config\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(**kwargs)\n+\n+        # Due to a mismatch at model addition-time, the `tie_word_embeddings` was saved in the text config, even\n+        # though it concerns the main model, while it was set to False by default in the main model... So we hardcode a fix here\n+        if not self.tie_word_embeddings and self.text_config.tie_word_embeddings:\n+            self.tie_word_embeddings = True\n+            self.text_config.tie_word_embeddings = False\n \n \n __all__ = [\"LlavaOnevisionConfig\"]"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 23,
        "deletions": 13
    }
}