{
    "author": "mahdibaghbanzadeh",
    "message": "Enhance DataCollatorForLanguageModeling with Configurable Token Replacement Probabilities (#35251)\n\n* DataCollatorForLanguageModeling class was updated with new parameters that provides more control over the token masking and relacing\r\n\r\n* DataCollatorForLanguageModeling class was updated with new parameters that provides more control over the token masking and relacing\r\n\r\n* Addressed review comments, modified the docstring and made a test for the DataCollatorForLanguageModeling",
    "sha": "c61fcde910c34c983ec2c3da2ed41bba3601313f",
    "files": [
        {
            "sha": "01a5896a2907e615c4f8bf7e65f2abd886c45b2d",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 93,
            "deletions": 20,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/c61fcde910c34c983ec2c3da2ed41bba3601313f/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c61fcde910c34c983ec2c3da2ed41bba3601313f/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=c61fcde910c34c983ec2c3da2ed41bba3601313f",
            "patch": "@@ -691,11 +691,17 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n             tokens and the value to predict for the masked token.\n         mlm_probability (`float`, *optional*, defaults to 0.15):\n             The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n+        mask_replace_prob (`float`, *optional*, defaults to 0.8):\n+            The probability with which masked tokens are replaced by the tokenizer's mask token (e.g., `[MASK]`).\n+            Defaults to 0.8, meaning 80% of the masked tokens will be replaced with `[MASK]`.\n+            Only works when `mlm` is set to `True`.\n+        random_replace_prob (`float`, *optional*, defaults to 0.1):\n+            The probability with which masked tokens are replaced by random tokens from the tokenizer's vocabulary.\n+            Defaults to 0.1, meaning 10% of the masked tokens will be replaced with random tokens. The remaining\n+            masked tokens (1 - mask_replace_prob - random_replace_prob) are left unchanged.\n+            Only works when `mlm` is set to `True`.\n         pad_to_multiple_of (`int`, *optional*):\n-            If set will pad the sequence to a multiple of the provided value.\n-\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.0 (Volta).\n+            If set, will pad the sequence to a multiple of the provided value.\n         return_tensors (`str`):\n             The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n \n@@ -705,11 +711,36 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n     BatchEncoding, with the `\"special_tokens_mask\"` key, as returned by a [`PreTrainedTokenizer`] or a\n     [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.\n \n-    </Tip>\"\"\"\n+    <Example Options and Expectations>\n+\n+    1. Default Behavior:\n+        - `mask_replace_prob=0.8`, `random_replace_prob=0.1`.\n+        - Expect 80% of masked tokens replaced with `[MASK]`, 10% replaced with random tokens, and 10% left unchanged.\n+\n+    2. All masked tokens replaced by `[MASK]`:\n+        - `mask_replace_prob=1.0`, `random_replace_prob=0.0`.\n+        - Expect all masked tokens to be replaced with `[MASK]`. No tokens are left unchanged or replaced with random tokens.\n+\n+    3. No `[MASK]` replacement, only random tokens:\n+        - `mask_replace_prob=0.0`, `random_replace_prob=1.0`.\n+        - Expect all masked tokens to be replaced with random tokens. No `[MASK]` replacements or unchanged tokens.\n+\n+    4. Balanced replacement:\n+        - `mask_replace_prob=0.5`, `random_replace_prob=0.4`.\n+        - Expect 50% of masked tokens replaced with `[MASK]`, 40% replaced with random tokens, and 10% left unchanged.\n+\n+    Note:\n+        The sum of `mask_replace_prob` and `random_replace_prob` must not exceed 1. If their sum is less than 1, the\n+        remaining proportion will consist of masked tokens left unchanged.\n+\n+    </Tip>\n+    \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n     mlm: bool = True\n     mlm_probability: float = 0.15\n+    mask_replace_prob: float = 0.8\n+    random_replace_prob: float = 0.1\n     pad_to_multiple_of: Optional[int] = None\n     tf_experimental_compile: bool = False\n     return_tensors: str = \"pt\"\n@@ -720,6 +751,15 @@ def __post_init__(self):\n                 \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n                 \"You should pass `mlm=False` to train on causal language modeling instead.\"\n             )\n+        if self.mlm_probability < 0 or self.mlm_probability > 1:\n+            raise ValueError(\"mlm_probability should be between 0 and 1.\")\n+        if self.mask_replace_prob + self.random_replace_prob > 1:\n+            raise ValueError(\"The sum of mask_replace_prob and random_replace_prob should not exceed 1\")\n+        if self.mask_replace_prob < 0 or self.mask_replace_prob > 1:\n+            raise ValueError(\"mask_replace_prob should be between 0 and 1.\")\n+        if self.random_replace_prob < 0 or self.random_replace_prob > 1:\n+            raise ValueError(\"random_replace_prob should be between 0 and 1.\")\n+\n         if self.tf_experimental_compile:\n             import tensorflow as tf\n \n@@ -749,18 +789,28 @@ def tf_mask_tokens(\n         # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n         labels = tf.where(masked_indices, inputs, -100)\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n+        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob) & masked_indices\n \n         inputs = tf.where(indices_replaced, mask_token_id, inputs)\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+        # random_replace_prob% of the time, we replace masked input tokens with random word\n+        indices_random = (\n+            self.tf_bernoulli(input_shape, random_replace_prob_scaled) & masked_indices & ~indices_replaced\n+        )\n         random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n \n         inputs = tf.where(indices_random, random_words, inputs)\n \n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n+        # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n     def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n@@ -849,16 +899,29 @@ def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n         masked_indices = torch.bernoulli(probability_matrix).bool()\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n+        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        indices_replaced = torch.bernoulli(torch.full(labels.shape, self.mask_replace_prob)).bool() & masked_indices\n         inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+\n+        # random_replace_prob% of the time, we replace masked input tokens with random word\n+        indices_random = (\n+            torch.bernoulli(torch.full(labels.shape, random_replace_prob_scaled)).bool()\n+            & masked_indices\n+            & ~indices_replaced\n+        )\n         random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n         inputs[indices_random] = random_words[indices_random]\n \n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n+        # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n     def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n@@ -905,14 +968,24 @@ def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n         masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n+        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        indices_replaced = (\n+            np.random.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n+        )\n         inputs[indices_replaced] = self.tokenizer.mask_token_id\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n         indices_random = (\n-            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n+            np.random.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n+            & masked_indices\n+            & ~indices_replaced\n         )\n         random_words = np.random.randint(\n             low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64"
        },
        {
            "sha": "c3e9b5a3badf210ecd417fa78d2369104cb3e378",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c61fcde910c34c983ec2c3da2ed41bba3601313f/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c61fcde910c34c983ec2c3da2ed41bba3601313f/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=c61fcde910c34c983ec2c3da2ed41bba3601313f",
            "patch": "@@ -1020,6 +1020,52 @@ def _test_no_pad_and_pad(self, no_pad_features, pad_features):\n         self.assertTrue(tf.reduce_any(masked_tokens))\n         # self.assertTrue(all(x == -100 for x in batch[\"labels\"].numpy()[~masked_tokens.numpy()].tolist()))\n \n+    def test_probability_sum_error(self):\n+        \"\"\"Test that the sum of mask_replace_prob and random_replace_prob exceeding 1 raises an error.\"\"\"\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        with self.assertRaises(ValueError):\n+            DataCollatorForLanguageModeling(tokenizer=tokenizer, mask_replace_prob=0.9, random_replace_prob=0.2)\n+\n+    def test_all_mask_replacement(self):\n+        \"\"\"Test behavior when mask_replace_prob=1.\"\"\"\n+        tokenizer = BertTokenizer(self.vocab_file)\n+\n+        # pytorch call\n+        collator = DataCollatorForLanguageModeling(\n+            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"pt\"\n+        )\n+\n+        inputs = torch.tensor([0, 1, 2, 3, 4, 5])\n+        features = [{\"input_ids\": inputs} for _ in range(8)]\n+        batch = collator(features)\n+\n+        # confirm that every token is either the original token or [MASK]\n+        self.assertTrue(torch.all((batch[\"input_ids\"] == inputs) | (batch[\"input_ids\"] == tokenizer.mask_token_id)))\n+\n+        # tf call\n+        collator = DataCollatorForLanguageModeling(\n+            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"tf\"\n+        )\n+        inputs = tf.constant([0, 1, 2, 3, 4, 5])\n+        features = [{\"input_ids\": inputs} for _ in range(8)]\n+        batch = collator(features)\n+\n+        # confirm that every token is either the original token or [MASK]\n+        self.assertTrue(\n+            tf.reduce_all((batch[\"input_ids\"] == inputs) | (batch[\"input_ids\"] == tokenizer.mask_token_id))\n+        )\n+\n+        # numpy call\n+        collator = DataCollatorForLanguageModeling(\n+            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"np\"\n+        )\n+        inputs = np.array([0, 1, 2, 3, 4, 5])\n+        features = [{\"input_ids\": inputs} for _ in range(8)]\n+        batch = collator(features)\n+\n+        # confirm that every token is either the original token or [MASK]\n+        self.assertTrue(np.all((batch[\"input_ids\"] == inputs) | (batch[\"input_ids\"] == tokenizer.mask_token_id)))\n+\n     def test_data_collator_for_language_modeling(self):\n         no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n         pad_features = [{\"input_ids\": list(range(5))}, {\"input_ids\": list(range(10))}]"
        }
    ],
    "stats": {
        "total": 159,
        "additions": 139,
        "deletions": 20
    }
}