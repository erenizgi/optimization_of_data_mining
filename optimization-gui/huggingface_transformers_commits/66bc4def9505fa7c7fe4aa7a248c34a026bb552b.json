{
    "author": "nbroad1881",
    "message": "add sdpa mbart (#32033)\n\n* add sdpa mbart\r\n\r\nuseful for donut\r\n\r\n* update sdpa docs\r\n\r\n* formatting\r\n\r\n* add self._use_sdpa in mbartencoder\r\n\r\n* use self.config to check attn\r\n\r\n* retrigger checks\r\n\r\n* [run-slow] mbart",
    "sha": "66bc4def9505fa7c7fe4aa7a248c34a026bb552b",
    "files": [
        {
            "sha": "dd3433f2cd486256e466a95d42f06a8ec94b0c2f",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66bc4def9505fa7c7fe4aa7a248c34a026bb552b/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66bc4def9505fa7c7fe4aa7a248c34a026bb552b/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=66bc4def9505fa7c7fe4aa7a248c34a026bb552b",
            "patch": "@@ -239,6 +239,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)\n * [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n * [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n+* [mBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n * [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)"
        },
        {
            "sha": "9455f21b2073ffc28f653064ef82f623e1dcac63",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 144,
            "deletions": 7,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/66bc4def9505fa7c7fe4aa7a248c34a026bb552b/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66bc4def9505fa7c7fe4aa7a248c34a026bb552b/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=66bc4def9505fa7c7fe4aa7a248c34a026bb552b",
            "patch": "@@ -24,7 +24,12 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -405,8 +410,116 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n+# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->MBart\n+class MBartSdpaAttention(MBartAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        if output_attentions or layer_head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"MBartModel is using MBartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                key_value_states=key_value_states,\n+                past_key_value=past_key_value,\n+                attention_mask=attention_mask,\n+                layer_head_mask=layer_head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states)\n+        # get key, value proj\n+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # the provided `key_value_states` to support prefix tuning\n+        if (\n+            is_cross_attention\n+            and past_key_value is not None\n+            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+        ):\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        else:\n+            # self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        query_states = self._shape(query_states, tgt_len, bsz)\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n+        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+\n+        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n+        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+\n+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n+        # partitioned across GPUs when using tensor-parallelism.\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n MBART_ATTENTION_CLASSES = {\n     \"eager\": MBartAttention,\n+    \"sdpa\": MBartSdpaAttention,\n     \"flash_attention_2\": MBartFlashAttention2,\n }\n \n@@ -632,6 +745,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -841,7 +955,7 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n             embed_dim,\n         )\n         self.layers = nn.ModuleList([MBartEncoderLayer(config) for _ in range(config.encoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self.config = config\n         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n@@ -929,9 +1043,13 @@ def forward(\n \n         # expand attention_mask\n         if attention_mask is not None:\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            if self._use_flash_attention_2:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n                 attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\" and head_mask is None and not output_attentions:\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n@@ -1021,7 +1139,8 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n             config.d_model,\n         )\n         self.layers = nn.ModuleList([MBartDecoderLayer(config) for _ in range(config.decoder_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self.config = config\n+\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n@@ -1141,9 +1260,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if self._use_flash_attention_2:\n+        if self.config._attn_implementation == \"flash_attention_2\":\n             # 2d mask is passed through the layers\n             attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self.config._attn_implementation == \"sdpa\" and not output_attentions and cross_attn_head_mask is None:\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n         else:\n             # 4d mask is passed through the layers\n             attention_mask = _prepare_4d_causal_attention_mask(\n@@ -1152,8 +1280,17 @@ def forward(\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self._use_flash_attention_2:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\" and cross_attn_head_mask is None and not output_attentions:\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask("
        }
    ],
    "stats": {
        "total": 152,
        "additions": 145,
        "deletions": 7
    }
}