{
    "author": "yonigozlan",
    "message": "uniformize git processor (#33668)\n\n* uniformize git processor\r\n\r\n* update doctring",
    "sha": "75b7485cc72bf7122094b943af9f7d26d69ff827",
    "files": [
        {
            "sha": "2d90b82069fd38f531a62b1a087d52a830b2b816",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75b7485cc72bf7122094b943af9f7d26d69ff827/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75b7485cc72bf7122094b943af9f7d26d69ff827/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=75b7485cc72bf7122094b943af9f7d26d69ff827",
            "patch": "@@ -1191,7 +1191,7 @@ def forward(\n \n         >>> text = \"this is an image of two cats\"\n \n-        >>> inputs = processor(text, images=image, return_tensors=\"pt\")\n+        >>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n \n         >>> outputs = model(**inputs)\n         >>> last_hidden_state = outputs.last_hidden_state"
        },
        {
            "sha": "3744d81a0aca812ad8a86cb97dddc55e90823e78",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 38,
            "deletions": 26,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/75b7485cc72bf7122094b943af9f7d26d69ff827/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75b7485cc72bf7122094b943af9f7d26d69ff827/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=75b7485cc72bf7122094b943af9f7d26d69ff827",
            "patch": "@@ -16,8 +16,16 @@\n Image/Text processor class for GIT\n \"\"\"\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from typing import List, Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class GitProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n \n \n class GitProcessor(ProcessorMixin):\n@@ -42,7 +50,14 @@ def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[GitProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n@@ -51,13 +66,13 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`, *optional*):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n \n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n@@ -68,37 +83,34 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n                 - `'jax'`: Return JAX `jnp.ndarray` objects.\n \n         Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n               `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n-        tokenizer_kwargs, image_processor_kwargs = {}, {}\n-        if kwargs:\n-            tokenizer_kwargs = {k: v for k, v in kwargs.items() if k not in self.image_processor._valid_processor_keys}\n-            image_processor_kwargs = {\n-                k: v for k, v in kwargs.items() if k in self.image_processor._valid_processor_keys\n-            }\n-\n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n \n-        if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **tokenizer_kwargs)\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            GitProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n \n+        data = {}\n+        if text is not None:\n+            text_features = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+            data.update(text_features)\n         if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **image_processor_kwargs)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            data.update(image_features)\n+        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"))\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 66,
        "additions": 39,
        "deletions": 27
    }
}