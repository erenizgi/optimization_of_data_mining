{
    "author": "cyyever",
    "message": "Enable ruff on benchmark and scripts (#40634)\n\n* Enable ruff on benchmark and scripts\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Cover benchmark_v2\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* correct\n\n* style\n\n* style\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
    "files": [
        {
            "sha": "58994409a06bd09e31da73e51ffb0c8387fab4e3",
            "filename": "Makefile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -3,7 +3,7 @@\n # make sure to test the local checkout in scripts and not the pre-installed one (don't use quotes!)\n export PYTHONPATH = src\n \n-check_dirs := examples tests src utils\n+check_dirs := examples tests src utils scripts benchmark benchmark_v2\n \n exclude_folders :=  \"\"\n "
        },
        {
            "sha": "c5ecb17ebbb1b7411eefa46ba027d2c8002fd0f2",
            "filename": "benchmark/benches/llama.py",
            "status": "modified",
            "additions": 20,
            "deletions": 11,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenches%2Fllama.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -11,25 +11,28 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from logging import Logger\n import os\n+import sys\n+from logging import Logger\n from threading import Event, Thread\n from time import perf_counter, sleep\n from typing import Optional\n-import sys\n+\n \n # Add the parent directory to Python path to import benchmarks_entrypoint\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n-from benchmarks_entrypoint import MetricsRecorder\n-\n import gpustat\n import psutil\n import psycopg2\n+from benchmarks_entrypoint import MetricsRecorder\n+\n \n # Optional heavy ML dependencies - only required when actually running the benchmark\n try:\n     import torch\n+\n     from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, StaticCache\n+\n     TRANSFORMERS_AVAILABLE = True\n except ImportError:\n     TRANSFORMERS_AVAILABLE = False\n@@ -63,19 +66,25 @@ def collect_metrics(benchmark_id, continue_metric_collection, metrics_recorder):\n \n \n def run_benchmark(\n-    logger: Logger, repository: str, branch: str, commit_id: str, commit_msg: str, metrics_recorder=None, num_tokens_to_generate=100\n+    logger: Logger,\n+    repository: str,\n+    branch: str,\n+    commit_id: str,\n+    commit_msg: str,\n+    metrics_recorder=None,\n+    num_tokens_to_generate=100,\n ):\n     # Check if required ML dependencies are available\n     if not TRANSFORMERS_AVAILABLE:\n         logger.error(\"Transformers and torch are required to run the LLaMA benchmark. Please install them with:\")\n         logger.error(\"pip install torch transformers\")\n         logger.error(\"Skipping LLaMA benchmark due to missing dependencies.\")\n         return\n-    \n+\n     continue_metric_collection = Event()\n     metrics_thread = None\n     model_id = \"meta-llama/Llama-2-7b-hf\"\n-    \n+\n     # If no metrics_recorder is provided, create one for backward compatibility\n     if metrics_recorder is None:\n         try:\n@@ -154,7 +163,7 @@ def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n         # First eager forward pass\n         logger.info(\"running first eager forward pass\")\n         start = perf_counter()\n-        outputs = model(**inputs)\n+        _ = model(**inputs)\n         torch.cuda.synchronize()\n         end = perf_counter()\n         first_eager_fwd_pass_time = end - start\n@@ -163,7 +172,7 @@ def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n         # Second eager forward pass (should be faster)\n         logger.info(\"running second eager forward pass\")\n         start = perf_counter()\n-        outputs = model(**inputs)\n+        _ = model(**inputs)\n         torch.cuda.synchronize()\n         end = perf_counter()\n         second_eager_fwd_pass_time = end - start\n@@ -339,7 +348,7 @@ def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n     continue_metric_collection.set()\n     if metrics_thread is not None:\n         metrics_thread.join()\n-    \n+\n     # Only close the recorder if we created it locally\n     if should_close_recorder:\n-        metrics_recorder.close() \n\\ No newline at end of file\n+        metrics_recorder.close()"
        },
        {
            "sha": "cef5b3138cf9d3c51c4d0ed2a998b44d3b28a86d",
            "filename": "benchmark/benchmark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmark.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -31,9 +31,7 @@\n from pathlib import Path\n \n from git import Repo\n-\n from huggingface_hub import HfApi\n-\n from optimum_benchmark import Benchmark\n from optimum_benchmark_wrapper import main\n "
        },
        {
            "sha": "8c581e9d75e8e968b2399d9e1f38d5256afce1b8",
            "filename": "benchmark/benchmarks_entrypoint.py",
            "status": "modified",
            "additions": 139,
            "deletions": 106,
            "changes": 245,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenchmarks_entrypoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Fbenchmarks_entrypoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmarks_entrypoint.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -13,19 +13,20 @@\n # limitations under the License.\n import argparse\n import importlib.util\n+import json\n import logging\n import os\n import sys\n-import json\n import uuid\n from datetime import datetime\n-from typing import Dict, Tuple, Optional, List\n \n import pandas as pd\n \n+\n try:\n     from psycopg2.extensions import register_adapter\n     from psycopg2.extras import Json\n+\n     register_adapter(dict, Json)\n     PSYCOPG2_AVAILABLE = True\n except ImportError:\n@@ -38,8 +39,14 @@ class ImportModuleException(Exception):\n \n class MetricsRecorder:\n     def __init__(\n-        self, connection, logger: logging.Logger, repository: str, branch: str, commit_id: str, commit_msg: str, \n-        collect_csv_data: bool = True\n+        self,\n+        connection,\n+        logger: logging.Logger,\n+        repository: str,\n+        branch: str,\n+        commit_id: str,\n+        commit_msg: str,\n+        collect_csv_data: bool = True,\n     ):\n         self.conn = connection\n         self.use_database = connection is not None\n@@ -51,27 +58,43 @@ def __init__(\n         self.commit_id = commit_id\n         self.commit_msg = commit_msg\n         self.collect_csv_data = collect_csv_data\n-        \n+\n         # For CSV export - store all data in pandas DataFrames (only if CSV collection is enabled)\n         if self.collect_csv_data:\n             # Initialize empty DataFrames with proper schemas\n-            self.benchmarks_df = pd.DataFrame(columns=[\n-                'benchmark_id', 'repository', 'branch', 'commit_id', 'commit_message', \n-                'metadata', 'created_at'\n-            ])\n-            self.device_measurements_df = pd.DataFrame(columns=[\n-                'benchmark_id', 'cpu_util', 'mem_megabytes', 'gpu_util', \n-                'gpu_mem_megabytes', 'time'\n-            ])\n-            self.model_measurements_df = pd.DataFrame(columns=[\n-                'benchmark_id', 'time', 'model_load_time', 'first_eager_forward_pass_time_secs',\n-                'second_eager_forward_pass_time_secs', 'first_eager_generate_time_secs',\n-                'second_eager_generate_time_secs', 'time_to_first_token_secs',\n-                'time_to_second_token_secs', 'time_to_third_token_secs',\n-                'time_to_next_token_mean_secs', 'first_compile_generate_time_secs',\n-                'second_compile_generate_time_secs', 'third_compile_generate_time_secs',\n-                'fourth_compile_generate_time_secs'\n-            ])\n+            self.benchmarks_df = pd.DataFrame(\n+                columns=[\n+                    \"benchmark_id\",\n+                    \"repository\",\n+                    \"branch\",\n+                    \"commit_id\",\n+                    \"commit_message\",\n+                    \"metadata\",\n+                    \"created_at\",\n+                ]\n+            )\n+            self.device_measurements_df = pd.DataFrame(\n+                columns=[\"benchmark_id\", \"cpu_util\", \"mem_megabytes\", \"gpu_util\", \"gpu_mem_megabytes\", \"time\"]\n+            )\n+            self.model_measurements_df = pd.DataFrame(\n+                columns=[\n+                    \"benchmark_id\",\n+                    \"time\",\n+                    \"model_load_time\",\n+                    \"first_eager_forward_pass_time_secs\",\n+                    \"second_eager_forward_pass_time_secs\",\n+                    \"first_eager_generate_time_secs\",\n+                    \"second_eager_generate_time_secs\",\n+                    \"time_to_first_token_secs\",\n+                    \"time_to_second_token_secs\",\n+                    \"time_to_third_token_secs\",\n+                    \"time_to_next_token_mean_secs\",\n+                    \"first_compile_generate_time_secs\",\n+                    \"second_compile_generate_time_secs\",\n+                    \"third_compile_generate_time_secs\",\n+                    \"fourth_compile_generate_time_secs\",\n+                ]\n+            )\n         else:\n             self.benchmarks_df = None\n             self.device_measurements_df = None\n@@ -83,36 +106,40 @@ def initialise_benchmark(self, metadata: dict[str, str]) -> str:\n         \"\"\"\n         # Generate a unique UUID for this benchmark\n         benchmark_id = str(uuid.uuid4())\n-        \n+\n         if self.use_database:\n             with self.conn.cursor() as cur:\n                 cur.execute(\n                     \"INSERT INTO benchmarks (benchmark_id, repository, branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s, %s, %s)\",\n                     (benchmark_id, self.repository, self.branch, self.commit_id, self.commit_msg, metadata),\n                 )\n                 self.logger.debug(f\"initialised benchmark #{benchmark_id}\")\n-        \n+\n         # Store benchmark data for CSV export (if enabled)\n         if self.collect_csv_data:\n             # Add row to pandas DataFrame\n-            new_row = pd.DataFrame([{\n-                'benchmark_id': benchmark_id,\n-                'repository': self.repository,\n-                'branch': self.branch,\n-                'commit_id': self.commit_id,\n-                'commit_message': self.commit_msg,\n-                'metadata': json.dumps(metadata),\n-                'created_at': datetime.utcnow().isoformat()\n-            }])\n+            new_row = pd.DataFrame(\n+                [\n+                    {\n+                        \"benchmark_id\": benchmark_id,\n+                        \"repository\": self.repository,\n+                        \"branch\": self.branch,\n+                        \"commit_id\": self.commit_id,\n+                        \"commit_message\": self.commit_msg,\n+                        \"metadata\": json.dumps(metadata),\n+                        \"created_at\": datetime.utcnow().isoformat(),\n+                    }\n+                ]\n+            )\n             self.benchmarks_df = pd.concat([self.benchmarks_df, new_row], ignore_index=True)\n-            \n+\n         mode_info = []\n         if self.use_database:\n             mode_info.append(\"database\")\n         if self.collect_csv_data:\n             mode_info.append(\"CSV\")\n         mode_str = \" + \".join(mode_info) if mode_info else \"no storage\"\n-        \n+\n         self.logger.debug(f\"initialised benchmark #{benchmark_id} ({mode_str} mode)\")\n         return benchmark_id\n \n@@ -123,24 +150,28 @@ def collect_device_measurements(self, benchmark_id: str, cpu_util, mem_megabytes\n         # Store device measurements for CSV export (if enabled)\n         if self.collect_csv_data:\n             # Add row to pandas DataFrame\n-            new_row = pd.DataFrame([{\n-                'benchmark_id': benchmark_id,\n-                'cpu_util': cpu_util,\n-                'mem_megabytes': mem_megabytes,\n-                'gpu_util': gpu_util,\n-                'gpu_mem_megabytes': gpu_mem_megabytes,\n-                'time': datetime.utcnow().isoformat()\n-            }])\n+            new_row = pd.DataFrame(\n+                [\n+                    {\n+                        \"benchmark_id\": benchmark_id,\n+                        \"cpu_util\": cpu_util,\n+                        \"mem_megabytes\": mem_megabytes,\n+                        \"gpu_util\": gpu_util,\n+                        \"gpu_mem_megabytes\": gpu_mem_megabytes,\n+                        \"time\": datetime.utcnow().isoformat(),\n+                    }\n+                ]\n+            )\n             self.device_measurements_df = pd.concat([self.device_measurements_df, new_row], ignore_index=True)\n-        \n+\n         # Store in database if available\n         if self.use_database:\n             with self.conn.cursor() as cur:\n                 cur.execute(\n                     \"INSERT INTO device_measurements (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes) VALUES (%s, %s, %s, %s, %s)\",\n                     (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes),\n                 )\n-            \n+\n         self.logger.debug(\n             f\"collected device measurements for benchmark #{benchmark_id} [CPU util: {cpu_util}, mem MBs: {mem_megabytes}, GPU util: {gpu_util}, GPU mem MBs: {gpu_mem_megabytes}]\"\n         )\n@@ -149,16 +180,13 @@ def collect_model_measurements(self, benchmark_id: str, measurements: dict[str,\n         # Store model measurements for CSV export (if enabled)\n         if self.collect_csv_data:\n             # Add row to pandas DataFrame with flattened measurements\n-            row_data = {\n-                'benchmark_id': benchmark_id,\n-                'time': datetime.utcnow().isoformat()\n-            }\n+            row_data = {\"benchmark_id\": benchmark_id, \"time\": datetime.utcnow().isoformat()}\n             # Flatten the measurements dict into the row\n             row_data.update(measurements)\n-            \n+\n             new_row = pd.DataFrame([row_data])\n             self.model_measurements_df = pd.concat([self.model_measurements_df, new_row], ignore_index=True)\n-        \n+\n         # Store in database if available\n         if self.use_database:\n             with self.conn.cursor() as cur:\n@@ -174,7 +202,7 @@ def collect_model_measurements(self, benchmark_id: str, measurements: dict[str,\n                         measurements,\n                     ),\n                 )\n-            \n+\n         self.logger.debug(f\"collected model measurements for benchmark #{benchmark_id}: {measurements}\")\n \n     def export_to_csv(self, output_dir: str = \"benchmark_results\"):\n@@ -184,19 +212,19 @@ def export_to_csv(self, output_dir: str = \"benchmark_results\"):\n         if not self.collect_csv_data:\n             self.logger.warning(\"CSV data collection is disabled - no CSV files will be generated\")\n             return\n-            \n+\n         if not os.path.exists(output_dir):\n             os.makedirs(output_dir)\n             self.logger.info(f\"Created output directory: {output_dir}\")\n-            \n+\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         files_created = []\n-        \n+\n         # Export using pandas DataFrames\n         self._export_pandas_data(output_dir, timestamp, files_created)\n-        \n+\n         self.logger.info(f\"CSV export complete! Created {len(files_created)} files in {output_dir}\")\n-    \n+\n     def _export_pandas_data(self, output_dir: str, timestamp: str, files_created: list):\n         \"\"\"\n         Export CSV files using pandas DataFrames\n@@ -206,24 +234,24 @@ def _export_pandas_data(self, output_dir: str, timestamp: str, files_created: li\n         self.benchmarks_df.to_csv(benchmarks_file, index=False)\n         files_created.append(benchmarks_file)\n         self.logger.info(f\"Exported {len(self.benchmarks_df)} benchmark records to {benchmarks_file}\")\n-        \n-        # Export device measurements  \n+\n+        # Export device measurements\n         device_file = os.path.join(output_dir, f\"device_measurements_{timestamp}.csv\")\n         self.device_measurements_df.to_csv(device_file, index=False)\n         files_created.append(device_file)\n         self.logger.info(f\"Exported {len(self.device_measurements_df)} device measurement records to {device_file}\")\n-        \n+\n         # Export model measurements (already flattened)\n         model_file = os.path.join(output_dir, f\"model_measurements_{timestamp}.csv\")\n         self.model_measurements_df.to_csv(model_file, index=False)\n         files_created.append(model_file)\n         self.logger.info(f\"Exported {len(self.model_measurements_df)} model measurement records to {model_file}\")\n-        \n+\n         # Create comprehensive summary using pandas operations\n         summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.csv\")\n         self._create_summary(summary_file)\n         files_created.append(summary_file)\n-    \n+\n     def _create_summary(self, summary_file: str):\n         \"\"\"\n         Create a comprehensive summary CSV using pandas operations\n@@ -234,36 +262,42 @@ def _create_summary(self, summary_file: str):\n             summary_df.to_csv(summary_file, index=False)\n             self.logger.info(f\"Created empty benchmark summary at {summary_file}\")\n             return\n-        \n+\n         # Start with benchmarks as the base\n         summary_df = self.benchmarks_df.copy()\n-        \n+\n         # Add model measurements (join on benchmark_id)\n         if len(self.model_measurements_df) > 0:\n             # Drop 'time' column from model measurements to avoid conflicts\n-            model_df = self.model_measurements_df.drop(columns=['time'], errors='ignore')\n-            summary_df = summary_df.merge(model_df, on='benchmark_id', how='left')\n-        \n+            model_df = self.model_measurements_df.drop(columns=[\"time\"], errors=\"ignore\")\n+            summary_df = summary_df.merge(model_df, on=\"benchmark_id\", how=\"left\")\n+\n         # Calculate device measurement aggregates using pandas groupby\n         if len(self.device_measurements_df) > 0:\n-            device_agg = self.device_measurements_df.groupby('benchmark_id').agg({\n-                'cpu_util': ['mean', 'max', 'std', 'count'],\n-                'mem_megabytes': ['mean', 'max', 'std'],\n-                'gpu_util': ['mean', 'max', 'std'],\n-                'gpu_mem_megabytes': ['mean', 'max', 'std']\n-            }).round(3)\n-            \n+            device_agg = (\n+                self.device_measurements_df.groupby(\"benchmark_id\")\n+                .agg(\n+                    {\n+                        \"cpu_util\": [\"mean\", \"max\", \"std\", \"count\"],\n+                        \"mem_megabytes\": [\"mean\", \"max\", \"std\"],\n+                        \"gpu_util\": [\"mean\", \"max\", \"std\"],\n+                        \"gpu_mem_megabytes\": [\"mean\", \"max\", \"std\"],\n+                    }\n+                )\n+                .round(3)\n+            )\n+\n             # Flatten column names\n             device_agg.columns = [f\"{col[0]}_{col[1]}\" for col in device_agg.columns]\n             device_agg = device_agg.reset_index()\n-            \n+\n             # Rename count column to be more descriptive\n-            if 'cpu_util_count' in device_agg.columns:\n-                device_agg = device_agg.rename(columns={'cpu_util_count': 'device_measurement_count'})\n-            \n+            if \"cpu_util_count\" in device_agg.columns:\n+                device_agg = device_agg.rename(columns={\"cpu_util_count\": \"device_measurement_count\"})\n+\n             # Merge with summary\n-            summary_df = summary_df.merge(device_agg, on='benchmark_id', how='left')\n-        \n+            summary_df = summary_df.merge(device_agg, on=\"benchmark_id\", how=\"left\")\n+\n         # Export the comprehensive summary\n         summary_df.to_csv(summary_file, index=False)\n         self.logger.info(f\"Created comprehensive benchmark summary with {len(summary_df)} records at {summary_file}\")\n@@ -312,23 +346,18 @@ def parse_arguments() -> tuple[str, str, str, str, bool, str]:\n         type=str,\n         help=\"The commit message associated with the commit, truncated to 70 characters.\",\n     )\n-    \n-    parser.add_argument(\n-        \"--csv\",\n-        action=\"store_true\",\n-        default=False,\n-        help=\"Enable CSV output files generation.\"\n-    )\n-    \n+\n+    parser.add_argument(\"--csv\", action=\"store_true\", default=False, help=\"Enable CSV output files generation.\")\n+\n     parser.add_argument(\n         \"--csv-output-dir\",\n         type=str,\n         default=\"benchmark_results\",\n-        help=\"Directory for CSV output files (default: benchmark_results).\"\n+        help=\"Directory for CSV output files (default: benchmark_results).\",\n     )\n \n     args = parser.parse_args()\n-    \n+\n     # CSV is disabled by default, only enabled when --csv is used\n     generate_csv = args.csv\n \n@@ -353,9 +382,10 @@ def create_database_connection():\n     if not PSYCOPG2_AVAILABLE:\n         logger.warning(\"psycopg2 not available - running in CSV-only mode\")\n         return None\n-        \n+\n     try:\n         import psycopg2\n+\n         conn = psycopg2.connect(\"dbname=metrics\")\n         logger.info(\"Successfully connected to database\")\n         return conn\n@@ -364,27 +394,28 @@ def create_database_connection():\n         return None\n \n \n-def create_global_metrics_recorder(repository: str, branch: str, commit_id: str, commit_msg: str, \n-                                   generate_csv: bool = False) -> MetricsRecorder:\n+def create_global_metrics_recorder(\n+    repository: str, branch: str, commit_id: str, commit_msg: str, generate_csv: bool = False\n+) -> MetricsRecorder:\n     \"\"\"\n     Create a global metrics recorder that will be used across all benchmarks.\n     \"\"\"\n     connection = create_database_connection()\n     recorder = MetricsRecorder(connection, logger, repository, branch, commit_id, commit_msg, generate_csv)\n-    \n+\n     # Log the storage mode\n     storage_modes = []\n     if connection is not None:\n         storage_modes.append(\"database\")\n     if generate_csv:\n         storage_modes.append(\"CSV\")\n-    \n+\n     if not storage_modes:\n         logger.warning(\"Running benchmarks with NO data storage (no database connection, CSV disabled)\")\n         logger.warning(\"Use --csv flag to enable CSV output when database is unavailable\")\n     else:\n         logger.info(f\"Running benchmarks with: {' + '.join(storage_modes)} storage\")\n-    \n+\n     return recorder\n \n \n@@ -393,29 +424,29 @@ def create_global_metrics_recorder(repository: str, branch: str, commit_id: str,\n     benches_folder_path = os.path.join(benchmarks_folder_path, \"benches\")\n \n     repository, branch, commit_id, commit_msg, generate_csv, csv_output_dir = parse_arguments()\n-    \n+\n     # Create a global metrics recorder\n     global_metrics_recorder = create_global_metrics_recorder(repository, branch, commit_id, commit_msg, generate_csv)\n-    \n+\n     successful_benchmarks = 0\n     failed_benchmarks = 0\n-    \n+\n     # Automatically discover all benchmark modules in benches/ folder\n     benchmark_modules = []\n-    \n+\n     if os.path.exists(benches_folder_path):\n         logger.debug(f\"Scanning for benchmarks in: {benches_folder_path}\")\n         for entry in os.scandir(benches_folder_path):\n             if not entry.name.endswith(\".py\"):\n                 continue\n             if entry.name.startswith(\"__\"):  # Skip __init__.py, __pycache__, etc.\n                 continue\n-                \n+\n             # Check if the file has a run_benchmark function\n             try:\n                 logger.debug(f\"checking if benches/{entry.name} has run_benchmark function\")\n                 module = import_from_path(entry.name.split(\".\")[0], entry.path)\n-                if hasattr(module, 'run_benchmark'):\n+                if hasattr(module, \"run_benchmark\"):\n                     benchmark_modules.append(entry.name)\n                     logger.debug(f\"discovered benchmark: {entry.name}\")\n                 else:\n@@ -436,16 +467,18 @@ def create_global_metrics_recorder(repository: str, branch: str, commit_id: str,\n             logger.debug(f\"loading: {module_name}\")\n             module = import_from_path(module_name.split(\".\")[0], module_path)\n             logger.info(f\"running benchmarks in: {module_name}\")\n-            \n+\n             # Check if the module has an updated run_benchmark function that accepts metrics_recorder\n             try:\n                 # Try the new signature first\n                 module.run_benchmark(logger, repository, branch, commit_id, commit_msg, global_metrics_recorder)\n             except TypeError:\n                 # Fall back to the old signature for backward compatibility\n-                logger.warning(f\"Module {module_name} using old run_benchmark signature - database connection will be created per module\")\n+                logger.warning(\n+                    f\"Module {module_name} using old run_benchmark signature - database connection will be created per module\"\n+                )\n                 module.run_benchmark(logger, repository, branch, commit_id, commit_msg)\n-            \n+\n             successful_benchmarks += 1\n         except ImportModuleException as e:\n             logger.error(e)\n@@ -461,7 +494,7 @@ def create_global_metrics_recorder(repository: str, branch: str, commit_id: str,\n             logger.info(f\"CSV reports have been generated and saved to the {csv_output_dir} directory\")\n         else:\n             logger.info(\"CSV generation disabled - no CSV files created (use --csv to enable)\")\n-        \n+\n         logger.info(f\"Benchmark run completed. Successful: {successful_benchmarks}, Failed: {failed_benchmarks}\")\n     except Exception as e:\n         logger.error(f\"Failed to export CSV results: {e}\")"
        },
        {
            "sha": "afa6804bf06bd5ceedf5d75d2c16adeeaeea4f26",
            "filename": "benchmark/optimum_benchmark_wrapper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Foptimum_benchmark_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark%2Foptimum_benchmark_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Foptimum_benchmark_wrapper.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -3,7 +3,11 @@\n \n \n def main(config_dir, config_name, args):\n-    subprocess.run([\"optimum-benchmark\", \"--config-dir\", f\"{config_dir}\", \"--config-name\", f\"{config_name}\"] + [\"hydra/job_logging=disabled\", \"hydra/hydra_logging=disabled\"] + args)\n+    subprocess.run(\n+        [\"optimum-benchmark\", \"--config-dir\", f\"{config_dir}\", \"--config-name\", f\"{config_name}\"]\n+        + [\"hydra/job_logging=disabled\", \"hydra/hydra_logging=disabled\"]\n+        + args\n+    )\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "64b106a00370c7f6a4b2c3912d0cd5de149a93e6",
            "filename": "benchmark_v2/benches/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenches%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenches%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenches%2F__init__.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -1 +1 @@\n-# Benchmark implementations directory \n\\ No newline at end of file\n+# Benchmark implementations directory"
        },
        {
            "sha": "23427a8549c7c3aaa8d82ac0d6f439932e8594ea",
            "filename": "benchmark_v2/benches/llama.py",
            "status": "modified",
            "additions": 56,
            "deletions": 46,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenches%2Fllama.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -12,55 +12,63 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import os\n import logging\n-from typing import Dict, Any, List\n+import os\n+from typing import Any\n \n+import torch\n from benchmark_framework import ModelBenchmark\n \n-import torch\n \n os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n torch.set_float32_matmul_precision(\"high\")\n \n+\n class LLaMABenchmark(ModelBenchmark):\n     \"\"\"Simplified LLaMA model benchmark implementation using the ModelBenchmark base class.\"\"\"\n-    \n+\n     def __init__(self, logger: logging.Logger):\n         super().__init__(logger)\n         self._default_prompt = \"Why dogs are so cute?\"  # Custom prompt for LLaMA\n-    \n \n-    \n-    def get_scenario_configs(self) -> List[Dict[str, Any]]:\n+    def get_scenario_configs(self) -> list[dict[str, Any]]:\n         \"\"\"\n         Get LLaMA-specific scenario configurations.\n-        \n+\n         Returns:\n             List of scenario configuration dictionaries\n         \"\"\"\n         return [\n             # Eager variants\n             {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            \n             # Compiled variants\n-            {\"variant\": \"compiled\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Compiled with max autotune\"},\n-            \n+            {\n+                \"variant\": \"compiled\",\n+                \"compile_mode\": \"max-autotune\",\n+                \"use_cache\": True,\n+                \"description\": \"Compiled with max autotune\",\n+            },\n             # Kernelized variant (if available)\n-            {\"variant\": \"kernelized\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Kernelized execution\"},\n+            {\n+                \"variant\": \"kernelized\",\n+                \"compile_mode\": \"max-autotune\",\n+                \"use_cache\": True,\n+                \"description\": \"Kernelized execution\",\n+            },\n         ]\n-    \n+\n     def _is_kernelization_available(self) -> bool:\n         \"\"\"Check if kernelization is available for LLaMA.\"\"\"\n         try:\n-            from kernels import Mode, kernelize\n+            from kernels import Mode, kernelize  # noqa: F401\n+\n             return True\n         except ImportError:\n             self.logger.debug(\"Kernelization not available: kernels module not found\")\n             return False\n-    \n-    def get_default_generation_config(self) -> Dict[str, Any]:\n+\n+    def get_default_generation_config(self) -> dict[str, Any]:\n         \"\"\"Get LLaMA-specific generation configuration.\"\"\"\n         return {\n             \"do_sample\": False,\n@@ -69,20 +77,19 @@ def get_default_generation_config(self) -> Dict[str, Any]:\n             \"repetition_penalty\": 1.0,\n             \"max_new_tokens\": None,  # Will be set per scenario\n         }\n-    \n-    def get_model_init_kwargs(self, config) -> Dict[str, Any]:\n+\n+    def get_model_init_kwargs(self, config) -> dict[str, Any]:\n         \"\"\"Get LLaMA-specific model initialization kwargs.\"\"\"\n-        from benchmark_framework import BenchmarkConfig\n         return {\n             \"torch_dtype\": getattr(torch, config.torch_dtype),\n             \"attn_implementation\": config.attn_implementation,\n             \"use_cache\": True,\n         }\n-    \n+\n     def get_default_torch_dtype(self) -> str:\n         \"\"\"Get default torch dtype for LLaMA.\"\"\"\n         return \"float16\"  # LLaMA works well with float16\n-    \n+\n     def get_default_device(self) -> str:\n         \"\"\"Get default device for LLaMA.\"\"\"\n         return \"cuda\"  # LLaMA prefers CUDA\n@@ -91,35 +98,37 @@ def get_default_device(self) -> str:\n def run_llama(logger, output_dir, **kwargs):\n     \"\"\"\n     Run LLaMA benchmark with the given configuration.\n-    \n+\n     Args:\n         logger: Logger instance\n         output_dir: Output directory for results\n         **kwargs: Additional configuration options\n-        \n+\n     Returns:\n         Path to output file if successful\n     \"\"\"\n     from benchmark_framework import BenchmarkRunner\n-    \n+\n     # Extract parameters with defaults\n-    model_id = kwargs.get('model_id', 'meta-llama/Llama-2-7b-hf')\n-    warmup_iterations = kwargs.get('warmup_iterations', 3)\n-    measurement_iterations = kwargs.get('measurement_iterations', 5)\n-    num_tokens_to_generate = kwargs.get('num_tokens_to_generate', 100)\n-    include_sdpa_variants = kwargs.get('include_sdpa_variants', True)\n-    device = kwargs.get('device', 'cuda')\n-    torch_dtype = kwargs.get('torch_dtype', 'float16')\n-    batch_size = kwargs.get('batch_size', 1)\n-    commit_id = kwargs.get('commit_id', None)\n-    \n+    model_id = kwargs.get(\"model_id\", \"meta-llama/Llama-2-7b-hf\")\n+    warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n+    measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n+    num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n+    include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n+    device = kwargs.get(\"device\", \"cuda\")\n+    torch_dtype = kwargs.get(\"torch_dtype\", \"float16\")\n+    batch_size = kwargs.get(\"batch_size\", 1)\n+    commit_id = kwargs.get(\"commit_id\")\n+\n     logger.info(f\"Starting LLaMA benchmark for model: {model_id}\")\n-    logger.info(f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\")\n-    \n+    logger.info(\n+        f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\"\n+    )\n+\n     try:\n         # Create benchmark instance\n         benchmark = LLaMABenchmark(logger)\n-        \n+\n         # Create scenarios\n         scenarios = benchmark.create_scenarios(\n             model_id=model_id,\n@@ -129,28 +138,29 @@ def run_llama(logger, output_dir, **kwargs):\n             include_sdpa_variants=include_sdpa_variants,\n             device=device,\n             torch_dtype=torch_dtype,\n-            batch_size=batch_size\n+            batch_size=batch_size,\n         )\n-        \n+\n         logger.info(f\"Created {len(scenarios)} benchmark scenarios\")\n-        \n+\n         # Create runner and execute benchmarks\n         runner = BenchmarkRunner(logger, output_dir)\n         results = runner.run_benchmark(benchmark, scenarios, commit_id=commit_id)\n-        \n+\n         if not results:\n             logger.warning(\"No successful benchmark results\")\n             return None\n-        \n+\n         # Save results\n-        model_name = model_id.split('/')[-1]  # Extract model name from ID\n+        model_name = model_id.split(\"/\")[-1]  # Extract model name from ID\n         output_file = runner.save_results(model_name, results)\n-        \n+\n         logger.info(f\"LLaMA benchmark completed successfully. Results saved to: {output_file}\")\n         return output_file\n-        \n+\n     except Exception as e:\n         logger.error(f\"LLaMA benchmark failed: {e}\")\n         import traceback\n+\n         logger.debug(traceback.format_exc())\n-        raise\n\\ No newline at end of file\n+        raise"
        },
        {
            "sha": "3e4005b9f4b0c759c70f76fa16ba7996fddce36d",
            "filename": "benchmark_v2/benchmark_framework.py",
            "status": "modified",
            "additions": 292,
            "deletions": 297,
            "changes": 589,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenchmark_framework.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Fbenchmark_framework.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenchmark_framework.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -14,28 +14,26 @@\n \n import gc\n import json\n+import logging\n import os\n-import subprocess\n-import sys\n-import time\n import statistics\n+import sys\n import threading\n+import time\n from abc import ABC, abstractmethod\n-from contextlib import nullcontext\n-from dataclasses import dataclass, field, asdict\n+from dataclasses import asdict, dataclass, field\n from datetime import datetime\n-from typing import Any, Callable, Dict, List, Optional, Union, TypedDict\n-import logging\n+from typing import Any, Optional, TypedDict, Union\n \n+import gpustat\n import numpy as np\n import psutil\n-import gpustat\n-\n import torch\n \n \n class GPUMetrics(TypedDict):\n     \"\"\"GPU monitoring result with GPU metrics.\"\"\"\n+\n     gpu_utilization_mean: float\n     gpu_utilization_max: float\n     gpu_utilization_min: float\n@@ -48,30 +46,31 @@ class GPUMetrics(TypedDict):\n \n class NoGPU(TypedDict):\n     \"\"\"GPU monitoring result without GPU metrics.\"\"\"\n+\n     gpu_monitoring_status: str\n     gpu_monitoring_reason: str\n \n \n class ArchAwareTimer:\n     \"\"\"Architecture-aware timer for supposedly better prescision\"\"\"\n-    \n+\n     def __init__(self, device: Optional[str] = None):\n         \"\"\"\n         Initialize architecture-aware timer.\n-        \n+\n         Args:\n             device: Device to use. If None, uses current device.\n         \"\"\"\n         self.device = device\n         self.use_cuda = torch.cuda.is_available()\n-        \n+\n         if self.use_cuda:\n             if device and device != \"cpu\":\n                 self.device_obj = torch.device(device)\n             else:\n                 # Fall back to CPU timing if device is CPU or CUDA not available\n                 self.use_cuda = False\n-        \n+\n         if self.use_cuda:\n             try:\n                 # Create CUDA events for timing\n@@ -80,31 +79,31 @@ def __init__(self, device: Optional[str] = None):\n             except RuntimeError:\n                 # Fall back to CPU timing if CUDA events fail\n                 self.use_cuda = False\n-        \n+\n         if not self.use_cuda:\n             self.start_time = None\n             self.end_time = None\n-    \n+\n     def start(self):\n         \"\"\"Start timing.\"\"\"\n         if self.use_cuda:\n             torch.cuda.synchronize(self.device_obj)\n             self.start_event.record(stream=torch.cuda.current_stream(self.device_obj))\n         else:\n             self.start_time = time.perf_counter()\n-    \n+\n     def stop(self):\n         \"\"\"Stop timing.\"\"\"\n         if self.use_cuda:\n             self.end_event.record(stream=torch.cuda.current_stream(self.device_obj))\n             torch.cuda.synchronize(self.device_obj)\n         else:\n             self.end_time = time.perf_counter()\n-    \n+\n     def elapsed_time(self) -> float:\n         \"\"\"\n         Get elapsed time in seconds.\n-        \n+\n         Returns:\n             Elapsed time in seconds\n         \"\"\"\n@@ -115,17 +114,17 @@ def elapsed_time(self) -> float:\n             if self.start_time is None or self.end_time is None:\n                 raise RuntimeError(\"Timer not properly started/stopped\")\n             return self.end_time - self.start_time\n-    \n+\n     @property\n     def timing_method(self) -> str:\n         \"\"\"Get the timing method being used.\"\"\"\n         return \"CUDA Events\" if self.use_cuda else \"CPU perf_counter\"\n-    \n+\n     def __enter__(self):\n         \"\"\"Context manager entry.\"\"\"\n         self.start()\n         return self\n-    \n+\n     def __exit__(self, exc_type, exc_val, exc_tb):\n         \"\"\"Context manager exit.\"\"\"\n         self.stop()\n@@ -134,6 +133,7 @@ def __exit__(self, exc_type, exc_val, exc_tb):\n @dataclass\n class BenchmarkConfig:\n     \"\"\"Configuration for a single benchmark scenario.\"\"\"\n+\n     name: str\n     model_id: str\n     variant: str = \"eager\"  # \"eager\", \"compiled\", \"kernelized\"\n@@ -143,36 +143,36 @@ class BenchmarkConfig:\n     device: str = \"cuda\"\n     torch_dtype: str = \"float16\"\n     compile_mode: Optional[str] = None  # None, \"default\", \"reduce-overhead\", \"max-autotune\"\n-    compile_options: Dict[str, Any] = field(default_factory=dict)\n+    compile_options: dict[str, Any] = field(default_factory=dict)\n     use_cache: bool = True\n     batch_size: int = 1\n     sequence_length: Optional[int] = None\n     attn_implementation: str = \"sdpa\"  # \"eager\", \"sdpa\", \"flash_attention_2\"\n     sdpa_backend: Optional[str] = None  # None, \"math\", \"flash_attention\", \"efficient_attention\", \"cudnn_attention\"\n-    custom_params: Dict[str, Any] = field(default_factory=dict)\n+    custom_params: dict[str, Any] = field(default_factory=dict)\n \n \n class BenchmarkScenario:\n     \"\"\"\n     A benchmark scenario that encapsulates both configuration and setup logic.\n     This makes it easier to define and adapt benchmarks for different models.\n     \"\"\"\n-    \n+\n     def __init__(self, name: str, config: BenchmarkConfig, description: str = \"\"):\n         self.name = name\n         self.config = config\n         self.description = description\n         self._setup_callbacks = []\n         self._teardown_callbacks = []\n-    \n+\n     def add_setup_callback(self, callback: callable):\n         \"\"\"Add a callback to be executed during scenario setup.\"\"\"\n         self._setup_callbacks.append(callback)\n-    \n+\n     def add_teardown_callback(self, callback: callable):\n         \"\"\"Add a callback to be executed during scenario teardown.\"\"\"\n         self._teardown_callbacks.append(callback)\n-    \n+\n     def setup(self, model, tokenizer, logger=None):\n         \"\"\"Execute setup callbacks for this scenario.\"\"\"\n         for callback in self._setup_callbacks:\n@@ -181,7 +181,7 @@ def setup(self, model, tokenizer, logger=None):\n             except Exception as e:\n                 if logger:\n                     logger.warning(f\"Setup callback failed for scenario {self.name}: {e}\")\n-    \n+\n     def teardown(self, model, tokenizer, logger=None):\n         \"\"\"Execute teardown callbacks for this scenario.\"\"\"\n         for callback in self._teardown_callbacks:\n@@ -190,29 +190,29 @@ def teardown(self, model, tokenizer, logger=None):\n             except Exception as e:\n                 if logger:\n                     logger.warning(f\"Teardown callback failed for scenario {self.name}: {e}\")\n-    \n+\n     def __repr__(self):\n         return f\"BenchmarkScenario(name='{self.name}', variant='{self.config.variant}')\"\n \n \n-\n-\n @dataclass\n class TimingResult:\n     \"\"\"Result from a timing measurement.\"\"\"\n+\n     time_to_first_token_seconds: Optional[float] = None\n     latency_seconds: float = 0.0\n     tokens_per_second: Optional[float] = None\n     time_per_output_token_seconds: Optional[float] = None\n     total_tokens_generated: int = 0\n-    metadata: Dict[str, Any] = field(default_factory=dict)\n+    metadata: dict[str, Any] = field(default_factory=dict)\n \n \n @dataclass\n class BenchmarkStatistics:\n     \"\"\"Statistical analysis of benchmark measurements.\"\"\"\n+\n     name: str\n-    measurements: List[float]\n+    measurements: list[float]\n     mean: float\n     median: float\n     std: float\n@@ -226,13 +226,13 @@ class BenchmarkStatistics:\n     unit: str = \"seconds\"\n \n     @classmethod\n-    def from_measurements(cls, name: str, measurements: List[float], unit: str = \"seconds\") -> 'BenchmarkStatistics':\n+    def from_measurements(cls, name: str, measurements: list[float], unit: str = \"seconds\") -> \"BenchmarkStatistics\":\n         \"\"\"Create statistics from a list of measurements.\"\"\"\n         if not measurements:\n             raise ValueError(\"Cannot create statistics from empty measurements\")\n-        \n+\n         measurements_array = np.array(measurements)\n-        \n+\n         return cls(\n             name=name,\n             measurements=measurements,\n@@ -246,13 +246,14 @@ def from_measurements(cls, name: str, measurements: List[float], unit: str = \"se\n             p90=float(np.percentile(measurements_array, 90)),\n             p95=float(np.percentile(measurements_array, 95)),\n             p99=float(np.percentile(measurements_array, 99)),\n-            unit=unit\n+            unit=unit,\n         )\n \n \n-@dataclass \n+@dataclass\n class HardwareInfo:\n     \"\"\"Hardware information collected during benchmarking.\"\"\"\n+\n     gpu_name: str\n     gpu_memory_total_mb: int\n     cpu_count: int\n@@ -265,6 +266,7 @@ class HardwareInfo:\n @dataclass\n class BenchmarkMetadata:\n     \"\"\"Metadata collected for each benchmark run.\"\"\"\n+\n     timestamp: str\n     commit_id: str\n     hardware_info: HardwareInfo\n@@ -273,8 +275,8 @@ class BenchmarkMetadata:\n \n class GPUMonitor:\n     \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n-    \n-    def __init__(self, sample_interval: float = 0.1, logger: logging.Logger = None):\n+\n+    def __init__(self, sample_interval: float = 0.1, logger: Optional[logging.Logger] = None):\n         self.sample_interval = sample_interval\n         self.logger = logger or logging.getLogger(__name__)\n         self.stop_event = threading.Event()\n@@ -284,10 +286,10 @@ def __init__(self, sample_interval: float = 0.1, logger: logging.Logger = None):\n         self.timestamps = []\n         self.gpu_available = False\n         self.warning_logged = False\n-        \n+\n         # Test GPU availability on initialization\n         self._test_gpu_availability()\n-        \n+\n     def _test_gpu_availability(self):\n         \"\"\"Test if GPU monitoring is available.\"\"\"\n         try:\n@@ -301,13 +303,13 @@ def _test_gpu_availability(self):\n         except Exception as e:\n             self.gpu_available = False\n             self.logger.debug(f\"GPU monitoring not available: {e}\")\n-        \n+\n     def start(self):\n         \"\"\"Start monitoring GPU metrics.\"\"\"\n         if not self.gpu_available:\n             self.logger.debug(\"GPU monitoring disabled: no GPUs available\")\n             return\n-            \n+\n         # Clear the stop event to enable monitoring\n         self.stop_event.clear()\n         self.gpu_utilization = []\n@@ -317,20 +319,17 @@ def start(self):\n         self.thread = threading.Thread(target=self._monitor_loop)\n         self.thread.start()\n         self.logger.debug(\"GPU monitoring started\")\n-        \n+\n     def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n         \"\"\"Stop monitoring and return collected metrics.\"\"\"\n         if not self.gpu_available:\n-            return NoGPU(\n-                gpu_monitoring_status=\"disabled\",\n-                gpu_monitoring_reason=\"no_gpus_available\"\n-            )\n-            \n+            return NoGPU(gpu_monitoring_status=\"disabled\", gpu_monitoring_reason=\"no_gpus_available\")\n+\n         # Signal the monitoring thread to stop\n         self.stop_event.set()\n         if self.thread:\n             self.thread.join()\n-        \n+\n         if self.gpu_utilization:\n             metrics = GPUMetrics(\n                 gpu_utilization_mean=statistics.mean(self.gpu_utilization),\n@@ -340,21 +339,18 @@ def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n                 gpu_memory_used_max=max(self.gpu_memory_used),\n                 gpu_memory_used_min=min(self.gpu_memory_used),\n                 sample_count=len(self.gpu_utilization),\n-                gpu_monitoring_status=\"success\"\n+                gpu_monitoring_status=\"success\",\n             )\n             self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n             return metrics\n         else:\n-            return NoGPU(\n-                gpu_monitoring_status=\"failed\",\n-                gpu_monitoring_reason=\"no_samples_collected\"\n-            )\n-    \n+            return NoGPU(gpu_monitoring_status=\"failed\", gpu_monitoring_reason=\"no_samples_collected\")\n+\n     def _monitor_loop(self):\n         \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n         consecutive_failures = 0\n         max_consecutive_failures = 5\n-        \n+\n         # Continue monitoring until stop_event is set\n         while not self.stop_event.is_set():\n             try:\n@@ -370,13 +366,13 @@ def _monitor_loop(self):\n                     if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n                         self.logger.warning(\"GPU monitoring: No GPU data returned by gpustat\")\n                         self.warning_logged = True\n-                        \n+\n             except Exception as e:\n                 consecutive_failures += 1\n                 if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n                     self.logger.warning(f\"GPU monitoring failed after {max_consecutive_failures} attempts: {e}\")\n                     self.warning_logged = True\n-            \n+\n             # Use Event.wait() with timeout instead of time.sleep()\n             # This allows for immediate response to stop signal while still maintaining sample interval\n             if self.stop_event.wait(timeout=self.sample_interval):\n@@ -388,7 +384,7 @@ def get_hardware_info() -> HardwareInfo:\n     \"\"\"Collect hardware information.\"\"\"\n     gpu_name = \"unknown\"\n     gpu_memory_total = 0\n-    \n+\n     try:\n         gpu_stats = gpustat.GPUStatCollection.new_query()\n         if gpu_stats and len(gpu_stats) > 0:\n@@ -397,27 +393,27 @@ def get_hardware_info() -> HardwareInfo:\n             gpu_memory_total = gpu[\"memory.total\"]\n     except Exception:\n         pass\n-    \n+\n     torch_version = torch.__version__\n     cuda_version = None\n-    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n+    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n         cuda_version = torch.version.cuda\n-    \n+\n     return HardwareInfo(\n         gpu_name=gpu_name,\n         gpu_memory_total_mb=gpu_memory_total,\n         cpu_count=psutil.cpu_count(),\n         memory_total_mb=int(psutil.virtual_memory().total / (1024 * 1024)),\n         python_version=f\"{sys.version.split()[0]}\",\n         torch_version=torch_version,\n-        cuda_version=cuda_version\n+        cuda_version=cuda_version,\n     )\n \n \n def flush_memory():\n     \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n     gc.collect()\n-    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n+    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n         torch.cuda.empty_cache()\n         torch.cuda.reset_max_memory_allocated()\n         torch.cuda.reset_peak_memory_stats()\n@@ -428,7 +424,7 @@ def get_sdpa_backend(backend_name: Optional[str]):\n     \"\"\"Get the SDPA backend enum from string name.\"\"\"\n     if backend_name is None:\n         return None\n-    \n+\n     try:\n         backend_map = {\n             \"math\": torch.nn.attention.SDPBackend.MATH,\n@@ -442,18 +438,15 @@ def get_sdpa_backend(backend_name: Optional[str]):\n         return None\n \n \n-\n-\n-\n class SDPAContext:\n     \"\"\"Context manager for SDPA kernel selection.\"\"\"\n-    \n-    def __init__(self, backend_name: Optional[str], logger: logging.Logger = None):\n+\n+    def __init__(self, backend_name: Optional[str], logger: Optional[logging.Logger] = None):\n         self.backend_name = backend_name\n         self.logger = logger or logging.getLogger(__name__)\n         self.backend = get_sdpa_backend(backend_name) if backend_name else None\n         self.context = None\n-        \n+\n     def __enter__(self):\n         if self.backend is not None:\n             try:\n@@ -466,9 +459,11 @@ def __enter__(self):\n                     self.logger.warning(f\"Failed to set SDPA backend {self.backend_name}: {e}\")\n                 self.context = None\n         elif self.backend_name and self.logger:\n-            self.logger.debug(f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\")\n+            self.logger.debug(\n+                f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\"\n+            )\n         return self\n-        \n+\n     def __exit__(self, exc_type, exc_val, exc_tb):\n         if self.context is not None:\n             try:\n@@ -481,44 +476,44 @@ def __exit__(self, exc_type, exc_val, exc_tb):\n \n class AbstractModelBenchmark(ABC):\n     \"\"\"Abstract base class for model benchmarks.\"\"\"\n-    \n+\n     def __init__(self, logger: logging.Logger):\n         self.logger = logger\n         self.model = None\n         self.tokenizer = None\n         self.device = None\n         self.scenarios = {}  # Map of scenario_name -> BenchmarkScenario\n-        \n+\n     @abstractmethod\n-    def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n         \"\"\"Create and return a dictionary of benchmark scenarios.\"\"\"\n         pass\n-        \n+\n     @abstractmethod\n     def setup_model(self, config: BenchmarkConfig) -> None:\n         \"\"\"Setup the model for benchmarking with the given configuration.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     def cleanup_model(self) -> None:\n         \"\"\"Cleanup model resources.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n         \"\"\"Measure time to first token generation.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n         \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n         pass\n-    \n+\n     def prepare_inputs(self, config: BenchmarkConfig) -> Any:\n         \"\"\"Prepare inputs for the model. Override if needed.\"\"\"\n         return None\n-    \n-    def get_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+\n+    def get_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n         \"\"\"Get benchmark scenarios. Creates them if they don't exist.\"\"\"\n         if not self.scenarios:\n             self.scenarios = self.create_scenarios(**kwargs)\n@@ -528,133 +523,135 @@ def get_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n class ModelBenchmark(AbstractModelBenchmark):\n     \"\"\"\n     Base class for HuggingFace Transformers model benchmarks.\n-    \n+\n     This class provides common scenario creation logic and handles the standard\n     patterns for eager, compiled, and kernelized execution variants with different\n     attention implementations and SDPA backends.\n     \"\"\"\n-    \n+\n     def __init__(self, logger: logging.Logger):\n         super().__init__(logger)\n         self.inputs = None\n         self.compiled_model = None\n         self.past_key_values = None\n         self.config = None\n         self._default_prompt = \"Why dogs are so cute?\"\n-        \n+\n     @property\n     def default_prompt(self) -> str:\n         \"\"\"Default prompt for text generation. Override in subclasses if needed.\"\"\"\n         return self._default_prompt\n-    \n \n-    \n-    def get_attention_configs(self, include_sdpa_variants: bool = True) -> List[Dict[str, Any]]:\n+    def get_attention_configs(self, include_sdpa_variants: bool = True) -> list[dict[str, Any]]:\n         \"\"\"\n         Get attention implementation configurations.\n-        \n+\n         Args:\n             include_sdpa_variants: Whether to include SDPA backend variants\n-            \n+\n         Returns:\n             List of attention configuration dictionaries\n         \"\"\"\n         attention_configs = [\n             {\"attn_implementation\": \"eager\", \"sdpa_backends\": [None], \"desc_suffix\": \" with eager attention\"},\n         ]\n-        \n+\n         # Add SDPA variants if requested\n         if include_sdpa_variants:\n-            attention_configs.append({\n-                \"attn_implementation\": \"sdpa\", \n-                \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n-                \"desc_suffix\": \"\"\n-            })\n-        \n+            attention_configs.append(\n+                {\n+                    \"attn_implementation\": \"sdpa\",\n+                    \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n+                    \"desc_suffix\": \"\",\n+                }\n+            )\n+\n         return attention_configs\n-    \n-    def get_scenario_configs(self) -> List[Dict[str, Any]]:\n+\n+    def get_scenario_configs(self) -> list[dict[str, Any]]:\n         \"\"\"\n         Get base scenario configurations. Override in subclasses to customize.\n-        \n+\n         Returns:\n             List of scenario configuration dictionaries\n         \"\"\"\n         return [\n             # Eager variants\n             {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            \n             # Compiled variants\n-            {\"variant\": \"compiled\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Compiled with max autotune\"},\n-            \n+            {\n+                \"variant\": \"compiled\",\n+                \"compile_mode\": \"max-autotune\",\n+                \"use_cache\": True,\n+                \"description\": \"Compiled with max autotune\",\n+            },\n             # Kernelized variant (if available)\n-            {\"variant\": \"kernelized\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Kernelized execution\"},\n+            {\n+                \"variant\": \"kernelized\",\n+                \"compile_mode\": \"max-autotune\",\n+                \"use_cache\": True,\n+                \"description\": \"Kernelized execution\",\n+            },\n         ]\n-    \n+\n     def _is_kernelization_available(self) -> bool:\n         \"\"\"Check if kernelization is available. Override in subclasses.\"\"\"\n         try:\n-            from kernels import Mode, kernelize\n+            from kernels import Mode, kernelize  # noqa: F401\n+\n             return True\n         except ImportError:\n             return False\n-    \n-    def get_default_generation_config(self) -> Dict[str, Any]:\n+\n+    def get_default_generation_config(self) -> dict[str, Any]:\n         \"\"\"Get default generation configuration. Override in subclasses for model-specific defaults.\"\"\"\n-        return {\n-            \"do_sample\": False,\n-            \"top_p\": 1.0,\n-            \"temperature\": 1.0\n-        }\n-    \n-    def get_model_init_kwargs(self, config: BenchmarkConfig) -> Dict[str, Any]:\n+        return {\"do_sample\": False, \"top_p\": 1.0, \"temperature\": 1.0}\n+\n+    def get_model_init_kwargs(self, config: BenchmarkConfig) -> dict[str, Any]:\n         \"\"\"Get model initialization kwargs. Override in subclasses for model-specific parameters.\"\"\"\n-        return {\n-            \"torch_dtype\": getattr(torch, config.torch_dtype),\n-            \"attn_implementation\": config.attn_implementation\n-        }\n-    \n+        return {\"torch_dtype\": getattr(torch, config.torch_dtype), \"attn_implementation\": config.attn_implementation}\n+\n     def get_default_torch_dtype(self) -> str:\n         \"\"\"Get default torch dtype. Override in subclasses.\"\"\"\n         return \"float16\"\n-    \n+\n     def get_default_device(self) -> str:\n         \"\"\"Get default device. Override in subclasses.\"\"\"\n         return \"cuda\"\n-    \n-    def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+\n+    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n         \"\"\"Create benchmark scenarios for HuggingFace models.\"\"\"\n         scenarios = {}\n-        \n+\n         # Extract parameters with model-specific defaults\n-        model_id = kwargs.get('model_id', 'microsoft/DialoGPT-medium')\n-        warmup_iterations = kwargs.get('warmup_iterations', 3)\n-        measurement_iterations = kwargs.get('measurement_iterations', 5)\n-        num_tokens_to_generate = kwargs.get('num_tokens_to_generate', 100)\n-        include_sdpa_variants = kwargs.get('include_sdpa_variants', True)\n-        device = kwargs.get('device', self.get_default_device())\n-        torch_dtype = kwargs.get('torch_dtype', self.get_default_torch_dtype())\n-        batch_size = kwargs.get('batch_size', 1)\n-        \n+        model_id = kwargs.get(\"model_id\", \"microsoft/DialoGPT-medium\")\n+        warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n+        measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n+        num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n+        include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n+        device = kwargs.get(\"device\", self.get_default_device())\n+        torch_dtype = kwargs.get(\"torch_dtype\", self.get_default_torch_dtype())\n+        batch_size = kwargs.get(\"batch_size\", 1)\n+\n         # Get configurations\n         attention_configs = self.get_attention_configs(include_sdpa_variants)\n         scenario_configs = self.get_scenario_configs()\n-        \n+\n         # Create scenarios for each attention config and variant combination\n         for attn_config in attention_configs:\n             attn_implementation = attn_config[\"attn_implementation\"]\n             sdpa_backends = attn_config[\"sdpa_backends\"]\n             desc_suffix = attn_config[\"desc_suffix\"]\n-            \n+\n             for scenario_config in scenario_configs:\n                 for sdpa_backend in sdpa_backends:\n                     # Skip kernelized if not available\n                     if scenario_config[\"variant\"] == \"kernelized\" and not self._is_kernelization_available():\n                         continue\n-                    \n+\n                     # Create unique config for this scenario\n                     config = BenchmarkConfig(\n-                        name=scenario_config['variant'],\n+                        name=scenario_config[\"variant\"],\n                         model_id=model_id,\n                         variant=scenario_config[\"variant\"],\n                         compile_mode=scenario_config[\"compile_mode\"],\n@@ -666,14 +663,14 @@ def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n                         torch_dtype=torch_dtype,\n                         batch_size=batch_size,\n                         attn_implementation=attn_implementation,\n-                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None\n+                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None,\n                     )\n-                    \n+\n                     # Create scenario name\n                     scenario_name_parts = [scenario_config[\"variant\"]]\n                     if scenario_config[\"compile_mode\"]:\n                         scenario_name_parts.append(f\"compile_{scenario_config['compile_mode']}\")\n-                    \n+\n                     # Add attention implementation to name\n                     if attn_implementation == \"eager\":\n                         scenario_name_parts.append(\"eager_attn\")\n@@ -682,9 +679,9 @@ def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n                             scenario_name_parts.append(f\"sdpa_{sdpa_backend}\")\n                         else:\n                             scenario_name_parts.append(\"sdpa_default\")\n-                    \n+\n                     scenario_name = \"_\".join(scenario_name_parts)\n-                    \n+\n                     # Create description\n                     description = scenario_config[\"description\"]\n                     if attn_implementation == \"sdpa\" and sdpa_backend:\n@@ -693,217 +690,202 @@ def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n                         description += \" with SDPA default backend\"\n                     else:\n                         description += desc_suffix\n-                    \n+\n                     # Create scenario\n-                    scenario = BenchmarkScenario(\n-                        name=scenario_name,\n-                        config=config,\n-                        description=description\n-                    )\n-                    \n+                    scenario = BenchmarkScenario(name=scenario_name, config=config, description=description)\n+\n                     # Add setup callbacks based on variant\n                     if scenario_config[\"variant\"] == \"compiled\":\n                         scenario.add_setup_callback(self._setup_compilation_callback)\n                     elif scenario_config[\"variant\"] == \"kernelized\":\n                         scenario.add_setup_callback(self._setup_kernelization_callback)\n-                    \n+\n                     scenarios[scenario_name] = scenario\n-        \n+\n         return scenarios\n-    \n+\n     def _setup_compilation_callback(self, model, tokenizer, config, logger):\n         \"\"\"Setup callback for compilation scenarios.\"\"\"\n         if logger:\n             logger.info(f\"Setting up compilation with mode: {config.compile_mode}\")\n-        \n+\n         # Perform torch.compile\n         if config.compile_mode is not None:\n-            self.compiled_model = torch.compile(\n-                model, \n-                mode=config.compile_mode, \n-                **config.compile_options\n-            )\n+            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)\n         else:\n             self.compiled_model = torch.compile(model, **config.compile_options)\n-        \n+\n         # Setup static cache for compiled mode if needed\n-        if config.use_cache and hasattr(self, 'inputs') and self.inputs is not None:\n+        if config.use_cache and hasattr(self, \"inputs\") and self.inputs is not None:\n             self._setup_static_cache(config)\n-    \n+\n     def _setup_kernelization_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for kernelization scenarios.\"\"\" \n+        \"\"\"Setup callback for kernelization scenarios.\"\"\"\n         if logger:\n             logger.info(\"Setting up kernelization\")\n-        \n+\n         try:\n             from kernels import Mode, kernelize\n-            self.compiled_model = kernelize(\n-                model,\n-                mode=Mode.INFERENCE\n-            )\n+\n+            self.compiled_model = kernelize(model, mode=Mode.INFERENCE)\n         except Exception as e:\n             if logger:\n                 logger.warning(f\"Failed to setup kernelized mode: {e}\")\n                 logger.warning(\"Falling back to eager mode\")\n             config.variant = \"eager\"\n-    \n+\n     def _setup_static_cache(self, config: BenchmarkConfig):\n         \"\"\"Setup static cache for compiled models. Override if needed.\"\"\"\n-        if hasattr(self, 'inputs') and self.inputs is not None:\n+        if hasattr(self, \"inputs\") and self.inputs is not None:\n             try:\n                 from transformers import StaticCache\n+\n                 seq_length = self.inputs[\"input_ids\"].shape[1]\n-                \n+\n                 # Get the actual device the model is on\n-                if hasattr(self.model, 'device'):\n+                if hasattr(self.model, \"device\"):\n                     cache_device = self.model.device\n                 else:\n                     cache_device = self.device\n-                \n+\n                 self.past_key_values = StaticCache(\n                     config=self.model.config,\n                     max_batch_size=config.batch_size,\n                     max_cache_len=seq_length + config.num_tokens_to_generate,\n                     device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype)\n+                    dtype=getattr(torch, config.torch_dtype),\n                 )\n                 self.logger.debug(f\"StaticCache created on device: {cache_device}\")\n             except (ImportError, TypeError) as e:\n                 # StaticCache not available or incompatible, continue without it\n                 self.logger.debug(f\"StaticCache setup failed: {e}, continuing without cache\")\n                 self.past_key_values = None\n-    \n+\n     def setup_model(self, config: BenchmarkConfig) -> None:\n         \"\"\"Setup the HuggingFace model for benchmarking with the given configuration.\"\"\"\n-        \n+\n         self.logger.info(f\"Setting up model: {config.model_id} with variant: {config.variant}\")\n         self.device = config.device\n         self.config = config\n-        \n+\n         # Load model and tokenizer\n         self._load_model_and_tokenizer(config)\n-        \n+\n         # Prepare inputs\n         self._prepare_model_inputs(config)\n-        \n+\n         # Configure generation settings\n         self._configure_generation(config)\n-        \n+\n         self.logger.info(\"Model setup complete\")\n-    \n+\n     def _load_model_and_tokenizer(self, config: BenchmarkConfig):\n         \"\"\"Load the model and tokenizer. Override in subclasses for custom loading.\"\"\"\n \n-        \n         from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-        \n+\n         # Load tokenizer\n         self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n         if self.tokenizer.pad_token is None:\n             self.tokenizer.pad_token = self.tokenizer.eos_token\n-        \n+\n         # Prepare generation config\n         generation_config_dict = self.get_default_generation_config()\n         gen_config = GenerationConfig(**generation_config_dict)\n-        \n+\n         # Load model\n         self.logger.info(\"Loading model...\")\n-        \n-        target_device = config.device    \n+\n+        target_device = config.device\n         # Get model initialization kwargs\n         model_init_kwargs = self.get_model_init_kwargs(config)\n-        model_init_kwargs.update({\n-            \"generation_config\": gen_config\n-        })\n-            \n-        self.model = AutoModelForCausalLM.from_pretrained(\n-            config.model_id, \n-            **model_init_kwargs\n-        ).eval()\n-        \n+        model_init_kwargs.update({\"generation_config\": gen_config})\n+\n+        self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()\n+\n         # Move model to target device\n         self.logger.info(f\"Moving model to device: {target_device}\")\n         self.model.to(target_device)\n         self.device = target_device  # Update device to match actual device used\n-    \n+\n     def _prepare_model_inputs(self, config: BenchmarkConfig):\n         \"\"\"Prepare model inputs. Override in subclasses for custom inputs.\"\"\"\n         # Prepare inputs\n         self.inputs = self.tokenizer(self.default_prompt, return_tensors=\"pt\")\n-        \n+\n         # Move inputs to the same device as the model\n-        if hasattr(self.model, 'device'):\n+        if hasattr(self.model, \"device\"):\n             # Model is on a single device\n             model_device = self.model.device\n         else:\n             # Model might be distributed, use self.device which was set during model loading\n             model_device = self.device\n-            \n+\n         self.inputs = {k: v.to(model_device) for k, v in self.inputs.items()}\n         self.logger.debug(f\"Moved inputs to device: {model_device}\")\n-    \n+\n     def _configure_generation(self, config: BenchmarkConfig):\n         \"\"\"Configure generation settings.\"\"\"\n         seq_length = self.inputs[\"input_ids\"].shape[1]\n         self.model.generation_config.max_length = seq_length + config.num_tokens_to_generate\n-    \n+\n     def cleanup_model(self) -> None:\n         \"\"\"Cleanup model resources.\"\"\"\n-        if hasattr(self, 'model') and self.model is not None:\n+        if hasattr(self, \"model\") and self.model is not None:\n             del self.model\n             self.model = None\n-        if hasattr(self, 'compiled_model') and self.compiled_model is not None:\n+        if hasattr(self, \"compiled_model\") and self.compiled_model is not None:\n             del self.compiled_model\n             self.compiled_model = None\n-        if hasattr(self, 'tokenizer') and self.tokenizer is not None:\n+        if hasattr(self, \"tokenizer\") and self.tokenizer is not None:\n             del self.tokenizer\n             self.tokenizer = None\n-        if hasattr(self, 'past_key_values') and self.past_key_values is not None:\n+        if hasattr(self, \"past_key_values\") and self.past_key_values is not None:\n             del self.past_key_values\n             self.past_key_values = None\n-        \n+\n         # Clear CUDA cache\n         flush_memory()\n-    \n+\n     def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n         \"\"\"Measure time to first token generation.\"\"\"\n         model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-        \n+\n         # Prepare generation kwargs\n         generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=1)\n-        \n+\n         # Use CUDA timer for high-precision measurement\n         with ArchAwareTimer(device=config.device) as timer:\n             # Use SDPA context if specified\n             with SDPAContext(config.sdpa_backend, self.logger):\n                 with torch.no_grad():\n-                    outputs = model_to_use.generate(**generation_kwargs)\n-        \n+                    _ = model_to_use.generate(**generation_kwargs)\n+\n         return timer.elapsed_time()\n-    \n+\n     def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n         \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n         model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-        \n+\n         # Prepare generation kwargs\n         generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=config.num_tokens_to_generate)\n-        \n+\n         # Use CUDA timer for high-precision measurement\n         with ArchAwareTimer(device=config.device) as timer:\n             # Use SDPA context if specified\n             with SDPAContext(config.sdpa_backend, self.logger):\n                 with torch.no_grad():\n                     outputs = model_to_use.generate(**generation_kwargs)\n-        \n+\n         # Calculate metrics\n         latency = timer.elapsed_time()\n         input_length = self.inputs[\"input_ids\"].shape[1]\n         output_length = outputs.shape[1]\n         tokens_generated = output_length - input_length\n-        \n+\n         tokens_per_second = tokens_generated / latency if latency > 0 else 0\n         time_per_output_token = latency / tokens_generated if tokens_generated > 0 else None\n-        \n+\n         return TimingResult(\n             latency_seconds=latency,\n             tokens_per_second=tokens_per_second,\n@@ -915,11 +897,11 @@ def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n                 \"variant\": config.variant,\n                 \"compile_mode\": config.compile_mode,\n                 \"attn_implementation\": config.attn_implementation,\n-                \"sdpa_backend\": config.sdpa_backend\n-            }\n+                \"sdpa_backend\": config.sdpa_backend,\n+            },\n         )\n-    \n-    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> Dict[str, Any]:\n+\n+    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> dict[str, Any]:\n         \"\"\"Get generation kwargs. Override in subclasses for custom generation.\"\"\"\n         generation_config_dict = self.get_default_generation_config()\n         generation_kwargs = {\n@@ -930,76 +912,76 @@ def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -\n             \"top_p\": generation_config_dict.get(\"top_p\", 1.0),\n             \"pad_token_id\": self.tokenizer.pad_token_id,\n         }\n-        \n+\n         # Handle static cache for compiled models\n         if self.past_key_values is not None and config.variant == \"compiled\":\n             try:\n                 from transformers import StaticCache\n+\n                 # Reset cache for each measurement\n                 seq_length = self.inputs[\"input_ids\"].shape[1]\n-                \n+\n                 # Get the actual device the model is on\n-                if hasattr(self.model, 'device'):\n+                if hasattr(self.model, \"device\"):\n                     cache_device = self.model.device\n                 else:\n                     cache_device = self.device\n-                \n+\n                 fresh_cache = StaticCache(\n                     config=self.model.config,\n                     max_batch_size=config.batch_size,\n                     max_cache_len=seq_length + max_new_tokens,\n                     device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype)\n+                    dtype=getattr(torch, config.torch_dtype),\n                 )\n                 generation_kwargs[\"past_key_values\"] = fresh_cache\n             except (ImportError, TypeError) as e:\n                 self.logger.debug(f\"Fresh StaticCache creation failed: {e}\")\n                 pass\n-        \n+\n         return generation_kwargs\n \n \n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n-    \n+\n     def __init__(self, logger: logging.Logger, output_dir: str = \"benchmark_results\"):\n         self.logger = logger\n         self.output_dir = output_dir\n         os.makedirs(output_dir, exist_ok=True)\n-        \n \n     def run_benchmark(\n-        self, \n-        benchmark: ModelBenchmark, \n-        scenarios: Dict[str, BenchmarkScenario],\n+        self,\n+        benchmark: ModelBenchmark,\n+        scenarios: dict[str, BenchmarkScenario],\n         collect_gpu_metrics: bool = True,\n-        commit_id: Optional[str] = None\n-    ) -> Dict[str, Dict[str, Any]]:\n+        commit_id: Optional[str] = None,\n+    ) -> dict[str, dict[str, Any]]:\n         \"\"\"\n         Run benchmarks using scenarios.\n-        \n+\n         Args:\n             benchmark: The benchmark instance to run\n             scenarios: Dictionary mapping scenario names to BenchmarkScenario instances\n             collect_gpu_metrics: Whether to collect GPU utilization metrics\n             commit_id: Git commit ID for metadata (if not provided, will auto-detect from git)\n-            \n+\n         Returns:\n             Dictionary mapping scenario names to results with statistics\n         \"\"\"\n         all_results = {}\n-        \n+\n         for scenario_name, scenario in scenarios.items():\n             self.logger.info(f\"Running benchmark scenario: {scenario_name}\")\n             config = scenario.config\n-            \n+\n             try:\n                 # Setup model for this configuration\n                 benchmark.setup_model(config)\n-                \n+\n                 # Run scenario setup callbacks\n                 scenario.setup(benchmark.model, benchmark.tokenizer, self.logger)\n-                \n+\n                 # Quick validation: try one measurement first to see if this scenario works\n                 try:\n                     flush_memory()\n@@ -1015,20 +997,20 @@ def run_benchmark(\n                     except Exception:\n                         pass\n                     continue\n-                \n+\n                 # Collect metadata\n                 metadata = BenchmarkMetadata(\n                     timestamp=datetime.utcnow().isoformat(),\n                     commit_id=commit_id,\n                     hardware_info=get_hardware_info(),\n-                    config=config\n+                    config=config,\n                 )\n-                \n+\n                 # Initialize GPU monitor\n                 gpu_monitor = None\n                 if collect_gpu_metrics:\n                     gpu_monitor = GPUMonitor(logger=self.logger)\n-                \n+\n                 # Warmup runs\n                 self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n                 warmup_failures = 0\n@@ -1037,92 +1019,108 @@ def run_benchmark(\n                         _ = benchmark.measure_latency(config)\n                     except Exception as e:\n                         warmup_failures += 1\n-                        self.logger.warning(f\"Warmup iteration {i+1} failed: {e}\")\n-                \n+                        self.logger.warning(f\"Warmup iteration {i + 1} failed: {e}\")\n+\n                 # If more than half the warmup iterations failed, skip this scenario\n                 if warmup_failures > config.warmup_iterations // 2:\n-                    self.logger.warning(f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\")\n+                    self.logger.warning(\n+                        f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\"\n+                    )\n                     try:\n                         scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n                         benchmark.cleanup_model()\n                     except Exception:\n                         pass\n                     continue\n-                \n+\n                 # Start GPU monitoring\n                 if gpu_monitor:\n                     gpu_monitor.start()\n-                \n+\n                 # Measurement runs for latency\n                 self.logger.info(f\"Measuring latency with {config.measurement_iterations} iterations...\")\n                 latency_measurements = []\n                 ttft_measurements = []\n                 tokens_per_sec_measurements = []\n                 itl_measurements = []  # Inter-Token Latency\n                 measurement_failures = 0\n-                \n+\n                 for i in range(config.measurement_iterations):\n-                    try:                        \n+                    try:\n                         # Measure time to first token\n                         ttft = benchmark.measure_time_to_first_token(config)\n                         ttft_measurements.append(ttft)\n-                        \n+\n                         # Measure full latency\n                         timing_result = benchmark.measure_latency(config)\n                         latency_measurements.append(timing_result.latency_seconds)\n-                        \n+\n                         if timing_result.tokens_per_second is not None:\n                             tokens_per_sec_measurements.append(timing_result.tokens_per_second)\n-                        \n+\n                         if timing_result.time_per_output_token_seconds is not None:\n                             itl_measurements.append(timing_result.time_per_output_token_seconds)\n-                        \n-                        itl_str = f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\" if timing_result.time_per_output_token_seconds else \"\"\n-                        self.logger.debug(f\"Iteration {i+1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\")\n-                        \n+\n+                        itl_str = (\n+                            f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\"\n+                            if timing_result.time_per_output_token_seconds\n+                            else \"\"\n+                        )\n+                        self.logger.debug(\n+                            f\"Iteration {i + 1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\"\n+                        )\n+\n                     except Exception as e:\n                         measurement_failures += 1\n-                        self.logger.warning(f\"Measurement iteration {i+1} failed: {e}\")\n-                \n+                        self.logger.warning(f\"Measurement iteration {i + 1} failed: {e}\")\n+\n                 # Stop GPU monitoring\n                 gpu_metrics = {}\n                 if gpu_monitor:\n                     gpu_metrics = gpu_monitor.stop_and_collect()\n-                \n+\n                 # If we don't have enough successful measurements, skip this scenario\n                 if not latency_measurements or len(latency_measurements) < config.measurement_iterations // 2:\n-                    self.logger.warning(f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\")\n+                    self.logger.warning(\n+                        f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\"\n+                    )\n                     try:\n                         scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n                         benchmark.cleanup_model()\n                     except Exception:\n                         pass\n                     continue\n-                \n+\n                 # Calculate statistics\n                 scenario_results = {\n                     \"metadata\": asdict(metadata),\n                     \"measurements\": {},\n                     \"gpu_metrics\": gpu_metrics,\n-                    \"scenario_description\": scenario.description\n+                    \"scenario_description\": scenario.description,\n                 }\n-                \n+\n                 if latency_measurements:\n                     latency_stats = BenchmarkStatistics.from_measurements(\"latency_seconds\", latency_measurements)\n                     scenario_results[\"measurements\"][\"latency_seconds\"] = asdict(latency_stats)\n-                \n+\n                 if ttft_measurements:\n-                    ttft_stats = BenchmarkStatistics.from_measurements(\"time_to_first_token_seconds\", ttft_measurements)\n+                    ttft_stats = BenchmarkStatistics.from_measurements(\n+                        \"time_to_first_token_seconds\", ttft_measurements\n+                    )\n                     scenario_results[\"measurements\"][\"time_to_first_token_seconds\"] = asdict(ttft_stats)\n-                \n+\n                 if tokens_per_sec_measurements:\n-                    tps_stats = BenchmarkStatistics.from_measurements(\"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\")\n+                    tps_stats = BenchmarkStatistics.from_measurements(\n+                        \"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\"\n+                    )\n                     scenario_results[\"measurements\"][\"tokens_per_second\"] = asdict(tps_stats)\n-                \n+\n                 if itl_measurements:\n-                    itl_stats = BenchmarkStatistics.from_measurements(\"time_per_output_token_seconds\", itl_measurements, \"seconds/token\")\n+                    itl_stats = BenchmarkStatistics.from_measurements(\n+                        \"time_per_output_token_seconds\", itl_measurements, \"seconds/token\"\n+                    )\n                     scenario_results[\"measurements\"][\"time_per_output_token_seconds\"] = asdict(itl_stats)\n-                \n+\n                 # Log summary\n                 if latency_measurements:\n                     self.logger.info(f\"Latency: {latency_stats.mean:.4f}{latency_stats.std:.4f}s (meanstd)\")\n@@ -1132,25 +1130,26 @@ def run_benchmark(\n                     self.logger.info(f\"Throughput: {tps_stats.mean:.2f}{tps_stats.std:.2f} tokens/sec (meanstd)\")\n                 if itl_measurements:\n                     self.logger.info(f\"ITL: {itl_stats.mean:.4f}{itl_stats.std:.4f}s/token (meanstd)\")\n-                \n+\n                 # Add note about partial results if some measurements failed\n                 if measurement_failures > 0:\n                     scenario_results[\"warnings\"] = [f\"Some measurements failed ({measurement_failures} failures)\"]\n                     self.logger.info(f\"Scenario completed with {measurement_failures} measurement failures\")\n-                \n+\n                 # Run scenario teardown callbacks\n                 scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                \n+\n                 # Cleanup model\n                 benchmark.cleanup_model()\n-                \n+\n                 all_results[scenario_name] = scenario_results\n-                \n+\n             except Exception as e:\n                 self.logger.warning(f\"Skipping scenario {scenario_name}: setup failed - {e}\")\n                 import traceback\n+\n                 self.logger.debug(traceback.format_exc())\n-                \n+\n                 # Try to clean up if possible\n                 try:\n                     scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n@@ -1164,41 +1163,37 @@ def run_benchmark(\n                     benchmark.cleanup_model()\n                 except Exception as cleanup_error:\n                     self.logger.warning(f\"Cleanup failed for scenario {scenario_name}: {cleanup_error}\")\n-                \n+\n                 flush_memory()\n-        \n+\n         return all_results\n-    \n-    def save_results(self, model_name: str, results: Dict[str, Dict[str, Any]]) -> str:\n+\n+    def save_results(self, model_name: str, results: dict[str, dict[str, Any]]) -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n         # Create model-specific subdirectory\n         model_dir = os.path.join(self.output_dir, model_name)\n         os.makedirs(model_dir, exist_ok=True)\n-        \n+\n         # Create filename with timestamp\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         filename = f\"{model_name}_benchmark_{timestamp}.json\"\n         filepath = os.path.join(model_dir, filename)\n-        \n+\n         # Prepare output structure\n-        output_data = {\n-            \"model_name\": model_name,\n-            \"benchmark_scenarios\": []\n-        }\n-        \n+        output_data = {\"model_name\": model_name, \"benchmark_scenarios\": []}\n+\n         for config_name, config_results in results.items():\n             scenario = {\n                 \"scenario_name\": config_name,\n                 \"metadata\": config_results[\"metadata\"],\n                 \"measurements\": config_results[\"measurements\"],\n-                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {})\n+                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {}),\n             }\n             output_data[\"benchmark_scenarios\"].append(scenario)\n-        \n+\n         # Save to JSON file\n-        with open(filepath, 'w') as f:\n+        with open(filepath, \"w\") as f:\n             json.dump(output_data, f, indent=2, default=str)\n-        \n+\n         self.logger.info(f\"Results saved to {filepath}\")\n         return filepath\n- \n\\ No newline at end of file"
        },
        {
            "sha": "26c816b9d16d42ed1056d01f53ad13a59b8b0c5c",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 125,
            "deletions": 170,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -14,350 +14,304 @@\n # limitations under the License.\n \n \"\"\"\n-Top-level benchmarking script that automatically discovers and runs all benchmarks \n+Top-level benchmarking script that automatically discovers and runs all benchmarks\n in the ./benches directory, organizing outputs into model-specific subfolders.\n \"\"\"\n \n import argparse\n import importlib.util\n+import json\n import logging\n import os\n import sys\n-import json\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional\n+from typing import Any, Optional\n \n \n def setup_logging(log_level: str = \"INFO\", enable_file_logging: bool = False) -> logging.Logger:\n     \"\"\"Setup logging configuration.\"\"\"\n     numeric_level = getattr(logging, log_level.upper(), None)\n     if not isinstance(numeric_level, int):\n-        raise ValueError(f'Invalid log level: {log_level}')\n-    \n+        raise ValueError(f\"Invalid log level: {log_level}\")\n+\n     handlers = [logging.StreamHandler(sys.stdout)]\n-    \n+\n     if enable_file_logging:\n-        handlers.append(\n-            logging.FileHandler(f'benchmark_run_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n-        )\n-    \n+        handlers.append(logging.FileHandler(f\"benchmark_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"))\n+\n     logging.basicConfig(\n-        level=numeric_level,\n-        format='[%(levelname)s - %(asctime)s] %(name)s: %(message)s',\n-        handlers=handlers\n+        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n     )\n-    \n+\n     return logging.getLogger(__name__)\n \n \n-def discover_benchmarks(benches_dir: str) -> List[Dict[str, Any]]:\n+def discover_benchmarks(benches_dir: str) -> list[dict[str, Any]]:\n     \"\"\"\n     Discover all benchmark modules in the benches directory.\n-    \n+\n     Returns:\n         List of dictionaries containing benchmark module info\n     \"\"\"\n     benchmarks = []\n     benches_path = Path(benches_dir)\n-    \n+\n     if not benches_path.exists():\n         raise FileNotFoundError(f\"Benches directory not found: {benches_dir}\")\n-    \n+\n     for py_file in benches_path.glob(\"*.py\"):\n         if py_file.name.startswith(\"__\"):\n             continue\n-            \n+\n         module_name = py_file.stem\n-        \n+\n         try:\n             # Import the module\n             spec = importlib.util.spec_from_file_location(module_name, py_file)\n             module = importlib.util.module_from_spec(spec)\n             spec.loader.exec_module(module)\n-            \n+\n             # Check if it has a benchmark runner function\n-            if hasattr(module, f'run_{module_name}'):\n-                benchmarks.append({\n-                    'name': module_name,\n-                    'path': str(py_file),\n-                    'module': module,\n-                    'runner_function': getattr(module, f'run_{module_name}')\n-                })\n-            elif hasattr(module, 'run_benchmark'):\n-                benchmarks.append({\n-                    'name': module_name,\n-                    'path': str(py_file),\n-                    'module': module,\n-                    'runner_function': getattr(module, 'run_benchmark')\n-                })\n+            if hasattr(module, f\"run_{module_name}\"):\n+                benchmarks.append(\n+                    {\n+                        \"name\": module_name,\n+                        \"path\": str(py_file),\n+                        \"module\": module,\n+                        \"runner_function\": getattr(module, f\"run_{module_name}\"),\n+                    }\n+                )\n+            elif hasattr(module, \"run_benchmark\"):\n+                benchmarks.append(\n+                    {\n+                        \"name\": module_name,\n+                        \"path\": str(py_file),\n+                        \"module\": module,\n+                        \"runner_function\": getattr(module, \"run_benchmark\"),\n+                    }\n+                )\n             else:\n                 logging.warning(f\"No runner function found in {py_file}\")\n-                \n+\n         except Exception as e:\n             logging.error(f\"Failed to import {py_file}: {e}\")\n-            \n+\n     return benchmarks\n \n \n def run_single_benchmark(\n-    benchmark_info: Dict[str, Any], \n-    output_dir: str,\n-    logger: logging.Logger,\n-    **kwargs\n+    benchmark_info: dict[str, Any], output_dir: str, logger: logging.Logger, **kwargs\n ) -> Optional[str]:\n     \"\"\"\n     Run a single benchmark and return the output file path.\n-    \n+\n     Args:\n         benchmark_info: Dictionary containing benchmark module info\n         output_dir: Base output directory\n         logger: Logger instance\n         **kwargs: Additional arguments to pass to the benchmark\n-        \n+\n     Returns:\n         Path to the output file if successful, None otherwise\n     \"\"\"\n-    benchmark_name = benchmark_info['name']\n-    runner_func = benchmark_info['runner_function']\n-    \n+    benchmark_name = benchmark_info[\"name\"]\n+    runner_func = benchmark_info[\"runner_function\"]\n+\n     logger.info(f\"Running benchmark: {benchmark_name}\")\n-    \n+\n     try:\n         # Check function signature to determine what arguments to pass\n         import inspect\n+\n         sig = inspect.signature(runner_func)\n-        \n+\n         # Prepare arguments based on function signature\n-        func_kwargs = {\n-            'logger': logger,\n-            'output_dir': output_dir\n-        }\n-        \n+        func_kwargs = {\"logger\": logger, \"output_dir\": output_dir}\n+\n         # Add other kwargs if the function accepts them\n         for param_name in sig.parameters:\n             if param_name in kwargs:\n                 func_kwargs[param_name] = kwargs[param_name]\n-        \n+\n         # Filter kwargs to only include parameters the function accepts\n         # If function has **kwargs, include all provided kwargs\n         has_var_kwargs = any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values())\n         if has_var_kwargs:\n             valid_kwargs = {**func_kwargs, **kwargs}\n         else:\n-            valid_kwargs = {k: v for k, v in func_kwargs.items() \n-                           if k in sig.parameters}\n-        \n+            valid_kwargs = {k: v for k, v in func_kwargs.items() if k in sig.parameters}\n+\n         # Run the benchmark\n         result = runner_func(**valid_kwargs)\n-        \n+\n         if isinstance(result, str):\n             # Function returned a file path\n             return result\n         else:\n             logger.info(f\"Benchmark {benchmark_name} completed successfully\")\n             return \"completed\"\n-            \n+\n     except Exception as e:\n         logger.error(f\"Benchmark {benchmark_name} failed: {e}\")\n         import traceback\n+\n         logger.debug(traceback.format_exc())\n         return None\n \n \n-def generate_summary_report(\n-    output_dir: str, \n-    benchmark_results: Dict[str, Any],\n-    logger: logging.Logger\n-) -> str:\n+def generate_summary_report(output_dir: str, benchmark_results: dict[str, Any], logger: logging.Logger) -> str:\n     \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n-    \n+\n     summary_data = {\n         \"run_metadata\": {\n             \"timestamp\": datetime.utcnow().isoformat(),\n             \"total_benchmarks\": len(benchmark_results),\n             \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n-            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None])\n+            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None]),\n         },\n         \"benchmark_results\": benchmark_results,\n-        \"output_directory\": output_dir\n+        \"output_directory\": output_dir,\n     }\n-    \n-    with open(summary_file, 'w') as f:\n+\n+    with open(summary_file, \"w\") as f:\n         json.dump(summary_data, f, indent=2, default=str)\n-    \n+\n     logger.info(f\"Summary report saved to: {summary_file}\")\n     return summary_file\n \n \n def main():\n     \"\"\"Main entry point for the benchmarking script.\"\"\"\n-    parser = argparse.ArgumentParser(\n-        description=\"Run all benchmarks in the ./benches directory\"\n-    )\n-    \n+    parser = argparse.ArgumentParser(description=\"Run all benchmarks in the ./benches directory\")\n+\n     parser.add_argument(\n         \"--output-dir\",\n         type=str,\n         default=\"benchmark_results\",\n-        help=\"Base output directory for benchmark results (default: benchmark_results)\"\n+        help=\"Base output directory for benchmark results (default: benchmark_results)\",\n     )\n-    \n+\n     parser.add_argument(\n         \"--benches-dir\",\n         type=str,\n         default=\"./benches\",\n-        help=\"Directory containing benchmark implementations (default: ./benches)\"\n+        help=\"Directory containing benchmark implementations (default: ./benches)\",\n     )\n-    \n+\n     parser.add_argument(\n         \"--log-level\",\n         type=str,\n         choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n         default=\"INFO\",\n-        help=\"Logging level (default: INFO)\"\n+        help=\"Logging level (default: INFO)\",\n     )\n-    \n-    parser.add_argument(\n-        \"--model-id\",\n-        type=str,\n-        help=\"Specific model ID to benchmark (if supported by benchmarks)\"\n-    )\n-    \n+\n+    parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n+\n+    parser.add_argument(\"--warmup-iterations\", type=int, default=3, help=\"Number of warmup iterations (default: 3)\")\n+\n     parser.add_argument(\n-        \"--warmup-iterations\",\n-        type=int,\n-        default=3,\n-        help=\"Number of warmup iterations (default: 3)\"\n+        \"--measurement-iterations\", type=int, default=5, help=\"Number of measurement iterations (default: 5)\"\n     )\n-    \n-    parser.add_argument(\n-        \"--measurement-iterations\",\n-        type=int,\n-        default=5,\n-        help=\"Number of measurement iterations (default: 5)\"\n-    )\n-    \n+\n     parser.add_argument(\n         \"--num-tokens-to-generate\",\n         type=int,\n         default=100,\n-        help=\"Number of tokens to generate in benchmarks (default: 100)\"\n-    )\n-    \n-    parser.add_argument(\n-        \"--include\",\n-        type=str,\n-        nargs=\"*\",\n-        help=\"Only run benchmarks matching these names\"\n+        help=\"Number of tokens to generate in benchmarks (default: 100)\",\n     )\n-    \n-    parser.add_argument(\n-        \"--exclude\",\n-        type=str,\n-        nargs=\"*\",\n-        help=\"Exclude benchmarks matching these names\"\n-    )\n-    \n-    parser.add_argument(\n-        \"--enable-mock\",\n-        action=\"store_true\",\n-        help=\"Enable mock benchmark (skipped by default)\"\n-    )\n-    \n-    parser.add_argument(\n-        \"--enable-file-logging\",\n-        action=\"store_true\",\n-        help=\"Enable file logging (disabled by default)\"\n-    )\n-    \n+\n+    parser.add_argument(\"--include\", type=str, nargs=\"*\", help=\"Only run benchmarks matching these names\")\n+\n+    parser.add_argument(\"--exclude\", type=str, nargs=\"*\", help=\"Exclude benchmarks matching these names\")\n+\n+    parser.add_argument(\"--enable-mock\", action=\"store_true\", help=\"Enable mock benchmark (skipped by default)\")\n+\n+    parser.add_argument(\"--enable-file-logging\", action=\"store_true\", help=\"Enable file logging (disabled by default)\")\n+\n     parser.add_argument(\n-        \"--commit-id\",\n-        type=str,\n-        help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n+        \"--commit-id\", type=str, help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n     )\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     # Setup logging\n     logger = setup_logging(args.log_level, args.enable_file_logging)\n-    \n+\n     logger.info(\"Starting benchmark discovery and execution\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n     logger.info(f\"Benches directory: {args.benches_dir}\")\n-    \n+\n     # Create output directory\n     os.makedirs(args.output_dir, exist_ok=True)\n-    \n+\n     try:\n         # Discover benchmarks\n         benchmarks = discover_benchmarks(args.benches_dir)\n         logger.info(f\"Discovered {len(benchmarks)} benchmark(s): {[b['name'] for b in benchmarks]}\")\n-        \n+\n         if not benchmarks:\n             logger.warning(\"No benchmarks found!\")\n             return 1\n-        \n+\n         # Filter benchmarks based on include/exclude\n         filtered_benchmarks = benchmarks\n-        \n+\n         if args.include:\n-            filtered_benchmarks = [b for b in filtered_benchmarks \n-                                 if any(pattern in b['name'] for pattern in args.include)]\n+            filtered_benchmarks = [\n+                b for b in filtered_benchmarks if any(pattern in b[\"name\"] for pattern in args.include)\n+            ]\n             logger.info(f\"Filtered to include: {[b['name'] for b in filtered_benchmarks]}\")\n-        \n+\n         if args.exclude:\n-            filtered_benchmarks = [b for b in filtered_benchmarks \n-                                 if not any(pattern in b['name'] for pattern in args.exclude)]\n+            filtered_benchmarks = [\n+                b for b in filtered_benchmarks if not any(pattern in b[\"name\"] for pattern in args.exclude)\n+            ]\n             logger.info(f\"After exclusion: {[b['name'] for b in filtered_benchmarks]}\")\n-        \n+\n         if not filtered_benchmarks:\n             logger.warning(\"No benchmarks remaining after filtering!\")\n             return 1\n-        \n+\n         # Prepare common kwargs for benchmarks\n         benchmark_kwargs = {\n-            'warmup_iterations': args.warmup_iterations,\n-            'measurement_iterations': args.measurement_iterations,\n-            'num_tokens_to_generate': args.num_tokens_to_generate\n+            \"warmup_iterations\": args.warmup_iterations,\n+            \"measurement_iterations\": args.measurement_iterations,\n+            \"num_tokens_to_generate\": args.num_tokens_to_generate,\n         }\n-        \n+\n         if args.model_id:\n-            benchmark_kwargs['model_id'] = args.model_id\n-        \n+            benchmark_kwargs[\"model_id\"] = args.model_id\n+\n         # Add enable_mock flag for mock benchmark\n-        benchmark_kwargs['enable_mock'] = args.enable_mock\n-        \n+        benchmark_kwargs[\"enable_mock\"] = args.enable_mock\n+\n         # Add commit_id if provided\n         if args.commit_id:\n-            benchmark_kwargs['commit_id'] = args.commit_id\n-        \n+            benchmark_kwargs[\"commit_id\"] = args.commit_id\n+\n         # Run benchmarks\n         benchmark_results = {}\n         successful_count = 0\n-        \n+\n         for benchmark_info in filtered_benchmarks:\n-            result = run_single_benchmark(\n-                benchmark_info,\n-                args.output_dir,\n-                logger,\n-                **benchmark_kwargs\n-            )\n-            \n-            benchmark_results[benchmark_info['name']] = result\n-            \n+            result = run_single_benchmark(benchmark_info, args.output_dir, logger, **benchmark_kwargs)\n+\n+            benchmark_results[benchmark_info[\"name\"]] = result\n+\n             if result is not None:\n                 successful_count += 1\n-        \n+\n         # Generate summary report\n         summary_file = generate_summary_report(args.output_dir, benchmark_results, logger)\n-        \n+\n         # Final summary\n         total_benchmarks = len(filtered_benchmarks)\n         failed_count = total_benchmarks - successful_count\n-        \n+\n         logger.info(\"=\" * 60)\n         logger.info(\"BENCHMARK RUN SUMMARY\")\n         logger.info(\"=\" * 60)\n@@ -366,20 +320,21 @@ def main():\n         logger.info(f\"Failed: {failed_count}\")\n         logger.info(f\"Output directory: {args.output_dir}\")\n         logger.info(f\"Summary report: {summary_file}\")\n-        \n+\n         if failed_count > 0:\n             logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n             return 1\n         else:\n             logger.info(\"All benchmarks completed successfully!\")\n             return 0\n-            \n+\n     except Exception as e:\n         logger.error(f\"Benchmark run failed: {e}\")\n         import traceback\n+\n         logger.debug(traceback.format_exc())\n         return 1\n \n \n if __name__ == \"__main__\":\n-    sys.exit(main()) \n\\ No newline at end of file\n+    sys.exit(main())"
        },
        {
            "sha": "38e6965f4f8085f533e48d2cc1da67874691dc1d",
            "filename": "scripts/check_tokenizers.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/scripts%2Fcheck_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/scripts%2Fcheck_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fcheck_tokenizers.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -4,8 +4,8 @@\n \n import transformers\n from transformers.convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS\n-from transformers.utils import logging\n from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n+from transformers.utils import logging\n \n \n logging.set_verbosity_info()\n@@ -22,7 +22,9 @@\n wrong = 0\n \n \n-def check_diff(spm_diff: list[int], tok_diff: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> bool:\n+def check_diff(\n+    spm_diff: list[int], tok_diff: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase\n+) -> bool:\n     if spm_diff == list(reversed(tok_diff)):\n         # AAA -> AA+A vs A+AA case.\n         return True\n@@ -54,7 +56,9 @@ def check_LTR_mark(line: str, idx: int, fast: PreTrainedTokenizerBase) -> bool:\n     return False\n \n \n-def check_details(line: str, spm_ids: list[int], tok_ids: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> bool:\n+def check_details(\n+    line: str, spm_ids: list[int], tok_ids: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase\n+) -> bool:\n     # Encoding can be the same with same result AAA -> A + AA vs AA + A\n     # We can check that we use at least exactly the same number of tokens.\n     for i, (spm_id, tok_id) in enumerate(zip(spm_ids, tok_ids)):\n@@ -90,7 +94,9 @@ def check_details(line: str, spm_ids: list[int], tok_ids: list[int], slow: PreTr\n                     if tok_ids[first + k : first + k + min_width] == spm_ids[first + i : first + i + min_width]\n                 ]\n                 for j in possible_matches:\n-                    if check_diff(spm_ids[first : first + i], tok_ids[first : first + j], slow, fast) and check_details(\n+                    if check_diff(\n+                        spm_ids[first : first + i], tok_ids[first : first + j], slow, fast\n+                    ) and check_details(\n                         line,\n                         spm_ids[first + i : last],\n                         tok_ids[first + j : last],\n@@ -140,9 +146,9 @@ def test_string(slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase, te\n     if skip_assert:\n         return\n \n-    assert (\n-        slow_ids == fast_ids\n-    ), f\"line {text} : \\n\\n{slow_ids}\\n{fast_ids}\\n\\n{slow.tokenize(text)}\\n{fast.tokenize(text)}\"\n+    assert slow_ids == fast_ids, (\n+        f\"line {text} : \\n\\n{slow_ids}\\n{fast_ids}\\n\\n{slow.tokenize(text)}\\n{fast.tokenize(text)}\"\n+    )\n \n \n def test_tokenizer(slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase) -> None:"
        },
        {
            "sha": "81bab2a5b2432962e5dbad4e2c6f13f3ac4536ef",
            "filename": "scripts/stale.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/scripts%2Fstale.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c/scripts%2Fstale.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fstale.py?ref=a5ecd94a3f2addbc4b79594db1e5e3b0f7fb101c",
            "patch": "@@ -15,6 +15,7 @@\n Script to close stale issue. Taken in part from the AllenNLP repository.\n https://github.com/allenai/allennlp.\n \"\"\"\n+\n import os\n from datetime import datetime as dt\n \n@@ -39,10 +40,11 @@ def main():\n \n     for i, issue in enumerate(open_issues):\n         print(i, issue)\n-        comments = sorted(list(issue.get_comments()), key=lambda i: i.created_at, reverse=True)\n+        comments = sorted(issue.get_comments(), key=lambda i: i.created_at, reverse=True)\n         last_comment = comments[0] if len(comments) > 0 else None\n         if (\n-            last_comment is not None and last_comment.user.login == \"github-actions[bot]\"\n+            last_comment is not None\n+            and last_comment.user.login == \"github-actions[bot]\"\n             and (dt.utcnow() - issue.updated_at.replace(tzinfo=None)).days > 7\n             and (dt.utcnow() - issue.created_at.replace(tzinfo=None)).days >= 30\n             and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())"
        }
    ],
    "stats": {
        "total": 1300,
        "additions": 656,
        "deletions": 644
    }
}