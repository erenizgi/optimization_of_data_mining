{
    "author": "yonigozlan",
    "message": "Falcon-H1 - Fix auto_docstring and add can_return_tuple decorator (#38260)\n\nFix auto_docstring and add can_return_tuple",
    "sha": "f5307272f531de6a4fec0f843480c4f46de0dfdf",
    "files": [
        {
            "sha": "0a4d8f4327772ff27762a7124945861cf34a8774",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 33,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5307272f531de6a4fec0f843480c4f46de0dfdf/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5307272f531de6a4fec0f843480c4f46de0dfdf/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=f5307272f531de6a4fec0f843480c4f46de0dfdf",
            "patch": "@@ -45,8 +45,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torchdynamo_compiling, logging, replace_return_docstrings\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -65,8 +64,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"FalconH1Config\"\n-\n \n class FalconHybridMambaAttentionDynamicCache(DynamicCache):\n     \"\"\"\n@@ -1224,13 +1221,6 @@ def compute_mup_vector(config):\n @auto_docstring\n # Adapted from transformers.models.jamba.modeling_jamba.JambaModel\n class FalconH1Model(FalconH1PreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`FalconH1DecoderLayer`]\n-\n-    Args:\n-        config: FalconH1Config\n-    \"\"\"\n-\n     def __init__(self, config: FalconH1Config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1264,6 +1254,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1275,7 +1266,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n@@ -1285,8 +1275,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1356,8 +1344,6 @@ def forward(\n \n         next_cache = None if not use_cache else past_key_values\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1526,9 +1512,8 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n+    @can_return_tuple\n     @auto_docstring\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1540,7 +1525,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n@@ -1550,14 +1534,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-            If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-            `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-            token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-            If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-            This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n \n         Example:\n \n@@ -1579,7 +1555,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n@@ -1591,7 +1566,6 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1605,10 +1579,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "bd0ecb1804d174a53edef20724dc8eb17aaa2aed",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 52,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5307272f531de6a4fec0f843480c4f46de0dfdf/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5307272f531de6a4fec0f843480c4f46de0dfdf/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=f5307272f531de6a4fec0f843480c4f46de0dfdf",
            "patch": "@@ -51,24 +51,11 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    auto_docstring,\n-    is_torchdynamo_compiling,\n-    logging,\n-    replace_return_docstrings,\n-)\n-from ...utils.deprecation import deprecate_kwarg\n-from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_flash_attn_2_available,\n-    is_mamba_2_ssm_available,\n-)\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n \n-if is_flash_attn_2_available():\n-    pass\n-\n if is_mamba_2_ssm_available():\n     from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n     from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n@@ -85,8 +72,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"FalconH1Config\"\n-\n \n class FalconHybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache):\n     \"\"\"\n@@ -1011,13 +996,6 @@ def compute_mup_vector(config):\n @auto_docstring\n # Adapted from transformers.models.jamba.modeling_jamba.JambaModel\n class FalconH1Model(FalconH1PreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`FalconH1DecoderLayer`]\n-\n-    Args:\n-        config: FalconH1Config\n-    \"\"\"\n-\n     def __init__(self, config: FalconH1Config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1051,6 +1029,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1062,7 +1041,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n@@ -1072,8 +1050,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1143,8 +1119,6 @@ def forward(\n \n         next_cache = None if not use_cache else past_key_values\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1281,9 +1255,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n \n class FalconH1ForCausalLM(LlamaForCausalLM):\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n-    @auto_docstring\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1295,25 +1266,11 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-            If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-            `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-            token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-            If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-            This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -1334,7 +1291,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n@@ -1346,7 +1302,6 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1360,10 +1315,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        }
    ],
    "stats": {
        "total": 91,
        "additions": 6,
        "deletions": 85
    }
}