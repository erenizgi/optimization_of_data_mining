{
    "author": "qubvel",
    "message": "Deprecate modeling_utils.py classes (#37298)\n\n* Move utils classes into models\n\n* Add deprecation warnings\n\n* Remove from docs\n\n* Update config attributes check",
    "sha": "4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
    "files": [
        {
            "sha": "fe6f961da946ceb002f3c442411633ddd9c99f29",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -33,23 +33,6 @@ Most of those are only useful if you are studying the code of the models in the\n \n [[autodoc]] pytorch_utils.Conv1D\n \n-[[autodoc]] modeling_utils.PoolerStartLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerEndLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerAnswerClass\n-    - forward\n-\n-[[autodoc]] modeling_utils.SquadHeadOutput\n-\n-[[autodoc]] modeling_utils.SQuADHead\n-    - forward\n-\n-[[autodoc]] modeling_utils.SequenceSummary\n-    - forward\n-\n ## PyTorch Helper Functions\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward"
        },
        {
            "sha": "6e8335623a0153d9d15d07d92a8eb75a3a9c0e9d",
            "filename": "docs/source/ja/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -25,23 +25,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.Conv1D\n \n-[[autodoc]] modeling_utils.PoolerStartLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerEndLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerAnswerClass\n-    - forward\n-\n-[[autodoc]] modeling_utils.SquadHeadOutput\n-\n-[[autodoc]] modeling_utils.SQuADHead\n-    - forward\n-\n-[[autodoc]] modeling_utils.SequenceSummary\n-    - forward\n-\n ## PyTorch Helper Functions\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward"
        },
        {
            "sha": "f84ae30cd6f54555e6390f2f9b9464450f651f4e",
            "filename": "docs/source/ko/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -25,23 +25,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.Conv1D\n \n-[[autodoc]] modeling_utils.PoolerStartLogits\n-   - forward\n-\n-[[autodoc]] modeling_utils.PoolerEndLogits\n-   - forward\n-\n-[[autodoc]] modeling_utils.PoolerAnswerClass\n-   - forward\n-\n-[[autodoc]] modeling_utils.SquadHeadOutput\n-\n-[[autodoc]] modeling_utils.SQuADHead\n-   - forward\n-\n-[[autodoc]] modeling_utils.SequenceSummary\n-   - forward\n-\n ## PyTorch 헬퍼(helper) 함수 [[transformers.apply_chunking_to_forward]]\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward"
        },
        {
            "sha": "2cc62711c717ec6cc93bfcd3215e2e5620631ed5",
            "filename": "docs/source/zh/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -25,23 +25,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.Conv1D\n \n-[[autodoc]] modeling_utils.PoolerStartLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerEndLogits\n-    - forward\n-\n-[[autodoc]] modeling_utils.PoolerAnswerClass\n-    - forward\n-\n-[[autodoc]] modeling_utils.SquadHeadOutput\n-\n-[[autodoc]] modeling_utils.SQuADHead\n-    - forward\n-\n-[[autodoc]] modeling_utils.SequenceSummary\n-    - forward\n-\n ## PyTorch帮助函数\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward"
        },
        {
            "sha": "50a200ae76bcfd1d252f35d9b98f31e260a15de7",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -5384,6 +5384,10 @@ class PoolerStartLogits(nn.Module):\n     def __init__(self, config: PretrainedConfig):\n         super().__init__()\n         self.dense = nn.Linear(config.hidden_size, 1)\n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `PoolerStartLogits` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMPoolerStartLogits`.\"\n+        )\n \n     def forward(\n         self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n@@ -5426,6 +5430,10 @@ def __init__(self, config: PretrainedConfig):\n         self.activation = nn.Tanh()\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dense_1 = nn.Linear(config.hidden_size, 1)\n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `PoolerEndLogits` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMPoolerEndLogits`.\"\n+        )\n \n     def forward(\n         self,\n@@ -5493,6 +5501,10 @@ def __init__(self, config):\n         self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n         self.activation = nn.Tanh()\n         self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `PoolerAnswerClass` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMPoolerAnswerClass`.\"\n+        )\n \n     def forward(\n         self,\n@@ -5574,6 +5586,12 @@ class SquadHeadOutput(ModelOutput):\n     end_top_index: Optional[torch.LongTensor] = None\n     cls_logits: Optional[torch.FloatTensor] = None\n \n+    def __post_init__(self):\n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `SquadHeadOutput` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMSquadHeadOutput`.\"\n+        )\n+\n \n class SQuADHead(nn.Module):\n     r\"\"\"\n@@ -5594,6 +5612,11 @@ def __init__(self, config):\n         self.end_logits = PoolerEndLogits(config)\n         self.answer_class = PoolerAnswerClass(config)\n \n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `SQuADHead` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMSQuADHead`.\"\n+        )\n+\n     @replace_return_docstrings(output_type=SquadHeadOutput, config_class=PretrainedConfig)\n     def forward(\n         self,\n@@ -5747,6 +5770,11 @@ def __init__(self, config: PretrainedConfig):\n         if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n             self.last_dropout = nn.Dropout(config.summary_last_dropout)\n \n+        logger.warning_once(\n+            \"[DEPRECATION WARNING] `SequenceSummary` is deprecated and will be removed in v4.53. \"\n+            \"Please use model-specific class, e.g. `XLMSequenceSummary`.\"\n+        )\n+\n     def forward(\n         self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n     ) -> torch.FloatTensor:"
        },
        {
            "sha": "bafaa36dd671bdd7c23d20b9c12fa79c3c2bf400",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 104,
            "deletions": 4,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -18,14 +18,14 @@\n import copy\n import math\n from dataclasses import dataclass\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Callable, Dict, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n-from ...activations import ACT2FN\n+from ...activations import ACT2FN, get_activation\n from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -34,7 +34,7 @@\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithCrossAttentions,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import Conv1D, isin_mps_friendly\n from ...utils import (\n     ModelOutput,\n@@ -499,6 +499,106 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Clvp\n+class ClvpSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`ClvpConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: ClvpConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n # Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP with GPT2->ClvpDecoderMLP\n class ClvpDecoderMLP(nn.Module):\n     def __init__(self, intermediate_size, config):\n@@ -884,7 +984,7 @@ def __init__(self, config: ClvpConfig):\n         self.rotary_pos_emb = ClvpRotaryPositionalEmbedding(config) if config.use_rotary_embedding else None\n         self.layers = nn.ModuleList([ClvpEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n \n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = ClvpSequenceSummary(config)\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n         self.projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)"
        },
        {
            "sha": "486e678b5964e7266cc18903579fb4071ed3c94d",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 103,
            "deletions": 3,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import os\n from operator import attrgetter\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -33,7 +33,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_convbert import ConvBertConfig\n@@ -683,6 +683,106 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->ConvBert\n+class ConvBertSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`ConvBertConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: ConvBertConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n CONVBERT_START_DOCSTRING = r\"\"\"\n     This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n     it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n@@ -1077,7 +1177,7 @@ def __init__(self, config):\n         super().__init__(config)\n \n         self.convbert = ConvBertModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = ConvBertSequenceSummary(config)\n         self.classifier = nn.Linear(config.hidden_size, 1)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "08cc3e530d6f4a1753dd1e178ec178993e6c951d",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 103,
            "deletions": 3,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -36,7 +36,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -946,6 +946,106 @@ def forward(self, features, **kwargs):\n         return x\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Electra\n+class ElectraSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`ElectraConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: ElectraConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     ELECTRA Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n@@ -1442,7 +1542,7 @@ def __init__(self, config):\n         super().__init__(config)\n \n         self.electra = ElectraModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = ElectraSequenceSummary(config)\n         self.classifier = nn.Linear(config.hidden_size, 1)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "b5a5ea793ae7e6b9fbc77b9844484df3d7f83bc1",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 437,
            "deletions": 12,
            "changes": 449,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -17,14 +17,14 @@\n import itertools\n import math\n from dataclasses import dataclass\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Callable, Dict, Optional, Tuple, Union\n \n import numpy as np\n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import gelu\n+from ...activations import gelu, get_activation\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -34,7 +34,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary, SQuADHead\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -329,6 +329,431 @@ def forward(self, x, y=None):\n         return outputs\n \n \n+@dataclass\n+# Copied from transformers.models.xlm.modeling_xlm.XLMSquadHeadOutput with XLM->Flaubert\n+class FlaubertSquadHeadOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of question answering models using a [`~modeling_utils.FlaubertSQuADHead`].\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n+            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n+            losses.\n+        start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n+        start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Indices for the top config.start_n_top start token possibilities (beam-search).\n+        end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n+            (beam-search).\n+        end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n+        cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the `is_impossible` label of the answers.\n+\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    start_top_log_probs: Optional[torch.FloatTensor] = None\n+    start_top_index: Optional[torch.LongTensor] = None\n+    end_top_log_probs: Optional[torch.FloatTensor] = None\n+    end_top_index: Optional[torch.LongTensor] = None\n+    cls_logits: Optional[torch.FloatTensor] = None\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerStartLogits with XLM->Flaubert\n+class FlaubertPoolerStartLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD start logits from sequence hidden states.\n+\n+    Args:\n+        config ([`FlaubertConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: FlaubertConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        Returns:\n+            `torch.FloatTensor`: The start logits for SQuAD.\n+        \"\"\"\n+        x = self.dense(hidden_states).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerEndLogits with XLM->Flaubert\n+class FlaubertPoolerEndLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD end logits from sequence hidden states.\n+\n+    Args:\n+        config ([`FlaubertConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n+            to use.\n+    \"\"\"\n+\n+    def __init__(self, config: FlaubertConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dense_1 = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        p_mask: Optional[torch.FloatTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The end logits for SQuAD.\n+        \"\"\"\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            slen, hsz = hidden_states.shape[-2:]\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n+            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n+\n+        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n+        x = self.activation(x)\n+        x = self.LayerNorm(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerAnswerClass with XLM->Flaubert\n+class FlaubertPoolerAnswerClass(nn.Module):\n+    \"\"\"\n+    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n+\n+    Args:\n+        config ([`FlaubertConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: FlaubertConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        cls_index: Optional[torch.LongTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The SQuAD 2.0 answer class.\n+        \"\"\"\n+        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n+        hsz = hidden_states.shape[-1]\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n+\n+        if cls_index is not None:\n+            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n+        else:\n+            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n+\n+        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n+        x = self.activation(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMSQuADHead with XLM->Flaubert\n+class FlaubertSQuADHead(nn.Module):\n+    r\"\"\"\n+    A SQuAD head inspired by XLNet.\n+\n+    Args:\n+        config ([`FlaubertConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n+            to use.\n+    \"\"\"\n+\n+    def __init__(self, config: FlaubertConfig):\n+        super().__init__()\n+        self.start_n_top = config.start_n_top\n+        self.end_n_top = config.end_n_top\n+\n+        self.start_logits = FlaubertPoolerStartLogits(config)\n+        self.end_logits = FlaubertPoolerEndLogits(config)\n+        self.answer_class = FlaubertPoolerAnswerClass(config)\n+\n+    @replace_return_docstrings(output_type=FlaubertSquadHeadOutput, config_class=FlaubertConfig)\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        cls_index: Optional[torch.LongTensor] = None,\n+        is_impossible: Optional[torch.LongTensor] = None,\n+        p_mask: Optional[torch.FloatTensor] = None,\n+        return_dict: bool = False,\n+    ) -> Union[FlaubertSquadHeadOutput, Tuple[torch.FloatTensor]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                Final hidden states of the model on the sequence tokens.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Positions of the first token for the labeled span.\n+            end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Positions of the last token for the labeled span.\n+            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n+            is_impossible (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Whether the question has a possible answer in the paragraph or not.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+            return_dict (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\n+        Returns:\n+        \"\"\"\n+        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n+\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n+            for x in (start_positions, end_positions, cls_index, is_impossible):\n+                if x is not None and x.dim() > 1:\n+                    x.squeeze_(-1)\n+\n+            # during training, compute the end logits based on the ground truth of the start position\n+            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n+\n+            loss_fct = CrossEntropyLoss()\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+            if cls_index is not None and is_impossible is not None:\n+                # Predict answerability from the representation of CLS and START\n+                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n+                loss_fct_cls = nn.BCEWithLogitsLoss()\n+                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n+\n+                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n+                total_loss += cls_loss * 0.5\n+\n+            return FlaubertSquadHeadOutput(loss=total_loss) if return_dict else (total_loss,)\n+\n+        else:\n+            # during inference, compute the end logits based on beam search\n+            bsz, slen, hsz = hidden_states.size()\n+            start_log_probs = nn.functional.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n+\n+            start_top_log_probs, start_top_index = torch.topk(\n+                start_log_probs, self.start_n_top, dim=-1\n+            )  # shape (bsz, start_n_top)\n+            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n+            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n+            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n+\n+            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n+                start_states\n+            )  # shape (bsz, slen, start_n_top, hsz)\n+            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n+            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n+            end_log_probs = nn.functional.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n+\n+            end_top_log_probs, end_top_index = torch.topk(\n+                end_log_probs, self.end_n_top, dim=1\n+            )  # shape (bsz, end_n_top, start_n_top)\n+            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n+            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n+\n+            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n+            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n+\n+            if not return_dict:\n+                return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n+            else:\n+                return FlaubertSquadHeadOutput(\n+                    start_top_log_probs=start_top_log_probs,\n+                    start_top_index=start_top_index,\n+                    end_top_log_probs=end_top_log_probs,\n+                    end_top_index=end_top_index,\n+                    cls_logits=cls_logits,\n+                )\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Flaubert\n+class FlaubertSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`FlaubertConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: FlaubertConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n # Copied from transformers.models.xlm.modeling_xlm.XLMPreTrainedModel with XLM->Flaubert\n class FlaubertPreTrainedModel(PreTrainedModel):\n     \"\"\"\n@@ -744,15 +1169,15 @@ def forward(\n     \"\"\",\n     FLAUBERT_START_DOCSTRING,\n )\n-# Copied transformers.models.xlm.modeling_xlm.XLMForSequenceClassification with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n+# Copied from transformers.models.xlm.modeling_xlm.XLMForSequenceClassification with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n class FlaubertForSequenceClassification(FlaubertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.config = config\n \n         self.transformer = FlaubertModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = FlaubertSequenceSummary(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1081,13 +1506,13 @@ class FlaubertForQuestionAnsweringOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-# Copied from transformer.models.xlm.modeling_xlm.XLMForQuestionAnswering with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n+# Copied from transformers.models.xlm.modeling_xlm.XLMForQuestionAnswering with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n class FlaubertForQuestionAnswering(FlaubertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n \n         self.transformer = FlaubertModel(config)\n-        self.qa_outputs = SQuADHead(config)\n+        self.qa_outputs = FlaubertSQuADHead(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1137,11 +1562,11 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from transformers import XLMTokenizer, XLMForQuestionAnswering\n+        >>> from transformers import AutoTokenizer, FlaubertForQuestionAnswering\n         >>> import torch\n \n-        >>> tokenizer = XLMTokenizer.from_pretrained(\"FacebookAI/xlm-mlm-en-2048\")\n-        >>> model = XLMForQuestionAnswering.from_pretrained(\"FacebookAI/xlm-mlm-en-2048\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-mlm-en-2048\")\n+        >>> model = FlaubertForQuestionAnswering.from_pretrained(\"FacebookAI/xlm-mlm-en-2048\")\n \n         >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(\n         ...     0\n@@ -1203,13 +1628,13 @@ def forward(\n     \"\"\",\n     FLAUBERT_START_DOCSTRING,\n )\n-# Copied from transformer.models.xlm.modeling_xlm.XLMForMultipleChoice with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n+# Copied from transformers.models.xlm.modeling_xlm.XLMForMultipleChoice with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n class FlaubertForMultipleChoice(FlaubertPreTrainedModel):\n     def __init__(self, config, *inputs, **kwargs):\n         super().__init__(config, *inputs, **kwargs)\n \n         self.transformer = FlaubertModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = FlaubertSequenceSummary(config)\n         self.logits_proj = nn.Linear(config.num_labels, 1)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "717962b3377a4b763eea0b576d69b2c648b3e6b6",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 103,
            "deletions": 3,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -26,7 +26,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import ACT2FN\n+from ...activations import ACT2FN, get_activation\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n@@ -36,7 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel, SequenceSummary\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n@@ -450,6 +450,106 @@ def forward(\n         return outputs  # hidden_states, present, (attentions, cross_attentions)\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->GPT2\n+class GPT2SequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`GPT2Config`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: GPT2Config):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n class GPT2PreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -1138,7 +1238,7 @@ def __init__(self, config):\n         config.num_labels = 1\n         self.transformer = GPT2Model(config)\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n-        self.multiple_choice_head = SequenceSummary(config)\n+        self.multiple_choice_head = GPT2SequenceSummary(config)\n \n         # Model parallel\n         self.model_parallel = False"
        },
        {
            "sha": "244ad6d50a2e9bcb3a9ef8899808627db26dcefb",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 104,
            "deletions": 4,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -19,16 +19,16 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import Any, Dict, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, Optional, Tuple, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import gelu_new, silu\n+from ...activations import gelu_new, get_activation, silu\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n@@ -262,6 +262,106 @@ def forward(self, x, attention_mask=None, head_mask=None, output_attentions=Fals\n         return outputs\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->OpenAIGPT\n+class OpenAIGPTSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`OpenAIGPTConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: OpenAIGPTConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n class OpenAIGPTPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -628,7 +728,7 @@ def __init__(self, config):\n         config.num_labels = 1\n         self.transformer = OpenAIGPTModel(config)\n         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n-        self.multiple_choice_head = SequenceSummary(config)\n+        self.multiple_choice_head = OpenAIGPTSequenceSummary(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "86465153b10e980490939bf454103362418f379a",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 104,
            "deletions": 4,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -16,15 +16,15 @@\n \n import math\n import os\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import ACT2FN\n+from ...activations import ACT2FN, get_activation\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -35,7 +35,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -620,6 +620,106 @@ def forward(\n         )\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->RoFormer\n+class RoFormerSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`RoFormerConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: RoFormerConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n class RoFormerPredictionHeadTransform(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1292,7 +1392,7 @@ def __init__(self, config):\n         super().__init__(config)\n \n         self.roformer = RoFormerModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = RoFormerSequenceSummary(config)\n         self.classifier = nn.Linear(config.hidden_size, 1)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "d105711b5dc3eb8254b0a438af48e0ec4a070ff9",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -20,7 +20,8 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...modeling_utils import ModelOutput, PreTrainedModel\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_univnet import UnivNetConfig\n "
        },
        {
            "sha": "16f1d4ec3ff5031780afa97929b67e55f719efae",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 426,
            "deletions": 7,
            "changes": 433,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -19,14 +19,14 @@\n import itertools\n import math\n from dataclasses import dataclass\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Callable, Dict, Optional, Tuple, Union\n \n import numpy as np\n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import gelu\n+from ...activations import gelu, get_activation\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -36,7 +36,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary, SQuADHead\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -88,6 +88,425 @@ def get_masks(slen, lengths, causal, padding_mask=None):\n     return mask, attn_mask\n \n \n+@dataclass\n+class XLMSquadHeadOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of question answering models using a [`~modeling_utils.XLMSQuADHead`].\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n+            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n+            losses.\n+        start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n+        start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Indices for the top config.start_n_top start token possibilities (beam-search).\n+        end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n+            (beam-search).\n+        end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n+        cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+            Log probabilities for the `is_impossible` label of the answers.\n+\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    start_top_log_probs: Optional[torch.FloatTensor] = None\n+    start_top_index: Optional[torch.LongTensor] = None\n+    end_top_log_probs: Optional[torch.FloatTensor] = None\n+    end_top_index: Optional[torch.LongTensor] = None\n+    cls_logits: Optional[torch.FloatTensor] = None\n+\n+\n+class XLMPoolerStartLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD start logits from sequence hidden states.\n+\n+    Args:\n+        config ([`XLMConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: XLMConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        Returns:\n+            `torch.FloatTensor`: The start logits for SQuAD.\n+        \"\"\"\n+        x = self.dense(hidden_states).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+class XLMPoolerEndLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD end logits from sequence hidden states.\n+\n+    Args:\n+        config ([`XLMConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n+            to use.\n+    \"\"\"\n+\n+    def __init__(self, config: XLMConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dense_1 = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        p_mask: Optional[torch.FloatTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The end logits for SQuAD.\n+        \"\"\"\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            slen, hsz = hidden_states.shape[-2:]\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n+            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n+\n+        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n+        x = self.activation(x)\n+        x = self.LayerNorm(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+class XLMPoolerAnswerClass(nn.Module):\n+    \"\"\"\n+    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n+\n+    Args:\n+        config ([`XLMConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: XLMConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        cls_index: Optional[torch.LongTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The SQuAD 2.0 answer class.\n+        \"\"\"\n+        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n+        hsz = hidden_states.shape[-1]\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n+\n+        if cls_index is not None:\n+            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n+        else:\n+            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n+\n+        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n+        x = self.activation(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        return x\n+\n+\n+class XLMSQuADHead(nn.Module):\n+    r\"\"\"\n+    A SQuAD head inspired by XLNet.\n+\n+    Args:\n+        config ([`XLMConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n+            to use.\n+    \"\"\"\n+\n+    def __init__(self, config: XLMConfig):\n+        super().__init__()\n+        self.start_n_top = config.start_n_top\n+        self.end_n_top = config.end_n_top\n+\n+        self.start_logits = XLMPoolerStartLogits(config)\n+        self.end_logits = XLMPoolerEndLogits(config)\n+        self.answer_class = XLMPoolerAnswerClass(config)\n+\n+    @replace_return_docstrings(output_type=XLMSquadHeadOutput, config_class=XLMConfig)\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        cls_index: Optional[torch.LongTensor] = None,\n+        is_impossible: Optional[torch.LongTensor] = None,\n+        p_mask: Optional[torch.FloatTensor] = None,\n+        return_dict: bool = False,\n+    ) -> Union[XLMSquadHeadOutput, Tuple[torch.FloatTensor]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                Final hidden states of the model on the sequence tokens.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Positions of the first token for the labeled span.\n+            end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Positions of the last token for the labeled span.\n+            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n+            is_impossible (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Whether the question has a possible answer in the paragraph or not.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+            return_dict (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\n+        Returns:\n+        \"\"\"\n+        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n+\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n+            for x in (start_positions, end_positions, cls_index, is_impossible):\n+                if x is not None and x.dim() > 1:\n+                    x.squeeze_(-1)\n+\n+            # during training, compute the end logits based on the ground truth of the start position\n+            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n+\n+            loss_fct = CrossEntropyLoss()\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+            if cls_index is not None and is_impossible is not None:\n+                # Predict answerability from the representation of CLS and START\n+                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n+                loss_fct_cls = nn.BCEWithLogitsLoss()\n+                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n+\n+                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n+                total_loss += cls_loss * 0.5\n+\n+            return XLMSquadHeadOutput(loss=total_loss) if return_dict else (total_loss,)\n+\n+        else:\n+            # during inference, compute the end logits based on beam search\n+            bsz, slen, hsz = hidden_states.size()\n+            start_log_probs = nn.functional.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n+\n+            start_top_log_probs, start_top_index = torch.topk(\n+                start_log_probs, self.start_n_top, dim=-1\n+            )  # shape (bsz, start_n_top)\n+            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n+            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n+            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n+\n+            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n+                start_states\n+            )  # shape (bsz, slen, start_n_top, hsz)\n+            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n+            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n+            end_log_probs = nn.functional.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n+\n+            end_top_log_probs, end_top_index = torch.topk(\n+                end_log_probs, self.end_n_top, dim=1\n+            )  # shape (bsz, end_n_top, start_n_top)\n+            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n+            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n+\n+            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n+            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n+\n+            if not return_dict:\n+                return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n+            else:\n+                return XLMSquadHeadOutput(\n+                    start_top_log_probs=start_top_log_probs,\n+                    start_top_index=start_top_index,\n+                    end_top_log_probs=end_top_log_probs,\n+                    end_top_index=end_top_index,\n+                    cls_logits=cls_logits,\n+                )\n+\n+\n+class XLMSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`XLMConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: XLMConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n class MultiHeadAttention(nn.Module):\n     NEW_ID = itertools.count()\n \n@@ -252,7 +671,7 @@ def _init_weights(self, module):\n @dataclass\n class XLMForQuestionAnsweringOutput(ModelOutput):\n     \"\"\"\n-    Base class for outputs of question answering models using a `SquadHead`.\n+    Base class for outputs of question answering models using a `XLMSQuADHead`.\n \n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n@@ -767,7 +1186,7 @@ def __init__(self, config):\n         self.config = config\n \n         self.transformer = XLMModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = XLMSequenceSummary(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -971,7 +1390,7 @@ def __init__(self, config):\n         super().__init__(config)\n \n         self.transformer = XLMModel(config)\n-        self.qa_outputs = SQuADHead(config)\n+        self.qa_outputs = XLMSQuADHead(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1176,7 +1595,7 @@ def __init__(self, config, *inputs, **kwargs):\n         super().__init__(config, *inputs, **kwargs)\n \n         self.transformer = XLMModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = XLMSequenceSummary(config)\n         self.logits_proj = nn.Linear(config.num_labels, 1)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "de7446e57bb6d5ae0c8ff2f6d2fe7c09c7a80943",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 283,
            "deletions": 8,
            "changes": 291,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -19,15 +19,15 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...activations import ACT2FN\n+from ...activations import ACT2FN, get_activation\n from ...generation import GenerationMixin\n-from ...modeling_utils import PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits, PreTrainedModel, SequenceSummary\n+from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     ModelOutput,\n@@ -529,6 +529,281 @@ def ff_chunk(self, output_x):\n         return output_x\n \n \n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerStartLogits with XLM->XLNet\n+class XLNetPoolerStartLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD start logits from sequence hidden states.\n+\n+    Args:\n+        config ([`XLNetConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: XLNetConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        Returns:\n+            `torch.FloatTensor`: The start logits for SQuAD.\n+        \"\"\"\n+        x = self.dense(hidden_states).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerEndLogits with XLM->XLNet\n+class XLNetPoolerEndLogits(nn.Module):\n+    \"\"\"\n+    Compute SQuAD end logits from sequence hidden states.\n+\n+    Args:\n+        config ([`XLNetConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n+            to use.\n+    \"\"\"\n+\n+    def __init__(self, config: XLNetConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dense_1 = nn.Linear(config.hidden_size, 1)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        p_mask: Optional[torch.FloatTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n+                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n+                should be masked.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The end logits for SQuAD.\n+        \"\"\"\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            slen, hsz = hidden_states.shape[-2:]\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n+            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n+\n+        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n+        x = self.activation(x)\n+        x = self.LayerNorm(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        if p_mask is not None:\n+            if p_mask.dtype == torch.float16:\n+                x = x * (1 - p_mask) - 65500 * p_mask\n+            else:\n+                x = x * (1 - p_mask) - 1e30 * p_mask\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMPoolerAnswerClass with XLM->XLNet\n+class XLNetPoolerAnswerClass(nn.Module):\n+    \"\"\"\n+    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n+\n+    Args:\n+        config ([`XLNetConfig`]):\n+            The config used by the model, will be used to grab the `hidden_size` of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: XLNetConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n+        self.activation = nn.Tanh()\n+        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        start_states: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        cls_index: Optional[torch.LongTensor] = None,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n+                The hidden states of the first tokens for the labeled span.\n+            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                The position of the first token for the labeled span.\n+            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n+\n+        <Tip>\n+\n+        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n+        `start_states`.\n+\n+        </Tip>\n+\n+        Returns:\n+            `torch.FloatTensor`: The SQuAD 2.0 answer class.\n+        \"\"\"\n+        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n+        hsz = hidden_states.shape[-1]\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n+        if start_positions is not None:\n+            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n+\n+        if cls_index is not None:\n+            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n+            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n+        else:\n+            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n+\n+        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n+        x = self.activation(x)\n+        x = self.dense_1(x).squeeze(-1)\n+\n+        return x\n+\n+\n+# Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->XLNet\n+class XLNetSequenceSummary(nn.Module):\n+    r\"\"\"\n+    Compute a single vector summary of a sequence hidden states.\n+\n+    Args:\n+        config ([`XLNetConfig`]):\n+            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n+            config class of your model for the default values it uses):\n+\n+            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n+\n+                - `\"last\"` -- Take the last token hidden state (like XLNet)\n+                - `\"first\"` -- Take the first token hidden state (like Bert)\n+                - `\"mean\"` -- Take the mean of all tokens hidden states\n+                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n+                - `\"attn\"` -- Not implemented now, use multi-head attention\n+\n+            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n+            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n+              (otherwise to `config.hidden_size`).\n+            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n+              another string or `None` will add no activation.\n+            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n+            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n+    \"\"\"\n+\n+    def __init__(self, config: XLNetConfig):\n+        super().__init__()\n+\n+        self.summary_type = getattr(config, \"summary_type\", \"last\")\n+        if self.summary_type == \"attn\":\n+            # We should use a standard multi-head attention module with absolute positional embedding for that.\n+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n+            raise NotImplementedError\n+\n+        self.summary = nn.Identity()\n+        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n+            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n+                num_classes = config.num_labels\n+            else:\n+                num_classes = config.hidden_size\n+            self.summary = nn.Linear(config.hidden_size, num_classes)\n+\n+        activation_string = getattr(config, \"summary_activation\", None)\n+        self.activation: Callable = get_activation(activation_string) if activation_string else nn.Identity()\n+\n+        self.first_dropout = nn.Identity()\n+        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n+            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n+\n+        self.last_dropout = nn.Identity()\n+        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n+            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n+\n+    def forward(\n+        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Compute a single vector summary of a sequence hidden states.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n+                The hidden states of the last layer.\n+            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n+                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n+\n+        Returns:\n+            `torch.FloatTensor`: The summary of the sequence hidden states.\n+        \"\"\"\n+        if self.summary_type == \"last\":\n+            output = hidden_states[:, -1]\n+        elif self.summary_type == \"first\":\n+            output = hidden_states[:, 0]\n+        elif self.summary_type == \"mean\":\n+            output = hidden_states.mean(dim=1)\n+        elif self.summary_type == \"cls_index\":\n+            if cls_index is None:\n+                cls_index = torch.full_like(\n+                    hidden_states[..., :1, :],\n+                    hidden_states.shape[-2] - 1,\n+                    dtype=torch.long,\n+                )\n+            else:\n+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n+        elif self.summary_type == \"attn\":\n+            raise NotImplementedError\n+\n+        output = self.first_dropout(output)\n+        output = self.summary(output)\n+        output = self.activation(output)\n+        output = self.last_dropout(output)\n+\n+        return output\n+\n+\n class XLNetPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -1502,7 +1777,7 @@ def __init__(self, config):\n         self.config = config\n \n         self.transformer = XLNetModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = XLNetSequenceSummary(config)\n         self.logits_proj = nn.Linear(config.d_model, config.num_labels)\n \n         # Initialize weights and apply final processing\n@@ -1696,7 +1971,7 @@ def __init__(self, config):\n         super().__init__(config)\n \n         self.transformer = XLNetModel(config)\n-        self.sequence_summary = SequenceSummary(config)\n+        self.sequence_summary = XLNetSequenceSummary(config)\n         self.logits_proj = nn.Linear(config.d_model, 1)\n \n         # Initialize weights and apply final processing\n@@ -1911,9 +2186,9 @@ def __init__(self, config):\n         self.end_n_top = config.end_n_top\n \n         self.transformer = XLNetModel(config)\n-        self.start_logits = PoolerStartLogits(config)\n-        self.end_logits = PoolerEndLogits(config)\n-        self.answer_class = PoolerAnswerClass(config)\n+        self.start_logits = XLNetPoolerStartLogits(config)\n+        self.end_logits = XLNetPoolerEndLogits(config)\n+        self.answer_class = XLNetPoolerAnswerClass(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "9865067bcdb4d54999fce718b9a8ce891e7f12ba",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9",
            "patch": "@@ -327,17 +327,6 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n                 is not None\n             ):\n                 attribute_used = True\n-            # `SequenceSummary` is called with `SequenceSummary(config)`\n-            elif attribute in [\n-                \"summary_type\",\n-                \"summary_use_proj\",\n-                \"summary_activation\",\n-                \"summary_last_dropout\",\n-                \"summary_proj_to_labels\",\n-                \"summary_first_dropout\",\n-            ]:\n-                if \"SequenceSummary\" in modeling_source:\n-                    attribute_used = True\n             if attribute_used:\n                 break\n         if attribute_used:"
        }
    ],
    "stats": {
        "total": 1925,
        "additions": 1797,
        "deletions": 128
    }
}