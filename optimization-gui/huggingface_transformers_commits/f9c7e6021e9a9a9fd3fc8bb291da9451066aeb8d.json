{
    "author": "wwwbai",
    "message": "Translate bertlogy.md into Chinese (#34908)\n\n* bertology translation\r\n\r\n* Update docs/source/zh/_toctree.yml\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/zh/bertology.md\r\n\r\nCo-authored-by: blueingman <15329507600@163.com>\r\n\r\n* Update docs/source/zh/bertology.md\r\n\r\nCo-authored-by: blueingman <15329507600@163.com>\r\n\r\n* Update docs/source/zh/bertology.md\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\n\r\n* Update docs/source/zh/bertology.md\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: blueingman <15329507600@163.com>\r\nCo-authored-by: Isotr0py <2037008807@qq.com>",
    "sha": "f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d",
    "files": [
        {
            "sha": "1d8d969ce61db0f73ac3bfa60d398826409c50e8",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d",
            "patch": "@@ -92,6 +92,8 @@\n     title: 分词器的摘要\n   - local: attention\n     title: 注意力机制\n+  - local: bertology\n+    title: 基于BERT进行的相关研究\n   title: 概念指南\n - sections:\n   - sections:"
        },
        {
            "sha": "9b39f948339474ea730527c2f4829cf13cb24314",
            "filename": "docs/source/zh/bertology.md",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d/docs%2Fsource%2Fzh%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d/docs%2Fsource%2Fzh%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fbertology.md?ref=f9c7e6021e9a9a9fd3fc8bb291da9451066aeb8d",
            "patch": "@@ -0,0 +1,33 @@\n+<!--版权2020年HuggingFace团队保留所有权利。\n+\n+根据Apache许可证第2.0版（“许可证”）许可；除非符合许可证，否则您不得使用此文件。您可以在以下网址获取许可证的副本：\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+除非适用法律要求或书面同意，否则按“按原样”分发的软件，无论是明示还是暗示的，都没有任何担保或条件。请参阅许可证以了解特定语言下的权限和限制。\n+\n+⚠️ 请注意，本文件虽然使用Markdown编写，但包含了特定的语法，适用于我们的doc-builder（类似于MDX），可能无法在您的Markdown查看器中正常渲染。\n+\n+-->\n+\n+# 基于BERT进行的相关研究（BERTology）\n+\n+当前，一个新兴的研究领域正致力于探索大规模 transformer 模型（如BERT）的内部工作机制，一些人称之为“BERTology”。以下是这个领域的一些典型示例：\n+\n+\n+- BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie Pavlick:\n+  https://arxiv.org/abs/1905.05950\n+- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: https://arxiv.org/abs/1905.10650\n+- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.\n+  Manning: https://arxiv.org/abs/1906.04341\n+- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://arxiv.org/abs/2210.04633\n+\n+\n+为了助力这一新兴领域的发展，我们在BERT/GPT/GPT-2模型中增加了一些附加功能，方便人们访问其内部表示，这些功能主要借鉴了Paul Michel的杰出工作(https://arxiv.org/abs/1905.10650)：\n+\n+\n+- 访问BERT/GPT/GPT-2的所有隐藏状态，\n+- 访问BERT/GPT/GPT-2每个注意力头的所有注意力权重，\n+- 检索注意力头的输出值和梯度，以便计算头的重要性得分并对头进行剪枝，详情可见论文：https://arxiv.org/abs/1905.10650。\n+\n+为了帮助您理解和使用这些功能，我们添加了一个具体的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，该脚本可以对一个在 GLUE 数据集上预训练的模型进行信息提取与剪枝。\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 35,
        "deletions": 0
    }
}