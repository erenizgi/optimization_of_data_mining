{
    "author": "mokeddembillel",
    "message": "Add Falcon3 documentation (#35307)\n\n* Add Falcon3 documentation\n\n* Update Falcon3 documentation\n\n* Change Falcon to Falcon3\n\n* Update docs and run make fix-copies\n\n* Add blog post and huggingface models links",
    "sha": "6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
    "files": [
        {
            "sha": "435b482df599cfc7984b66b1273eaa0b22566076",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
            "patch": "@@ -396,6 +396,8 @@\n         title: ESM\n       - local: model_doc/falcon\n         title: Falcon\n+      - local: model_doc/falcon3\n+        title: Falcon3\n       - local: model_doc/falcon_mamba\n         title: FalconMamba\n       - local: model_doc/fastspeech2_conformer"
        },
        {
            "sha": "3bd1c286d43240f3025ec2b7069be7f92f523481",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
            "patch": "@@ -141,6 +141,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                           [ESM](model_doc/esm)                           |       ✅        |         ✅         |      ❌      |\n |              [FairSeq Machine-Translation](model_doc/fsmt)               |       ✅        |         ❌         |      ❌      |\n |                        [Falcon](model_doc/falcon)                        |       ✅        |         ❌         |      ❌      |\n+|                       [Falcon3](model_doc/falcon3)                       |       ✅        |         ❌         |      ✅      |\n |                  [FalconMamba](model_doc/falcon_mamba)                   |       ✅        |         ❌         |      ❌      |\n |         [FastSpeech2Conformer](model_doc/fastspeech2_conformer)          |       ✅        |         ❌         |      ❌      |\n |                       [FLAN-T5](model_doc/flan-t5)                       |       ✅        |         ✅         |      ✅      |"
        },
        {
            "sha": "813533dd7f4d0af0f572e48f4904636528420150",
            "filename": "docs/source/en/model_doc/falcon3.md",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon3.md?ref=6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
            "patch": "@@ -0,0 +1,29 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Falcon3\n+\n+## Overview\n+\n+Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities. This iteration includes five base models: Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base. In developing these models, we incorporated several key innovations aimed at improving the models' performances while reducing training costs:\n+\n+One pre-training: We conducted a single large-scale pretraining run on the 7B model, using 2048 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data.\n+Depth up-scaling for improved reasoning: Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2TT of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot and few-shot performance for models under 13B parameters.\n+Knowledge distillation for better tiny models: To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency.\n+\n+## Resources\n+- [Blog post](https://huggingface.co/blog/falcon3)\n+- [Models on Huggingface](https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026)"
        },
        {
            "sha": "d7d8281c2e3f03c4081165cca3e28adac9086c7b",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
            "patch": "@@ -415,6 +415,7 @@\n         (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),\n         (\"falcon\", \"Falcon\"),\n+        (\"falcon3\", \"Falcon3\"),\n         (\"falcon_mamba\", \"FalconMamba\"),\n         (\"fastspeech2_conformer\", \"FastSpeech2Conformer\"),\n         (\"flan-t5\", \"FLAN-T5\"),"
        },
        {
            "sha": "957bfd5af6af6f0fca5f00c19550c25471d24691",
            "filename": "utils/check_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/utils%2Fcheck_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c08b3b6e5bd1a6cc4253115c4e76889ea108afc/utils%2Fcheck_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_table.py?ref=6c08b3b6e5bd1a6cc4253115c4e76889ea108afc",
            "patch": "@@ -157,6 +157,7 @@ def _center_text(text: str, width: int) -> str:\n     \"LayoutXLM\": \"LayoutLMv2\",\n     \"Llama2\": \"LLaMA\",\n     \"Llama3\": \"LLaMA\",\n+    \"Falcon3\": \"LLaMA\",\n     \"MADLAD-400\": \"T5\",\n     \"MatCha\": \"Pix2Struct\",\n     \"mBART-50\": \"mBART\","
        }
    ],
    "stats": {
        "total": 34,
        "additions": 34,
        "deletions": 0
    }
}