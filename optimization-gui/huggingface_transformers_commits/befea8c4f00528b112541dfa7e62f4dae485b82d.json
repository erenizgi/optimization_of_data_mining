{
    "author": "Wauplin",
    "message": "ðŸš¨ Remove cache migration script (#35810)\n\n* Remove cache migration script\r\n\r\n* remove dummy move_cache",
    "sha": "befea8c4f00528b112541dfa7e62f4dae485b82d",
    "files": [
        {
            "sha": "2773a43c2ec2c810b0ea3f77265dc9b3a26b9aed",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/befea8c4f00528b112541dfa7e62f4dae485b82d/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/befea8c4f00528b112541dfa7e62f4dae485b82d/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=befea8c4f00528b112541dfa7e62f4dae485b82d",
            "patch": "@@ -96,7 +96,6 @@\n     http_user_agent,\n     is_offline_mode,\n     is_remote_url,\n-    move_cache,\n     send_example_telemetry,\n     try_to_load_from_cache,\n )"
        },
        {
            "sha": "21d6d04489e5953407e3ad6f09b1fac66c56f23e",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 0,
            "deletions": 196,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/befea8c4f00528b112541dfa7e62f4dae485b82d/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/befea8c4f00528b112541dfa7e62f4dae485b82d/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=befea8c4f00528b112541dfa7e62f4dae485b82d",
            "patch": "@@ -18,10 +18,8 @@\n import json\n import os\n import re\n-import shutil\n import sys\n import tempfile\n-import traceback\n import warnings\n from concurrent import futures\n from pathlib import Path\n@@ -40,7 +38,6 @@\n     create_branch,\n     create_commit,\n     create_repo,\n-    get_hf_file_metadata,\n     hf_hub_download,\n     hf_hub_url,\n     try_to_load_from_cache,\n@@ -86,7 +83,6 @@ def is_offline_mode():\n \n torch_cache_home = os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n default_cache_path = constants.default_cache_path\n-old_default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n \n # Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.\n # The best way to set the cache path is with the environment variable HF_HOME. For more details, checkout this\n@@ -100,23 +96,6 @@ def is_offline_mode():\n PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n \n-# Onetime move from the old location to the new one if no ENV variable has been set.\n-if (\n-    os.path.isdir(old_default_cache_path)\n-    and not os.path.isdir(constants.HF_HUB_CACHE)\n-    and \"PYTORCH_PRETRAINED_BERT_CACHE\" not in os.environ\n-    and \"PYTORCH_TRANSFORMERS_CACHE\" not in os.environ\n-    and \"TRANSFORMERS_CACHE\" not in os.environ\n-):\n-    logger.warning(\n-        \"In Transformers v4.22.0, the default path to cache downloaded models changed from\"\n-        \" '~/.cache/torch/transformers' to '~/.cache/huggingface/hub'. Since you don't seem to have\"\n-        \" overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to\"\n-        \" '~/.cache/huggingface/hub' to avoid redownloading models you have already in the cache. You should\"\n-        \" only see this message once.\"\n-    )\n-    shutil.move(old_default_cache_path, constants.HF_HUB_CACHE)\n-\n HF_MODULES_CACHE = os.getenv(\"HF_MODULES_CACHE\", os.path.join(constants.HF_HOME, \"modules\"))\n TRANSFORMERS_DYNAMIC_MODULE_NAME = \"transformers_modules\"\n SESSION_ID = uuid4().hex\n@@ -1087,47 +1066,6 @@ def get_checkpoint_shard_files(\n     return cached_filenames, sharded_metadata\n \n \n-# All what is below is for conversion between old cache format and new cache format.\n-\n-\n-def get_all_cached_files(cache_dir=None):\n-    \"\"\"\n-    Returns a list for all files cached with appropriate metadata.\n-    \"\"\"\n-    if cache_dir is None:\n-        cache_dir = TRANSFORMERS_CACHE\n-    else:\n-        cache_dir = str(cache_dir)\n-    if not os.path.isdir(cache_dir):\n-        return []\n-\n-    cached_files = []\n-    for file in os.listdir(cache_dir):\n-        meta_path = os.path.join(cache_dir, f\"{file}.json\")\n-        if not os.path.isfile(meta_path):\n-            continue\n-\n-        with open(meta_path, encoding=\"utf-8\") as meta_file:\n-            metadata = json.load(meta_file)\n-            url = metadata[\"url\"]\n-            etag = metadata[\"etag\"].replace('\"', \"\")\n-            cached_files.append({\"file\": file, \"url\": url, \"etag\": etag})\n-\n-    return cached_files\n-\n-\n-def extract_info_from_url(url):\n-    \"\"\"\n-    Extract repo_name, revision and filename from an url.\n-    \"\"\"\n-    search = re.search(r\"^https://huggingface\\.co/(.*)/resolve/([^/]*)/(.*)$\", url)\n-    if search is None:\n-        return None\n-    repo, revision, filename = search.groups()\n-    cache_repo = \"--\".join([\"models\"] + repo.split(\"/\"))\n-    return {\"repo\": cache_repo, \"revision\": revision, \"filename\": filename}\n-\n-\n def create_and_tag_model_card(\n     repo_id: str,\n     tags: Optional[List[str]] = None,\n@@ -1168,88 +1106,6 @@ def create_and_tag_model_card(\n     return model_card\n \n \n-def clean_files_for(file):\n-    \"\"\"\n-    Remove, if they exist, file, file.json and file.lock\n-    \"\"\"\n-    for f in [file, f\"{file}.json\", f\"{file}.lock\"]:\n-        if os.path.isfile(f):\n-            os.remove(f)\n-\n-\n-def move_to_new_cache(file, repo, filename, revision, etag, commit_hash):\n-    \"\"\"\n-    Move file to repo following the new huggingface hub cache organization.\n-    \"\"\"\n-    os.makedirs(repo, exist_ok=True)\n-\n-    # refs\n-    os.makedirs(os.path.join(repo, \"refs\"), exist_ok=True)\n-    if revision != commit_hash:\n-        ref_path = os.path.join(repo, \"refs\", revision)\n-        with open(ref_path, \"w\") as f:\n-            f.write(commit_hash)\n-\n-    # blobs\n-    os.makedirs(os.path.join(repo, \"blobs\"), exist_ok=True)\n-    blob_path = os.path.join(repo, \"blobs\", etag)\n-    shutil.move(file, blob_path)\n-\n-    # snapshots\n-    os.makedirs(os.path.join(repo, \"snapshots\"), exist_ok=True)\n-    os.makedirs(os.path.join(repo, \"snapshots\", commit_hash), exist_ok=True)\n-    pointer_path = os.path.join(repo, \"snapshots\", commit_hash, filename)\n-    huggingface_hub.file_download._create_relative_symlink(blob_path, pointer_path)\n-    clean_files_for(file)\n-\n-\n-def move_cache(cache_dir=None, new_cache_dir=None, token=None):\n-    if new_cache_dir is None:\n-        new_cache_dir = TRANSFORMERS_CACHE\n-    if cache_dir is None:\n-        # Migrate from old cache in .cache/huggingface/transformers\n-        old_cache = Path(TRANSFORMERS_CACHE).parent / \"transformers\"\n-        if os.path.isdir(str(old_cache)):\n-            cache_dir = str(old_cache)\n-        else:\n-            cache_dir = new_cache_dir\n-    cached_files = get_all_cached_files(cache_dir=cache_dir)\n-    logger.info(f\"Moving {len(cached_files)} files to the new cache system\")\n-\n-    hub_metadata = {}\n-    for file_info in tqdm(cached_files):\n-        url = file_info.pop(\"url\")\n-        if url not in hub_metadata:\n-            try:\n-                hub_metadata[url] = get_hf_file_metadata(url, token=token)\n-            except requests.HTTPError:\n-                continue\n-\n-        etag, commit_hash = hub_metadata[url].etag, hub_metadata[url].commit_hash\n-        if etag is None or commit_hash is None:\n-            continue\n-\n-        if file_info[\"etag\"] != etag:\n-            # Cached file is not up to date, we just throw it as a new version will be downloaded anyway.\n-            clean_files_for(os.path.join(cache_dir, file_info[\"file\"]))\n-            continue\n-\n-        url_info = extract_info_from_url(url)\n-        if url_info is None:\n-            # Not a file from huggingface.co\n-            continue\n-\n-        repo = os.path.join(new_cache_dir, url_info[\"repo\"])\n-        move_to_new_cache(\n-            file=os.path.join(cache_dir, file_info[\"file\"]),\n-            repo=repo,\n-            filename=url_info[\"filename\"],\n-            revision=url_info[\"revision\"],\n-            etag=etag,\n-            commit_hash=commit_hash,\n-        )\n-\n-\n class PushInProgress:\n     \"\"\"\n     Internal class to keep track of a push in progress (which might contain multiple `Future` jobs).\n@@ -1271,55 +1127,3 @@ def cancel(self) -> None:\n             # Cancel the job if it wasn't started yet and remove cancelled/done jobs from the list\n             if not (job.cancel() or job.done())\n         ]\n-\n-\n-cache_version_file = os.path.join(TRANSFORMERS_CACHE, \"version.txt\")\n-if not os.path.isfile(cache_version_file):\n-    cache_version = 0\n-else:\n-    with open(cache_version_file) as f:\n-        try:\n-            cache_version = int(f.read())\n-        except ValueError:\n-            cache_version = 0\n-\n-cache_is_not_empty = os.path.isdir(TRANSFORMERS_CACHE) and len(os.listdir(TRANSFORMERS_CACHE)) > 0\n-\n-if cache_version < 1 and cache_is_not_empty:\n-    if is_offline_mode():\n-        logger.warning(\n-            \"You are offline and the cache for model files in Transformers v4.22.0 has been updated while your local \"\n-            \"cache seems to be the one of a previous version. It is very likely that all your calls to any \"\n-            \"`from_pretrained()` method will fail. Remove the offline mode and enable internet connection to have \"\n-            \"your cache be updated automatically, then you can go back to offline mode.\"\n-        )\n-    else:\n-        logger.warning(\n-            \"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a \"\n-            \"one-time only operation. You can interrupt this and resume the migration later on by calling \"\n-            \"`transformers.utils.move_cache()`.\"\n-        )\n-    try:\n-        if TRANSFORMERS_CACHE != constants.HF_HUB_CACHE:\n-            # Users set some env variable to customize cache storage\n-            move_cache(TRANSFORMERS_CACHE, TRANSFORMERS_CACHE)\n-        else:\n-            move_cache()\n-    except Exception as e:\n-        trace = \"\\n\".join(traceback.format_tb(e.__traceback__))\n-        logger.error(\n-            f\"There was a problem when trying to move your cache:\\n\\n{trace}\\n{e.__class__.__name__}: {e}\\n\\nPlease \"\n-            \"file an issue at https://github.com/huggingface/transformers/issues/new/choose and copy paste this whole \"\n-            \"message and we will do our best to help.\"\n-        )\n-\n-if cache_version < 1:\n-    try:\n-        os.makedirs(TRANSFORMERS_CACHE, exist_ok=True)\n-        with open(cache_version_file, \"w\") as f:\n-            f.write(\"1\")\n-    except Exception:\n-        logger.warning(\n-            f\"There was a problem when trying to write in your cache folder ({TRANSFORMERS_CACHE}). You should set \"\n-            \"the environment variable TRANSFORMERS_CACHE to a writable directory.\"\n-        )"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 0,
        "deletions": 197
    }
}