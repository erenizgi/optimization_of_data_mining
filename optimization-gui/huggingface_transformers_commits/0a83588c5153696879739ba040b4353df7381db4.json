{
    "author": "rootonchair",
    "message": "Bridgetower fast image processor (#37373)\n\n* add support for fast tokenizer\n\n* make style\n\n* fix according to reviews\n\n* make style\n\n* relax slow_fast_equivalence mean diff\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "0a83588c5153696879739ba040b4353df7381db4",
    "files": [
        {
            "sha": "4b8601bf8a9534aca5f0ca654f701a6b1f7236a4",
            "filename": "docs/source/en/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -147,6 +147,11 @@ Tips:\n [[autodoc]] BridgeTowerImageProcessor\n     - preprocess\n \n+## BridgeTowerImageProcessorFast\n+\n+[[autodoc]] BridgeTowerImageProcessorFast\n+    - preprocess\n+\n ## BridgeTowerProcessor\n \n [[autodoc]] BridgeTowerProcessor"
        },
        {
            "sha": "c210d4666f362e429a55e5c41bd89cf18cfce778",
            "filename": "docs/source/ja/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -144,6 +144,11 @@ BridgeTower は、ビジュアル エンコーダー、テキスト エンコー\n [[autodoc]] BridgeTowerImageProcessor\n     - preprocess\n \n+## BridgeTowerImageProcessorFast\n+\n+[[autodoc]] BridgeTowerImageProcessorFast\n+    - preprocess\n+\n ## BridgeTowerProcessor\n \n [[autodoc]] BridgeTowerProcessor"
        },
        {
            "sha": "296c3dad10d5a01e1eab8ceb9fd012813189fbb6",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -62,7 +62,7 @@\n             (\"bit\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"blip-2\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"bridgetower\", (\"BridgeTowerImageProcessor\",)),\n+            (\"bridgetower\", (\"BridgeTowerImageProcessor\", \"BridgeTowerImageProcessorFast\")),\n             (\"chameleon\", (\"ChameleonImageProcessor\",)),\n             (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "8ca84a320fdc4aa1f4cf99aef78154849514c8a6",
            "filename": "src/transformers/models/bridgetower/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2F__init__.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_bridgetower import *\n     from .image_processing_bridgetower import *\n+    from .image_processing_bridgetower_fast import *\n     from .modeling_bridgetower import *\n     from .processing_bridgetower import *\n else:"
        },
        {
            "sha": "95eaa9f88b9266fbcece143803d7fd4f3a9485e0",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -28,8 +28,8 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n-    is_batched,\n     is_scaled_image,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -455,7 +455,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n         do_pad = do_pad if do_pad is not None else self.do_pad\n-        do_center_crop if do_center_crop is not None else self.do_center_crop\n+        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n         # For backwards compatibility. Initial version of this processor was cropping to the \"size\" argument, which\n         # it should default to if crop_size is undefined.\n         crop_size = (\n@@ -464,9 +464,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n-\n-        if not is_batched(images):\n-            images = [images]\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "7af3213854fd1795bfa59b35c612a6894b925d57",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "added",
            "additions": 345,
            "deletions": 0,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -0,0 +1,345 @@\n+# coding=utf-8\n+# Copyright 2025 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for BridgeTower.\"\"\"\n+\n+from typing import Dict, Iterable, Optional, Tuple, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    ImageInput,\n+    SizeDict,\n+    TensorType,\n+    Unpack,\n+    get_max_height_width,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n+from ...utils import add_start_docstrings, is_torch_available, is_torchvision_available, is_torchvision_v2_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+def make_pixel_mask(\n+    image: \"torch.Tensor\",\n+    output_size: Tuple[int, int],\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+\n+    Args:\n+        image (`np.ndarray`):\n+            Image to make the pixel mask for.\n+        output_size (`Tuple[int, int]`):\n+            Output size of the mask.\n+    \"\"\"\n+    input_height, input_width = image.shape[-2:]\n+    batch_size = image.size(0)\n+    mask = torch.zeros((batch_size, *output_size), dtype=torch.long)\n+    mask[:input_height, :input_width] = 1\n+    return mask\n+\n+\n+def get_resize_output_image_size(\n+    input_image: \"torch.Tensor\",\n+    shorter: int = 800,\n+    longer: int = 1333,\n+    size_divisor: int = 32,\n+) -> Tuple[int, int]:\n+    input_height, input_width = input_image.shape[-2:]\n+    min_size, max_size = shorter, longer\n+\n+    scale = min_size / min(input_height, input_width)\n+\n+    if input_height < input_width:\n+        new_height = min_size\n+        new_width = scale * input_width\n+    else:\n+        new_height = scale * input_height\n+        new_width = min_size\n+\n+    if max(new_height, new_width) > max_size:\n+        scale = max_size / max(new_height, new_width)\n+        new_height = scale * new_height\n+        new_width = scale * new_width\n+\n+    new_height, new_width = int(new_height + 0.5), int(new_width + 0.5)\n+    new_height = new_height // size_divisor * size_divisor\n+    new_width = new_width // size_divisor * size_divisor\n+\n+    return new_height, new_width\n+\n+\n+class BridgeTowerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    size_divisor: Optional[int]\n+    do_pad: Optional[bool]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast BridgeTower image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        size_divisor (`int`, *optional*, defaults to 32):\n+            The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n+            is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to the `(max_height, max_width)` of the images in the batch. Can be overridden by\n+            the `do_pad` parameter in the `preprocess` method.\n+    \"\"\",\n+)\n+class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"shortest_edge\": 288}\n+    default_to_square = False\n+    crop_size = {\"shortest_edge\": 288}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = True\n+    size_divisor = 32\n+    valid_kwargs = BridgeTowerFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            size_divisor (`int`, *optional*, defaults to 32):\n+                The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n+                is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n+            do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the image to the `(max_height, max_width)` of the images in the batch. Can be overridden by\n+                the `do_pad` parameter in the `preprocess` method.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        size_divisor: int = 32,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image.\n+\n+        Resizes the shorter side of the image to `size[\"shortest_edge\"]` while preserving the aspect ratio. If the\n+        longer side is larger than the max size `(int(`size[\"shortest_edge\"]` * 1333 / 800))`, the longer side is then\n+        resized to the max size while preserving the aspect ratio.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            size_divisor (`int`, *optional*, defaults to 32):\n+                The image is resized to a size that is a multiple of this value.\n+            resample (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if not size.shortest_edge:\n+            raise ValueError(f\"The `size` dictionary must contain the key `shortest_edge`. Got {size.keys()}\")\n+        shorter = size.shortest_edge\n+        longer = int(1333 / 800 * shorter)\n+        output_size = get_resize_output_image_size(\n+            image,\n+            shorter=shorter,\n+            longer=longer,\n+            size_divisor=size_divisor,\n+        )\n+        return F.resize(image, output_size, interpolation=interpolation, antialias=antialias)\n+\n+    def center_crop(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: Dict[str, int],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Center crop an image to `(size[\"height\"], size[\"width\"])`. If the input size is smaller than `crop_size` along\n+        any edge, the image is padded with 0's and then center cropped.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to center crop.\n+            size (`Dict[str, int]`):\n+                Size of the output image in the form `{\"height\": h, \"width\": w}`.\n+        \"\"\"\n+        output_size = size.shortest_edge\n+        return F.center_crop(\n+            image,\n+            output_size=(output_size, output_size),\n+            **kwargs,\n+        )\n+\n+    def _pad_image(\n+        self,\n+        image: \"torch.Tensor\",\n+        output_size: Tuple[int, int],\n+        constant_values: Union[float, Iterable[float]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pad an image with zeros to the given size.\n+        \"\"\"\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = output_size\n+\n+        pad_bottom = output_height - input_height\n+        pad_right = output_width - input_width\n+        padding = (0, 0, pad_right, pad_bottom)\n+        padded_image = F.pad(\n+            image,\n+            padding,\n+            fill=constant_values,\n+        )\n+        return padded_image\n+\n+    def pad(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        constant_values: Union[float, Iterable[float]] = 0,\n+        return_pixel_mask: bool = True,\n+    ) -> tuple:\n+        \"\"\"\n+        Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n+        in the batch and optionally returns their corresponding pixel mask.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to pad.\n+            constant_values (`float` or `Iterable[float]`, *optional*):\n+                The value to use for the padding if `mode` is `\"constant\"`.\n+            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n+                Whether to return a pixel mask.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+        \"\"\"\n+        pad_size = get_max_height_width(images)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        processed_images_grouped = {}\n+        processed_masks_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            stacked_images = self._pad_image(\n+                stacked_images,\n+                pad_size,\n+                constant_values=constant_values,\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+            if return_pixel_mask:\n+                stacked_masks = make_pixel_mask(image=stacked_images, output_size=pad_size)\n+                processed_masks_grouped[shape] = stacked_masks\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        processed_masks = None\n+        if return_pixel_mask:\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+\n+        return processed_images, processed_masks\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        size_divisor: Optional[int],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_pad: bool,\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, size_divisor=size_divisor, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        data = {}\n+        if do_pad:\n+            processed_images, processed_masks = self.pad(processed_images, return_pixel_mask=True)\n+            processed_masks = torch.stack(processed_masks, dim=0) if return_tensors else processed_masks\n+            data[\"pixel_mask\"] = processed_masks\n+\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        data[\"pixel_values\"] = processed_images\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def to_dict(self):\n+        encoder_dict = super().to_dict()\n+        encoder_dict.pop(\"_valid_processor_keys\", None)\n+        encoder_dict.pop(\"crop_size\", None)\n+        return encoder_dict\n+\n+\n+__all__ = [\"BridgeTowerImageProcessorFast\"]"
        },
        {
            "sha": "388bb65f69dce344cd24dda217361a9418319345",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 67,
            "deletions": 49,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -16,19 +16,25 @@\n import unittest\n from typing import Optional, Union\n \n-import numpy as np\n+import requests\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n     from PIL import Image\n \n     from transformers import BridgeTowerImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import BridgeTowerImageProcessorFast\n+\n \n class BridgeTowerImageProcessingTester:\n     def __init__(\n@@ -76,46 +82,7 @@ def prepare_image_processor_dict(self):\n         }\n \n     def get_expected_values(self, image_inputs, batched=False):\n-        \"\"\"\n-        This function computes the expected height and width when providing images to BridgeTowerImageProcessor,\n-        assuming do_resize is set to True with a scalar size and size_divisor.\n-        \"\"\"\n-        if not batched:\n-            size = self.size[\"shortest_edge\"]\n-            image = image_inputs[0]\n-            if isinstance(image, Image.Image):\n-                w, h = image.size\n-            elif isinstance(image, np.ndarray):\n-                h, w = image.shape[0], image.shape[1]\n-            else:\n-                h, w = image.shape[1], image.shape[2]\n-            scale = size / min(w, h)\n-            if h < w:\n-                newh, neww = size, scale * w\n-            else:\n-                newh, neww = scale * h, size\n-\n-            max_size = int((1333 / 800) * size)\n-            if max(newh, neww) > max_size:\n-                scale = max_size / max(newh, neww)\n-                newh = newh * scale\n-                neww = neww * scale\n-\n-            newh, neww = int(newh + 0.5), int(neww + 0.5)\n-            expected_height, expected_width = (\n-                newh // self.size_divisor * self.size_divisor,\n-                neww // self.size_divisor * self.size_divisor,\n-            )\n-\n-        else:\n-            expected_values = []\n-            for image in image_inputs:\n-                expected_height, expected_width = self.get_expected_values([image])\n-                expected_values.append((expected_height, expected_width))\n-            expected_height = max(expected_values, key=lambda item: item[0])[0]\n-            expected_width = max(expected_values, key=lambda item: item[1])[1]\n-\n-        return expected_height, expected_width\n+        return self.size[\"shortest_edge\"], self.size[\"shortest_edge\"]\n \n     def expected_output_image_shape(self, images):\n         height, width = self.get_expected_values(images, batched=True)\n@@ -137,6 +104,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class BridgeTowerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = BridgeTowerImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = BridgeTowerImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -147,10 +115,60 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+\n+    def _assertEquivalence(self, a, b):\n+        self.assertTrue(torch.allclose(a, b, atol=1e-1))\n+        self.assertLessEqual(torch.mean(torch.abs(a - b)).item(), 1e-3)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assertEquivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assertEquivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assertEquivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assertEquivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())"
        },
        {
            "sha": "2580953841dfc700cdbd9e3746024c20591a0d34",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a83588c5153696879739ba040b4353df7381db4/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a83588c5153696879739ba040b4353df7381db4/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=0a83588c5153696879739ba040b4353df7381db4",
            "patch": "@@ -181,7 +181,7 @@ def test_slow_fast_equivalence(self):\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n         self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n         self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 5e-3\n         )\n \n     @require_vision\n@@ -207,7 +207,7 @@ def test_slow_fast_equivalence_batched(self):\n \n         self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n         self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 5e-3\n         )\n \n     @require_vision"
        }
    ],
    "stats": {
        "total": 486,
        "additions": 429,
        "deletions": 57
    }
}