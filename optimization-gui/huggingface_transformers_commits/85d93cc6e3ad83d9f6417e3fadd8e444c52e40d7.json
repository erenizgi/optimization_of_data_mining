{
    "author": "gante",
    "message": "[serve] Cursor support, move docs into separate page, add more examples (#39133)\n\n* jan docs\n\n* rm\n\n* [cursor] tmp commit\n\n* Cursor working :D\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update src/transformers/commands/serving.py\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* cursor docs\n\n* try to fix agents/tools docs?\n\n* try to fix agents/tools docs?\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* add transformers chat example with transformers serve\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>",
    "sha": "85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
    "files": [
        {
            "sha": "4b3a17d4b52d7a9e9ef1eabe016a7c0c7ab2b2aa",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -100,8 +100,6 @@\n       title: Distributed inference\n     - local: perf_infer_cpu\n       title: CPU\n-    - local: tf_xla\n-      title: XLA\n     title: Optimization\n   - local: agents\n     title: Agents\n@@ -141,8 +139,6 @@\n       title: GPU\n     - local: perf_train_cpu\n       title: CPU\n-    - local: perf_train_tpu_tf\n-      title: TPU\n     - local: perf_train_special\n       title: Apple Silicon\n     - local: perf_train_gaudi\n@@ -1144,4 +1140,3 @@\n       title: Environment Variables\n     title: Reference\n   title: API\n-"
        },
        {
            "sha": "f8a60b5fc325b6251cdf99b7798d9d7cd8aba3a3",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -14,5 +14,9 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+# Agents\n+\n+(deprecated)\n+\n > [!WARNING]\n > Agents and tools were spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. They were removed from `transformers` in v4.52."
        },
        {
            "sha": "3c5734b60e4571cb83f64952eab81111e751ba6d",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 2,
            "deletions": 66,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -25,10 +25,7 @@ Check model leaderboards like [OpenLLM](https://hf.co/spaces/HuggingFaceH4/open_\n \n This guide shows you how to quickly start chatting with Transformers from the command line, how build and format a conversation, and how to chat using the [`TextGenerationPipeline`].\n \n-## transformers CLI\n-\n-\n-### Interactive chat session\n+## chat CLI\n \n After you've [installed Transformers](./installation.md), chat with a model directly from the command line as shown below. It launches an interactive session with a model, with a few base commands listed at the start of the session.\n \n@@ -52,68 +49,7 @@ For a full list of options, run the command below.\n transformers chat -h\n ```\n \n-The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating).\n-\n-\n-### Serving a model and using MCP tools\n-\n-> [!WARNING]\n-> This section is experimental and subject to changes in future versions\n-\n-Powering the `chat` interface, we have a server that takes user messages and returns completions. The server has a chat completion API compatible with the OpenAI SDK, so you can also quickly experiment with `transformers` models on existing aplications. To launch a server separately, use the `transformers serve` CLI:\n-\n-```bash\n-transformers serve Menlo/Jan-nano\n-```\n-\n-Under the hood, the `chat` CLI launches and uses `transformers serve`. This server is also an MCP client, which can receive information available MCP servers (i.e. tools), massage their information into the model prompt, and prepare calls to these tools when the model commands to do so. Naturally, this requires a model that is trained to use tools.\n-\n-At the moment, MCP tool usage in `transformers` has the following constraints:\n-- `chat` can't handle tools, but the [`tiny-agents`](https://huggingface.co/blog/python-tiny-agents) CLI can;\n-- Only the `qwen` family of models is supported.\n-\n-The first step to use MCP tools is to let the model know which tools are available. As an example, let's consider a `tiny-agents` configuration file with a reference to an [image generation MCP server](https://evalstate-flux1-schnell.hf.space/).\n-\n-> [!TIP]\n-> Many Hugging Face Spaces can be used as MCP servers. You can find all compatible Spaces [here](https://huggingface.co/spaces?filter=mcp-server).\n-\n-```json\n-{\n-    \"model\": \"http://localhost:8000\",\n-    \"provider\": \"local\",\n-    \"servers\": [\n-        {\n-            \"type\": \"sse\",\n-            \"config\": {\n-                \"url\": \"https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse\"\n-            }\n-        }\n-    ]\n-}\n-```\n-\n-You can then launch your `tiny-agents` chat interface with the following command.\n-\n-```bash\n-tiny-agents run path/to/your/config.json\n-```\n-\n-If you have a server (from `transformers serve`) running in the background, you're ready to use MCP tools from a local model! For instance, here's the example of a chat session:\n-\n-```bash\n-Agent loaded with 1 tools:\n- • flux1_schnell_infer\n-»  Generate an image of a cat on the moon\n-<Tool req_0_tool_call>flux1_schnell_infer {\"prompt\": \"a cat on the moon\", \"seed\": 42, \"randomize_seed\": true, \"width\": 1024, \"height\": 1024, \"num_inference_steps\": 4}\n-\n-Tool req_0_tool_call\n-[Binary Content: Image image/webp, 57732 bytes]\n-The task is complete and the content accessible to the User\n-Image URL: https://evalstate-flux1-schnell.hf.space/gradio_api/file=/tmp/gradio/3dbddc0e53b5a865ed56a4e3dbdd30f3f61cf3b8aabf1b456f43e5241bd968b8/image.webp\n-380576952\n-\n-I have generated an image of a cat on the moon using the Flux 1 Schnell Image Generator. The image is 1024x1024 pixels and was created with 4 inference steps. Let me know if you would like to make any changes or need further assistance!\n-```\n+The chat is implemented on top of the [AutoClass](./model_doc/auto), using tooling from [text generation](./llm_tutorial) and [chat](./chat_templating). It uses the `transformers serve` CLI under the hood ([docs](./serving.md#serve-cli)).\n \n \n ## TextGenerationPipeline"
        },
        {
            "sha": "286ff530a817b040736736e3f966b6f856d28623",
            "filename": "docs/source/en/perf_train_tpu_tf.md",
            "status": "removed",
            "additions": 0,
            "deletions": 355,
            "changes": 355,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/docs%2Fsource%2Fen%2Fperf_train_tpu_tf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/docs%2Fsource%2Fen%2Fperf_train_tpu_tf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_tpu_tf.md?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -1,355 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TPU\n-\n-TPU (Tensor Processing Unit) is a type of hardware designed to accelerate tensor computations for training and inference. TPUs are generally accessed through Google cloud services, but smaller TPUs are also available for free from [Google Colab](https://colab.research.google.com/notebooks/tpu.ipynb) or [Kaggle](https://www.kaggle.com/docs/tpu).\n-\n-This guide focuses on training a Keras model for sequence classification on a TPU from Google Colab. Make sure the TPU runtime is enabled by going to **Runtime > Change runtime type** and selecting a TPU.\n-\n-Run the command below to install the latest version of Transformers and [Datasets](https://huggingface.co/docs/datasets).\n-\n-```py\n-!pip install --U transformers datasets\n-```\n-\n-Create an instance of [tf.distribute.cluster_resolver.TPUClusterResolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver), and then connect to the remote cluster and initialize the TPUs.\n-\n-```py\n-import tensorflow as tf\n-\n-resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n-tf.config.experimental_connect_to_cluster(resolver)\n-tf.tpu.experimental.initialize_tpu_system(resolver)\n-```\n-\n-There are various distribution strategies for running your model on multiple TPUs. The [tpu.distribute.TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) offers synchronized distributed training.\n-\n-```py\n-strategy = tf.distribute.TPUStrategy(resolver)\n-```\n-\n-Load and tokenize a dataset - this example uses [CoLA](https://huggingface.co/datasets/nyu-mll/glue/viewer/cola) from the GLUE benchmark - and pad all samples to the maximum length so it is easier to load as an array and to avoid [XLA compilation issues](#xla).\n-\n-```py\n-from transformers import AutoTokenizer\n-from datasets import load_dataset\n-import numpy as np\n-\n-dataset = load_dataset(\"glue\", \"cola\")[\"train\"]\n-tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n-\n-train_data = tokenizer(\n-    dataset[\"sentence\"],\n-    padding=\"max_length\",\n-    truncation=True,\n-    max_length=128,\n-    return_tensors=\"np\",\n-)\n-train_data = dict(train_data)\n-train_labels = np.array(dataset[\"label\"])\n-```\n-\n-The model **must** be created inside [Strategy.scope](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#scope) in order to replicate the model layers on each TPU device.\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-\n-with strategy.scope():\n-    model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n-    model.compile(optimizer=\"adam\")\n-```\n-\n-TPUs only accept [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) inputs unlike the Keras [fit](https://keras.io/api/models/model_training_apis/#fit-method) method which accepts a broader range of inputs.\n-\n-```py\n-BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n-\n-tf_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n-tf_dataset = tf_dataset.shuffle(len(tf_dataset))\n-tf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)\n-```\n-\n-Finally, call [fit](https://keras.io/api/models/model_training_apis/#fit-method) to start training.\n-\n-```py\n-model.fit(tf_dataset)\n-```\n-\n-## Large datasets\n-\n-The dataset created above pads every sample to the maximum length and loads the whole dataset into memory. This may not be possible if you're working with larger datasets. When training on large datasets, you may want to create a [tf.TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) or stream the data.\n-\n-### tf.TFRecord\n-\n-[tf.TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) is the standard [tf.data](https://www.tensorflow.org/guide/data) format for storing training data. For very large training jobs, it's worth preprocessing your data and storing it in the `tf.TFRecord` format and building a `tf.data` pipeline on top. Refer to the table below to help you decide whether `tf.TFRecord` is helpful for you.\n-\n-| pros | cons |\n-|---|---|\n-| works on all TPU instances | costs associated with cloud storage |\n-| supports huge datasets and massive throughput | some data types (images) can take a lot of space to store |\n-| suitable for training on entire TPU pods |  |\n-| preprocessing is done in advance, maximizing training speed |  |\n-\n-Preprocess and tokenize the dataset before writing it to a `tf.TFRecord` to avoid writing every time the data is loaded.\n-\n-An exception is made for *train-time augmentations*, because augmentations applied after writing to a `tf.TFRecord` results in the same augmentation for each epoch. Instead, apply augmentations in the `tf.data` pipeline that loads the data.\n-\n-> [!TIP]\n-> In practice, you probably won't be able to load the entire dataset in memory. Load a chunk of the dataset at a time and convert it to `TFRecord`, and repeat until the entire dataset is in the `TFRecord` format. Then you can use a list of all the files to create a `TFRecordDataset`. The example below demonstrates a single file for simplicity.\n-\n-```py\n-tokenized_data = tokenizer(\n-    dataset[\"sentence\"],\n-    padding=\"max_length\",\n-    truncation=True,\n-    max_length=128,\n-    return_tensors=\"np\",\n-)\n-labels = dataset[\"label\"]\n-\n-with tf.io.TFRecordWriter(\"dataset.tfrecords\") as file_writer:\n-    for i in range(len(labels)):\n-        features = {\n-            \"input_ids\": tf.train.Feature(\n-                int64_list=tf.train.Int64List(value=tokenized_data[\"input_ids\"][i])\n-            ),\n-            \"attention_mask\": tf.train.Feature(\n-                int64_list=tf.train.Int64List(value=tokenized_data[\"attention_mask\"][i])\n-            ),\n-            \"labels\": tf.train.Feature(\n-                int64_list=tf.train.Int64List(value=[labels[i]])\n-            ),\n-        }\n-        features = tf.train.Features(feature=features)\n-        example = tf.train.Example(features=features)\n-        record_bytes = example.SerializeToString()\n-        file_writer.write(record_bytes)\n-```\n-\n-Build a [TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) using the saved filename to load it.\n-\n-```py\n-def decode_fn(sample):\n-    features = {\n-        \"input_ids\": tf.io.FixedLenFeature((128,), dtype=tf.int64),\n-        \"attention_mask\": tf.io.FixedLenFeature((128,), dtype=tf.int64),\n-        \"labels\": tf.io.FixedLenFeature((1,), dtype=tf.int64),\n-    }\n-    return tf.io.parse_example(sample, features)\n-\n-# TFRecordDataset can handle gs:// paths\n-tf_dataset = tf.data.TFRecordDataset([\"gs://matt-tf-tpu-tutorial-datasets/cola/dataset.tfrecords\"])\n-tf_dataset = tf_dataset.map(decode_fn)\n-tf_dataset = tf_dataset.shuffle(len(dataset)).batch(BATCH_SIZE, drop_remainder=True)\n-tf_dataset = tf_dataset.apply(\n-    tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n-)\n-```\n-\n-The dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n-\n-```py\n-model.fit(tf_dataset)\n-```\n-\n-### Stream from raw data\n-\n-Data can be stored in its native format and preprocessed in a [tf.data](https://www.tensorflow.org/guide/data) pipeline as the data is loaded. This approach isn't supported for many models with complex tokenization schemes, but some models like BERT are supported because their tokenization can be compiled. Refer to the table below to help you decide whether this approach is helpful for you.\n-\n-| pros | cons |\n-|---|---|\n-| suitable for highly compressed big data in native format (images, audio) | requires writing a full preprocessing pipeline |\n-| convenient if raw data is available in a public cloud bucket | complex preprocessing on-the-fly can hurt throughput |\n-| works on all TPU instances if data is stored in Google Cloud | must place data in cloud storage if not already there |\n-|  | not as suitable for text data because writing a tokenization pipeline is hard (use `TFRecord` for text) |\n-\n-The example below demonstrates streaming data for an image model.\n-\n-Load an image dataset and get a list of the underlying image file paths and labels.\n-\n-```py\n-from datasets import load_dataset\n-\n-image_dataset = load_dataset(\"beans\", split=\"train\")\n-filenames = image_dataset[\"image_file_path\"]\n-labels = image_dataset[\"labels\"]\n-```\n-\n-Convert the local filenames in the dataset into `gs://` paths in Google Cloud Storage.\n-\n-```py\n-# strip everything but the category directory and filenames\n-base_filenames = ['/'.join(filename.split('/')[-2:]) for filename in filenames]\n-# prepend the Google Cloud base path to everything instead\n-gs_paths = [\"gs://matt-tf-tpu-tutorial-datasets/beans/\"+filename for filename in base_filenames]\n-\n-# create tf_dataset\n-tf_dataset = tf.data.Dataset.from_tensor_slices(\n-    {\"filename\": gs_paths, \"labels\": labels}\n-)\n-tf_dataset = tf_dataset.shuffle(len(tf_dataset))\n-```\n-\n-Transformers preprocessing classes like [`AutoImageProcessor`] are framework-agnostic and can't be compiled into a pipeline by `tf.data`. To get around this, get the normalization values (`mean` and `std`) from the [`AutoImageProcessor`] and use them in the `tf.data` pipeline.\n-\n-```py\n-from transformers import AutoImageProcessor\n-\n-processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n-image_size = (processor.size[\"height\"], processor.size[\"width\"])\n-image_mean = processor.image_mean\n-image_std = processor.image_std\n-```\n-\n-Use these normalization values to create a function to load and preprocess the images.\n-\n-```py\n-BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n-\n-def decode_fn(sample):\n-    image_data = tf.io.read_file(sample[\"filename\"])\n-    image = tf.io.decode_jpeg(image_data, channels=3)\n-    image = tf.image.resize(image, image_size)\n-    array = tf.cast(image, tf.float32)\n-    array /= 255.0\n-    array = (array - image_mean) / image_std\n-    array = tf.transpose(array, perm=[2, 0, 1])\n-    return {\"pixel_values\": array, \"labels\": sample[\"labels\"]}\n-\n-tf_dataset = tf_dataset.map(decode_fn)\n-tf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)\n-print(tf_dataset.element_spec)\n-```\n-\n-The dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n-\n-```py\n-from transformers import TFAutoModelForImageClassification\n-\n-with strategy.scope():\n-    model = TFAutoModelForImageClassification.from_pretrained(image_model_checkpoint)\n-    model.compile(optimizer=\"adam\")\n-\n-model.fit(tf_dataset)\n-```\n-\n-### Stream with prepare_tf_dataset\n-\n-[`~TFPreTrainedModel.prepare_tf_dataset`] creates a `tf.data` pipeline that loads samples from [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). The pipeline uses [tf.numpy_function]() or [`~datasets.Dataset.from_generator`], which can't be compiled by TensorFlow, to access the underlying `tf.data.Dataset`. It also won't work on a Colab TPU or TPU Nodes because the pipeline streams data from a local disk. Refer to the table below to help you decide whether this approach is helpful for you.\n-\n-| pros | cons |\n-|---|---|\n-| simple code | only works on TPU VM |\n-| same approach on TPU/GPU | data must be available as a Hugging Face Dataset |\n-| dataset doesn't have to fit in memory | data must fit on local storage |\n-| supports variable padding | data loading may be a bottleneck on a big TPU pod slice |\n-\n-[`~TFPreTrainedModel.prepare_tf_dataset`] only works on [TPU VM](#tpu-types). Add the tokenizer output as columns in the dataset since the dataset is stored on disk, which means it can handle data larger than the available memory. Use [`~TFPreTrainedModel.prepare_tf_dataset`] to stream data from the dataset by wrapping it with a `tf.data` pipeline.\n-\n-```py\n-def tokenize_function(examples):\n-    return tokenizer(\n-        examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128\n-    )\n-# add the tokenizer output to the dataset as new columns\n-dataset = dataset.map(tokenize_function)\n-\n-# prepare_tf_dataset() chooses columns that match the models input names\n-tf_dataset = model.prepare_tf_dataset(\n-    dataset, batch_size=BATCH_SIZE, shuffle=True, tokenizer=tokenizer\n-)\n-```\n-\n-The dataset can now be passed to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n-\n-```py\n-from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n-with strategy.scope():\n-    model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n-    model.compile(optimizer=\"adam\")\n-\n-model.fit(tf_dataset)\n-```\n-\n-## TPU types\n-\n-There are two types of TPUs, a TPU Node and a TPU VM.\n-\n-A TPU Node indirectly accesses a remote TPU. It requires a separate VM to initialize your network and data pipeline, and then forwards it to the remote node. Google Colab TPUs are an example of a TPU Node. You can't use local data because the TPU is remotely located, and data must be stored in Google Cloud Storage where the data pipeline can access it.\n-\n-TPU VM are connected directly to the machine the TPU is located on, and they are generally easier to work with, especially when it comes to your data pipeline.\n-\n-> [!TIP]\n-> We recommend avoiding TPU Nodes if possible because it is more difficult to debug than TPU VMs. TPU Nodes may also be unsupported in the future and become a legacy access method.\n-\n-A single TPU (v2-8, v3-8, v4-8) runs 8 replicas. TPUs can exist in **pods** which run hundreds or even thousands of replicas simultaneously. When you only use a portion of a pod, it is referred to as a **pod slice**. On Google Colab, you'll typically get a single v2-8 TPU.\n-\n-## XLA\n-\n-[XLA](https://openxla.org/xla) is a linear algebra compiler for high-performance execution and it is used by default to improve performance on TPUs.\n-\n-Before executing your code on a TPU, it's a good idea to try it first on a CPU or GPU because it is easier to debug. You can train for a few steps to make sure the model and data pipeline work as expected. Set `jit_compile=True` in the [compile](https://keras.io/api/models/model_training_apis/#compile-method) method to enable XLA compilation (but remember to remove this line of code before running on a TPU).\n-\n-The section below outlines three rules for making your code XLA-compatible. Transformers enforce the first two rules for models and loss functions by default, but don't forget about them if you're writing your own models and loss functions.\n-\n-### Data dependent conditionals\n-\n-Any `if` statements cannot depend on values inside a [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor). The code below can't be compiled by XLA.\n-\n-```py\n-if tf.reduce_sum(tensor) > 10:\n-    tensor = tensor / 2.0\n-```\n-\n-To compile with XLA, use [tf.cond](https://www.tensorflow.org/api_docs/python/tf/cond) or remove the conditional and use indicator variables instead as shown below.\n-\n-```py\n-sum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\n-tensor = tensor / (1.0 + sum_over_10)\n-```\n-\n-### Data dependent shapes\n-\n-The shape of a [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) cannot depend on their values. For example, [tf.unique](https://www.tensorflow.org/api_docs/python/tf/unique) can't be compiled because it returns a tensor containing an instance of each unique value in the input. The shape of this output depends on how repetitive the input [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) is.\n-\n-This is an issue during **label masking**, where labels are set to a negative value to indicate they should be ignored when computing the loss. The code below can't be compiled by XLA because the shape of `masked_outputs` and `masked_labels` depend on how many positions are masked.\n-\n-```py\n-label_mask = labels >= 0\n-masked_outputs = outputs[label_mask]\n-masked_labels = labels[label_mask]\n-loss = compute_loss(masked_outputs, masked_labels)\n-mean_loss = torch.mean(loss)\n-```\n-\n-To compile with XLA, avoid the data-dependent shapes by computing the loss for every position and zeroing out the masked positions in both the numerator and denominator when calculating the mean. Convert `tf.bool` to `tf.float32` as an indicator variable to make your code XLA-compatible.\n-\n-```py\n-label_mask = tf.cast(labels >= 0, tf.float32)\n-loss = compute_loss(outputs, labels)\n-loss = loss * label_mask\n-mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)\n-```\n-\n-### Recompile different input shapes\n-\n-XLA recompiles your model if input shapes are variable which create huge performance problems. It is especially common in text models because input texts have variable lengths after tokenization.\n-\n-> [!WARNING]\n-> Execessive padding can also severely slow down training because requires more compute and memory to process.\n-\n-To avoid different shapes, use padding to pad all your inputs to the same length and use an `attention_mask`. Try padding batches of samples to a multiple of 32 or 64 tokens. Use the parameters `padding=\"max_length\"`, `padding=\"longest\"`, or `pad_to_multiple_of` to help with padding. This often increases the number of tokens by a small amount, but it significantly reduces the number of unique input shapes because every input shape is a multiple of 32 or 64. Fewer unique input shapes requires fewer recompilation.\n\\ No newline at end of file"
        },
        {
            "sha": "15cf6f18659a49d814b4747fd07a3b547060afa4",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 167,
            "deletions": 2,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -16,7 +16,9 @@ rendered properly in your Markdown viewer.\n \n # Serving\n \n-Transformer models can be served for inference with specialized libraries such as Text Generation Inference (TGI) and vLLM. These libraries are specifically designed to optimize performance with LLMs and include many unique optimization features that may not be included in Transformers.\n+Transformer models can be efficiently deployed using libraries such as vLLM, Text Generation Inference (TGI), and others. These libraries are designed for production-grade user-facing services, and can scale to multiple servers and millions of concurrent users.\n+\n+You can also serve transformer models easily using the `transformers serve` CLI. This is ideal for experimentation purposes, or to run models locally for personal and private use.\n \n ## TGI\n \n@@ -61,4 +63,167 @@ vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n     --task generate \\\n     --model-impl transformers \\\n     --trust-remote-code\n-```\n\\ No newline at end of file\n+```\n+\n+## Serve CLI\n+\n+> [!WARNING]\n+> This section is experimental and subject to change in future versions\n+\n+<!-- TODO: LLMs -> models, after we add audio/image input/output support -->\n+You can serve LLMs supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers a chat Completions API compatible with the OpenAI SDK, which is the _de facto_ standard for LLM conversations. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n+\n+To launch a server, simply use the `transformers serve` CLI command:\n+\n+```shell\n+transformers serve\n+```\n+\n+The simplest way to interact with the server is through our `transformers chat` CLI\n+\n+```shell\n+transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-4B\n+```\n+\n+or by sending an HTTP request with `cURL`, e.g.\n+\n+```shell\n+curl -X POST http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 1000, \"stream\": true, \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\"}'\n+```\n+\n+from which you'll receive multiple chunks in the Completions API format\n+\n+```shell\n+data: {\"object\": \"chat.completion.chunk\", \"id\": \"req_0\", \"created\": 1751377863, \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\", \"system_fingerprint\": \"\", \"choices\": [{\"delta\": {\"role\": \"assistant\", \"content\": \"\", \"tool_call_id\": null, \"tool_calls\": null}, \"index\": 0, \"finish_reason\": null, \"logprobs\": null}]}\n+\n+data: {\"object\": \"chat.completion.chunk\", \"id\": \"req_0\", \"created\": 1751377863, \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\", \"system_fingerprint\": \"\", \"choices\": [{\"delta\": {\"role\": \"assistant\", \"content\": \"\", \"tool_call_id\": null, \"tool_calls\": null}, \"index\": 0, \"finish_reason\": null, \"logprobs\": null}]}\n+\n+(...)\n+```\n+\n+The server is also an MCP client, so it can interact with MCP tools in agentic use cases. This, of course, requires the use of an LLM that is designed to use tools.\n+\n+> [!TIP]\n+> At the moment, MCP tool usage in `transformers` is limited to the `qwen` family of models.\n+\n+<!-- TODO: example with a minimal python example, and explain that it is possible to pass a full generation config in the request -->\n+\n+\n+### Usage example 1: apps with local requests (feat. Jan)\n+\n+This example shows how to use `transformers serve` as a local LLM provider for the [Jan](https://jan.ai/) app. Jan is a ChatGPT-alternative graphical interface, fully running on your machine. The requests to `transformers serve` come directly from the local app -- while this section focuses on Jan, you can extrapolate some instructions to other apps that make local requests.\n+\n+To connect `transformers serve` with Jan, you'll need to set up a new model provider (\"Settings\" > \"Model Providers\"). Click on \"Add Provider\", and set a new name. In your new model provider page, all you need to set is the \"Base URL\" to the following pattern:\n+\n+```shell\n+http://[host]:[port]/v1\n+```\n+\n+where `host` and `port` are the `transformers serve` CLI parameters (`localhost:8000` by default). After setting this up, you should be able to see some models in the \"Models\" section, hitting \"Refresh\". Make sure you add some text in the \"API key\" text field too -- this data is not actually used, but the field can't be empty. Your custom model provider page should look like this:\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_jan_model_providers.png\"/>\n+</h3>\n+\n+You are now ready to chat!\n+\n+> [!TIP]\n+> You can add any `transformers`-compatible model to Jan through `transformers serve`. In the custom model provider you created, click on the \"+\" button in the \"Models\" section and add its Hub repository name, e.g. `Qwen/Qwen3-4B`.\n+\n+To conclude this example, let's look into a more advanced use-case. If you have a beefy machine to serve models with, but prefer using Jan on a different device, you need to add port forwarding. If you have `ssh` access from your Jan machine into your server, this can be accomplished by typing the following to your Jan machine's terminal\n+\n+```\n+ssh -N -f -L 8000:localhost:8000 your_server_account@your_server_IP -p port_to_ssh_into_your_server\n+```\n+\n+Port forwarding is not Jan-specific: you can use it to connect `transformers serve` running in a different machine with an app of your choice.\n+\n+\n+### Usage example 2: apps with external requests (feat. Cursor)\n+\n+This example shows how to use `transformers serve` as a local LLM provider for [Cursor](https://cursor.com/), the popular IDE. Unlike in the previous example, requests to `transformers serve` will come from an external IP (Cursor's server IPs), which requires some additional setup. Furthermore, some of Cursor's requests require [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS), which is disabled by default for security reasons.\n+\n+To launch our server with CORS enabled, run\n+\n+```shell\n+transformers serve --enable-cors\n+```\n+\n+We'll also need to expose our server to external IPs. A potential solution is to use [`ngrok`](https://ngrok.com/), which has a permissive free tier. After setting up your `ngrok` account and authenticating on your server machine, you run\n+\n+```shell\n+ngrok http [port]\n+```\n+\n+where `port` is the port used by `transformers serve` (`8000` by default). On the terminal where you launched `ngrok`, you'll see an https address in the \"Forwarding\" row, as in the image below. This is the address to send requests to.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_ngrok.png\"/>\n+</h3>\n+\n+We're now ready to set things up on the app side! In Cursor, while we can't set a new provider, we can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set our `transformers serve` endpoint, follow this order:\n+1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n+2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n+3. Add some random text to OpenAI API Key. This field won't be used, but it can’t be empty;\n+4. Add the https address from `ngrok` to the \"Override OpenAI Base URL\" field, appending `/v1` to the address (i.e. `https://(...).ngrok-free.app/v1`);\n+5. Hit \"Verify\".\n+\n+After you follow these steps, your \"Models\" tab should look like the image below. Your server should also have received a few requests from the verification step.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor.png\"/>\n+</h3>\n+\n+You are now ready to use your local model in Cursor! For instance, if you toggle the AI Pane, you can select the model you added and ask it questions about your local files.\n+\n+<h3 align=\"center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_serve_cursor_chat.png\"/>\n+</h3>\n+\n+\n+### Usage example 3: `tiny-agents` CLI and MCP Tools\n+\n+To showcase the use of MCP tools, let's see how to integrate the `transformers serve` server with the [`tiny-agents`](https://huggingface.co/blog/python-tiny-agents) CLI.\n+\n+> [!TIP]\n+> Many Hugging Face Spaces can be used as MCP servers, as in this example. You can find all compatible Spaces [here](https://huggingface.co/spaces?filter=mcp-server).\n+\n+The first step to use MCP tools is to let the model know which tools are available. As an example, let's consider a `tiny-agents` configuration file with a reference to an [image generation MCP server](https://evalstate-flux1-schnell.hf.space/).\n+\n+```json\n+{\n+    \"model\": \"Menlo/Jan-nano\",\n+    \"endpointUrl\": \"http://localhost:8000\",\n+    \"servers\": [\n+        {\n+            \"type\": \"sse\",\n+            \"config\": {\n+                \"url\": \"https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse\"\n+            }\n+        }\n+    ]\n+}\n+```\n+\n+You can then launch your `tiny-agents` chat interface with the following command.\n+\n+```bash\n+tiny-agents run path/to/your/config.json\n+```\n+\n+If you have `transformers serve` running in the background, you're ready to use MCP tools from a local model! For instance, here's the example of a chat session with `tiny-agents`:\n+\n+```bash\n+Agent loaded with 1 tools:\n+ • flux1_schnell_infer\n+»  Generate an image of a cat on the moon\n+<Tool req_0_tool_call>flux1_schnell_infer {\"prompt\": \"a cat on the moon\", \"seed\": 42, \"randomize_seed\": true, \"width\": 1024, \"height\": 1024, \"num_inference_steps\": 4}\n+\n+Tool req_0_tool_call\n+[Binary Content: Image image/webp, 57732 bytes]\n+The task is complete and the content accessible to the User\n+Image URL: https://evalstate-flux1-schnell.hf.space/gradio_api/file=/tmp/gradio/3dbddc0e53b5a865ed56a4e3dbdd30f3f61cf3b8aabf1b456f43e5241bd968b8/image.webp\n+380576952\n+\n+I have generated an image of a cat on the moon using the Flux 1 Schnell Image Generator. The image is 1024x1024 pixels and was created with 4 inference steps. Let me know if you would like to make any changes or need further assistance!\n+```"
        },
        {
            "sha": "c8fb13ff6aecc903efb97c93bd6d8c1a32dee970",
            "filename": "docs/source/en/tf_xla.md",
            "status": "removed",
            "additions": 0,
            "deletions": 129,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15b06d8dc6fa132550311d63c9758b580f39bcc/docs%2Fsource%2Fen%2Ftf_xla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15b06d8dc6fa132550311d63c9758b580f39bcc/docs%2Fsource%2Fen%2Ftf_xla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftf_xla.md?ref=e15b06d8dc6fa132550311d63c9758b580f39bcc",
            "patch": "@@ -1,129 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# XLA\n-\n-[[open-in-colab]]\n-\n-[Accelerated Linear Algebra (XLA)](https://openxla.org/xla) is a linear algebra compiler that optimizes model runtime across different hardware and frameworks.\n-\n-This guide will look specifically at how to accelerate *TensorFlow* models with XLA.\n-\n-## TensorFlow\n-\n-XLA can potentially accelerate a TensorFlow model without making any source code changes. It is already packaged with the TensorFlow library, and it is triggered with `jit_compile` in any graph creating function such as [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n-\n-If you're using Keras methods like [fit](https://keras.io/api/models/model_training_apis/#fit-method) and [predict](https://keras.io/api/models/model_training_apis/#predict-method), enable XLA by passing `jit_compile=True` to [compile](https://keras.io/api/models/model_training_apis/#compile-method).\n-\n-```py\n-model.compile(jit_compile=True)\n-```\n-\n-XLA can be used to accelerate any arbitrary [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n-\n-Models with a TensorFlow implementation like [GPT2](./model_doc/gpt2), [T5](./model_doc/t5), [OPT](./model_doc/opt), and [Whisper](./model_doc/whisper) are XLA compatible. The speed up depends on a model, but in general, TensorFlow models in Transformers get a ~100x speed up.\n-\n-### Functions\n-\n-A typical forward pass in a TensorFlow model is shown below. To run a forward pass with XLA, wrap the model with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function) and set `jit_compile=True`.\n-\n-```diff\n-import tensorflow as tf\n-\n-model = tf.keras.Sequential(\n-    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n-)\n-# Generate random inputs for the model.\n-batch_size = 16\n-input_vector_dim = 10\n-random_inputs = tf.random.normal((batch_size, input_vector_dim))\n-\n-# Run a forward pass.\n-- _ = model(random_inputs)\n-+ xla_fn = tf.function(model, jit_compile=True)\n-+ _ = xla_fn(random_inputs)\n-```\n-\n-The default `call` function of the model is used to compile the XLA graph. But if there's any other model function you want to compile with XLA, wrap them with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n-\n-```py\n-my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n-```\n-\n-### Text generation\n-\n-You could also compile other model functions with XLA. For example, enable XLA for text generation by wrapping [`~TFGenerationMixin.generate`] with [tf.function](https://www.tensorflow.org/api_docs/python/tf/function).\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-# Will error if the minimal version of Transformers is not installed.\n-from transformers.utils import check_min_version\n-\n-check_min_version(\"4.21.0\")\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-tokenized_input = tokenizer(input_string, return_tensors=\"tf\")\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-\"Generated -- TensorFlow is an open-source, open-source, distributed-source application framework for the\"\n-```\n-\n-## Tracing\n-\n-When executing an XLA-enabled function for the first time, it tries to infer the computation graph in a process known as *tracing*. This is a time-consuming step, but any consecutive calls to the function will be much faster because it won't have to trace the computation graph again.\n-\n-To ensure a function is only traced once, the inputs must have the same shape as when the graph was built. This usually isn't an issue for fixed input shapes like images, but it can be an issue for inputs with variable shapes like text.\n-\n-One way to handle this is to pad your text so it always has the same shape. Configure padding options such as [pad_to_multiple_of](https://hf.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad.pad_to_multiple_of) in the tokenizer.\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-# Call tokenizer with padding options.\n-tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-```\n-\n-In addition to the input shape, any changes to the generation options at any point also triggers tracing.\n-\n-## Resources\n-\n-Learn more about XLA with the following resources.\n-\n-- A [notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) demonstrating XLA-compatible encoder-decoder and decoder-only text generation models.\n-- The [Faster Text Generation with TensorFlow and XLA](https://hf.co/blog/tf-xla-generate) blog post compares benchmarks for XLA-compatible models and provides a friendly introduction to XLA in TensorFlow.\n-- The [How Hugging Face improved Text Generation performance with XLA](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) blog post discusses the design philosophy behind adding XLA to TensorFlow models in Transformers.\n-- The [Introduction to graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs) guide.\n-- The [Better performance with tf.function](https://www.tensorflow.org/guide/function) guide.\n-- The [XLA](https://openxla.org/xla) documentation."
        },
        {
            "sha": "b43f9e4aef6a2df5538d309c958d6bc81ebfd244",
            "filename": "docs/source/en/tools.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Ftools.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/docs%2Fsource%2Fen%2Ftools.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftools.md?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -14,5 +14,9 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+# Tools\n+\n+(deprecated)\n+\n > [!WARNING]\n > Agents and tools were spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. They were removed from `transformers` in v4.52."
        },
        {
            "sha": "a8ecfdae9a5663857db791e3933ed647f02f58ba",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 82,
            "deletions": 32,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -21,8 +21,6 @@\n from typing import Any, Optional\n \n from huggingface_hub import (\n-    ChatCompletionStreamOutputChoice,\n-    ChatCompletionStreamOutputDelta,\n     ChatCompletionStreamOutputDeltaToolCall,\n     ChatCompletionStreamOutputFunction,\n     ModelInfo,\n@@ -52,6 +50,7 @@\n if is_pydantic_available() and is_fastapi_available() and is_uvicorn_available():\n     import uvicorn\n     from fastapi import FastAPI\n+    from fastapi.middleware.cors import CORSMiddleware\n     from fastapi.responses import JSONResponse, StreamingResponse\n     from pydantic import BaseModel\n \n@@ -133,15 +132,17 @@ def create_generation_config_from_req(req: \"ChatCompletionInput\") -> \"Generation\n         generation_config = GenerationConfig()\n \n     if req.frequency_penalty is not None:\n-        generation_config.repetition_penalty = req.frequency_penalty\n+        generation_config.repetition_penalty = float(req.frequency_penalty)\n     if req.logit_bias is not None:\n         generation_config.sequence_bias = req.logit_bias\n     if req.stop is not None:\n         generation_config.stop_strings = req.stop\n     if req.temperature is not None:\n-        generation_config.temperature = req.temperature\n+        generation_config.temperature = float(req.temperature)\n+        if float(req.temperature) == 0.0:\n+            generation_config.do_sample = False\n     if req.top_p is not None:\n-        generation_config.top_p = req.top_p\n+        generation_config.top_p = float(req.top_p)\n     if req.seed is not None:\n         torch.manual_seed(req.seed)\n \n@@ -202,13 +203,22 @@ class ServeArguments:\n     use_bnb_nested_quant: bool = field(default=False, metadata={\"help\": \"Whether to use nested quantization.\"})\n \n     # Serving settings\n-    host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to..\"})\n+    host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n \n     # Other settings\n     log_level: str = field(\n         default=\"info\", metadata={\"help\": \"Logging level as a string. Example: 'info' or 'warning'.\"}\n     )\n+    enable_cors: bool = field(\n+        default=False,\n+        metadata={\n+            \"help\": (\n+                \"Whether to enable CORS. Some apps that make requests from external domains (e.g. Cursor) require \"\n+                \"CORS to be enabled.\"\n+            ),\n+        },\n+    )\n \n \n class ServeCommand(BaseTransformersCLICommand):\n@@ -236,6 +246,7 @@ def __init__(self, args: ServeArguments):\n \n         self.args = args\n         self.use_continuous_batching = self.args.attn_implementation == \"sdpa_paged\"\n+        self.enable_cors = self.args.enable_cors\n \n         # State: preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n         # cache and avoid re-running prefil\n@@ -250,36 +261,66 @@ def __init__(self, args: ServeArguments):\n \n     def build_chunk(\n         self,\n-        content: str,\n         request_id: str,\n+        content: Optional[str] = None,\n         role: Optional[str] = None,\n         finish_reason: Optional[str] = None,\n         tool_calls: Optional[list[ChatCompletionStreamOutputDeltaToolCall]] = None,\n     ) -> str:\n+        \"\"\"\n+        Builds a chunk of a streaming response.\n+\n+        IMPORTANT: The built chunk won't contain empty fields (fields with `None`). Some downstream apps, like Cursor,\n+        assume that when the field exists, it has data.\n+\n+        Args:\n+            request_id (`str`):\n+                The request ID.\n+            content (`str`, *optional*):\n+                Content of the response from the model.\n+            role (`str`, *optional*):\n+                The role of the next content, until a new role is defined.\n+            finish_reason (`str`, *optional*):\n+                The reason the generation by the model has finished.\n+            tool_calls (`list[ChatCompletionStreamOutputDeltaToolCall]`, *optional*):\n+                Data about the tool calls, when they are triggered.\n+\n+        Returns:\n+            `str`: The built chunk, a string containing a JSON string with the payload.\n+        \"\"\"\n         payload = {\n             \"object\": \"chat.completion.chunk\",\n             \"id\": request_id,\n             \"created\": int(time.time()),\n             \"model\": self.loaded_model,\n+            \"choices\": [{\"delta\": {}, \"index\": 0}],\n             \"system_fingerprint\": \"\",\n-            \"choices\": [\n-                ChatCompletionStreamOutputChoice(\n-                    delta=ChatCompletionStreamOutputDelta(\n-                        role=role,\n-                        content=content,\n-                        tool_calls=tool_calls,\n-                    ),\n-                    index=0,\n-                    logprobs=None,\n-                    finish_reason=finish_reason,\n-                ),\n-            ],\n         }\n+        if content is not None:\n+            payload[\"choices\"][0][\"delta\"][\"content\"] = content\n+        if role is not None:\n+            payload[\"choices\"][0][\"delta\"][\"role\"] = role\n+        if tool_calls is not None:\n+            payload[\"choices\"][0][\"delta\"][\"tool_calls\"] = tool_calls\n+        if finish_reason is not None:\n+            payload[\"choices\"][0][\"finish_reason\"] = finish_reason\n+\n         return f\"data: {json.dumps(payload)}\\n\\n\"\n \n     def run(self):\n         app = FastAPI()\n \n+        # Some apps that make requests from external domains (e.g. Cursor) require CORS to be enabled. However, for\n+        # security purposes, it's disabled by default\n+        if self.enable_cors:\n+            app.add_middleware(\n+                CORSMiddleware,\n+                allow_origins=[\"*\"],\n+                allow_credentials=True,\n+                allow_methods=[\"*\"],\n+                allow_headers=[\"*\"],\n+            )\n+\n         if self.use_continuous_batching:\n             self.continuous_batching(app)\n         else:\n@@ -361,7 +402,7 @@ def _serve(req: \"ChatCompletionInput\"):\n \n             def stream_response(_inputs):\n                 try:\n-                    max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 256\n+                    max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n                     request_id = manager.add_request(_inputs, request_id=req.request_id, max_new_tokens=max_new_tokens)\n                     queue_is_flushed = False\n \n@@ -373,7 +414,9 @@ def stream_response(_inputs):\n                                 queue_is_flushed = True\n \n                         finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n-                        yield self.build_chunk(result.next_token, request_id=request_id, finish_reason=finish_reason)\n+                        yield self.build_chunk(\n+                            request_id=request_id, content=result.next_token, finish_reason=finish_reason\n+                        )\n \n                         if result.status == RequestStatus.FINISHED:\n                             break\n@@ -401,7 +444,7 @@ def is_continuation(self, req: \"ChatCompletionInput\") -> bool:\n         # No cached messages: this is a new request\n         if self.last_messages is None:\n             req_continues_last_messages = False\n-        # The new request has fewer rounds of conversation: this is a new request\n+        # The new request has no new rounds of conversation: this is a new request\n         elif len(self.last_messages) >= len(req.messages):\n             req_continues_last_messages = False\n         # Otherwise, check that the last messages are a subset of the new request\n@@ -417,6 +460,7 @@ def is_continuation(self, req: \"ChatCompletionInput\") -> bool:\n     def generate(self, app):\n         @app.post(\"/v1/chat/completions\")\n         def _serve(req: \"ChatCompletionInput\"):\n+            logger.debug(f\"Received request: {req}\")\n             update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n \n             if update_model:\n@@ -454,7 +498,7 @@ def _serve(req: \"ChatCompletionInput\"):\n             generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n \n             generation_config = create_generation_config_from_req(req)\n-            max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 256\n+            max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n             generation_config.max_new_tokens = max_new_tokens\n \n             last_kv_cache = None\n@@ -482,7 +526,14 @@ def generate_with_cache(**kwargs):\n                     thread.start()\n                     tool_state = ToolState()\n \n+                    # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n+                    # they come from the assistant.\n+                    logger.debug(\"Starting model output\")\n+                    yield self.build_chunk(_request_id, role=\"assistant\")\n+\n                     for result in streamer:\n+                        logger.debug(f\"Model output: {result}\")\n+\n                         # ====== TOOL CALL LOGIC ======\n                         if tool_model_family is not None:\n                             # Start of a tool call: reset state variables, set `inside_tool_call`\n@@ -493,7 +544,7 @@ def generate_with_cache(**kwargs):\n                             # End of tool call: reset `inside_tool_call`, emit a `finish_reason`\n                             if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"end\"]:\n                                 tool_state.reset()\n-                                yield self.build_chunk(\"\", _request_id, role=None, finish_reason=\"tool_calls\")\n+                                yield self.build_chunk(_request_id, finish_reason=\"tool_calls\")\n                                 continue\n \n                             # Inside a tool call\n@@ -512,7 +563,6 @@ def generate_with_cache(**kwargs):\n                                     tool = ChatCompletionStreamOutputDeltaToolCall(\n                                         function=ChatCompletionStreamOutputFunction(\n                                             name=tool_name,\n-                                            arguments=None,\n                                         ),\n                                         index=0,\n                                         type=\"function\",\n@@ -543,16 +593,16 @@ def generate_with_cache(**kwargs):\n                                         ),\n                                         index=0,\n                                         type=\"function\",\n-                                        id=None,\n                                     )\n \n-                                yield self.build_chunk(None, _request_id, role=None, tool_calls=[tool])\n+                                yield self.build_chunk(_request_id, tool_calls=[tool])\n                                 continue\n                         # ====== END OF TOOL CALL LOGIC ======\n \n-                        # All non-tool related tokens are emitted as assistant messages\n-                        yield self.build_chunk(result, _request_id, role=\"assistant\")\n-                    yield self.build_chunk(None, _request_id, role=None, finish_reason=\"stop\")\n+                        # All non-tool related tokens are emitted as assistant messages. Empty text is skipped.\n+                        if result != \"\":\n+                            yield self.build_chunk(_request_id, content=result)\n+                    yield self.build_chunk(_request_id, finish_reason=\"stop\")\n \n                     thread.join()\n                 except Exception as e:\n@@ -620,8 +670,8 @@ def load_model_and_tokenizer(\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n \n-        if model.generation_config.max_new_tokens is not None and model.generation_config.max_new_tokens < 256:\n-            model.generation_config.max_new_tokens = 256\n+        if model.generation_config.max_new_tokens is not None and model.generation_config.max_new_tokens < 1024:\n+            model.generation_config.max_new_tokens = 1024\n \n         if getattr(model, \"hf_device_map\", None) is None:\n             model = model.to(args.device)"
        },
        {
            "sha": "a03693922f219391ab5832dce109e55a86a08f5d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -1776,6 +1776,10 @@ def _prepare_generation_config(\n                     ):\n                         modified_values[key] = model_gen_config_value\n                         setattr(generation_config, key, model_gen_config_value)\n+                # edge case: we may set `temperature=0.0` and `do_sample=False`, but the model defaults to\n+                # `do_sample=True`\n+                if generation_config.temperature == 0.0:\n+                    generation_config.do_sample = False\n                 if use_model_defaults is None and len(modified_values) > 0:\n                     logger.warning_once(\n                         f\"`generation_config` default values have been modified to match model-specific defaults: \""
        },
        {
            "sha": "b35127b9167fa45423063d464927939c90f31aa6",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=85d93cc6e3ad83d9f6417e3fadd8e444c52e40d7",
            "patch": "@@ -278,7 +278,6 @@ docs/source/en/perf_train_cpu_many.md\n docs/source/en/perf_train_gpu_many.md\n docs/source/en/perf_train_gpu_one.md\n docs/source/en/perf_train_special.md\n-docs/source/en/perf_train_tpu_tf.md\n docs/source/en/perplexity.md\n docs/source/en/philosophy.md\n docs/source/en/pipeline_webserver.md\n@@ -307,7 +306,6 @@ docs/source/en/tasks/video_classification.md\n docs/source/en/tasks/visual_question_answering.md\n docs/source/en/tasks/zero_shot_image_classification.md\n docs/source/en/tasks/zero_shot_object_detection.md\n-docs/source/en/tf_xla.md\n docs/source/en/tflite.md\n docs/source/en/tokenizer_summary.md\n docs/source/en/torchscript.md"
        }
    ],
    "stats": {
        "total": 854,
        "additions": 263,
        "deletions": 591
    }
}