{
    "author": "sbucaille",
    "message": "[image-processing] deprecate `plot_keypoint_matching`, make `visualize_keypoint_matching` as a standard (#39830)\n\n* fix: deprecate plot_keypoint_matching and make visualize_keypoint_matching for all Keypoint Matching models\n\n* refactor: added copied from\n\n* fix: make style\n\n* fix: repo consistency\n\n* fix: make style\n\n* docs: added missing method in SuperGlue docs",
    "sha": "1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
    "files": [
        {
            "sha": "02002857324a51fbd8c5d539e7674c89fc702e24",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -107,7 +107,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n \n     ```py\n     # Easy visualization using the built-in plotting method\n-    processor.plot_keypoint_matching(images, processed_outputs)\n+    processor.visualize_keypoint_matching(images, processed_outputs)\n     ```\n \n <div class=\"flex justify-center\">\n@@ -128,7 +128,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n \n - preprocess\n - post_process_keypoint_matching\n-- plot_keypoint_matching\n+- visualize_keypoint_matching\n \n <frameworkcontent>\n <pt>"
        },
        {
            "sha": "acbf3561ca365b66e9d18d7384c3b717b516a4d2",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 4,
            "deletions": 30,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -103,38 +103,11 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n             print(f\"Keypoint at {keypoint0.numpy()} matches with keypoint at {keypoint1.numpy()} with score {matching_score}\")\n     ```\n \n-- The example below demonstrates how to visualize matches between two images.\n+- Visualize the matches between the images using the built-in plotting functionality.\n \n     ```py\n-    import matplotlib.pyplot as plt\n-    import numpy as np\n-\n-    # Create side by side image\n-    merged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\n-    merged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\n-    merged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\n-    plt.imshow(merged_image)\n-    plt.axis(\"off\")\n-\n-    # Retrieve the keypoints and matches\n-    output = processed_outputs[0]\n-    keypoints0 = output[\"keypoints0\"]\n-    keypoints1 = output[\"keypoints1\"]\n-    matching_scores = output[\"matching_scores\"]\n-\n-    # Plot the matches\n-    for keypoint0, keypoint1, matching_score in zip(keypoints0, keypoints1, matching_scores):\n-        plt.plot(\n-            [keypoint0[0], keypoint1[0] + image1.width],\n-            [keypoint0[1], keypoint1[1]],\n-            color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n-            alpha=0.9,\n-            linewidth=0.5,\n-        )\n-        plt.scatter(keypoint0[0], keypoint0[1], c=\"black\", s=2)\n-        plt.scatter(keypoint1[0] + image1.width, keypoint1[1], c=\"black\", s=2)\n-\n-    plt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\n+    # Easy visualization using the built-in plotting method\n+    processor.visualize_keypoint_matching(images, processed_outputs)\n     ```\n \n <div class=\"flex justify-center\">\n@@ -155,6 +128,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n \n - preprocess\n - post_process_keypoint_matching\n+- visualize_keypoint_matching\n \n <frameworkcontent>\n <pt>"
        },
        {
            "sha": "5663e21fcae531ae3fb9cf3305f636ad34214c66",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -408,7 +408,7 @@ def visualize_keypoint_matching(\n             images (`ImageInput`):\n                 Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n                 images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            outputs (List[Dict[str, torch.Tensor]]]):\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n                 A post processed keypoint matching output\n \n         Returns:"
        },
        {
            "sha": "c389929eea309350436957c10266b53c914751d4",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 75,
            "deletions": 1,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -17,6 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n from typing import Optional, Union\n \n import numpy as np\n@@ -44,6 +45,9 @@\n from .modeling_lightglue import LightGlueKeypointMatchingOutput\n \n \n+if is_vision_available():\n+    from PIL import Image, ImageDraw\n+\n if is_vision_available():\n     import PIL\n \n@@ -402,18 +406,88 @@ def post_process_keypoint_matching(\n \n         return results\n \n+    def visualize_keypoint_matching(\n+        self,\n+        images: ImageInput,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n+            plot_image[:height0, :width0] = image_pair[0]\n+            plot_image[:height1, width0:] = image_pair[1]\n+\n+            plot_image_pil = Image.fromarray(plot_image)\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return (r, g, b)\n+\n     def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n         \"\"\"\n         Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n         matplotlib to be installed.\n \n+        .. deprecated::\n+            `plot_keypoint_matching` is deprecated and will be removed in a future version. Use `visualize_keypoint_matching` instead.\n+\n         Args:\n             images (`ImageInput`):\n                 Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n                 a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            outputs ([`LightGlueKeypointMatchingOutput`]):\n+            keypoint_matching_output ([`LightGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n         \"\"\"\n+        warnings.warn(\n+            \"`plot_keypoint_matching` is deprecated and will be removed in transformers v. \"\n+            \"Use `visualize_keypoint_matching` instead.\",\n+            FutureWarning,\n+        )\n+\n         if is_matplotlib_available():\n             import matplotlib.pyplot as plt\n         else:"
        },
        {
            "sha": "2801727e43fbdb46e4dbd6de0899c21df51e7527",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 79,
            "deletions": 2,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n \n@@ -20,7 +21,7 @@\n from torch.nn.utils.rnn import pad_sequence\n \n from ...configuration_utils import PretrainedConfig\n-from ...image_utils import ImageInput, to_numpy_array\n+from ...image_utils import ImageInput, is_vision_available, to_numpy_array\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -35,6 +36,10 @@\n from ..superpoint import SuperPointConfig\n \n \n+if is_vision_available():\n+    from PIL import Image, ImageDraw\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -220,18 +225,90 @@ def post_process_keypoint_matching(\n     ) -> list[dict[str, torch.Tensor]]:\n         return super().post_process_keypoint_matching(outputs, target_sizes, threshold)\n \n+    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor.visualize_keypoint_matching with EfficientLoFTR->LightGlue\n+    def visualize_keypoint_matching(\n+        self,\n+        images: ImageInput,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n+            plot_image[:height0, :width0] = image_pair[0]\n+            plot_image[:height1, width0:] = image_pair[1]\n+\n+            plot_image_pil = Image.fromarray(plot_image)\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor._get_color\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return (r, g, b)\n+\n     def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n         \"\"\"\n         Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n         matplotlib to be installed.\n \n+        .. deprecated::\n+            `plot_keypoint_matching` is deprecated and will be removed in a future version. Use `visualize_keypoint_matching` instead.\n+\n         Args:\n             images (`ImageInput`):\n                 Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n                 a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            outputs ([`LightGlueKeypointMatchingOutput`]):\n+            keypoint_matching_output ([`LightGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n         \"\"\"\n+        warnings.warn(\n+            \"`plot_keypoint_matching` is deprecated and will be removed in transformers v. \"\n+            \"Use `visualize_keypoint_matching` instead.\",\n+            FutureWarning,\n+        )\n+\n         if is_matplotlib_available():\n             import matplotlib.pyplot as plt\n         else:"
        },
        {
            "sha": "f02e2a9f65c04402f2046e4514aec04ad8f58302",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec0feccdd7423fb99f13ead4a7e79fdea0a1890/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=1ec0feccdd7423fb99f13ead4a7e79fdea0a1890",
            "patch": "@@ -47,6 +47,7 @@\n \n if is_vision_available():\n     import PIL\n+    from PIL import Image, ImageDraw\n \n logger = logging.get_logger(__name__)\n \n@@ -406,5 +407,68 @@ def post_process_keypoint_matching(\n \n         return results\n \n+    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor.visualize_keypoint_matching with EfficientLoFTR->SuperGlue\n+    def visualize_keypoint_matching(\n+        self,\n+        images: ImageInput,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `SuperGlueImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n+            plot_image[:height0, :width0] = image_pair[0]\n+            plot_image[:height1, width0:] = image_pair[1]\n+\n+            plot_image_pil = Image.fromarray(plot_image)\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor._get_color\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return (r, g, b)\n+\n \n __all__ = [\"SuperGlueImageProcessor\"]"
        }
    ],
    "stats": {
        "total": 261,
        "additions": 225,
        "deletions": 36
    }
}