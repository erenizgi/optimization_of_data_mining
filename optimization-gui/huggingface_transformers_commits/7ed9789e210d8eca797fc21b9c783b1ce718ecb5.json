{
    "author": "zucchini-nlp",
    "message": "Fix: `num_logits_to_keep` in composite models (#33168)\n\n* fix\r\n\r\n* paligemma",
    "sha": "7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
    "files": [
        {
            "sha": "d22d6d208bc467c8e855c24cd492a0e6c22297bc",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1118,7 +1118,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1169,14 +1169,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "92f3c57d86703e33e6ced8b9f7978ec5940b3953",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1382,7 +1382,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1433,14 +1433,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "23eaee57857a44af07b8751a3f0ba814db751a71",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1131,7 +1131,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1181,14 +1181,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "3035afdf94051c1c097958368b93c7a5b99755aa",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1094,7 +1094,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1148,14 +1148,17 @@ def prepare_inputs_for_generation(\n                 cache_position=cache_position,\n                 batch_size=batch_size,\n             )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "f57bdd27fee6d9b21a62bb971733eb931a0134d3",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1643,7 +1643,7 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         inputs_embeds=None,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         past_length = 0\n@@ -1687,6 +1687,9 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids}\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         image_hidden_states = kwargs.get(\"image_hidden_states\", None)\n         if image_hidden_states is not None:\n             pixel_values = None\n@@ -1703,7 +1706,6 @@ def prepare_inputs_for_generation(\n                 \"pixel_values\": pixel_values,\n                 \"pixel_attention_mask\": pixel_attention_mask,\n                 \"image_hidden_states\": image_hidden_states,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "c273b021d736647fa9601a9a82bd7e65e236df7b",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1348,7 +1348,7 @@ def prepare_inputs_for_generation(\n         output_router_logits=False,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1373,6 +1373,9 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n@@ -1381,7 +1384,6 @@ def prepare_inputs_for_generation(\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n                 \"output_router_logits\": output_router_logits,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "051b4f539cdd94e108ac28823c72304b269724b1",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1244,7 +1244,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1295,14 +1295,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "ae53156d9ba2cdd014142101869d7a1611f44e4e",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -377,6 +377,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -385,6 +386,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+\n         Returns:\n \n         Example:\n@@ -518,6 +525,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -558,6 +566,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         attention_mask=None,\n         cache_position=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n@@ -572,6 +581,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "5fe029f13e73494f68d9e5fb088bc22eb17c8e4e",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -721,6 +721,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -729,6 +730,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -890,6 +896,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -931,6 +938,7 @@ def prepare_inputs_for_generation(\n         image_sizes=None,\n         attention_mask=None,\n         cache_position=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         legacy_processing = (\n@@ -944,6 +952,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "78e8c5a077233bd0321f398a59ad76568f3b4427",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -767,6 +767,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -778,6 +779,10 @@ def forward(\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n \n         Returns:\n \n@@ -973,6 +978,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -1014,6 +1020,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_sizes=None,\n         attention_mask=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         if past_key_values is not None:\n@@ -1057,6 +1064,9 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids}\n \n+        if \"num_logits_to_keep\" != None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,"
        },
        {
            "sha": "99cc80bdaec357927c207d3611277f5daf85e320",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1087,7 +1087,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1115,14 +1115,16 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "22aa9010692a3947655a0977e9be65e02ef7e39d",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1348,7 +1348,7 @@ def prepare_inputs_for_generation(\n         output_router_logits=False,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1373,6 +1373,9 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n@@ -1381,7 +1384,6 @@ def prepare_inputs_for_generation(\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n                 \"output_router_logits\": output_router_logits,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "719f3ff2fd172dd59e01970c46bad6ded2d8dac0",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1123,7 +1123,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1174,14 +1174,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "ccc376232a9daa225127b9345b0dfafe31bc8faa",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1162,7 +1162,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1213,14 +1213,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "7c09177e27d791b1c3022a572722fa41ca767543",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -356,6 +356,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -364,6 +365,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -458,6 +464,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs.logits\n@@ -503,6 +510,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         token_type_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n@@ -511,6 +519,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "193dc860bd13dc159f56126b0ca6ffb828dbe19c",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -972,7 +972,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1023,14 +1023,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "f20a0074702f935948286edd71f7cdf5e3af0255",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1264,7 +1264,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1315,14 +1315,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "c1857b73ec393dab0cbafba4c59de1dacd69c273",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1304,7 +1304,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1355,14 +1355,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "55f7c00bb394ff3eb57e5f4437a23938f7e31d45",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1162,7 +1162,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1213,14 +1213,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "3e3c331e91cb47fcea75dbc9239018416139e779",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1358,7 +1358,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1409,14 +1409,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "d86770c408cddbe0d94bc3026023a3e751c7fa3a",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1250,7 +1250,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1301,14 +1301,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "b904102f45080f0dc118513525faed9332551189",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -1138,7 +1138,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1189,14 +1189,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "b9263ad15cbf93b802d13499ca68fc3152531439",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -417,6 +417,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -425,6 +426,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -627,6 +633,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -668,6 +675,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         attention_mask=None,\n         cache_position=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n@@ -682,6 +690,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "e036d6fb766744252f1d23d1d2861079e4c93e2e",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ed9789e210d8eca797fc21b9c783b1ce718ecb5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=7ed9789e210d8eca797fc21b9c783b1ce718ecb5",
            "patch": "@@ -379,6 +379,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -387,6 +388,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+\n         Returns:\n \n         Example:\n@@ -512,6 +519,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -552,6 +560,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         attention_mask=None,\n         cache_position=None,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n@@ -566,6 +575,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n "
        }
    ],
    "stats": {
        "total": 166,
        "additions": 130,
        "deletions": 36
    }
}