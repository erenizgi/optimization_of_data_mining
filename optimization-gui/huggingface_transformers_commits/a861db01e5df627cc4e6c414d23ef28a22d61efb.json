{
    "author": "MekkCyber",
    "message": "Fix Device map for bitsandbytes tests (#36800)\n\nfix",
    "sha": "a861db01e5df627cc4e6c414d23ef28a22d61efb",
    "files": [
        {
            "sha": "bf137a6af55731c8c027944476ed228151780913",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/a861db01e5df627cc4e6c414d23ef28a22d61efb/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a861db01e5df627cc4e6c414d23ef28a22d61efb/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=a861db01e5df627cc4e6c414d23ef28a22d61efb",
            "patch": "@@ -528,9 +528,39 @@ def test_multi_gpu_loading(self):\n         This tests that the model has been loaded and can be used correctly on a multi-GPU setup.\n         Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice\n         \"\"\"\n+        device_map = {\n+            \"transformer.word_embeddings\": 0,\n+            \"transformer.word_embeddings_layernorm\": 0,\n+            \"lm_head\": 0,\n+            \"transformer.h.0\": 0,\n+            \"transformer.h.1\": 0,\n+            \"transformer.h.2\": 0,\n+            \"transformer.h.3\": 0,\n+            \"transformer.h.4\": 0,\n+            \"transformer.h.5\": 0,\n+            \"transformer.h.6\": 0,\n+            \"transformer.h.7\": 0,\n+            \"transformer.h.8\": 0,\n+            \"transformer.h.9\": 0,\n+            \"transformer.h.10\": 1,\n+            \"transformer.h.11\": 1,\n+            \"transformer.h.12\": 1,\n+            \"transformer.h.13\": 1,\n+            \"transformer.h.14\": 1,\n+            \"transformer.h.15\": 1,\n+            \"transformer.h.16\": 1,\n+            \"transformer.h.17\": 0,\n+            \"transformer.h.18\": 0,\n+            \"transformer.h.19\": 0,\n+            \"transformer.h.20\": 0,\n+            \"transformer.h.21\": 0,\n+            \"transformer.h.22\": 0,\n+            \"transformer.h.23\": 1,\n+            \"transformer.ln_f\": 0,\n+        }\n \n         model_parallel = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, load_in_4bit=True, device_map=\"balanced\"\n+            self.model_name, load_in_4bit=True, device_map=device_map\n         )\n \n         # Check correct device map"
        },
        {
            "sha": "bc7804de9b87d064146e6a46567b0746920bdc37",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/a861db01e5df627cc4e6c414d23ef28a22d61efb/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a861db01e5df627cc4e6c414d23ef28a22d61efb/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=a861db01e5df627cc4e6c414d23ef28a22d61efb",
            "patch": "@@ -682,9 +682,39 @@ def test_multi_gpu_loading(self):\n         This tests that the model has been loaded and can be used correctly on a multi-GPU setup.\n         Let's just try to load a model on 2 GPUs and see if it works. The model we test has ~2GB of total, 3GB should suffice\n         \"\"\"\n+        device_map = {\n+            \"transformer.word_embeddings\": 0,\n+            \"transformer.word_embeddings_layernorm\": 0,\n+            \"lm_head\": 0,\n+            \"transformer.h.0\": 0,\n+            \"transformer.h.1\": 0,\n+            \"transformer.h.2\": 0,\n+            \"transformer.h.3\": 0,\n+            \"transformer.h.4\": 0,\n+            \"transformer.h.5\": 0,\n+            \"transformer.h.6\": 0,\n+            \"transformer.h.7\": 0,\n+            \"transformer.h.8\": 0,\n+            \"transformer.h.9\": 0,\n+            \"transformer.h.10\": 1,\n+            \"transformer.h.11\": 1,\n+            \"transformer.h.12\": 1,\n+            \"transformer.h.13\": 1,\n+            \"transformer.h.14\": 1,\n+            \"transformer.h.15\": 1,\n+            \"transformer.h.16\": 1,\n+            \"transformer.h.17\": 0,\n+            \"transformer.h.18\": 0,\n+            \"transformer.h.19\": 0,\n+            \"transformer.h.20\": 0,\n+            \"transformer.h.21\": 0,\n+            \"transformer.h.22\": 0,\n+            \"transformer.h.23\": 1,\n+            \"transformer.ln_f\": 0,\n+        }\n \n         model_parallel = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, load_in_8bit=True, device_map=\"balanced\"\n+            self.model_name, load_in_8bit=True, device_map=device_map\n         )\n \n         # Check correct device map"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 62,
        "deletions": 2
    }
}