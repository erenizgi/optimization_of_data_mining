{
    "author": "Rocketknight1",
    "message": "Post-PR fixes! (#38868)\n\n* Post-PR fixes!\n\n* make fix-copies",
    "sha": "d058f81e5bac1a52a7e9acfd30526f7bcbcae40a",
    "files": [
        {
            "sha": "feb7c790113db2239fdb34dfbe35fad85e95b63d",
            "filename": "src/transformers/models/lightglue/convert_lightglue_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py?ref=d058f81e5bac1a52a7e9acfd30526f7bcbcae40a",
            "patch": "@@ -15,7 +15,6 @@\n import gc\n import os\n import re\n-from typing import List\n \n import torch\n from datasets import load_dataset\n@@ -90,7 +89,7 @@ def verify_model_outputs(model, device):\n }\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: List[str]):\n+def convert_old_keys_to_new_keys(state_dict_keys: list[str]):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "ca9189210bac3359110531ff0b4466552bc6f7cd",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=d058f81e5bac1a52a7e9acfd30526f7bcbcae40a",
            "patch": "@@ -17,7 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -139,7 +139,7 @@ class LightGlueImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n             Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n             `True`. Can be overridden by `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n@@ -159,7 +159,7 @@ class LightGlueImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -180,7 +180,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         **kwargs,\n@@ -191,7 +191,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Dictionary of the form `{\"height\": int, \"width\": int}`, specifying the size of the output image.\n             data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the output image. If not provided, it will be inferred from the input\n@@ -220,7 +220,7 @@ def preprocess(\n         self,\n         images,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -240,7 +240,7 @@ def preprocess(\n                 `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the output image after `resize` has been applied. If `size[\"shortest_edge\"]` >= 384, the image\n                 is resized to `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Otherwise, the smaller edge of the\n                 image will be matched to `int(size[\"shortest_edge\"]/ crop_pct)`, after which the image is cropped to\n@@ -337,31 +337,31 @@ def preprocess(\n     def post_process_keypoint_matching(\n         self,\n         outputs: LightGlueKeypointMatchingOutput,\n-        target_sizes: Union[TensorType, List[Tuple]],\n+        target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n-    ) -> List[Dict[str, torch.Tensor]]:\n+    ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n         Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n             outputs ([`KeypointMatchingOutput`]):\n                 Raw outputs of the model.\n-            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n-                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+            target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the\n                 target size `(height, width)` of each image in the batch. This must be the original image size (before\n                 any processing).\n             threshold (`float`, *optional*, defaults to 0.0):\n                 Threshold to filter out the matches with low scores.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            `list[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n             of the pair, the matching scores and the matching indices.\n         \"\"\"\n         if outputs.mask.shape[0] != len(target_sizes):\n             raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n         if not all(len(target_size) == 2 for target_size in target_sizes):\n             raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n \n-        if isinstance(target_sizes, List):\n+        if isinstance(target_sizes, list):\n             image_pair_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n         else:\n             if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:"
        },
        {
            "sha": "4df4888621e14805e1b18e9a356763c2cfb22bae",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=d058f81e5bac1a52a7e9acfd30526f7bcbcae40a",
            "patch": "@@ -18,7 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -74,8 +74,8 @@ class LightGlueKeypointMatchingOutput(ModelOutput):\n     keypoints: Optional[torch.FloatTensor] = None\n     prune: Optional[torch.IntTensor] = None\n     mask: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n class LightGluePositionalEncoder(nn.Module):\n@@ -85,7 +85,7 @@ def __init__(self, config: LightGlueConfig):\n \n     def forward(\n         self, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         projected_keypoints = self.projector(keypoints)\n         embeddings = projected_keypoints.repeat_interleave(2, dim=-1)\n         cosines = torch.cos(embeddings)\n@@ -200,12 +200,12 @@ def __init__(self, config: LightGlueConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -274,7 +274,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         output_hidden_states: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[tuple[torch.Tensor]], Optional[tuple[torch.Tensor]]]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -435,7 +435,7 @@ def _init_weights(self, module: nn.Module) -> None:\n             module.weight.data.fill_(1.0)\n \n \n-def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> Tuple[torch.Tensor, torch.Tensor]:\n+def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\"\n     batch_size, _, _ = scores.shape\n     # For each keypoint, get the best match\n@@ -548,7 +548,7 @@ def _get_confidence_threshold(self, layer_index: int) -> float:\n \n     def _keypoint_processing(\n         self, descriptors: torch.Tensor, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n-    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n         descriptors = descriptors.detach().contiguous()\n         projected_descriptors = self.input_projection(descriptors)\n         keypoint_encoding_output = self.positional_encoder(keypoints, output_hidden_states=output_hidden_states)\n@@ -659,7 +659,7 @@ def _do_final_keypoint_pruning(\n         matches: torch.Tensor,\n         matching_scores: torch.Tensor,\n         num_keypoints: torch.Tensor,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         # (batch_size, num_keypoints) -> (batch_size // 2, 2, num_keypoints) -> 2 * (batch_size // 2, num_keypoints) to\n         # have tensors from\n         batch_size, _ = indices.shape\n@@ -699,7 +699,7 @@ def _match_image_pair(\n         mask: torch.Tensor = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple, Tuple]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple, tuple]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -875,7 +875,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[Tuple, LightGlueKeypointMatchingOutput]:\n+    ) -> Union[tuple, LightGlueKeypointMatchingOutput]:\n         loss = None\n         if labels is not None:\n             raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")"
        },
        {
            "sha": "96a389194b46a1c85b95a8335ff2494fbdf9abd4",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d058f81e5bac1a52a7e9acfd30526f7bcbcae40a/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=d058f81e5bac1a52a7e9acfd30526f7bcbcae40a",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n-from typing import Callable, Dict, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -196,17 +196,17 @@ class LightGlueKeypointMatchingOutput(ModelOutput):\n     keypoints: Optional[torch.FloatTensor] = None\n     prune: Optional[torch.IntTensor] = None\n     mask: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n class LightGlueImageProcessor(SuperGlueImageProcessor):\n     def post_process_keypoint_matching(\n         self,\n         outputs: LightGlueKeypointMatchingOutput,\n-        target_sizes: Union[TensorType, List[Tuple]],\n+        target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n-    ) -> List[Dict[str, torch.Tensor]]:\n+    ) -> list[dict[str, torch.Tensor]]:\n         return super().post_process_keypoint_matching(outputs, target_sizes, threshold)\n \n     def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n@@ -263,7 +263,7 @@ def __init__(self, config: LightGlueConfig):\n \n     def forward(\n         self, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         projected_keypoints = self.projector(keypoints)\n         embeddings = projected_keypoints.repeat_interleave(2, dim=-1)\n         cosines = torch.cos(embeddings)\n@@ -277,12 +277,12 @@ class LightGlueAttention(LlamaAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -348,7 +348,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         output_hidden_states: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[tuple[torch.Tensor]], Optional[tuple[torch.Tensor]]]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -509,7 +509,7 @@ def _init_weights(self, module: nn.Module) -> None:\n             module.weight.data.fill_(1.0)\n \n \n-def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> Tuple[torch.Tensor, torch.Tensor]:\n+def get_matches_from_scores(scores: torch.Tensor, threshold: float) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"obtain matches from a score matrix [Bx M+1 x N+1]\"\"\"\n     batch_size, _, _ = scores.shape\n     # For each keypoint, get the best match\n@@ -622,7 +622,7 @@ def _get_confidence_threshold(self, layer_index: int) -> float:\n \n     def _keypoint_processing(\n         self, descriptors: torch.Tensor, keypoints: torch.Tensor, output_hidden_states: Optional[bool] = False\n-    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n         descriptors = descriptors.detach().contiguous()\n         projected_descriptors = self.input_projection(descriptors)\n         keypoint_encoding_output = self.positional_encoder(keypoints, output_hidden_states=output_hidden_states)\n@@ -733,7 +733,7 @@ def _do_final_keypoint_pruning(\n         matches: torch.Tensor,\n         matching_scores: torch.Tensor,\n         num_keypoints: torch.Tensor,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         # (batch_size, num_keypoints) -> (batch_size // 2, 2, num_keypoints) -> 2 * (batch_size // 2, num_keypoints) to\n         # have tensors from\n         batch_size, _ = indices.shape\n@@ -773,7 +773,7 @@ def _match_image_pair(\n         mask: torch.Tensor = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple, Tuple]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple, tuple]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -949,7 +949,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[Tuple, LightGlueKeypointMatchingOutput]:\n+    ) -> Union[tuple, LightGlueKeypointMatchingOutput]:\n         loss = None\n         if labels is not None:\n             raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 40,
        "deletions": 41
    }
}