{
    "author": "Cyrilvallez",
    "message": "Simplify Mixtral and its modular children (#39252)\n\n* simplify mixtral a lot\n\n* fix\n\n* other moes\n\n* mixtral\n\n* qwen3\n\n* back\n\n* Update modular_qwen3_moe.py",
    "sha": "93747d89eab1441af877064d0313bac536d60109",
    "files": [
        {
            "sha": "09ed01ffe2508b7c76ba86cfb70d56fe89d174d2",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 8,
            "deletions": 66,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -47,6 +47,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import OutputRecorder\n from .configuration_minimax import MiniMaxConfig\n \n \n@@ -555,7 +556,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n@@ -571,18 +572,10 @@ def forward(\n         # Fully Connected\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         residual = hidden_states\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states, _ = self.block_sparse_moe(hidden_states)\n         hidden_states = residual * self.mlp_alpha_factor + hidden_states * self.mlp_beta_factor\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -600,8 +593,9 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n         \"hidden_states\": MiniMaxDecoderLayer,\n-        \"attentions\": MiniMaxAttention,\n+        \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n     }\n \n     def _init_weights(self, module):\n@@ -683,34 +677,16 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[MiniMaxCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         if use_cache and past_key_values is None:\n             past_key_values = MiniMaxCache()\n         elif use_cache and not isinstance(past_key_values, MiniMaxCache):\n@@ -744,54 +720,29 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n         for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             if decoder_layer.layer_type == \"full_attention\":\n                 input_attention_mask = causal_mask\n             else:\n                 # lightning attention uses original attention_mask, and uses it only for the first step\n                 input_attention_mask = attention_mask\n \n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n \n@@ -924,8 +875,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -954,15 +903,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -971,8 +915,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n             **kwargs,"
        },
        {
            "sha": "9e2ee0b17b03d73b111d70e02c689eba1328f668",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 16,
            "deletions": 56,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.generic import OutputRecorder\n from ..mixtral.configuration_mixtral import MixtralConfig\n from ..mixtral.modeling_mixtral import (\n     MixtralAttention,\n@@ -41,6 +42,7 @@\n     MixtralModel,\n     MixtralPreTrainedModel,\n     MixtralRMSNorm,\n+    MixtralSparseMoeBlock,\n )\n \n \n@@ -381,6 +383,10 @@ class MiniMaxAttention(MixtralAttention):\n     pass\n \n \n+class MiniMaxSparseMoeBlock(MixtralSparseMoeBlock):\n+    pass\n+\n+\n class MiniMaxDecoderLayer(MixtralDecoderLayer, GradientCheckpointingLayer):\n     def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n@@ -441,7 +447,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n@@ -457,24 +463,21 @@ def forward(\n         # Fully Connected\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         residual = hidden_states\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states, _ = self.block_sparse_moe(hidden_states)\n         hidden_states = residual * self.mlp_alpha_factor + hidden_states * self.mlp_beta_factor\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n     _supports_cache_class = True  # Note: only supports MiniMaxCache\n     _supports_static_cache = False\n     _supports_quantized_cache = False\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n+        \"hidden_states\": MiniMaxDecoderLayer,\n+        \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n+    }\n \n \n class MiniMaxModel(MixtralModel):\n@@ -483,34 +486,16 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[MiniMaxCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         if use_cache and past_key_values is None:\n             past_key_values = MiniMaxCache()\n         elif use_cache and not isinstance(past_key_values, MiniMaxCache):\n@@ -544,54 +529,29 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n         for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             if decoder_layer.layer_type == \"full_attention\":\n                 input_attention_mask = causal_mask\n             else:\n                 # lightning attention uses original attention_mask, and uses it only for the first step\n                 input_attention_mask = attention_mask\n \n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n "
        },
        {
            "sha": "b6bda2e0819a0027f7bbecbe08c8ccb1a1901428",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 13,
            "deletions": 99,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -51,6 +51,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import OutputRecorder\n from .configuration_mixtral import MixtralConfig\n \n \n@@ -311,51 +312,24 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -364,18 +338,10 @@ def forward(\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states, _ = self.block_sparse_moe(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MixtralRotaryEmbedding(nn.Module):\n@@ -427,6 +393,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n         \"hidden_states\": MixtralDecoderLayer,\n         \"attentions\": MixtralAttention,\n     }\n@@ -476,34 +443,15 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n \n@@ -533,48 +481,23 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        return MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n \n@@ -707,8 +630,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -737,15 +658,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -754,8 +670,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n             **kwargs,"
        },
        {
            "sha": "04b95de419005b32e662b2508eefe55411d142aa",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 101,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -27,13 +27,13 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.generic import OutputRecorder\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n     MistralForCausalLM,\n@@ -240,51 +240,24 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -293,18 +266,10 @@ def forward(\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states, _ = self.block_sparse_moe(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MixtralRotaryEmbedding(MistralRotaryEmbedding):\n@@ -313,6 +278,11 @@ class MixtralRotaryEmbedding(MistralRotaryEmbedding):\n \n class MixtralPreTrainedModel(MistralPreTrainedModel):\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n+        \"hidden_states\": MixtralDecoderLayer,\n+        \"attentions\": MixtralAttention,\n+    }\n \n \n class MixtralModel(MistralModel):\n@@ -321,34 +291,15 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n \n@@ -378,48 +329,23 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        return MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n \n@@ -442,8 +368,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -472,15 +396,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -489,8 +408,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n             **kwargs,"
        },
        {
            "sha": "15fcf92355fd768c7ce6dcbc65a882eda1f2a523",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 83,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -44,7 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n@@ -308,16 +308,13 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> torch.FloatTensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -342,46 +339,32 @@ def forward(\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n         \"\"\"\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n-            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-\n         hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n         if isinstance(hidden_states, tuple):\n-            hidden_states, router_logits = hidden_states\n-        else:\n-            router_logits = None\n-\n+            hidden_states, _ = hidden_states\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Qwen3MoeRotaryEmbedding(nn.Module):\n@@ -433,6 +416,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Qwen3MoeSparseMoeBlock, index=1),\n         \"hidden_states\": Qwen3MoeDecoderLayer,\n         \"attentions\": Qwen3MoeAttention,\n     }\n@@ -482,34 +466,15 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n \n@@ -539,48 +504,23 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        return MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n \n@@ -713,8 +653,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -743,15 +681,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -760,8 +693,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n             **kwargs,"
        },
        {
            "sha": "5618f2d0aff6724b3c52ba98c0c2310ac379840b",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 62,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93747d89eab1441af877064d0313bac536d60109/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=93747d89eab1441af877064d0313bac536d60109",
            "patch": "@@ -141,88 +141,43 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n-            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-\n         hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n         if isinstance(hidden_states, tuple):\n-            hidden_states, router_logits = hidden_states\n-        else:\n-            router_logits = None\n-\n+            hidden_states, _ = hidden_states\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Qwen3MoeModel(MixtralModel):\n-    def __init__(self, config: Qwen3MoeConfig):\n-        super().__init__(config)\n-        self.layers = nn.ModuleList(\n-            [Qwen3MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n+    pass\n \n \n class Qwen3MoeForCausalLM(MixtralForCausalLM):\n@@ -240,8 +195,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -270,15 +223,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -287,8 +235,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n             **kwargs,"
        }
    ],
    "stats": {
        "total": 544,
        "additions": 77,
        "deletions": 467
    }
}