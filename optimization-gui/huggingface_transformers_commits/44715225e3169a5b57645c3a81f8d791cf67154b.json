{
    "author": "threewebcode",
    "message": "fix typos in the code comments and error messages (#36993)\n\n* chore: enhance code comments\n\n* chore: enhance code comments\n\n* chore: enhance code comments\n\n* chore: enhance code comments\n\n* chore: enhance code comments\n\n* chore: enhance code comments\n\n* chore: enhance code comments",
    "sha": "44715225e3169a5b57645c3a81f8d791cf67154b",
    "files": [
        {
            "sha": "90d8aa631e90cb497b4831a5d09f027c5051259c",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -798,7 +798,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "707eb66c427c0c5368b537d607eacef7bca6240f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -267,7 +267,7 @@ def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n     output = torch.zeros(num_tokens, out_features, dtype=token_states.dtype, device=token_states.device)\n \n     cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=0)\n-    # Insert zero at the begining for offset index's convenience\n+    # Insert zero at the beginning for offset index's convenience\n     zero_tensor = torch.zeros(1, dtype=torch.long, device=cumsum_num_tokens.device)\n     cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n "
        },
        {
            "sha": "d341970e2988688082721678bd8e398bf1eefcf0",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -86,7 +86,7 @@ def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n     output = torch.zeros(num_tokens, out_features, dtype=token_states.dtype, device=token_states.device)\n \n     cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=0)\n-    # Insert zero at the begining for offset index's convenience\n+    # Insert zero at the beginning for offset index's convenience\n     zero_tensor = torch.zeros(1, dtype=torch.long, device=cumsum_num_tokens.device)\n     cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n "
        },
        {
            "sha": "c13d9097e6ce52f9d1618c5509f5b7d22d3a258d",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -373,7 +373,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "b806eea5e65734292d66e3b33c91569dcf8697ad",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -341,7 +341,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "d06f9a7d432d97e3536385b744a663365da34b15",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -1016,7 +1016,7 @@ def forward(self, x, mask=None, bias=None, indices=None):\n         use mask.\n \n         Inputs:\n-            x: batch of input sequneces (.. x L x C) mask: batch of boolean masks where 1=valid, 0=padding position (..\n+            x: batch of input sequences (.. x L x C) mask: batch of boolean masks where 1=valid, 0=padding position (..\n             x L_k) bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads)\n \n         Outputs:"
        },
        {
            "sha": "4d0f2f69b350ce7589a1c82e896cb08ec5808f41",
            "filename": "src/transformers/models/esm/openfold_utils/rigid_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -989,10 +989,10 @@ def map_tensor_fn(self, fn: Callable[[torch.Tensor], torch.Tensor]) -> Rigid:\n \n     def to_tensor_4x4(self) -> torch.Tensor:\n         \"\"\"\n-        Converts a transformation to a homogenous transformation tensor.\n+        Converts a transformation to a homogeneous transformation tensor.\n \n         Returns:\n-            A [*, 4, 4] homogenous transformation tensor\n+            A [*, 4, 4] homogeneous transformation tensor\n         \"\"\"\n         tensor = self._trans.new_zeros((*self.shape, 4, 4))\n         tensor[..., :3, :3] = self._rots.get_rot_mats()\n@@ -1003,10 +1003,10 @@ def to_tensor_4x4(self) -> torch.Tensor:\n     @staticmethod\n     def from_tensor_4x4(t: torch.Tensor) -> Rigid:\n         \"\"\"\n-        Constructs a transformation from a homogenous transformation tensor.\n+        Constructs a transformation from a homogeneous transformation tensor.\n \n         Args:\n-            t: [*, 4, 4] homogenous transformation tensor\n+            t: [*, 4, 4] homogeneous transformation tensor\n         Returns:\n             T object with shape [*]\n         \"\"\""
        },
        {
            "sha": "4099920f4028909c1ae69bb5bc0b6e33a3881612",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -80,7 +80,7 @@ class FalconMambaConfig(PretrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the cache should be used.\n         use_mambapy (`bool`, *optional*, defaults to `False`):\n-            Determines the fallback strategy during training if the CUDA-based official implementation of FalconMamba is not avaiable. If `True`, the falcon_mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n+            Determines the fallback strategy during training if the CUDA-based official implementation of FalconMamba is not available. If `True`, the falcon_mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n         mixer_rms_eps (`float`, *optional*, defaults to 1e-06):\n             The RMS norm epsilon value that is used in the Mixer RMS norm for B, C and dt states.\n     Example:"
        },
        {
            "sha": "bf8695d2a9820b9e648689a2a1b6cd5f51998fad",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -119,7 +119,7 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n \n         # projection of the input hidden states\n         self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n         self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n         # time step projection (discretization)\n         self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n@@ -768,7 +768,7 @@ def prepare_inputs_for_generation(\n         attention_mask: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ):\n-        # Overwitten -- uses `cache_params` as opposed to `past_key_values`\n+        # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n \n         if use_cache:\n             # `cache_position` should have been initialized in `generate`"
        },
        {
            "sha": "0d37d04e812a9cdf7ddc7da1d103d62e1794be95",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -791,7 +791,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "c01591b5a69183067ae9c888ace65ba5dfa376c1",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -237,7 +237,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "b74f060ced8e2f38e020ff03ca8e01565ade1e68",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -543,7 +543,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "d0769264061ed3bee19b28de1f21e46d92022ed1",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -235,7 +235,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # LongT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "c5389cdfd41139c50036670cdc836ca516528ab8",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -98,7 +98,7 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n \n         # projection of the input hidden states\n         self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n         self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n         # time step projection (discretization)\n         self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n@@ -708,7 +708,7 @@ def prepare_inputs_for_generation(\n         attention_mask: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ):\n-        # Overwitten -- uses `cache_params` as opposed to `past_key_values`\n+        # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n \n         if use_cache:\n             # `cache_position` should have been initialized in `generate`"
        },
        {
            "sha": "2df643b78140f62b4f2c39b108ba0fab8ca382ac",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -135,7 +135,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # MT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "675859f3102b5e4c05d5a78ce973fd4512ddca03",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -72,7 +72,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "2e4734aed8640f593ab022cf02fb1659bf2942e7",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -164,7 +164,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # Pop2Piano uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "71d304ea96c6619c9b62b5981b885efa398fd0d3",
            "filename": "src/transformers/models/switch_transformers/convert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -119,7 +119,7 @@ def rename_keys(s_dict):\n \n \n def convert_gin_to_config(gin_file, num_experts):\n-    # Convert a google style config to the hugging face fromat\n+    # Convert a google style config to the hugging face format\n     import regex as re\n \n     with open(gin_file, \"r\") as f:"
        },
        {
            "sha": "d2d9929b91288cc6230c6c767eb916fc6e738c0e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -230,7 +230,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # SwitchTransformers uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n \n@@ -297,12 +297,12 @@ def forward(self, hidden_states):\n         expert the corresponding hidden states.\n \n         \"\"\"\n-        # Step 1: Get the router_mask from the router as wel as the probabilities\n+        # Step 1: Get the router_mask from the router as well as the probabilities\n         router_mask, router_probs, router_logits = self.router(hidden_states)\n         expert_index = torch.argmax(router_mask, dim=-1)\n \n         # The routers introduced might not always map all the tokens, to a router, which means that some hidden states\n-        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.\n+        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the selected ones.\n \n         next_states = hidden_states.clone()\n "
        },
        {
            "sha": "12498359d21b30f05c09d639a277586b5daa3682",
            "filename": "src/transformers/models/t5/convert_t5x_checkpoint_to_flax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_flax.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -218,7 +218,7 @@ def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_f\n         flax_model.params[\"lm_head\"][\"kernel\"] = t5x_model[\"target\"][\"decoder\"][\"logits_dense\"][\"kernel\"]\n \n     flax_model.save_pretrained(flax_dump_folder_path)\n-    print(\"T5X Model was sucessfully converted!\")\n+    print(\"T5X Model was successfully converted!\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "39c8101f92a6c45176d42463f41932adbecbd3ae",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -249,7 +249,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "8238cd38a9a19f7cb439cedad09b1c024caf4f88",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -524,7 +524,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # Udop uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "8c22500a7cee826e6ce413888e79214c6f14cb42",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -73,7 +73,7 @@ def __init__(self, hidden_size, eps=1e-6):\n \n     def forward(self, hidden_states):\n         # UMT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n-        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n+        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus variance is calculated\n         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n         # half-precision inputs is done in fp32\n "
        },
        {
            "sha": "f4b5e1cfda311b51f62b85a059e420841feda708",
            "filename": "src/transformers/models/vivit/convert_vivit_flax_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -73,7 +73,7 @@ def transform_attention(current: np.ndarray):\n         return transform_attention_kernel(current)\n \n     else:\n-        raise Exception(f\"Invalid number of dimesions: {np.ndim(current)}\")\n+        raise Exception(f\"Invalid number of dimensions: {np.ndim(current)}\")\n \n \n def transform_attention_bias(current: np.ndarray):"
        },
        {
            "sha": "2f27e19dd58007a1cd4f665b039259566bad53e2",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44715225e3169a5b57645c3a81f8d791cf67154b/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=44715225e3169a5b57645c3a81f8d791cf67154b",
            "patch": "@@ -300,7 +300,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 32,
        "deletions": 32
    }
}