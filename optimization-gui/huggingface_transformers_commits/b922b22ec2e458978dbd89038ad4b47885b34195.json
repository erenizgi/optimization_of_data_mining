{
    "author": "samrae7",
    "message": "36978 | Fast image processor for DPT model (#37481)\n\n* chore: ran codegen script\n\n* test: test_image_processor_properties\n\n* test: test_image_processor_from_dict_with_kwargs\n\n* test: wip - test_padding\n\n* test: test_padding\n\n* test: test_keep_aspect_ratio\n\n* wip\n\n* test\n\n* test: wip\n\n* test: wip\n\n* test: test_call_segmentation_maps, wip\n\n* chore: tidy up\n\n* test: test_call_segmentation_maps\n\n* fix: test_save_load_fast_slow\n\n* test: reduce labels\n\n* chore: make fixup\n\n* chore: rm comment\n\n* chore: tidy\n\n* chore remove comment\n\n* refactor: no need to infer channel dimesnion\n\n* refactor: encapsulate logic for preparing segmentation maps\n\n* refactor: improve readability of segmentation_map preparation\n\n* improvement: batched version of pad_image\n\n* chore: fixup\n\n* docs\n\n* chore: make quality\n\n* chore: remove unecessary comment\n\n* fix: add SemanticSegmentationMixin\n\n* feat: add post_process_depth_estimation to fast dpt image processor\n\n* chore: fix formatting\n\n* remove max_height, max_width\n\n* fix: better way of processin segmentation maps\n- copied from Beit Fast processor\n\n* chore: formatting + remove TODO\n\n* chore: fixup styles\n\n* chore: remove unecessary line break\n\n* chore: core review suggestion to remove autodocstring\n\n* fix: add do_reduce_labels logic + refactor\n- refactor preprocess logic to make it consistent with other processors\n- add missing reduce labels logic\n\n* refactor: remove deprecated mixin\n\n* chore: fixup\n\n* use modular for dpt + final nit changes\n\n* fix style\n\n---------\n\nCo-authored-by: Samuel Rae <samuelrae@Samuels-Air.fritz.box>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "b922b22ec2e458978dbd89038ad4b47885b34195",
    "files": [
        {
            "sha": "a763e2af62f0e766d553c18db9b534ad6432760c",
            "filename": "docs/source/en/model_doc/dpt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -78,7 +78,13 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] DPTImageProcessor\n     - preprocess\n+\n+## DPTImageProcessorFast\n+\n+[[autodoc]] DPTImageProcessorFast\n+    - preprocess\n     - post_process_semantic_segmentation\n+    - post_process_depth_estimation\n \n ## DPTModel\n "
        },
        {
            "sha": "69087ec68f91ba48b53573101df1c1291ce1cf78",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -74,14 +74,14 @@\n             (\"data2vec-vision\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n             (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n-            (\"depth_anything\", (\"DPTImageProcessor\",)),\n+            (\"depth_anything\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"depth_pro\", (\"DepthProImageProcessor\", \"DepthProImageProcessorFast\")),\n             (\"deta\", (\"DetaImageProcessor\",)),\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n-            (\"dpt\", (\"DPTImageProcessor\",)),\n+            (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),"
        },
        {
            "sha": "97eff4f5adfa0b8e82379fdc17208a9984f2a603",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -174,11 +174,6 @@ def _preprocess_segmentation_maps(\n         processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n         return processed_segmentation_maps\n \n-    def __call__(self, images, segmentation_maps=None, **kwargs):\n-        # Overrides the `__call__` method of the `Preprocessor` class such that the images and segmentation maps can both\n-        # be passed in as positional arguments.\n-        return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n-\n     @auto_docstring\n     def preprocess(\n         self,"
        },
        {
            "sha": "ce0070f270f3604afd0661e0cd8aaa4fa2141217",
            "filename": "src/transformers/models/dpt/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_dpt import *\n     from .feature_extraction_dpt import *\n     from .image_processing_dpt import *\n+    from .image_processing_dpt_fast import *\n     from .modeling_dpt import *\n else:\n     import sys"
        },
        {
            "sha": "c9db9c51852f5ae4b27476ad81a09fb1b1b6aad8",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "added",
            "additions": 474,
            "deletions": 0,
            "changes": 474,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -0,0 +1,474 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/dpt/modular_dpt.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dpt.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Iterable\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+from transformers.image_processing_base import BatchFeature\n+from transformers.image_transforms import group_images_by_shape, reorder_images\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    is_torch_tensor,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    requires_backends,\n+)\n+\n+\n+if TYPE_CHECKING:\n+    from ...modeling_outputs import DepthEstimatorOutput\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    ensure_multiple_of (`int`, *optional*, defaults to 1):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+        by `ensure_multiple_of` in `preprocess`.\n+    do_pad (`bool`, *optional*, defaults to `False`):\n+        Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n+        combination with DPT.\n+    size_divisor (`int`, *optional*):\n+        If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n+        DINOv2 paper, which uses the model in combination with DPT.\n+    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n+        be overidden by `keep_aspect_ratio` in `preprocess`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    ensure_multiple_of: Optional[int]\n+    size_divisor: Optional[int]\n+    do_pad: Optional[bool]\n+    keep_aspect_ratio: Optional[bool]\n+    do_reduce_labels: Optional[bool]\n+\n+\n+def get_resize_output_image_size(\n+    input_image: \"torch.Tensor\",\n+    output_size: Union[int, Iterable[int]],\n+    keep_aspect_ratio: bool,\n+    multiple: int,\n+) -> SizeDict:\n+    def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n+        x = round(val / multiple) * multiple\n+\n+        if max_val is not None and x > max_val:\n+            x = math.floor(val / multiple) * multiple\n+\n+        if x < min_val:\n+            x = math.ceil(val / multiple) * multiple\n+\n+        return x\n+\n+    input_height, input_width = input_image.shape[-2:]\n+    output_height, output_width = output_size\n+\n+    # determine new height and width\n+    scale_height = output_height / input_height\n+    scale_width = output_width / input_width\n+\n+    if keep_aspect_ratio:\n+        # scale as little as possible\n+        if abs(1 - scale_width) < abs(1 - scale_height):\n+            # fit width\n+            scale_height = scale_width\n+        else:\n+            # fit height\n+            scale_width = scale_height\n+\n+    new_height = constrain_to_multiple_of(scale_height * input_height, multiple=multiple)\n+    new_width = constrain_to_multiple_of(scale_width * input_width, multiple=multiple)\n+\n+    return SizeDict(height=new_height, width=new_width)\n+\n+\n+@auto_docstring\n+class DPTImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    default_to_square = True\n+    crop_size = None\n+    do_resize = True\n+    do_center_crop = None\n+    do_rescale = True\n+    do_normalize = True\n+    do_reduce_labels = None\n+\n+    valid_kwargs = DPTFastImageProcessorKwargs\n+    do_pad = False\n+    rescale_factor = 1 / 255\n+    ensure_multiple_of = 1\n+    keep_aspect_ratio = False\n+\n+    def __init__(self, **kwargs: Unpack[DPTFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+        return label\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        keep_aspect_ratio: bool,\n+        ensure_multiple_of: Optional[int],\n+        do_pad: bool,\n+        size_divisor: Optional[int],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=size,\n+                    interpolation=interpolation,\n+                    ensure_multiple_of=ensure_multiple_of,\n+                    keep_aspect_ratio=keep_aspect_ratio,\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            if do_pad:\n+                stacked_images = self.pad_image(stacked_images, size_divisor)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return processed_images\n+\n+    def _preprocess_images(\n+        self,\n+        images,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses images.\"\"\"\n+        kwargs[\"do_reduce_labels\"] = False\n+        processed_images = self._preprocess(images=images, **kwargs)\n+        return processed_images\n+\n+    def _preprocess_segmentation_maps(\n+        self,\n+        segmentation_maps,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses segmentation maps.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_normalize\"] = False\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n+        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+        return processed_segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        images = self._preprocess_images(\n+            images=images,\n+            **kwargs,\n+        )\n+        data = {\"pixel_values\": images}\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_segmentation_maps(\n+                segmentation_maps=segmentation_maps,\n+                **kwargs,\n+            )\n+            data[\"labels\"] = segmentation_maps\n+\n+        return BatchFeature(data=data)\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+        \"\"\"\n+        Converts the output of [`DPTForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DPTForSemanticSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        # TODO: add support for other frameworks\n+        logits = outputs.logits\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            if is_torch_tensor(target_sizes):\n+                target_sizes = target_sizes.numpy()\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(logits)):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = logits.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        ensure_multiple_of: Optional[int] = 1,\n+        keep_aspect_ratio: bool = False,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing when resizing the image\n+            ensure_multiple_of (`int`, *optional*):\n+                If `do_resize` is `True`, the image is resized to a size that is a multiple of this value\n+            keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+                If `True`, and `do_resize` is `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        if not size.height or not size.width:\n+            raise ValueError(f\"The size dictionary must contain the keys 'height' and 'width'. Got {size.keys()}\")\n+\n+        output_size = get_resize_output_image_size(\n+            image,\n+            output_size=(size.height, size.width),\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            multiple=ensure_multiple_of,\n+        )\n+        return super().resize(image, output_size, interpolation=interpolation, antialias=antialias)\n+\n+    def pad_image(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int = 1,\n+    ) -> \"torch.Tensor\":\n+        r\"\"\"\n+        Center pad a batch of images to be a multiple of `size_divisor`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to pad.  Can be a batch of images of dimensions (N, C, H, W) or a single image of dimensions (C, H, W).\n+            size_divisor (`int`):\n+                The width and height of the image will be padded to a multiple of this number.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+\n+        def _get_pad(size, size_divisor):\n+            new_size = math.ceil(size / size_divisor) * size_divisor\n+            pad_size = new_size - size\n+            pad_size_left = pad_size // 2\n+            pad_size_right = pad_size - pad_size_left\n+            return pad_size_left, pad_size_right\n+\n+        pad_top, pad_bottom = _get_pad(height, size_divisor)\n+        pad_left, pad_right = _get_pad(width, size_divisor)\n+        padding = (pad_left, pad_top, pad_right, pad_bottom)\n+        return F.pad(image, padding)\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+    ) -> list[dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                depth = torch.nn.functional.interpolate(\n+                    depth.unsqueeze(0).unsqueeze(1), size=target_size, mode=\"bicubic\", align_corners=False\n+                ).squeeze()\n+\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"DPTImageProcessorFast\"]"
        },
        {
            "sha": "43aeffb260833e86c6a5cc124c45b6f8ed1540c8",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "added",
            "additions": 313,
            "deletions": 0,
            "changes": 313,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -0,0 +1,313 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Iterable\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+from transformers.image_processing_base import BatchFeature\n+from transformers.image_transforms import group_images_by_shape, reorder_images\n+from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast\n+\n+from ...image_processing_utils_fast import (\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    requires_backends,\n+)\n+\n+\n+if TYPE_CHECKING:\n+    from ...modeling_outputs import DepthEstimatorOutput\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+def get_resize_output_image_size(\n+    input_image: \"torch.Tensor\",\n+    output_size: Union[int, Iterable[int]],\n+    keep_aspect_ratio: bool,\n+    multiple: int,\n+) -> SizeDict:\n+    def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n+        x = round(val / multiple) * multiple\n+\n+        if max_val is not None and x > max_val:\n+            x = math.floor(val / multiple) * multiple\n+\n+        if x < min_val:\n+            x = math.ceil(val / multiple) * multiple\n+\n+        return x\n+\n+    input_height, input_width = input_image.shape[-2:]\n+    output_height, output_width = output_size\n+\n+    # determine new height and width\n+    scale_height = output_height / input_height\n+    scale_width = output_width / input_width\n+\n+    if keep_aspect_ratio:\n+        # scale as little as possible\n+        if abs(1 - scale_width) < abs(1 - scale_height):\n+            # fit width\n+            scale_height = scale_width\n+        else:\n+            # fit height\n+            scale_width = scale_height\n+\n+    new_height = constrain_to_multiple_of(scale_height * input_height, multiple=multiple)\n+    new_width = constrain_to_multiple_of(scale_width * input_width, multiple=multiple)\n+\n+    return SizeDict(height=new_height, width=new_width)\n+\n+\n+class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    ensure_multiple_of (`int`, *optional*, defaults to 1):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+        by `ensure_multiple_of` in `preprocess`.\n+    do_pad (`bool`, *optional*, defaults to `False`):\n+        Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n+        combination with DPT.\n+    size_divisor (`int`, *optional*):\n+        If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n+        DINOv2 paper, which uses the model in combination with DPT.\n+    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n+        be overidden by `keep_aspect_ratio` in `preprocess`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    ensure_multiple_of: Optional[int]\n+    size_divisor: Optional[int]\n+    do_pad: Optional[bool]\n+    keep_aspect_ratio: Optional[bool]\n+    do_reduce_labels: Optional[bool]\n+\n+\n+@auto_docstring\n+class DPTImageProcessorFast(BeitImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = False\n+    rescale_factor = 1 / 255\n+    ensure_multiple_of = 1\n+    keep_aspect_ratio = False\n+    do_reduce_labels = False\n+    crop_size = None\n+    do_center_crop = None\n+    do_reduce_labels = None\n+\n+    valid_kwargs = DPTFastImageProcessorKwargs\n+\n+    def from_dict():\n+        raise NotImplementedError(\"No need to override this method\")\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        ensure_multiple_of: Optional[int] = 1,\n+        keep_aspect_ratio: bool = False,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing when resizing the image\n+            ensure_multiple_of (`int`, *optional*):\n+                If `do_resize` is `True`, the image is resized to a size that is a multiple of this value\n+            keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+                If `True`, and `do_resize` is `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        if not size.height or not size.width:\n+            raise ValueError(f\"The size dictionary must contain the keys 'height' and 'width'. Got {size.keys()}\")\n+\n+        output_size = get_resize_output_image_size(\n+            image,\n+            output_size=(size.height, size.width),\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            multiple=ensure_multiple_of,\n+        )\n+        return BeitImageProcessorFast().resize(image, output_size, interpolation=interpolation, antialias=antialias)\n+\n+    def pad_image(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int = 1,\n+    ) -> \"torch.Tensor\":\n+        r\"\"\"\n+        Center pad a batch of images to be a multiple of `size_divisor`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to pad.  Can be a batch of images of dimensions (N, C, H, W) or a single image of dimensions (C, H, W).\n+            size_divisor (`int`):\n+                The width and height of the image will be padded to a multiple of this number.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+\n+        def _get_pad(size, size_divisor):\n+            new_size = math.ceil(size / size_divisor) * size_divisor\n+            pad_size = new_size - size\n+            pad_size_left = pad_size // 2\n+            pad_size_right = pad_size - pad_size_left\n+            return pad_size_left, pad_size_right\n+\n+        pad_top, pad_bottom = _get_pad(height, size_divisor)\n+        pad_left, pad_right = _get_pad(width, size_divisor)\n+        padding = (pad_left, pad_top, pad_right, pad_bottom)\n+        return F.pad(image, padding)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        keep_aspect_ratio: bool,\n+        ensure_multiple_of: Optional[int],\n+        do_pad: bool,\n+        size_divisor: Optional[int],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=size,\n+                    interpolation=interpolation,\n+                    ensure_multiple_of=ensure_multiple_of,\n+                    keep_aspect_ratio=keep_aspect_ratio,\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            if do_pad:\n+                stacked_images = self.pad_image(stacked_images, size_divisor)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return processed_images\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+    ) -> list[dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                depth = torch.nn.functional.interpolate(\n+                    depth.unsqueeze(0).unsqueeze(1), size=target_size, mode=\"bicubic\", align_corners=False\n+                ).squeeze()\n+\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"DPTImageProcessorFast\"]"
        },
        {
            "sha": "f0a80a6e14b72fba8c057540182cb763a9df60b4",
            "filename": "tests/models/dpt/test_image_processing_dpt.py",
            "status": "modified",
            "additions": 222,
            "deletions": 151,
            "changes": 373,
            "blob_url": "https://github.com/huggingface/transformers/blob/b922b22ec2e458978dbd89038ad4b47885b34195/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b922b22ec2e458978dbd89038ad4b47885b34195/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py?ref=b922b22ec2e458978dbd89038ad4b47885b34195",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,6 +33,9 @@\n \n     from transformers import DPTImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import DPTImageProcessorFast\n+\n \n class DPTImageProcessingTester:\n     def __init__(\n@@ -114,6 +118,7 @@ def prepare_semantic_batch_inputs():\n @require_vision\n class DPTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = DPTImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DPTImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -124,170 +129,236 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processing_class = image_processing_class(**self.image_processor_dict)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_padding(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image = np.random.randn(3, 249, 491)\n-\n-        # test individual method\n-        image = image_processing.pad_image(image, size_divisor=4)\n-        self.assertTrue(image.shape[1] % 4 == 0)\n-        self.assertTrue(image.shape[2] % 4 == 0)\n-\n-        # test by calling\n-        pixel_values = image_processing.preprocess(\n-            image, do_rescale=False, do_resize=False, do_pad=True, size_divisor=4, return_tensors=\"pt\"\n-        ).pixel_values\n-        self.assertTrue(pixel_values.shape[2] % 4 == 0)\n-        self.assertTrue(pixel_values.shape[3] % 4 == 0)\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == DPTImageProcessorFast:\n+                image = torch.arange(0, 366777, 1, dtype=torch.uint8).reshape(3, 249, 491)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                padded_image = image_processor.pad_image(image, size_divisor=4)\n+                self.assertTrue(padded_image.shape[1] % 4 == 0)\n+                self.assertTrue(padded_image.shape[2] % 4 == 0)\n+                pixel_values = image_processor.preprocess(\n+                    image, do_rescale=False, do_resize=False, do_pad=True, size_divisor=4, return_tensors=\"pt\"\n+                ).pixel_values\n+                self.assertTrue(pixel_values.shape[2] % 4 == 0)\n+                self.assertTrue(pixel_values.shape[3] % 4 == 0)\n+            else:\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                image = np.random.randn(3, 249, 491)\n+                image = image_processor.pad_image(image, size_divisor=4)\n+                self.assertTrue(image.shape[1] % 4 == 0)\n+                self.assertTrue(image.shape[2] % 4 == 0)\n+                pixel_values = image_processor.preprocess(\n+                    image, do_rescale=False, do_resize=False, do_pad=True, size_divisor=4, return_tensors=\"pt\"\n+                ).pixel_values\n+                self.assertTrue(pixel_values.shape[2] % 4 == 0)\n+                self.assertTrue(pixel_values.shape[3] % 4 == 0)\n \n     def test_keep_aspect_ratio(self):\n         size = {\"height\": 512, \"width\": 512}\n-        image_processor = DPTImageProcessor(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n \n-        image = np.zeros((489, 640, 3))\n+            image = np.zeros((489, 640, 3))\n \n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n \n     # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_call_segmentation_maps\n     def test_call_segmentation_maps(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        maps = []\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-            maps.append(torch.zeros(image.shape[-2:]).long())\n-\n-        # Test not batched input\n-        encoding = image_processor(image_inputs[0], maps[0], return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched\n-        encoding = image_processor(image_inputs, maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test not batched input (PIL images)\n-        image, segmentation_map = prepare_semantic_single_inputs()\n-\n-        encoding = image_processor(image, segmentation_map, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched input (PIL images)\n-        images, segmentation_maps = prepare_semantic_batch_inputs()\n-\n-        encoding = image_processor(images, segmentation_maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processor(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processor(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processor(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processor(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n-    # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_reduce_labels\n     def test_reduce_labels(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-\n-        # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n-        image, map = prepare_semantic_single_inputs()\n-        encoding = image_processor(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n-\n-        image_processor.do_reduce_labels = True\n-        encoding = image_processor(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+            image, map = prepare_semantic_single_inputs()\n+            encoding = image_processor(image, map, return_tensors=\"pt\")\n+            labels_no_reduce = encoding[\"labels\"].clone()\n+            self.assertTrue(labels_no_reduce.min().item() >= 0)\n+            self.assertTrue(labels_no_reduce.max().item() <= 150)\n+            # Get the first non-zero label coords and value, for comparison when do_reduce_labels is True\n+            non_zero_positions = (labels_no_reduce > 0).nonzero()\n+            first_non_zero_coords = tuple(non_zero_positions[0].tolist())\n+            first_non_zero_value = labels_no_reduce[first_non_zero_coords].item()\n+\n+            image_processor.do_reduce_labels = True\n+            encoding = image_processor(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+            # Compare with non-reduced label to see if it's reduced by 1\n+            self.assertEqual(encoding[\"labels\"][first_non_zero_coords].item(), first_non_zero_value - 1)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(image_encoding_slow.pixel_values - image_encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+        self.assertTrue(torch.allclose(image_encoding_slow.labels, image_encoding_fast.labels, atol=1e-1))\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )"
        }
    ],
    "stats": {
        "total": 1176,
        "additions": 1018,
        "deletions": 158
    }
}