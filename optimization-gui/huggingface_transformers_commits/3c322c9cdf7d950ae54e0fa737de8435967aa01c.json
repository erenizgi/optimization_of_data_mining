{
    "author": "SunMarc",
    "message": "fix gemma3 grad acc (#37208)\n\n* fix gemma3 grad acc\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* rmv print\n\n* rm\n\n* Update setup.py\n\n* Apply style fixes\n\n* propagate the changes\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "3c322c9cdf7d950ae54e0fa737de8435967aa01c",
    "files": [
        {
            "sha": "51a2ac085be74bed07ab2f46b8c43cd1b66a1260",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=3c322c9cdf7d950ae54e0fa737de8435967aa01c",
            "patch": "@@ -777,6 +777,8 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n )\n class Gemma3Model(Gemma3PreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config: Gemma3Config):\n         super().__init__(config)"
        },
        {
            "sha": "f748461dc46ef59e9307a770ba4f72f42b9da109",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=3c322c9cdf7d950ae54e0fa737de8435967aa01c",
            "patch": "@@ -727,6 +727,9 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n \n class Gemma3Model(PaliGemmaModel):\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    accepts_loss_kwargs = False\n+\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space."
        },
        {
            "sha": "6aabb3a3d802c5dc3303085dc25880052773f794",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=3c322c9cdf7d950ae54e0fa737de8435967aa01c",
            "patch": "@@ -132,6 +132,8 @@ def _init_weights(self, module):\n )\n class PaliGemmaModel(PaliGemmaPreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config: PaliGemmaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "74e3b65d15560bab173d7f06df666ff7bf0414da",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c322c9cdf7d950ae54e0fa737de8435967aa01c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3c322c9cdf7d950ae54e0fa737de8435967aa01c",
            "patch": "@@ -629,18 +629,16 @@ def __init__(\n \n         # Just in case the model was wrapped outside of the `Trainer`\n         unwrapped_model = self.accelerator.unwrap_model(model)\n-        model_forward = (\n-            unwrapped_model.forward\n-            if not _is_peft_model(unwrapped_model)\n-            else unwrapped_model.get_base_model().forward\n-        )\n-        forward_params = inspect.signature(model_forward).parameters\n+        # We also unwrap peft model\n+        if _is_peft_model(unwrapped_model):\n+            unwrapped_model = unwrapped_model.get_base_model()\n \n         # Check if the model has explicit setup for loss kwargs,\n         # if not, check if `**kwargs` are in model.forward\n-        if hasattr(model, \"accepts_loss_kwargs\"):\n-            self.model_accepts_loss_kwargs = model.accepts_loss_kwargs\n+        if hasattr(unwrapped_model, \"accepts_loss_kwargs\"):\n+            self.model_accepts_loss_kwargs = unwrapped_model.accepts_loss_kwargs\n         else:\n+            forward_params = inspect.signature(unwrapped_model.forward).parameters\n             self.model_accepts_loss_kwargs = any(\n                 k.kind == inspect.Parameter.VAR_KEYWORD for k in forward_params.values()\n             )"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 13,
        "deletions": 8
    }
}