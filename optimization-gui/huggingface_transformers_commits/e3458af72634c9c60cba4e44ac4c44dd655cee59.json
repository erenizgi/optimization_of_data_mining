{
    "author": "SilverSoldier",
    "message": "Save checkpoint to temporary directory to handle partial saves during failures (#35580)\n\nSave checkpoint to temporary folder first\r\n\r\nSince partial/missing files due to failures throw error during load",
    "sha": "e3458af72634c9c60cba4e44ac4c44dd655cee59",
    "files": [
        {
            "sha": "2009ebe8f254ae67ea424ef3ddeffc38262fdbef",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 31,
            "deletions": 20,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3458af72634c9c60cba4e44ac4c44dd655cee59/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3458af72634c9c60cba4e44ac4c44dd655cee59/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=e3458af72634c9c60cba4e44ac4c44dd655cee59",
            "patch": "@@ -18,6 +18,7 @@\n \n import contextlib\n import copy\n+import errno\n import functools\n import glob\n import importlib.metadata\n@@ -3128,31 +3129,41 @@ def _save_checkpoint(self, model, trial):\n             self.store_flos()\n \n         run_dir = self._get_output_dir(trial=trial)\n-        output_dir = os.path.join(run_dir, checkpoint_folder)\n-        self.save_model(output_dir, _internal_call=True)\n+        checkpoint_dir = os.path.join(run_dir, checkpoint_folder)\n+        with tempfile.TemporaryDirectory(prefix=f\"tmp-{PREFIX_CHECKPOINT_DIR}-\", dir=run_dir) as output_dir:\n+            self.save_model(output_dir, _internal_call=True)\n \n-        if not self.args.save_only_model:\n-            # Save optimizer and scheduler\n-            self._save_optimizer_and_scheduler(output_dir)\n-            # Save RNG state\n-            self._save_rng_state(output_dir)\n+            if not self.args.save_only_model:\n+                # Save optimizer and scheduler\n+                self._save_optimizer_and_scheduler(output_dir)\n+                # Save RNG state\n+                self._save_rng_state(output_dir)\n \n-        # Save the Trainer state\n-        if self.args.should_save:\n-            # Update `ExportableState` callbacks and `TrainerControl` state to where we are currently\n-            for cb in [\n-                cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)\n-            ]:\n-                cb_name = cb.__class__.__name__\n-                cb_state = cb.state()\n-                if isinstance(self.state.stateful_callbacks[cb_name], list):\n-                    self.state.stateful_callbacks[cb_name].append(cb_state)\n+            # Save the Trainer state\n+            if self.args.should_save:\n+                # Update `ExportableState` callbacks and `TrainerControl` state to where we are currently\n+                for cb in [\n+                    cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)\n+                ]:\n+                    cb_name = cb.__class__.__name__\n+                    cb_state = cb.state()\n+                    if isinstance(self.state.stateful_callbacks[cb_name], list):\n+                        self.state.stateful_callbacks[cb_name].append(cb_state)\n+                    else:\n+                        self.state.stateful_callbacks[cb_name] = cb_state\n+                self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n+\n+            try:\n+                os.renames(output_dir, checkpoint_dir)\n+            except OSError as e:\n+                if e.errno in [errno.ENOTEMPTY, errno.EEXIST]:  # Directory/File already exists\n+                    shutil.rmtree(checkpoint_dir)\n+                    os.renames(output_dir, checkpoint_dir)\n                 else:\n-                    self.state.stateful_callbacks[cb_name] = cb_state\n-            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n+                    raise\n \n         if self.args.push_to_hub:\n-            self._push_from_checkpoint(output_dir)\n+            self._push_from_checkpoint(checkpoint_dir)\n \n         # Maybe delete some older checkpoints.\n         if self.args.should_save:"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 31,
        "deletions": 20
    }
}