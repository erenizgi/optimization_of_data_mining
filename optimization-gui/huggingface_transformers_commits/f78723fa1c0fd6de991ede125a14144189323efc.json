{
    "author": "ArthurZucker",
    "message": "Small migration guide update (#42510)\n\n* update\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n---------\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "f78723fa1c0fd6de991ede125a14144189323efc",
    "files": [
        {
            "sha": "49fadf5820fc5489ff4e480df062f0e8b6fed7a4",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/f78723fa1c0fd6de991ede125a14144189323efc/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f78723fa1c0fd6de991ede125a14144189323efc/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=f78723fa1c0fd6de991ede125a14144189323efc",
            "patch": "@@ -275,6 +275,76 @@ For legacy implementations, the original BERT Python tokenizer code (including `\n - https://github.com/huggingface/transformers/pull/41626\n \n \n+## Disclaimers for the RC0\n+\n+### PEFT + MoE:\n+Because we are switching from the naive MOE (`nn.ModuleList` for experts) we currently have an issue with MoEs that have adapters. For more details see https://github.com/huggingface/transformers/issues/42491#issuecomment-3591485649. \n+\n+### Custom pretrained models:\n+For anyone inheriting from a `transformers` `PreTrainedModel`, the weights are automatically initialized with the common scheme: \n+```python\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        \"\"\"\n+        Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n+        initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\n+        `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\n+        \"\"\"\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range or 0.02\n+        elif hasattr(self.config, \"init_std\"):\n+            std = self.config.init_std\n+        elif hasattr(self.config, \"initializer_factor\"):\n+            std = self.config.initializer_factor\n+        else:\n+            # 0.02 is the standard default value across the library\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n+            if getattr(module, \"weight\", None) is not None:\n+                init.normal_(module.weight, mean=0.0, std=std)\n+            if getattr(module, \"bias\", None) is not None:\n+                init.zeros_(module.bias)\n+        elif isinstance(module, nn.Embedding):\n+            if getattr(module, \"weight\", None) is not None:\n+                init.normal_(module.weight, mean=0.0, std=std)\n+                # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+                if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                    init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, nn.MultiheadAttention):\n+            # This uses torch's original init\n+            module._reset_parameters()\n+        # We cannot use `isinstance` on the RMSNorms or LayerNorms, as they usually are custom modules which change names\n+        # between modelings (because they are prefixed with the model name)\n+        elif (\n+            isinstance(module, (nn.GroupNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\n+            or \"LayerNorm\" in module.__class__.__name__\n+            or \"RMSNorm\" in module.__class__.__name__\n+        ):\n+            # Norms can exist without weights (in which case they are None from torch primitives)\n+            if hasattr(module, \"weight\") and module.weight is not None:\n+                init.ones_(module.weight)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                init.zeros_(module.bias)\n+```\n+\n+If you want to avoid that, for now you should just do:\n+\n+```\n+class CustomModel(Qwen3VLForConditionalGeneration):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.action_head = nn.Linear(1024, 7)\n+        self.positional_embedding = nn.Parameter(torch.randn(16, 1152))\n+        self.post_init()\n+    \n+    def _init_weights(self, module):\n+        pass \n+\n+```\n+There is a tracker for that here: https://github.com/huggingface/transformers/issues/42418.\n+\n ## Library-wide changes with lesser impact\n \n ### `use_auth_token`"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 70,
        "deletions": 0
    }
}