{
    "author": "co63oc",
    "message": "Fix typos in comments (#37694)\n\nSigned-off-by: co63oc <co63oc@users.noreply.github.com>",
    "sha": "0302aa1c6ef1052697671e7b6eece53fb841c668",
    "files": [
        {
            "sha": "831ec424e3dec3e4dad8d3fa77fea4874833365d",
            "filename": "benchmark/benchmark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/benchmark%2Fbenchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/benchmark%2Fbenchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmark.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -90,7 +90,7 @@ def summarize(run_dir, metrics, expand_metrics=False):\n \n         model = benchmark.config.backend[\"model\"]\n \n-        # Ths looks like `benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5`.\n+        # This looks like `benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5`.\n         # (we rely on the usage of hydra's `${hydra.job.override_dirname}`.)\n         benchmark_name = re.sub(f\"backend.model={model},*\", \"\", report_dir)\n         benchmark_name = str(Path(benchmark_name).parts[-1])"
        },
        {
            "sha": "6a477de7fbf4592e74fd9a459876bb01b11c178b",
            "filename": "benchmark/llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/benchmark%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/benchmark%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fllama.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -293,7 +293,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n                 max_cache_len=seq_length + 128,\n             )\n \n-            # 3nd call\n+            # 3rd call\n             start = perf_counter()\n             output = model.generate(**inputs, past_key_values=past_key_values)\n             end = perf_counter()"
        },
        {
            "sha": "0084709c7754e5e3d6082c0ed5ef45b888a4c265",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -37,15 +37,15 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n \n     Args:\n         audio (`str` or `np.ndarray`):\n-            The audio to be laoded to the numpy array format.\n+            The audio to be loaded to the numpy array format.\n         sampling_rate (`int`, *optional*, defaults to 16000):\n-            The samlping rate to be used when loading the audio. It should be same as the\n+            The sampling rate to be used when loading the audio. It should be same as the\n             sampling rate the model you will be using further was trained with.\n         timeout (`float`, *optional*):\n             The timeout value in seconds for the URL request.\n \n     Returns:\n-        `np.ndarray`: A numpy artay representing the audio.\n+        `np.ndarray`: A numpy array representing the audio.\n     \"\"\"\n     requires_backends(load_audio, [\"librosa\"])\n "
        },
        {
            "sha": "cb2c8c5cdc294231bfbb5f2ae4ddaeeb8a57fc24",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -1919,7 +1919,7 @@ def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k\n             full_key_states = torch.cat((k_out[:, :, 1:, :], key_states), dim=-2)\n             full_value_states = torch.cat((v_out[:, :, 1:, :], value_states), dim=-2)\n             # Fast decoding path -> here as the effective size is still sliding window, it is extremely important\n-            # to return `self.key_cache[layer_idx]` and `self.value_cache[layer_idx]`, as they have the fixed adress\n+            # to return `self.key_cache[layer_idx]` and `self.value_cache[layer_idx]`, as they have the fixed address\n             # in memory (the values are the same as the full states, but not the address!!)\n             if key_states.shape[-2] == 1:\n                 self.key_cache[layer_idx].copy_(full_key_states)\n@@ -2031,7 +2031,7 @@ def __init__(\n         self.active_device_layer = 0\n \n     def initialise_cache_layer(self, layer_idx, key_states):\n-        \"\"\"Overriden to use the correct device if offloaded layer (and pin memory).\"\"\"\n+        \"\"\"Overridden to use the correct device if offloaded layer (and pin memory).\"\"\"\n         if len(self.key_cache) > layer_idx:\n             return\n \n@@ -2243,7 +2243,7 @@ class OffloadedStaticCache(StaticCache):\n             The device to offload to. Defaults to CPU.\n         layer_device_map (`Dict[int, Union[str, torch.device, int]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n-            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n \n     Example:"
        },
        {
            "sha": "a395ee793d66c3f91a4a7ebd77c2400fda76553a",
            "filename": "src/transformers/debug_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fdebug_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fdebug_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdebug_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -80,7 +80,7 @@ class DebugUnderflowOverflow:\n     You can see here, that `T5DenseGatedGeluDense.forward` resulted in output activations, whose absolute max value was\n     around 62.7K, which is very close to fp16's top limit of 64K. In the next frame we have `Dropout` which\n     renormalizes the weights, after it zeroed some of the elements, which pushes the absolute max value to more than\n-    64K, and we get an overlow.\n+    64K, and we get an overflow.\n \n     As you can see it's the previous frames that we need to look into when the numbers start going into very large for\n     fp16 numbers."
        },
        {
            "sha": "67aed15f0e5267c7b568194fe8520977b1e6815d",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -848,7 +848,7 @@ def load_tf_shard(model, model_layer_map, resolved_archive_file, ignore_mismatch\n                 f\"Unable to load weights from TF checkpoint file for '{resolved_archive_file}' \"\n                 f\"at '{resolved_archive_file}'. \"\n                 \"If you tried to load a TF model from a sharded checkpoint, you should try converting the model \"\n-                \"by loading it in pytorch and saving it locally. A convertion script should be released soon.\"\n+                \"by loading it in pytorch and saving it locally. A conversion script should be released soon.\"\n             )\n \n \n@@ -980,10 +980,10 @@ def load_tf_weights_from_h5(model, resolved_archive_file, ignore_mismatched_size\n                 for symbolic_weight in symbolic_weights:\n                     # TF names always start with the model name so we ignore it\n                     if _prefix is not None:\n-                        delimeter = len(_prefix.split(\"/\"))\n+                        delimiter = len(_prefix.split(\"/\"))\n                         symbolic_weight_name = \"/\".join(\n-                            symbolic_weight.name.split(\"/\")[:delimeter]\n-                            + symbolic_weight.name.split(\"/\")[delimeter + 1 :]\n+                            symbolic_weight.name.split(\"/\")[:delimiter]\n+                            + symbolic_weight.name.split(\"/\")[delimiter + 1 :]\n                         )\n                     else:\n                         symbolic_weight_name = \"/\".join(symbolic_weight.name.split(\"/\")[1:])\n@@ -2042,7 +2042,7 @@ def _v2_resized_token_embeddings(self, new_num_tokens: Optional[int] = None) ->\n         return model_embeds\n \n     def _get_word_embedding_weight(model, embedding_layer):\n-        # TODO (joao): flagged for delection due to embeddings refactor\n+        # TODO (joao): flagged for detection due to embeddings refactor\n \n         # If the variable holds the weights themselves, return them\n         if isinstance(embedding_layer, tf.Tensor):\n@@ -3312,7 +3312,7 @@ class TFSharedEmbeddings(keras.layers.Layer):\n             Additional keyword arguments passed along to the `__init__` of `keras.layers.Layer`.\n     \"\"\"\n \n-    # TODO (joao): flagged for delection due to embeddings refactor\n+    # TODO (joao): flagged for detection due to embeddings refactor\n \n     def __init__(self, vocab_size: int, hidden_size: int, initializer_range: Optional[float] = None, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "930d072fce7554df6fb7892c83ba421438568414",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -856,7 +856,7 @@ def _get_resolved_checkpoint_files(\n ) -> Tuple[Optional[List[str]], Optional[Dict]]:\n     \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n     checkpoints are sharded.\n-    This function will download the data if necesary.\n+    This function will download the data if necessary.\n     \"\"\"\n     is_sharded = False\n \n@@ -3296,7 +3296,7 @@ def save_pretrained(\n                 the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n             save_peft_format (`bool`, *optional*, defaults to `True`):\n                 For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n-                keys of the state dict of adapters needs to be pre-pended with `base_model.model`. Advanced users can\n+                keys of the state dict of adapters needs to be prepended with `base_model.model`. Advanced users can\n                 disable this behaviours by setting `save_peft_format` to `False`.\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n@@ -3400,7 +3400,7 @@ def save_pretrained(\n \n                 if save_peft_format:\n                     logger.info(\n-                        \"To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\"\n+                        \"To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\"\n                     )\n                     peft_state_dict = {}\n                     for key, value in state_dict.items():\n@@ -5887,14 +5887,14 @@ def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, hf_quantizer: Optional[HfQuantizer]):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n-    the model, which is actually the loading speed botteneck.\n+    the model, which is actually the loading speed bottleneck.\n     Calling this function allows to cut the model loading time by a very large margin.\n \n     A few facts related to loading speed (taking into account the use of this function):\n     - When loading a model the first time, it is usually slower than the subsequent times, because the OS is very likely\n-    to cache the different state dicts (if enough ressources/RAM are available)\n+    to cache the different state dicts (if enough resources/RAM are available)\n     - Trying to force the OS to cache the files in advance (by e.g. accessing a small portion of them) is really hard,\n-    and not a good idea in general as this is low level OS optimizations that depend on ressource usage anyway\n+    and not a good idea in general as this is low level OS optimizations that depend on resource usage anyway\n     - As of 18/03/2025, loading a Llama 70B model with TP takes ~1 min without file cache, and ~13s with full file cache.\n     The baseline, i.e. only loading the tensor shards on device and adjusting dtype (i.e. copying them) is ~5s with full cache.\n     These numbers are reported for TP on 4 H100 GPUs.\n@@ -5935,7 +5935,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n             index = device.index if device.index is not None else torch.cuda.current_device()\n             device_memory = torch.cuda.mem_get_info(index)[0]\n             # Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\n-            # than that amount might sometimes lead to unecesary cuda OOM, if the last parameter to be loaded on the device is large,\n+            # than that amount might sometimes lead to unnecessary cuda OOM, if the last parameter to be loaded on the device is large,\n             # and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\n             # the param size, instead of using the remaining reserved part, and allocating only the difference, which can lead\n             # to OOM. See https://github.com/huggingface/transformers/issues/37436#issuecomment-2808982161 for more details."
        },
        {
            "sha": "2f7e445241c2a41c9c83b2dcb24b4f985e22fbe3",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -56,24 +56,24 @@ class ConvNextImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 384}`):\n             Resolution of the output image after `resize` is applied. If `size[\"shortest_edge\"]` >= 384, the image is\n             resized to `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Otherwise, the smaller edge of the image will\n             be matched to `int(size[\"shortest_edge\"]/crop_pct)`, after which the image is cropped to\n             `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Only has an effect if `do_resize` is set to `True`. Can\n-            be overriden by `size` in the `preprocess` method.\n+            be overridden by `size` in the `preprocess` method.\n         crop_pct (`float` *optional*, defaults to 224 / 256):\n             Percentage of the image to crop. Only has an effect if `do_resize` is `True` and size < 384. Can be\n-            overriden by `crop_pct` in the `preprocess` method.\n+            overridden by `crop_pct` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Resampling filter to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`"
        },
        {
            "sha": "c91b99e56cc0eada5e993c7a8819a29e19a2ce7f",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -91,7 +91,7 @@ class Data2VecAudioConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n         mask_time_length (`int`, *optional*, defaults to 10):\n             Length of vector span along the time axis.\n@@ -102,7 +102,7 @@ class Data2VecAudioConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "624e52cedd708c439446ad3c310130526ec9ceaf",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -79,7 +79,7 @@ class TvltImageProcessor(BaseImageProcessor):\n             `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n             Size of the output image after resizing. The shortest edge of the image will be resized to\n-            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overriden by\n+            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overridden by\n             `size` in the `preprocess` method.\n         patch_size (`List[int]` *optional*, defaults to [16,16]):\n             The patch size of image patch embedding."
        },
        {
            "sha": "2d0d2af79ebc74f169207cbda58fd3fcd8e652ec",
            "filename": "src/transformers/models/hubert/configuration_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -107,7 +107,7 @@ class HubertConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -119,7 +119,7 @@ class HubertConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "e67a0845a737a384a4de0898d70e7dba520c94a7",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -377,7 +377,7 @@ def __call__(\n         add_eos_token = output_kwargs[\"text_kwargs\"].pop(\"add_eos_token\", False)\n         add_end_of_utterance_token = output_kwargs[\"text_kwargs\"].pop(\"add_end_of_utterance_token\", None)\n \n-        # if the value isn't overriden by the user, check if the tokenizer was trained with this token and then use it\n+        # if the value isn't overridden by the user, check if the tokenizer was trained with this token and then use it\n         if add_end_of_utterance_token is None:\n             add_end_of_utterance_token = self.tokenizer_was_trained_with_end_of_utterance_token\n         # turn non-batched prompts into batched"
        },
        {
            "sha": "af13a2d317930249373f9fb944c432d3297f8f4d",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -66,7 +66,7 @@ class ImageGPTImageProcessor(BaseImageProcessor):\n \n     Args:\n         clusters (`np.ndarray` or `List[List[int]]`, *optional*):\n-            The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overriden by `clusters`\n+            The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n             in `preprocess`.\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's dimensions to `(size[\"height\"], size[\"width\"])`. Can be overridden by"
        },
        {
            "sha": "71b88ef1ade26ad1ae2dc100c24a213258f0414d",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -214,21 +214,21 @@ class Owlv2ImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_pad (`bool`, *optional*, defaults to `True`):\n-            Whether to pad the image to a square with gray pixels on the bottom and the right. Can be overriden by\n+            Whether to pad the image to a square with gray pixels on the bottom and the right. Can be overridden by\n             `do_pad` in the `preprocess` method.\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 960, \"width\": 960}`):\n-            Size to resize the image to. Can be overriden by `size` in the `preprocess` method.\n+            Size to resize the image to. Can be overridden by `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Resampling method to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+            Resampling method to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method."
        },
        {
            "sha": "bab5073be5872e59ab8588f60d2f023825ad7481",
            "filename": "src/transformers/models/sew/configuration_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -100,7 +100,7 @@ class SEWConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -112,7 +112,7 @@ class SEWConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "515b6adc373a411c18d03be8adb9419c367e5a29",
            "filename": "src/transformers/models/sew_d/configuration_sew_d.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -113,7 +113,7 @@ class SEWDConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -125,7 +125,7 @@ class SEWDConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "809252fbb6268274499b94685d3e35fd70e43bc7",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -335,7 +335,7 @@ def _process_messages_for_chat_template(\n         Used within `apply_chat_template` when a model has special way to process conversation history. For example,\n         video models might want to specify in the prompt the duration of video or which frame indices at which timestamps\n         were sampled. This information cannot be accessed before the video is loaded.\n-        For most models it is a no-op, must be overriden by model processors which require special processing.\n+        For most models it is a no-op, must be overridden by model processors which require special processing.\n         Args:\n             conversation (`List[Dict, str, str]`):\n                 The conversation to process. Always comes in batched format."
        },
        {
            "sha": "2025805983beaf65cd3770e3ad787530a898fea7",
            "filename": "src/transformers/models/speecht5/configuration_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -109,7 +109,7 @@ class SpeechT5Config(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -121,7 +121,7 @@ class SpeechT5Config(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "4a858db8f4e9d0a64cb4a14a492e89797225eaad",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -139,21 +139,21 @@ class SuperGlueImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n             Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n-            `True`. Can be overriden by `size` in the `preprocess` method.\n+            `True`. Can be overridden by `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Resampling filter to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_grayscale (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n+            Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]"
        },
        {
            "sha": "77802d2e5c76bab58a3661f28c6046ebabb3eea8",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -102,19 +102,19 @@ class SuperPointImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n             Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n-            `True`. Can be overriden by `size` in the `preprocess` method.\n+            `True`. Can be overridden by `size` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_grayscale (`bool`, *optional*, defaults to `False`):\n-            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n+            Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]"
        },
        {
            "sha": "be19d893a66edcd09fdab1f30a66a4f3e2de31c4",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -91,7 +91,7 @@ class TvpImageProcessor(BaseImageProcessor):\n             `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"longest_edge\": 448}`):\n             Size of the output image after resizing. The longest edge of the image will be resized to\n-            `size[\"longest_edge\"]` while maintaining the aspect ratio of the original image. Can be overriden by\n+            `size[\"longest_edge\"]` while maintaining the aspect ratio of the original image. Can be overridden by\n             `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n             Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the"
        },
        {
            "sha": "549767c0fbe043137c379098c2ce4ec73c191e9e",
            "filename": "src/transformers/models/unispeech/configuration_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -106,7 +106,7 @@ class UniSpeechConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -118,7 +118,7 @@ class UniSpeechConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "6e736960bc9f3f67db06c0485052c119a3487c7b",
            "filename": "src/transformers/models/unispeech_sat/configuration_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -107,7 +107,7 @@ class UniSpeechSatConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -119,7 +119,7 @@ class UniSpeechSatConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "a7043d5ada55e3b7dadcfe4a582afc4e25520678",
            "filename": "src/transformers/models/videomae/image_processing_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -72,7 +72,7 @@ class VideoMAEImageProcessor(BaseImageProcessor):\n             `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n             Size of the output image after resizing. The shortest edge of the image will be resized to\n-            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overriden by\n+            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overridden by\n             `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n             Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the"
        },
        {
            "sha": "8320ad2d6d19d0d7c13dca0bb38fd72297f482c4",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -334,11 +334,11 @@ class VitPoseImageProcessor(BaseImageProcessor):\n             Whether to apply an affine transformation to the input images.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 256, \"width\": 192}`):\n             Resolution of the image after `affine_transform` is applied. Only has an effect if `do_affine_transform` is set to `True`. Can\n-            be overriden by `size` in the `preprocess` method.\n+            be overridden by `size` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.).\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether or not to normalize the input with mean and standard deviation."
        },
        {
            "sha": "abf0a5808e128e6d3cdf1da5fba9579e264f3c95",
            "filename": "src/transformers/models/vivit/image_processing_vivit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -73,7 +73,7 @@ class VivitImageProcessor(BaseImageProcessor):\n             `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 256}`):\n             Size of the output image after resizing. The shortest edge of the image will be resized to\n-            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overriden by\n+            `size[\"shortest_edge\"]` while maintaining the aspect ratio of the original image. Can be overridden by\n             `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n             Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n@@ -91,7 +91,7 @@ class VivitImageProcessor(BaseImageProcessor):\n             Defines the scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter\n             in the `preprocess` method.\n         offset (`bool`, *optional*, defaults to `True`):\n-            Whether to scale the image in both negative and positive directions. Can be overriden by the `offset` in\n+            Whether to scale the image in both negative and positive directions. Can be overridden by the `offset` in\n             the `preprocess` method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`"
        },
        {
            "sha": "3b717f6af9a843328ed4e064acd1b07a6ae2c550",
            "filename": "src/transformers/models/wav2vec2/configuration_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -106,7 +106,7 @@ class Wav2Vec2Config(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -118,7 +118,7 @@ class Wav2Vec2Config(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "4904030606d8a9b77c97f4b1f3516643af7fbac5",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -1630,7 +1630,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n         state_dict = {k: v.to(adapter_weights[k]) for k, v in state_dict.items()}\n         self.load_state_dict(state_dict, strict=False)\n \n-        # set target language corectly\n+        # set target language correctly\n         self.target_lang = target_lang\n \n "
        },
        {
            "sha": "1f468150775705567ec16571d8598c652b0a34e2",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -100,7 +100,7 @@ class Wav2Vec2CTCTokenizerOutput(ModelOutput):\n             Decoded logits in text from. Usually the speech transcription.\n         char_offsets (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str, Union[int, str]]]`):\n             Offsets of the decoded characters. In combination with sampling rate and model downsampling rate char\n-            offsets can be used to compute time stamps for each charater. Total logit score of the beam associated with\n+            offsets can be used to compute time stamps for each character. Total logit score of the beam associated with\n             produced text.\n         word_offsets (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str, Union[int, str]]]`):\n             Offsets of the decoded words. In combination with sampling rate and model downsampling rate word offsets"
        },
        {
            "sha": "cff7c5cce346e381a3bd9366dcb21c9e4e4847c3",
            "filename": "src/transformers/models/wav2vec2_bert/configuration_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -76,7 +76,7 @@ class Wav2Vec2BertConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates `mask_time_prob*len(time_axis)/mask_time_length ``independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -88,7 +88,7 @@ class Wav2Vec2BertConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "d324614e76083e9980693e629d83fb107c5eb477",
            "filename": "src/transformers/models/wav2vec2_conformer/configuration_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -103,7 +103,7 @@ class Wav2Vec2ConformerConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -115,7 +115,7 @@ class Wav2Vec2ConformerConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "7706819ab44128d77f39799400e290d7e2211be5",
            "filename": "src/transformers/models/whisper/configuration_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -146,7 +146,7 @@ class WhisperConfig(PretrainedConfig):\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n             procecure generates `mask_time_prob*len(time_axis)/mask_time_length` independent masks over the axis. If\n-            reasoning from the propability of each feature vector to be chosen as the start of the vector span to be\n+            reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment == True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -158,7 +158,7 @@ class WhisperConfig(PretrainedConfig):\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n             masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n-            the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector\n+            the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is\n             True`."
        },
        {
            "sha": "d84964342c62d94271c0c04579b0b523e47a3d4a",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -555,7 +555,7 @@ def __init__(\n \n         if input_path is not None:\n             if not exists(abspath(self.input_path)):\n-                raise OSError(f\"{self.input_path} doesnt exist on disk\")\n+                raise OSError(f\"{self.input_path} doesn't exist on disk\")\n \n     @abstractmethod\n     def __iter__(self):"
        },
        {
            "sha": "b84bd4001c296b79cf6ba3b299ef1de51e1583e8",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -761,7 +761,7 @@ def get_processor_dict(\n         resolved_additional_chat_template_files = {}\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_processor_file = pretrained_model_name_or_path\n-            # cant't load chat-template when given a file as pretrained_model_name_or_path\n+            # can't load chat-template when given a file as pretrained_model_name_or_path\n             resolved_chat_template_file = None\n             resolved_raw_chat_template_file = None\n             is_local = True"
        },
        {
            "sha": "687d745775e2a70325c95c79c604cd1f54d00bef",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0302aa1c6ef1052697671e7b6eece53fb841c668/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=0302aa1c6ef1052697671e7b6eece53fb841c668",
            "patch": "@@ -2664,7 +2664,7 @@ def wrapper(*args, **kwargs):\n def run_first(test_case):\n     \"\"\"\n     Decorator marking a test with order(1). When pytest-order plugin is installed, tests marked with this decorator\n-    are garanteed to run first.\n+    are guaranteed to run first.\n \n     This is especially useful in some test settings like on a Gaudi instance where a Gaudi device can only be used by a\n     single process at a time. So we make sure all tests that run in a subprocess are launched first, to avoid device"
        }
    ],
    "stats": {
        "total": 164,
        "additions": 82,
        "deletions": 82
    }
}