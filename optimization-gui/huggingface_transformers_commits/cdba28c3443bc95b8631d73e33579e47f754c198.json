{
    "author": "YangKai0616",
    "message": "[XPU] Add MXFP4 support for XPU (#41117)\n\n* XPU supports gpt-oss MXFP4\n\n* Complete MXFP4 UT file and comment information\n\n* Complete MXFP4 UT file and comment information\n\n* Fix code style\n\n* Fix code style\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "cdba28c3443bc95b8631d73e33579e47f754c198",
    "files": [
        {
            "sha": "ac6ab780ffc04c8b8ec5d22d4cab94a117d36f52",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 27,
            "deletions": 4,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdba28c3443bc95b8631d73e33579e47f754c198/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdba28c3443bc95b8631d73e33579e47f754c198/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=cdba28c3443bc95b8631d73e33579e47f754c198",
            "patch": "@@ -23,6 +23,7 @@\n     from accelerate import init_empty_weights\n \n import re\n+from contextlib import contextmanager\n \n \n logger = logging.get_logger(__name__)\n@@ -47,6 +48,28 @@\n ]\n \n \n+@contextmanager\n+def on_device(dev):\n+    if is_torch_available():\n+        import torch\n+\n+        if isinstance(dev, torch.Tensor):\n+            dev = dev.device\n+        elif isinstance(dev, str):\n+            dev = torch.device(dev)\n+        dev_type = getattr(dev, \"type\", None)\n+        if dev_type == \"cuda\":\n+            with torch.cuda.device(dev):\n+                yield\n+                return\n+        if dev_type == \"xpu\" and hasattr(torch, \"xpu\"):\n+            with torch.xpu.device(dev):\n+                yield\n+                return\n+    # other: CPU\n+    yield\n+\n+\n # Copied from GPT_OSS repo and vllm\n def quantize_to_mxfp4(w, triton_kernels_hub):\n     downcast_to_mxfp_torch = triton_kernels_hub.numerics_details.mxfp.downcast_to_mxfp_torch\n@@ -173,7 +196,7 @@ def forward(self, hidden_states: torch.Tensor, routing_data, gather_idx, scatter\n         )\n         swiglu_fn = triton_kernels_hub.swiglu.swiglu_fn\n \n-        with torch.cuda.device(hidden_states.device):\n+        with on_device(hidden_states.device):\n             act = FusedActivation(FnSpecs(\"swiglu\", swiglu_fn, (\"alpha\", \"limit\")), (self.alpha, self.limit), 2)\n \n             intermediate_cache1 = matmul_ogs(\n@@ -214,7 +237,7 @@ def routing_torch_dist(\n         triton_kernels_hub.routing.compute_expt_data_torch,\n     )\n \n-    with torch.cuda.device(logits.device):\n+    with on_device(logits.device):\n         world_size = torch.distributed.get_world_size()\n         rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n         replace_value = -1\n@@ -281,7 +304,7 @@ def mlp_forward(self, hidden_states):\n     hidden_states = hidden_states.reshape(-1, self.router.hidden_dim)\n     router_logits = nn.functional.linear(hidden_states, self.router.weight, self.router.bias)\n \n-    with torch.cuda.device(router_logits.device):\n+    with on_device(router_logits.device):\n         routing_data, gather_idx, scatter_idx = routing(router_logits, self.router.top_k)\n \n     routed_out = self.experts(hidden_states, routing_data, gather_idx, scatter_idx)\n@@ -376,7 +399,7 @@ def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, trito\n             target_device = \"cuda\"\n         blocks = blocks.to(target_device).contiguous()\n         scales = scales.to(target_device).contiguous()\n-        with torch.cuda.device(target_device):\n+        with on_device(target_device):\n             triton_weight_tensor, weight_scale = swizzle_mxfp4(\n                 blocks.transpose(-2, -1), scales.transpose(-2, -1), triton_kernels_hub\n             )"
        },
        {
            "sha": "f4657d8b54f6894f237d571b187499b23f97cf72",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdba28c3443bc95b8631d73e33579e47f754c198/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdba28c3443bc95b8631d73e33579e47f754c198/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=cdba28c3443bc95b8631d73e33579e47f754c198",
            "patch": "@@ -72,7 +72,7 @@ def validate_environment(self, *args, **kwargs):\n         if self.quantization_config.dequantize:\n             return\n \n-        if not torch.cuda.is_available():\n+        if not (torch.cuda.is_available() or torch.xpu.is_available()):\n             if self.pre_quantized:\n                 logger.warning_once(\n                     \"Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16\"\n@@ -85,43 +85,49 @@ def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\"Using mxfp4 requires Accelerate: `pip install accelerate`\")\n \n-        compute_capability = torch.cuda.get_device_capability()\n-        gpu_is_supported = compute_capability >= (7, 5)\n-        kernels_available = is_triton_available(\"3.4.0\") and is_kernels_available()\n+        if torch.xpu.is_available():\n+            gpu_is_supported = True\n+            kernels_available = is_triton_available(\"3.5.0\") and is_kernels_available()\n+        else:\n+            compute_capability = torch.cuda.get_device_capability()\n+            gpu_is_supported = compute_capability >= (7, 5)\n+            kernels_available = is_triton_available(\"3.4.0\") and is_kernels_available()\n \n         if self.pre_quantized:\n             # On unsupported GPUs or without kernels, we will dequantize the model to bf16\n             if not gpu_is_supported:\n                 logger.warning_once(\n-                    \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200). \"\n+                    \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200) or XPUs (e.g Intel® Data Center GPU Max Series) \"\n                     \"We will default to dequantizing the model to bf16.\"\n                 )\n                 self.quantization_config.dequantize = True\n                 return\n \n             if not kernels_available:\n                 logger.warning_once(\n-                    \"MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\"\n+                    \"MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\"\n                 )\n                 self.quantization_config.dequantize = True\n                 return\n         elif not gpu_is_supported:\n             # we can't quantize the model in this case so we raise an error\n             raise ValueError(\n-                \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200)\"\n+                \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200) or XPUs (e.g Intel® Data Center GPU Max Series) \"\n             )\n         elif not kernels_available:\n             # we can't quantize the model in this case so we raise an error\n-            raise ValueError(\"MXFP4 quantization requires triton >= 3.4.0 and kernels installed\")\n+            raise ValueError(\n+                \"MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0\"\n+            )\n \n         if not self.pre_quantized:\n             self._lazy_import_kernels()\n \n         device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n-                \"You have loaded an FP4 model on CPU and have a CUDA device available, make sure to set \"\n-                \"your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \"\n+                \"You have loaded an FP4 model on CPU and have a CUDA/XPU device available, make sure to set \"\n+                \"your model on a GPU/XPU device in order to run your model. To remove this warning, pass device_map = 'cuda' or device_map = 'xpu'. \"\n             )\n         elif device_map is not None:\n             if (\n@@ -263,6 +269,8 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n         # clean cache due to triton ops\n         if torch.cuda.is_available():\n             torch.cuda.empty_cache()\n+        elif torch.xpu.is_available():\n+            torch.xpu.empty_cache()\n \n     def update_expected_keys(self, model: \"PreTrainedModel\", expected_keys: list[str], checkpoint_keys: list[str]):\n         # Replace expected_keys for experts' gate_up_proj and down_proj with their _blocks and _scales variants"
        },
        {
            "sha": "eddd15b3b744c2b0c7f48d4371d586465a1e7ae0",
            "filename": "tests/quantization/mxfp4/test_mxfp4.py",
            "status": "modified",
            "additions": 50,
            "deletions": 25,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdba28c3443bc95b8631d73e33579e47f754c198/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdba28c3443bc95b8631d73e33579e47f754c198/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py?ref=cdba28c3443bc95b8631d73e33579e47f754c198",
            "patch": "@@ -15,14 +15,15 @@\n import gc\n import tempfile\n import unittest\n+from contextlib import ExitStack, contextmanager\n from unittest.mock import patch\n \n from transformers import AutoTokenizer, GptOssForCausalLM, Mxfp4Config\n from transformers.testing_utils import (\n     require_kernels,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_large_gpu,\n+    require_torch_large_accelerator,\n     require_triton,\n     slow,\n )\n@@ -35,6 +36,30 @@\n     import torch\n \n \n+if torch.cuda.is_available():\n+    REQUIRE_TRITON_MXFP4 = require_triton(min_version=\"3.4.0\")\n+elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n+    REQUIRE_TRITON_MXFP4 = require_triton(min_version=\"3.5.0\")\n+else:\n+    REQUIRE_TRITON_MXFP4 = unittest.skip(\"test requires CUDA or XPU\")\n+\n+\n+def _empty_accelerator_cache():\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n+        torch.xpu.empty_cache()\n+\n+\n+@contextmanager\n+def _patch_no_accelerator():\n+    with ExitStack() as stack:\n+        stack.enter_context(patch(\"torch.cuda.is_available\", return_value=False))\n+        if hasattr(torch, \"xpu\"):\n+            stack.enter_context(patch(\"torch.xpu.is_available\", return_value=False))\n+        yield\n+\n+\n class Mxfp4ConfigTest(unittest.TestCase):\n     def test_basic_config_creation(self):\n         \"\"\"Test basic configuration creation with default values\"\"\"\n@@ -82,8 +107,7 @@ class Mxfp4QuantizerTest(unittest.TestCase):\n \n     def setUp(self):\n         gc.collect()\n-        if torch.cuda.is_available():\n-            torch.cuda.empty_cache()\n+        _empty_accelerator_cache()\n \n     def test_quantizer_validation_no_torch(self):\n         \"\"\"Test quantizer validation when torch is not available\"\"\"\n@@ -96,9 +120,9 @@ def test_quantizer_validation_no_torch(self):\n             with self.assertRaises(ImportError):\n                 quantizer.validate_environment()\n \n-    def test_quantizer_validation_no_cuda(self):\n-        \"\"\"Test quantizer validation when CUDA is not available\"\"\"\n-        with patch(\"torch.cuda.is_available\", return_value=False):\n+    def test_quantizer_validation_no_accelerator(self):\n+        \"\"\"Test quantizer validation when CUDA/XPU is not available\"\"\"\n+        with _patch_no_accelerator():\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n             config = Mxfp4Config()\n@@ -108,8 +132,9 @@ def test_quantizer_validation_no_cuda(self):\n             with self.assertRaises(RuntimeError):\n                 quantizer.validate_environment()\n \n+    @require_torch_gpu\n     def test_quantizer_validation_low_compute_capability(self):\n-        \"\"\"Test quantizer validation with low compute capability\"\"\"\n+        \"\"\"Test quantizer validation with CUDA low compute capability\"\"\"\n         with patch(\"torch.cuda.get_device_capability\", return_value=(7, 0)):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -120,8 +145,9 @@ def test_quantizer_validation_low_compute_capability(self):\n             with self.assertRaises(ValueError):\n                 quantizer.validate_environment()\n \n+    @require_torch_gpu\n     def test_quantizer_validation_low_compute_capability_with_prequantized(self):\n-        \"\"\"Test quantizer validation with low compute capability\"\"\"\n+        \"\"\"Test quantizer validation with CUDA low compute capability\"\"\"\n         with patch(\"torch.cuda.get_device_capability\", return_value=(7, 0)):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -132,8 +158,9 @@ def test_quantizer_validation_low_compute_capability_with_prequantized(self):\n             quantizer.validate_environment()\n             self.assertTrue(quantizer.quantization_config.dequantize)\n \n+    @require_torch_gpu\n     def test_quantizer_validation_low_compute_capability_with_dequantize(self):\n-        \"\"\"Test quantizer validation with low compute capability but dequantize enabled\"\"\"\n+        \"\"\"Test quantizer validation with CUDA low compute capability but dequantize enabled\"\"\"\n         with patch(\"torch.cuda.get_device_capability\", return_value=(7, 0)):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -147,20 +174,20 @@ def test_quantizer_validation_low_compute_capability_with_dequantize(self):\n                 if \"compute capability\" in str(e):\n                     self.fail(\"Should not raise compute capability error when dequantize=True\")\n \n-    def test_quantizer_validation_order_dequantize_before_cuda_check(self):\n-        \"\"\"Test that dequantize check happens before CUDA availability check\"\"\"\n+    def test_quantizer_validation_order_dequantize_before_accelerator_check(self):\n+        \"\"\"Test that dequantize check happens before CUDA/XPU availability check\"\"\"\n         # Mock torch.cuda.is_available\n-        with patch(\"torch.cuda.is_available\", return_value=False):\n+        with _patch_no_accelerator():\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n-            # Test with dequantize=True - should pass even without CUDA and accelerate\n+            # Test with dequantize=True - should pass even without CUDA/XPU and accelerate\n             config = Mxfp4Config(dequantize=True)\n             quantizer = Mxfp4HfQuantizer(config)\n \n             # This should not raise any error because dequantize check comes first\n             quantizer.validate_environment()\n \n-            # Test with dequantize=False - should still fail due to missing CUDA\n+            # Test with dequantize=False - should still fail due to missing CUDA/XPU\n             config = Mxfp4Config(dequantize=False)\n             quantizer = Mxfp4HfQuantizer(config)\n             quantizer.pre_quantized = False\n@@ -313,9 +340,8 @@ def test_convert_moe_packed_tensors(self):\n         self.assertEqual(result.shape, (2, 8 * 16 * 2, 4))\n         self.assertEqual(result.dtype, torch.bfloat16)\n \n-    @require_triton(min_version=\"3.4.0\")\n+    @REQUIRE_TRITON_MXFP4\n     @require_kernels\n-    @require_torch_gpu\n     @require_torch\n     def test_quantize_to_mxfp4(self):\n         \"\"\"Test quantization function\"\"\"\n@@ -326,7 +352,8 @@ def test_quantize_to_mxfp4(self):\n         quantizer = Mxfp4HfQuantizer(config)\n \n         # Create dummy weight tensor\n-        w = torch.randn(32, 64, 128, dtype=torch.bfloat16, device=torch.device(\"cuda\"))\n+        device = \"xpu\" if (hasattr(torch, \"xpu\") and torch.xpu.is_available()) else \"cuda\"\n+        w = torch.randn(32, 64, 128, dtype=torch.bfloat16, device=torch.device(device))\n \n         quantized_w, w_scale = quantize_to_mxfp4(w, quantizer._lazy_import_kernels())\n \n@@ -335,8 +362,8 @@ def test_quantize_to_mxfp4(self):\n \n \n @require_torch\n-@require_torch_large_gpu\n-@require_triton(min_version=\"3.4.0\")\n+@require_torch_large_accelerator\n+@REQUIRE_TRITON_MXFP4\n @require_kernels\n @slow\n class Mxfp4ModelTest(unittest.TestCase):\n@@ -353,13 +380,11 @@ class Mxfp4ModelTest(unittest.TestCase):\n \n     def setUp(self):\n         gc.collect()\n-        if torch.cuda.is_available():\n-            torch.cuda.empty_cache()\n+        _empty_accelerator_cache()\n \n     def tearDown(self):\n         gc.collect()\n-        if torch.cuda.is_available():\n-            torch.cuda.empty_cache()\n+        _empty_accelerator_cache()\n \n     def check_inference_correctness_quantized(self, model, tokenizer):\n         # Check that inference pass works on the model\n@@ -454,7 +479,7 @@ def test_save_mxfp4(self):\n         with tempfile.TemporaryDirectory() as tmp:\n             # Save the model in mxfp4 format\n             model.save_pretrained(tmp)\n-            torch.cuda.empty_cache()\n+            _empty_accelerator_cache()\n             gc.collect()\n             # test quantized model\n             loaded_model = GptOssForCausalLM.from_pretrained(\n@@ -486,7 +511,7 @@ def test_save_mxfp4_non_quantized(self):\n         # save the quantized model\n         with tempfile.TemporaryDirectory() as tmp:\n             loaded_model.save_pretrained(tmp)\n-            torch.cuda.empty_cache()\n+            _empty_accelerator_cache()\n             gc.collect()\n             # load it back to check with everything works as expected\n             loaded_model = GptOssForCausalLM.from_pretrained("
        }
    ],
    "stats": {
        "total": 134,
        "additions": 95,
        "deletions": 39
    }
}