{
    "author": "ctcanbol",
    "message": "Support loading Qwen3 MoE GGUF (#39638)\n\n* support loading qwen3 gguf\n\n* qwen3moe test cases\n\n* fix whitespaces\n\n* fix ggml tests",
    "sha": "fb141e2c906066453dd8be59deade90c9fdf8264",
    "files": [
        {
            "sha": "ecf34bbf5e19594ba3192906ab5145854838f079",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb141e2c906066453dd8be59deade90c9fdf8264/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb141e2c906066453dd8be59deade90c9fdf8264/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=fb141e2c906066453dd8be59deade90c9fdf8264",
            "patch": "@@ -102,6 +102,20 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"qwen3moe\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+        \"expert_count\": \"num_experts\",\n+        \"expert_used_count\": \"num_experts_per_tok\",\n+    },\n     \"falcon\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n@@ -689,6 +703,7 @@ def converted(self) -> Tokenizer:\n     \"qwen2\": GGUFQwen2Converter,\n     \"qwen2_moe\": GGUFQwen2Converter,\n     \"qwen3\": GGUFQwen2Converter,\n+    \"qwen3_moe\": GGUFQwen2Converter,\n     \"phi3\": GGUFPhi3Converter,\n     \"bloom\": GGUFGPTConverter,\n     \"falcon\": GGUFGPTConverter,"
        },
        {
            "sha": "408ab47a629627a47612cfbe0282c62cdfac473b",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb141e2c906066453dd8be59deade90c9fdf8264/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb141e2c906066453dd8be59deade90c9fdf8264/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=fb141e2c906066453dd8be59deade90c9fdf8264",
            "patch": "@@ -302,6 +302,7 @@ class GgufModelTests(unittest.TestCase):\n     gemma3_text_model_id = \"unsloth/gemma-3-1b-it-GGUF\"\n     gemma3_vision_model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n     qwen3_model_id = \"Qwen/Qwen3-0.6B-GGUF\"\n+    qwen3moe_model_id = \"Qwen/Qwen3-30B-A3B-GGUF\"\n \n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n@@ -335,6 +336,7 @@ class GgufModelTests(unittest.TestCase):\n     bf16_gemma3_text_model_id = \"gemma-3-1b-it-BF16.gguf\"\n     bf16_gemma3_vision_model_id = \"gemma-3-4b-it-BF16.gguf\"\n     q8_0_qwen3_model_id = \"Qwen3-0.6B-Q8_0.gguf\"\n+    q4_k_m_qwen3moe_model_id = \"Qwen3-30B-A3B-Q4_K_M.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -973,3 +975,17 @@ def test_qwen3_q8_0(self):\n \n         EXPECTED_TEXT = \"HelloED\\nI need to find the value of the\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_qwen3moe_q4_k_m(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.qwen3moe_model_id, gguf_file=self.q4_k_m_qwen3moe_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.qwen3moe_model_id,\n+            gguf_file=self.q4_k_m_qwen3moe_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I am a 20 year old male\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 31,
        "deletions": 0
    }
}