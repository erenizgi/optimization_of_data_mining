{
    "author": "SangbumChoi",
    "message": "Add Segment Anything 2 (SAM2) (#32317)\n\n* initial comment\n\n* test\n\n* initial conversion for outline\n\n* intermediate commit for configuration\n\n* chore:init files for sam2\n\n* adding arbitary undefined config\n\n* check\n\n* add vision\n\n* make style\n\n* init sam2 base model\n\n* Fix imports\n\n* Linting\n\n* chore:sam to sam2 classes\n\n* Linting\n\n* Add sam2 to models.__init__\n\n* chore:match prompt encoder with sam2 code\n\n* chore:prepare kwargs for mask decoder\n\n* Add image/video predictors\n\n* Add CUDA kernel\n\n* Add output classes\n\n* linting\n\n* Add logging info\n\n* tmp commit\n\n* docs for sam2\n\n* enable image processing\n\n* check difference of original SAM2\n- difference is the order of ToTensor()\n- please see https://pytorch.org/vision/main/_modules/torchvision/transforms/functional.html#resize\n\n* enable promptencoder of sam2\n\n* fix promprencoder\n\n* Confirmed that PromptEncoder is exactly same (Be aware of bfloat16 and float32 difference)\n\n* Confirmed that ImageEncoder is exactly same (Be aware the linting of init)\n\n* Confirmed that MaskDecoder is exactly same (TO DO: lint variable name)\n\n* SamModel is now available (Need more chore for name)\n\n* make fix-copies\n\n* make style\n\n* make CI happy\n\n* Refactor VisionEncoder and PostioinEmbedding\n\n* TO DO : fix the image_embeddings and sparse_embeddings part\n\n* pure image inference done\n\n* reusable features fix and make style\n\n* styling\n\n* refactor memoryattention\n\n* tmp\n\n* tmp\n\n* refactor memoryencoder\nTO DO : convert and inference the video pipeline\n\n* TO DO : fix the image_encoder shape\n\n* conversion finish\nTO DO: need to check video inference\n\n* make style\n\n* remove video model\n\n* lint\n\n* change\n\n* python utils/check_docstringspy --check_all\n\n* python utils/check_config_attributes.py\n\n* remove copies for sam2promptencoder due to configuration\n\n* change __init__.py\n\n* remove tensorflow version\n\n* fix that to not use direct comparison\n\n* make style\n\n* add missing import\n\n* fix image_embedding_size\n\n* refactor Sam2 Attention\n\n* add fully working video inference (refactoring todo)\n\n* clarify _prepare_memory_conditioned_features\n\n* simplify modeling code, remove unused paths\n\n* use one model\n\n* use auto_docstring\n\n* refactor rope embeddings\n\n* nit\n\n* not using multimask when several points given\n\n* add all sam2.1\n\n* add video tmp\n\n* add Sam2VideoSessionState + fast image proc + video proc\n\n* remove init_states from model\n\n* fix batch inference\n\n* add image integration tests\n\n* uniformize modeling code with other sam models and use modular\n\n* pass vision tests an most model tests\n\n* All tests passing\n\n* add offloading inference state and video to cpu\n\n* fix inference from image embedding and existing mask\n\n* fix multi_boxes mask inference\n\n* Fix batch images + batch boxes inference\n\n* improve processing for image inference\n\n* add support for mask generation pipeline\n\n* add support for get_connected_components post processing in mask generation\n\n* add fast image processor sam, image processor tests and use modular for sam2 image processor\n\n* fix mistake in sam after #39120\n\n* fix init weights\n\n* refactor convert\n\n* add integration tests for video + other improvements\n\n* add needed missing docstrings\n\n* Improve docstrings and\n\n* improve inference speed by avoiding cuda sync\n\n* add test\n\n* skip test for vision_model\n\n* minor fix for vision_model\n\n* fix vision_model by adding sam2model and change the torch dependencies\n\n* remove patch_size\n\n* remove image_embedding_size\n\n* fix patch_size\n\n* fix test\n\n* make style\n\n* Separate hieradet and vision encoder in sam2\n\n* fixup\n\n* review changes part 1\n\n* remove MemoryEncoderConfig and MemoryAttentionConfig\n\n* pass q_stride instead of q_pool module\n\n* add inference on streamed videos\n\n* explicitely process streamed frames\n\n* nit\n\n* Improve docstrings in Sam2Model\n\n* update sam2 modeling with better gestion of inference state and cache, and separate Sam2Model and Sam2VideoModel\n\n* improve video inference api\n\n* change inference_state to inference_session\n\n* use modular for Sam2Model\n\n* fix convert sam2 hf\n\n* modular\n\n* Update src/transformers/models/sam2/video_processing_sam2.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* fix minor config\n\n* fix attention loading error\n\n* update modeling tests to use hub checkpoints\n\n* Use CI A10 runner for integration tests values + higher tolerance for video integration tests\n\n* PR review part 1\n\n* fix doc\n\n* nit improvements\n\n* enforce one input format for points, labels and boxes\n\n* nit\n\n* last few nits from PR review\n\n* fix style\n\n* fix the input type\n\n* fix docs\n\n* add sam2 model as conversion script\n\n* improve sam2 doc\n\n* nit fixes + optimization\n\n* split sam2 and sam2_video in two models\n\n* PR review part 1\n\n* fix None for default slow processor of sam2\n\n* remove unecessary code path in sam2_video\n\n* refactor/simplify RoPE\n\n* replace embedding module list with embedding matrix\n\n* fix tests\n\n* remove kernel\n\n* nit\n\n* use lru_cache for sine_pos_embeddings\n\n* reorder sam2_video methods\n\n* simplify sam2_video\n\n* PR review part 1\n\n* simplify sam2 video a lot\n\n* more simplification\n\n* update integration tests with updated conftest\n\n* more explicit config for hieradet\n\n* do post_processing outside of sam2 video model\n\n* Improve Sam2VideoVisionRotaryEmbedding\n\n* fix tests\n\n* update docs and fix mask2former/oneformer\n\n* avoid unnecessary reshapes/permute\n\n* fix device concatenating points\n\n* small dtype fix\n\n* PR review\n\n* nit\n\n* fix style and finish up doc\n\n* fix style\n\n* fix docstrings\n\n* fix modular\n\n---------\n\nCo-authored-by: RUFFY-369 <prakarshkaushik369@gmail.com>\nCo-authored-by: Haitham Khedr <haithamkhedr@meta.com>\nCo-authored-by: sangbum choi <sangbumchoi@sangbumui-MacBookAir.local>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "68a13cd4a65d0624a5b87827c6e0709a882613f0",
    "files": [
        {
            "sha": "18704b846df24e191e650f72d962c3b5c5887e77",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -1097,6 +1097,10 @@\n         title: Qwen2Audio\n       - local: model_doc/qwen2_vl\n         title: Qwen2VL\n+      - local: model_doc/sam2\n+        title: SAM2\n+      - local: model_doc/sam2_video\n+        title: SAM2 Video\n       - local: model_doc/sam\n         title: Segment Anything\n       - local: model_doc/sam_hq"
        },
        {
            "sha": "a9273ff730b478535528ebedd6c119cb1fe60b6a",
            "filename": "docs/source/en/model_doc/sam2.md",
            "status": "added",
            "additions": 349,
            "deletions": 0,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,349 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+# SAM2\n+\n+## Overview\n+\n+SAM2 (Segment Anything Model 2) was proposed in [Segment Anything in Images and Videos](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr DollÃ¡r, Christoph Feichtenhofer.\n+\n+The model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.\n+\n+![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif)\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.*\n+\n+Tips:\n+\n+- Batch & Video Support: SAM2 natively supports batch processing and seamless video segmentation, while original SAM is designed for static images and simpler one-image-at-a-time workflows.\n+- Accuracy & Generalization: SAM2 shows improved segmentation quality, robustness, and zero-shot generalization to new domains compared to the original SAM, especially with mixed prompts.\n+\n+This model was contributed by [sangbumchoi](https://github.com/SangbumChoi) and [yonigozlan](https://huggingface.co/yonigozlan).\n+The original code can be found [here](https://github.com/facebookresearch/sam2/tree/main).\n+\n+## Usage example\n+\n+### Automatic Mask Generation with Pipeline\n+\n+SAM2 can be used for automatic mask generation to segment all objects in an image using the `mask-generation` pipeline:\n+\n+```python\n+>>> from transformers import pipeline\n+\n+>>> generator = pipeline(\"mask-generation\", model=\"yonigozlan/sam2.1_hiera_large_hf\", device=0)\n+>>> image_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+>>> outputs = generator(image_url, points_per_batch=64)\n+\n+>>> len(outputs[\"masks\"])  # Number of masks generated\n+39\n+```\n+\n+### Basic Image Segmentation\n+\n+#### Single Point Click\n+\n+You can segment objects by providing a single point click on the object you want to segment:\n+\n+```python\n+>>> from transformers import Sam2Processor, Sam2Model\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+>>> model = Sam2Model.from_pretrained(\"yonigozlan/sam2.1_hiera_large_hf\").to(device)\n+>>> processor = Sam2Processor.from_pretrained(\"yonigozlan/sam2.1_hiera_large_hf\")\n+\n+>>> image_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+>>> raw_image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+\n+>>> input_points = [[[[500, 375]]]]  # Single point click, 4 dimensions (image_dim, object_dim, point_per_object_dim, coordinates)\n+>>> input_labels = [[[1]]]  # 1 for positive click, 0 for negative click, 3 dimensions (image_dim, object_dim, point_label)\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+\n+>>> # The model outputs multiple mask predictions ranked by quality score\n+>>> print(f\"Generated {masks.shape[1]} masks with shape {masks.shape}\")\n+Generated 3 masks with shape torch.Size(1, 3, 1500, 2250)\n+```\n+\n+#### Multiple Points for Refinement\n+\n+You can provide multiple points to refine the segmentation:\n+\n+```python\n+>>> # Add both positive and negative points to refine the mask\n+>>> input_points = [[[[500, 375], [1125, 625]]]]  # Multiple points for refinement\n+>>> input_labels = [[[1, 1]]]  # Both positive clicks\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+```\n+\n+#### Bounding Box Input\n+\n+SAM2 also supports bounding box inputs for segmentation:\n+\n+```python\n+>>> # Define bounding box as [x_min, y_min, x_max, y_max]\n+>>> input_boxes = [[[75, 275, 1725, 850]]]\n+\n+>>> inputs = processor(images=raw_image, input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+```\n+\n+#### Multiple Objects Segmentation\n+\n+You can segment multiple objects simultaneously:\n+\n+```python\n+>>> # Define points for two different objects\n+>>> input_points = [[[[500, 375]], [[650, 750]]]]  # Points for two objects in same image\n+>>> input_labels = [[[1], [1]]]  # Positive clicks for both objects\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> # Each object gets its own mask\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+>>> print(f\"Generated masks for {masks.shape[0]} objects\")\n+Generated masks for 2 objects\n+```\n+\n+### Batch Inference\n+\n+#### Batched Images\n+\n+Process multiple images simultaneously for improved efficiency:\n+\n+```python\n+>>> from transformers import Sam2Processor, Sam2Model\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+>>> model = Sam2Model.from_pretrained(\"yonigozlan/sam2.1_hiera_large_hf\").to(device)\n+>>> processor = Sam2Processor.from_pretrained(\"yonigozlan/sam2.1_hiera_large_hf\")\n+\n+>>> # Load multiple images\n+>>> image_urls = [\n+...     \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\",\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+... ]\n+>>> raw_images = [Image.open(requests.get(url, stream=True).raw).convert(\"RGB\") for url in image_urls]\n+\n+>>> # Single point per image\n+>>> input_points = [[[[500, 375]]], [[[770, 200]]]]  # One point for each image\n+>>> input_labels = [[[1]], [[1]]]  # Positive clicks for both images\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> # Post-process masks for each image\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+>>> print(f\"Processed {len(all_masks)} images, each with {all_masks[0].shape[0]} objects\")\n+Processed 2 images, each with 1 objects\n+```\n+\n+#### Batched Objects per Image\n+\n+Segment multiple objects within each image using batch inference:\n+\n+```python\n+>>> # Multiple objects per image - different numbers of objects per image\n+>>> input_points = [\n+...     [[[500, 375]], [[650, 750]]],  # Truck image: 2 objects\n+...     [[[770, 200]]]  # Dog image: 1 object\n+... ]\n+>>> input_labels = [\n+...     [[1], [1]],  # Truck image: positive clicks for both objects\n+...     [[1]]  # Dog image: positive click for the object\n+... ]\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+```\n+\n+#### Batched Images with Batched Objects and Multiple Points\n+\n+Handle complex batch scenarios with multiple points per object:\n+\n+```python\n+>>> # Add groceries image for more complex example\n+>>> groceries_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+>>> groceries_image = Image.open(requests.get(groceries_url, stream=True).raw).convert(\"RGB\")\n+>>> raw_images = [raw_images[0], groceries_image]  # Use truck and groceries images\n+\n+>>> # Complex batching: multiple images, multiple objects, multiple points per object\n+>>> input_points = [\n+...     [[[500, 375]], [[650, 750]]],  # Truck image: 2 objects with 1 point each\n+...     [[[400, 300]], [[630, 300], [550, 300]]]  # Groceries image: obj1 has 1 point, obj2 has 2 points\n+... ]\n+>>> input_labels = [\n+...     [[1], [1]],  # Truck image: positive clicks\n+...     [[1], [1, 1]]  # Groceries image: positive clicks for refinement\n+... ]\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+```\n+\n+#### Batched Bounding Boxes\n+\n+Process multiple images with bounding box inputs:\n+\n+```python\n+>>> # Multiple bounding boxes per image (using truck and groceries images)\n+>>> input_boxes = [\n+...     [[75, 275, 1725, 850], [425, 600, 700, 875], [1375, 550, 1650, 800], [1240, 675, 1400, 750]],  # Truck image: 4 boxes\n+...     [[450, 170, 520, 350], [350, 190, 450, 350], [500, 170, 580, 350], [580, 170, 640, 350]]  # Groceries image: 4 boxes\n+... ]\n+\n+>>> # Update images for this example\n+>>> raw_images = [raw_images[0], groceries_image]  # truck and groceries\n+\n+>>> inputs = processor(images=raw_images, input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+>>> print(f\"Processed {len(input_boxes)} images with {len(input_boxes[0])} and {len(input_boxes[1])} boxes respectively\")\n+Processed 2 images with 4 and 4 boxes respectively\n+```\n+\n+### Using Previous Masks as Input\n+\n+SAM2 can use masks from previous predictions as input to refine segmentation:\n+\n+```python\n+>>> # Get initial segmentation\n+>>> input_points = [[[[500, 375]]]]\n+>>> input_labels = [[[1]]]\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Use the best mask as input for refinement\n+>>> mask_input = outputs.pred_masks[:, :, torch.argmax(outputs.iou_scores.squeeze())]\n+\n+>>> # Add additional points with the mask input\n+>>> new_input_points = [[[[500, 375], [450, 300]]]]\n+>>> new_input_labels = [[[1, 1]]]\n+>>> inputs = processor(\n+...     input_points=new_input_points,\n+...     input_labels=new_input_labels,\n+...     original_sizes=inputs[\"original_sizes\"],\n+...     return_tensors=\"pt\",\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     refined_outputs = model(\n+...         **inputs,\n+...         input_masks=mask_input,\n+...         image_embeddings=outputs.image_embeddings,\n+...         multimask_output=False,\n+...     )\n+```\n+\n+<!-- TODO replace with sam2 resources -->\n+<!-- ## Resources -->\n+<!-- A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM.\n+\n+- [Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything_2.ipynb) for using the model. -->\n+\n+## Sam2Config\n+\n+[[autodoc]] Sam2Config\n+\n+## Sam2HieraDetConfig\n+\n+[[autodoc]] Sam2HieraDetConfig\n+\n+## Sam2VisionConfig\n+\n+[[autodoc]] Sam2VisionConfig\n+\n+## Sam2MaskDecoderConfig\n+\n+[[autodoc]] Sam2MaskDecoderConfig\n+\n+## Sam2PromptEncoderConfig\n+\n+[[autodoc]] Sam2PromptEncoderConfig\n+\n+## Sam2Processor\n+\n+[[autodoc]] Sam2Processor\n+    - __call__\n+    - post_process_masks\n+\n+## Sam2ImageProcessorFast\n+\n+[[autodoc]] Sam2ImageProcessorFast\n+\n+## Sam2HieraDetModel\n+\n+[[autodoc]] Sam2HieraDetModel\n+    - forward\n+\n+## Sam2VisionModel\n+\n+[[autodoc]] Sam2VisionModel\n+    - forward\n+\n+## Sam2Model\n+\n+[[autodoc]] Sam2Model\n+    - forward"
        },
        {
            "sha": "e1bc201b976402b1d7dfbda4418b81cf5cdadbf8",
            "filename": "docs/source/en/model_doc/sam2_video.md",
            "status": "added",
            "additions": 314,
            "deletions": 0,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,314 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+# SAM2 Video\n+\n+## Overview\n+\n+SAM2 (Segment Anything Model 2) was proposed in [Segment Anything in Images and Videos](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr DollÃ¡r, Christoph Feichtenhofer.\n+\n+The model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.\n+\n+![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif)\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.*\n+\n+Tips:\n+\n+- Batch & Video Support: SAM2 natively supports batch processing and seamless video segmentation, while original SAM is designed for static images and simpler one-image-at-a-time workflows.\n+- Accuracy & Generalization: SAM2 shows improved segmentation quality, robustness, and zero-shot generalization to new domains compared to the original SAM, especially with mixed prompts.\n+\n+This model was contributed by [sangbumchoi](https://github.com/SangbumChoi) and [yonigozlan](https://huggingface.co/yonigozlan).\n+The original code can be found [here](https://github.com/facebookresearch/sam2/tree/main).\n+\n+## Usage example\n+\n+### Video Segmentation and Tracking\n+\n+SAM2's key strength is its ability to track objects across video frames. Here's how to use it for video segmentation:\n+\n+#### Basic Video Tracking\n+\n+```python\n+>>> from transformers import Sam2VideoModel, Sam2VideoProcessor\n+>>> import torch\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = Sam2VideoModel.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\").to(device, dtype=torch.bfloat16)\n+>>> processor = Sam2VideoProcessor.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\")\n+\n+>>> # Load video frames (example assumes you have a list of PIL Images)\n+>>> # video_frames = [Image.open(f\"frame_{i:05d}.jpg\") for i in range(num_frames)]\n+\n+>>> # For this example, we'll use the video loading utility\n+>>> from transformers.video_utils import load_video\n+>>> video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+>>> video_frames, _ = load_video(video_url)\n+\n+>>> # Initialize video inference session\n+>>> inference_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     torch_dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add click on first frame to select object\n+>>> ann_frame_idx = 0\n+>>> ann_obj_id = 1\n+>>> points = [[[[210, 350]]]]\n+>>> labels = [[[1]]]\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=ann_obj_id,\n+...     input_points=points,\n+...     input_labels=labels,\n+... )\n+\n+>>> # Segment the object on the first frame\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+>>> video_res_masks = processor.post_process_masks(\n+...     [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+... )[0]\n+>>> print(f\"Segmentation shape: {video_res_masks.shape}\")\n+Segmentation shape: torch.Size([1, 1, 480, 854])\n+\n+>>> # Propagate through the entire video\n+>>> video_segments = {}\n+>>> for sam2_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam2_video_output.frame_idx] = video_res_masks\n+\n+>>> print(f\"Tracked object through {len(video_segments)} frames\")\n+Tracked object through 180 frames\n+```\n+\n+#### Multi-Object Video Tracking\n+\n+Track multiple objects simultaneously across video frames:\n+\n+```python\n+>>> # Reset for new tracking session\n+>>> inference_session.reset_inference_session()\n+\n+>>> # Add multiple objects on the first frame\n+>>> ann_frame_idx = 0\n+>>> obj_ids = [2, 3]\n+>>> input_points = [[[[200, 300]], [[400, 150]]]]  # Points for two objects (batched)\n+>>> input_labels = [[[1], [1]]]\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=obj_ids,\n+...     input_points=input_points,\n+...     input_labels=input_labels,\n+... )\n+\n+>>> # Get masks for both objects on first frame\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+\n+>>> # Propagate both objects through video\n+>>> video_segments = {}\n+>>> for sam2_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam2_video_output.frame_idx] = {\n+...         obj_id: video_res_masks[i]\n+...         for i, obj_id in enumerate(inference_session.obj_ids)\n+...     }\n+\n+>>> print(f\"Tracked {len(inference_session.obj_ids)} objects through {len(video_segments)} frames\")\n+Tracked 2 objects through 180 frames\n+```\n+\n+#### Refining Video Segmentation\n+\n+You can add additional clicks on any frame to refine the tracking:\n+\n+```python\n+>>> # Add refinement click on a later frame\n+>>> refine_frame_idx = 50\n+>>> ann_obj_id = 2  # Refining first object\n+>>> points = [[[[220, 280]]]]  # Additional point\n+>>> labels = [[[1]]]  # Positive click\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=refine_frame_idx,\n+...     obj_ids=ann_obj_id,\n+...     input_points=points,\n+...     input_labels=labels,\n+... )\n+\n+>>> # Re-propagate with the additional information\n+>>> video_segments = {}\n+>>> for sam2_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam2_video_output.frame_idx] = video_res_masks\n+```\n+\n+### Streaming Video Inference\n+\n+For real-time applications, SAM2 supports processing video frames as they arrive:\n+\n+```python\n+>>> # Initialize session for streaming\n+>>> inference_session = processor.init_video_session(\n+...     inference_device=device,\n+...     torch_dtype=torch.bfloat16,\n+... )\n+\n+>>> # Process frames one by one\n+>>> for frame_idx, frame in enumerate(video_frames[:10]):  # Process first 10 frames\n+...     inputs = processor(images=frame, device=device, return_tensors=\"pt\")\n+...\n+...     if frame_idx == 0:\n+...         # Add point input on first frame\n+...         processor.add_inputs_to_inference_session(\n+...             inference_session=inference_session,\n+...             frame_idx=0,\n+...             obj_ids=1,\n+...             input_points=[[[[210, 350], [250, 220]]]],\n+...             input_labels=[[[1, 1]]],\n+...             original_size=inputs.original_sizes[0], # need to be provided when using streaming video inference\n+...         )\n+...\n+...     # Process current frame\n+...     sam2_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[0])\n+...\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam2_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=False\n+...     )[0]\n+...     print(f\"Frame {frame_idx}: mask shape {video_res_masks.shape}\")\n+```\n+\n+#### Video Batch Processing for Multiple Objects\n+\n+Track multiple objects simultaneously in video by adding them all at once:\n+\n+```python\n+>>> # Initialize video session\n+>>> inference_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     torch_dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add multiple objects on the first frame using batch processing\n+>>> ann_frame_idx = 0\n+>>> obj_ids = [2, 3]  # Track two different objects\n+>>> input_points = [\n+...     [[[200, 300], [230, 250], [275, 175]], [[400, 150]]]\n+... ]  # Object 2: 3 points (2 positive, 1 negative); Object 3: 1 point\n+>>> input_labels = [\n+...     [[1, 1, 0], [1]]\n+... ]  # Object 2: positive, positive, negative; Object 3: positive\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=obj_ids,\n+...     input_points=input_points,\n+...     input_labels=input_labels,\n+... )\n+\n+>>> # Get masks for all objects on the first frame\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+>>> video_res_masks = processor.post_process_masks(\n+...     [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+... )[0]\n+>>> print(f\"Generated masks for {video_res_masks.shape[0]} objects\")\n+Generated masks for 2 objects\n+\n+>>> # Propagate all objects through the video\n+>>> video_segments = {}\n+>>> for sam2_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam2_video_output.frame_idx] = {\n+...         obj_id: video_res_masks[i]\n+...         for i, obj_id in enumerate(inference_session.obj_ids)\n+...     }\n+\n+>>> print(f\"Tracked {len(inference_session.obj_ids)} objects through {len(video_segments)} frames\")\n+Tracked 2 objects through 180 frames\n+```\n+\n+<!-- TODO replace with sam2 resources -->\n+<!-- ## Resources -->\n+<!-- A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM.\n+\n+- [Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything_2.ipynb) for using the model. -->\n+\n+## Sam2VideoConfig\n+\n+[[autodoc]] Sam2VideoConfig\n+\n+## Sam2VideoMaskDecoderConfig\n+\n+[[autodoc]] Sam2VideoMaskDecoderConfig\n+\n+## Sam2VideoPromptEncoderConfig\n+\n+[[autodoc]] Sam2VideoPromptEncoderConfig\n+\n+## Sam2VideoProcessor\n+\n+[[autodoc]] Sam2VideoProcessor\n+    - __call__\n+    - post_process_masks\n+    - init_video_session\n+    - add_inputs_to_inference_session\n+\n+## Sam2VideoVideoProcessor\n+\n+[[autodoc]] Sam2VideoVideoProcessor\n+\n+## Sam2VideoInferenceSession\n+\n+[[autodoc]] Sam2VideoInferenceSession\n+\n+## Sam2VideoModel\n+\n+[[autodoc]] Sam2VideoModel\n+    - forward\n+    - propagate_in_video_iterator"
        },
        {
            "sha": "bded5ca6e0f360754ed6519df7e0c50f09e1cae9",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -211,10 +211,7 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     valid_kwargs = DefaultFastImageProcessorKwargs\n     unused_kwargs = None\n \n-    def __init__(\n-        self,\n-        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n-    ) -> None:\n+    def __init__(self, **kwargs: Unpack[DefaultFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n         kwargs = self.filter_out_unused_kwargs(kwargs)\n         size = kwargs.pop(\"size\", self.size)"
        },
        {
            "sha": "662edfe269a89afa59de8cd8af9f710604a2f614",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -284,6 +284,8 @@\n     from .rt_detr_v2 import *\n     from .rwkv import *\n     from .sam import *\n+    from .sam2 import *\n+    from .sam2_video import *\n     from .sam_hq import *\n     from .seamless_m4t import *\n     from .seamless_m4t_v2 import *"
        },
        {
            "sha": "01c85be8642e866ae7ff76ff7327d8fd5a850986",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -331,6 +331,10 @@\n         (\"rt_detr_v2\", \"RTDetrV2Config\"),\n         (\"rwkv\", \"RwkvConfig\"),\n         (\"sam\", \"SamConfig\"),\n+        (\"sam2\", \"Sam2Config\"),\n+        (\"sam2_hiera_det_model\", \"Sam2HieraDetConfig\"),\n+        (\"sam2_video\", \"Sam2VideoConfig\"),\n+        (\"sam2_vision_model\", \"Sam2VisionConfig\"),\n         (\"sam_hq\", \"SamHQConfig\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionConfig\"),\n         (\"sam_vision_model\", \"SamVisionConfig\"),\n@@ -752,6 +756,10 @@\n         (\"rt_detr_v2\", \"RT-DETRv2\"),\n         (\"rwkv\", \"RWKV\"),\n         (\"sam\", \"SAM\"),\n+        (\"sam2\", \"SAM2\"),\n+        (\"sam2_hiera_det_model\", \"Sam2HieraDetModel\"),\n+        (\"sam2_video\", \"Sam2VideoModel\"),\n+        (\"sam2_vision_model\", \"Sam2VisionModel\"),\n         (\"sam_hq\", \"SAM-HQ\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n@@ -916,6 +924,8 @@\n         (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n         (\"qwen2_vl_text\", \"qwen2_vl\"),\n         (\"sam_vision_model\", \"sam\"),\n+        (\"sam2_vision_model\", \"sam2\"),\n+        (\"sam2_hiera_det_model\", \"sam2\"),\n         (\"sam_hq_vision_model\", \"sam_hq\"),\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),"
        },
        {
            "sha": "9a983d68f83f769be1565ceaf54ada5954a173d1",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -156,6 +156,7 @@\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n             (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n+            (\"sam2\", (None, \"Sam2ImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"seggpt\", (\"SegGptImageProcessor\", None)),"
        },
        {
            "sha": "b9fe86a45b49e2fe65f3a1c7849ec1a4bdc57241",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -322,6 +322,10 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"rt_detr_v2\", \"RTDetrV2Model\"),\n         (\"rwkv\", \"RwkvModel\"),\n         (\"sam\", \"SamModel\"),\n+        (\"sam2\", \"Sam2Model\"),\n+        (\"sam2_hiera_det_model\", \"Sam2HieraDetModel\"),\n+        (\"sam2_video\", \"Sam2VideoModel\"),\n+        (\"sam2_vision_model\", \"Sam2VisionModel\"),\n         (\"sam_hq\", \"SamHQModel\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n@@ -1635,6 +1639,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(\n     [\n         (\"sam\", \"SamModel\"),\n+        (\"sam2\", \"Sam2Model\"),\n+        (\"sam2_video\", \"Sam2Model\"),\n         (\"sam_hq\", \"SamHQModel\"),\n     ]\n )"
        },
        {
            "sha": "dc49532a864a43bb6336dd10f190f3d45af06a63",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -117,6 +117,7 @@\n         (\"qwen2_audio\", \"Qwen2AudioProcessor\"),\n         (\"qwen2_vl\", \"Qwen2VLProcessor\"),\n         (\"sam\", \"SamProcessor\"),\n+        (\"sam2\", \"Sam2Processor\"),\n         (\"sam_hq\", \"SamHQProcessor\"),\n         (\"seamless_m4t\", \"SeamlessM4TProcessor\"),\n         (\"sew\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "5d9b58b51a91a72b43cdd90c31e8a775502aece2",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -55,6 +55,7 @@\n             (\"qwen2_5_omni\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_5_vl\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_vl\", \"Qwen2VLVideoProcessor\"),\n+            (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),"
        },
        {
            "sha": "883f6d818e9554bc1a5dcbfb027d4d6d813b418d",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -49,14 +49,14 @@\n \n if is_torch_available():\n     import torch\n-    from torch.nn import functional as F_t\n+    from torch.nn import functional as F\n \n-if is_torchvision_available() and is_torchvision_v2_available():\n+if is_torchvision_v2_available():\n     from torchvision.ops.boxes import batched_nms\n-    from torchvision.transforms.v2 import functional as F\n+    from torchvision.transforms.v2 import functional as F_t\n elif is_torchvision_available():\n     from torchvision.ops.boxes import batched_nms\n-    from torchvision.transforms import functional as F\n+    from torchvision.transforms import functional as F_t\n \n \n class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n@@ -108,7 +108,7 @@ def pad_image(self, images: \"torch.Tensor\", pad_size: SizeDict):\n         pad_width = output_width - input_width\n         pad_height = output_height - input_height\n         padding = (0, 0, pad_width, pad_height)\n-        return F.pad(images, padding)\n+        return F_t.pad(images, padding)\n \n     def _get_preprocess_shape(self, old_shape: tuple[int, int], longest_edge: int):\n         \"\"\"\n@@ -122,7 +122,7 @@ def _get_preprocess_shape(self, old_shape: tuple[int, int], longest_edge: int):\n         return (newh, neww)\n \n     def resize(\n-        self, image: \"torch.Tensor\", size: SizeDict, interpolation: Optional[\"F.InterpolationMode\"], **kwargs\n+        self, image: \"torch.Tensor\", size: SizeDict, interpolation: Optional[\"F_t.InterpolationMode\"], **kwargs\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Resize an image to `(size[\"height\"], size[\"width\"])`.\n@@ -135,7 +135,7 @@ def resize(\n                 edge of the image will be resized to the specified size, while the other edge will be resized to\n                 maintain the aspect ratio.\n             interpolation:\n-                `F.InterpolationMode` filter to use when resizing the image e.g. `F.InterpolationMode.BICUBIC`.\n+                `F_t.InterpolationMode` filter to use when resizing the image e.g. `F_t.InterpolationMode.BICUBIC`.\n \n         Returns:\n             `torch.Tensor`: The resized image.\n@@ -262,7 +262,7 @@ def _preprocess(\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n         size: SizeDict,\n-        interpolation: Optional[\"F.InterpolationMode\"],\n+        interpolation: Optional[\"F_t.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n@@ -477,9 +477,9 @@ def post_process_masks(\n                 masks[i] = torch.from_numpy(masks[i])\n             elif not isinstance(masks[i], torch.Tensor):\n                 raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n-            interpolated_mask = F_t.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n+            interpolated_mask = F.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n             interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n-            interpolated_mask = F_t.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n+            interpolated_mask = F.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n             if binarize:\n                 interpolated_mask = interpolated_mask > mask_threshold\n             output_masks.append(interpolated_mask)"
        },
        {
            "sha": "23e617c9bbf77d31e1f7c8dd1a52c36ef4df64e1",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -402,7 +402,7 @@ def forward(\n                 attention_similarity=attention_similarity,\n                 **kwargs,\n             )\n-        # Apply the final attenion layer from the points to the image\n+        # Apply the final attention layer from the points to the image\n         query = queries + point_embeddings\n         key = keys + image_positional_embeddings\n \n@@ -1110,6 +1110,7 @@ def forward(\n @auto_docstring(\n     custom_intro=\"\"\"\n     Segment Anything Model (SAM) for generating segmentation masks, given an input image and\n+    input points and labels, boxes, or masks.\n     \"\"\"\n )\n class SamModel(SamPreTrainedModel):\n@@ -1118,7 +1119,7 @@ class SamModel(SamPreTrainedModel):\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(SamTwoWayAttentionBlock, index=2)}\n \n-    def __init__(self, config):\n+    def __init__(self, config: SamConfig):\n         super().__init__(config)\n         self.shared_image_embedding = SamPositionalEmbedding(config.vision_config)\n "
        },
        {
            "sha": "38e6621b063af7a706186a1bf810e3f4709dc1b8",
            "filename": "src/transformers/models/sam2/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2F__init__.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam2 import *\n+    from .image_processing_sam2_fast import *\n+    from .modeling_sam2 import *\n+    from .processing_sam2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "39fbc9dfc2f53aa8b614d23e79a038fa50289e26",
            "filename": "src/transformers/models/sam2/configuration_sam2.py",
            "status": "added",
            "additions": 457,
            "deletions": 0,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,457 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"SAM2 model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Sam2HieraDetConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2HieraDetModel`]. It is used to instantiate\n+    a HieraDet model as defined in the original sam2 repo according to the specified arguments, defining the model architecture.\n+    Instantiating a configuration defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny\n+    [facebook/sam2.1-hiera-tiny](https://huggingface.co/facebook/sam2.1-hiera-tiny) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 96):\n+            The hidden dimension of the image encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 1):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of channels in the image.\n+        image_size (`list[int]`, *optional*, defaults to `[1024, 1024]`):\n+            The size of the image.\n+        patch_kernel_size (`list[int]`, *optional*, defaults to `[7, 7]`):\n+            The kernel size of the patch.\n+        patch_stride (`list[int]`, *optional*, defaults to `[4, 4]`):\n+            The stride of the patch.\n+        patch_padding (`list[int]`, *optional*, defaults to `[3, 3]`):\n+            The padding of the patch.\n+        query_stride (`list[int]`, *optional*, defaults to `[2, 2]`):\n+            The downsample stride between stages.\n+        window_positional_embedding_background_size (`list[int]`, *optional*, defaults to `[7, 7]`):\n+            The window size per stage when not using global attention.\n+        num_query_pool_stages (`int`, *optional*, defaults to 3):\n+            The number of query pool stages.\n+        blocks_per_stage (`list[int]`, *optional*, defaults to `[1, 2, 7, 2]`):\n+            The number of blocks per stage.\n+        embed_dim_per_stage (`list[int]`, *optional*, defaults to `[96, 192, 384, 768]`):\n+            The embedding dimension per stage.\n+        num_attention_heads_per_stage (`list[int]`, *optional*, defaults to `[1, 2, 4, 8]`):\n+            The number of attention heads per stage.\n+        window_size_per_stage (`list[int]`, *optional*, defaults to `[8, 4, 14, 7]`):\n+            The window size per stage.\n+        global_attention_blocks (`list[int]`, *optional*, defaults to `[5, 7, 9]`):\n+            The blocks where global attention is used.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            The ratio of the MLP hidden dimension to the embedding dimension.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the neck.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon for the layer normalization.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    \"\"\"\n+\n+    base_config_key = \"backbone_config\"\n+    model_type = \"sam2_hiera_det_model\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=96,\n+        num_attention_heads=1,\n+        num_channels=3,\n+        image_size=None,\n+        patch_kernel_size=None,\n+        patch_stride=None,\n+        patch_padding=None,\n+        query_stride=None,\n+        window_positional_embedding_background_size=None,\n+        num_query_pool_stages=3,\n+        blocks_per_stage=None,\n+        embed_dim_per_stage=None,\n+        num_attention_heads_per_stage=None,\n+        window_size_per_stage=None,\n+        global_attention_blocks=None,\n+        mlp_ratio=4.0,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        image_size = image_size if image_size is not None else [1024, 1024]\n+        patch_kernel_size = patch_kernel_size if patch_kernel_size is not None else [7, 7]\n+        patch_stride = patch_stride if patch_stride is not None else [4, 4]\n+        patch_padding = patch_padding if patch_padding is not None else [3, 3]\n+        query_stride = query_stride if query_stride is not None else [2, 2]\n+        window_positional_embedding_background_size = (\n+            window_positional_embedding_background_size\n+            if window_positional_embedding_background_size is not None\n+            else [7, 7]\n+        )\n+        blocks_per_stage = blocks_per_stage if blocks_per_stage is not None else [1, 2, 7, 2]\n+        embed_dim_per_stage = embed_dim_per_stage if embed_dim_per_stage is not None else [96, 192, 384, 768]\n+        num_attention_heads_per_stage = (\n+            num_attention_heads_per_stage if num_attention_heads_per_stage is not None else [1, 2, 4, 8]\n+        )\n+        window_size_per_stage = window_size_per_stage if window_size_per_stage is not None else [8, 4, 14, 7]\n+        global_attention_blocks = global_attention_blocks if global_attention_blocks is not None else [5, 7, 9]\n+\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_kernel_size = patch_kernel_size\n+        self.patch_stride = patch_stride\n+        self.patch_padding = patch_padding\n+        self.query_stride = query_stride\n+        self.window_positional_embedding_background_size = window_positional_embedding_background_size\n+        self.num_query_pool_stages = num_query_pool_stages\n+        self.blocks_per_stage = blocks_per_stage\n+        self.embed_dim_per_stage = embed_dim_per_stage\n+        self.num_attention_heads_per_stage = num_attention_heads_per_stage\n+        self.window_size_per_stage = window_size_per_stage\n+        self.global_attention_blocks = global_attention_blocks\n+        self.mlp_ratio = mlp_ratio\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2VisionModel`]. It is used to instantiate a SAM\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny\n+    [facebook/sam2.1-hiera-tiny](https://huggingface.co/facebook/sam2.1-hiera-tiny) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        backbone_config (`Union[dict, \"PretrainedConfig\"]`, *optional*):\n+            Configuration for the vision backbone. This is used to instantiate the backbone using\n+            `AutoModel.from_config`.\n+        backbone_channel_list (`List[int]`, *optional*, defaults to `[768, 384, 192, 96]`):\n+            The list of channel dimensions for the backbone.\n+        backbone_feature_sizes (`List[List[int]]`, *optional*, defaults to `[[256, 256], [128, 128], [64, 64]]`):\n+            The spatial sizes of the feature maps from the backbone.\n+        fpn_hidden_size (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the FPN.\n+        fpn_kernel_size (`int`, *optional*, defaults to 1):\n+            The kernel size for the convolutions in the neck.\n+        fpn_stride (`int`, *optional*, defaults to 1):\n+            The stride for the convolutions in the neck.\n+        fpn_padding (`int`, *optional*, defaults to 0):\n+            The padding for the convolutions in the neck.\n+        fpn_top_down_levels (`List[int]`, *optional*, defaults to `[2, 3]`):\n+            The levels for the top-down FPN connections.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of feature levels from the FPN to use.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the neck.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon for the layer normalization.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    \"\"\"\n+\n+    base_config_key = \"vision_config\"\n+    model_type = \"sam2_vision_model\"\n+    sub_configs = {\n+        \"backbone_config\": AutoConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        backbone_config=None,\n+        backbone_channel_list=None,\n+        backbone_feature_sizes=None,\n+        fpn_hidden_size=256,\n+        fpn_kernel_size=1,\n+        fpn_stride=1,\n+        fpn_padding=0,\n+        fpn_top_down_levels=None,\n+        num_feature_levels=3,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        backbone_channel_list = [768, 384, 192, 96] if backbone_channel_list is None else backbone_channel_list\n+        backbone_feature_sizes = (\n+            [[256, 256], [128, 128], [64, 64]] if backbone_feature_sizes is None else backbone_feature_sizes\n+        )\n+        fpn_top_down_levels = [2, 3] if fpn_top_down_levels is None else fpn_top_down_levels\n+\n+        if isinstance(backbone_config, dict):\n+            backbone_config[\"model_type\"] = backbone_config.get(\"model_type\", \"sam2_hiera_det_model\")\n+            backbone_config = CONFIG_MAPPING[backbone_config[\"model_type\"]](**backbone_config)\n+        elif isinstance(backbone_config, Sam2HieraDetConfig):\n+            backbone_config = backbone_config\n+        elif backbone_config is None:\n+            backbone_config = Sam2HieraDetConfig()\n+\n+        self.backbone_config = backbone_config\n+\n+        # Neck\n+        self.backbone_channel_list = backbone_channel_list\n+        self.backbone_feature_sizes = backbone_feature_sizes\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.fpn_kernel_size = fpn_kernel_size\n+        self.fpn_stride = fpn_stride\n+        self.fpn_padding = fpn_padding\n+        self.fpn_top_down_levels = fpn_top_down_levels\n+        self.num_feature_levels = num_feature_levels\n+\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam2PromptEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2PromptEncoder`]. The [`Sam2PromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1024,\n+        patch_size=16,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.scale = scale\n+\n+\n+class Sam2MaskDecoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2MaskDecoder`]. It is used to instantiate a SAM2\n+    memory encoder according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the SAM2 mask decoder.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            The dimension of the MLP in the two-way transformer.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            The number of hidden layers in the two-way transformer.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads in the two-way transformer.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsample rate for the attention layers.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of multimask outputs.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The depth of the IoU head.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the IoU head.\n+        dynamic_multimask_via_stability (`bool`, *optional*, defaults to `True`):\n+            Whether to use dynamic multimask via stability.\n+        dynamic_multimask_stability_delta (`float`, *optional*, defaults to 0.05):\n+            The stability delta for the dynamic multimask.\n+        dynamic_multimask_stability_thresh (`float`, *optional*, defaults to 0.98):\n+            The stability threshold for the dynamic multimask.\n+\n+    \"\"\"\n+\n+    base_config_key = \"mask_decoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        hidden_act=\"gelu\",\n+        mlp_dim=2048,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=256,\n+        dynamic_multimask_via_stability=True,\n+        dynamic_multimask_stability_delta=0.05,\n+        dynamic_multimask_stability_thresh=0.98,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.hidden_act = hidden_act\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.dynamic_multimask_via_stability = dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = dynamic_multimask_stability_thresh\n+\n+        # TwoWayTransformer configuration\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_dim = mlp_dim\n+        self.attention_downsample_rate = attention_downsample_rate\n+\n+\n+class Sam2Config(PretrainedConfig):\n+    r\"\"\"\n+    [`Sam2Config`] is the configuration class to store the configuration of a [`Sam2Model`]. It is used to instantiate a\n+    SAM2 model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n+    configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n+    [facebook/sam2.1-hiera-tiny](https://huggingface.co/facebook/sam2.1-hiera-tiny) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `Sam2VisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2VisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `Sam2PromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2PromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `Sam2MaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2MaskDecoderConfig`].\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation for parameter initialization.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     Sam2VisionConfig,\n+    ...     Sam2PromptEncoderConfig,\n+    ...     Sam2MaskDecoderConfig,\n+    ...     Sam2Model,\n+    ... )\n+\n+    >>> # Initializing a Sam2Config with `\"facebook/sam2.1_hiera_tiny\"` style configuration\n+    >>> configuration = Sam2config()\n+\n+    >>> # Initializing a Sam2Model (with random weights) from the `\"facebook/sam2.1_hiera_tiny\"` style configuration\n+    >>> model = Sam2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Sam2Config from a Sam2VisionConfig, Sam2PromptEncoderConfig, and Sam2MaskDecoderConfig\n+\n+    >>> # Initializing SAM2 vision encoder, memory attention, and memory encoder configurations\n+    >>> vision_config = Sam2VisionConfig()\n+    >>> prompt_encoder_config = Sam2PromptEncoderConfig()\n+    >>> mask_decoder_config = Sam2MaskDecoderConfig()\n+\n+    >>> config = Sam2Config(vision_config, prompt_encoder_config, mask_decoder_config)\n+    ```\"\"\"\n+\n+    model_type = \"sam2\"\n+    sub_configs = {\n+        \"vision_config\": AutoConfig,\n+        \"prompt_encoder_config\": Sam2PromptEncoderConfig,\n+        \"mask_decoder_config\": Sam2MaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        vision_config = vision_config if vision_config is not None else {}\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam2_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif isinstance(vision_config, PretrainedConfig):\n+            vision_config = vision_config\n+        if isinstance(prompt_encoder_config, Sam2PromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam2MaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam2PromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam2MaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+\n+\n+__all__ = [\n+    \"Sam2Config\",\n+    \"Sam2HieraDetConfig\",\n+    \"Sam2VisionConfig\",\n+    \"Sam2PromptEncoderConfig\",\n+    \"Sam2MaskDecoderConfig\",\n+]"
        },
        {
            "sha": "382c8bfc0bafc82217ff334e9851a4596df47028",
            "filename": "src/transformers/models/sam2/convert_sam2_to_hf.py",
            "status": "added",
            "additions": 324,
            "deletions": 0,
            "changes": 324,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fconvert_sam2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fconvert_sam2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fconvert_sam2_to_hf.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,324 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Convert SAM checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/segment-anything-2.\n+\"\"\"\n+\n+import argparse\n+import re\n+\n+import numpy as np\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+\n+from transformers import (\n+    Sam2Config,\n+    Sam2HieraDetConfig,\n+    Sam2ImageProcessorFast,\n+    Sam2MaskDecoderConfig,\n+    Sam2Model,\n+    Sam2Processor,\n+    Sam2PromptEncoderConfig,\n+    Sam2VisionConfig,\n+)\n+\n+\n+def get_config(model_name):\n+    if \"hiera_tiny\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig()\n+        vision_config = Sam2VisionConfig(backbone_config=hiera_det_config)\n+    elif \"hiera_small\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(blocks_per_stage=[1, 2, 11, 2], global_attention_blocks=[7, 10, 13])\n+        vision_config = Sam2VisionConfig(backbone_config=hiera_det_config)\n+    elif \"hiera_base_plus\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(\n+            hidden_size=112,\n+            embed_dim_per_stage=[112, 224, 448, 896],\n+            num_attention_heads_per_stage=[2, 4, 8, 16],\n+            blocks_per_stage=[2, 3, 16, 3],\n+            global_attention_blocks=[12, 16, 20],\n+            window_positional_embedding_background_size=(14, 14),\n+        )\n+        vision_config = Sam2VisionConfig(\n+            backbone_config=hiera_det_config,\n+            backbone_channel_list=[896, 448, 224, 112],\n+        )\n+    elif \"hiera_large\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(\n+            hidden_size=144,\n+            embed_dim_per_stage=[144, 288, 576, 1152],\n+            num_attention_heads_per_stage=[2, 4, 8, 16],\n+            blocks_per_stage=[2, 6, 36, 4],\n+            global_attention_blocks=[23, 33, 43],\n+            window_positional_embedding_background_size=(7, 7),\n+            window_size_per_stage=[8, 4, 16, 8],\n+        )\n+        vision_config = Sam2VisionConfig(\n+            backbone_config=hiera_det_config,\n+            backbone_channel_list=[1152, 576, 288, 144],\n+        )\n+    prompt_encoder_config = Sam2PromptEncoderConfig()\n+    mask_decoder_config = Sam2MaskDecoderConfig()\n+\n+    if \"sam2.1\" in model_name:\n+        enable_temporal_pos_encoding_for_object_pointers = True\n+        enable_occlusion_spatial_embedding = True\n+    else:\n+        enable_temporal_pos_encoding_for_object_pointers = False\n+        enable_occlusion_spatial_embedding = False\n+\n+    config = Sam2Config(\n+        vision_config=vision_config,\n+        prompt_encoder_config=prompt_encoder_config,\n+        mask_decoder_config=mask_decoder_config,\n+        enable_temporal_pos_encoding_for_object_pointers=enable_temporal_pos_encoding_for_object_pointers,\n+        enable_occlusion_spatial_embedding=enable_occlusion_spatial_embedding,\n+    )\n+\n+    return config\n+\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"iou_prediction_head.layers.0\": \"iou_prediction_head.proj_in\",\n+    \"iou_prediction_head.layers.1\": \"iou_prediction_head.layers.0\",\n+    \"iou_prediction_head.layers.2\": \"iou_prediction_head.proj_out\",\n+    \"mask_decoder.output_upscaling.0\": \"mask_decoder.upscale_conv1\",\n+    \"mask_decoder.output_upscaling.1\": \"mask_decoder.upscale_layer_norm\",\n+    \"mask_decoder.output_upscaling.3\": \"mask_decoder.upscale_conv2\",\n+    \"mask_downscaling.0\": \"mask_embed.conv1\",\n+    \"mask_downscaling.1\": \"mask_embed.layer_norm1\",\n+    \"mask_downscaling.3\": \"mask_embed.conv2\",\n+    \"mask_downscaling.4\": \"mask_embed.layer_norm2\",\n+    \"mask_downscaling.6\": \"mask_embed.conv3\",\n+    \"dwconv\": \"depthwise_conv\",\n+    \"pwconv\": \"pointwise_conv\",\n+    \"fuser\": \"memory_fuser\",\n+    \"point_embeddings\": \"point_embed\",\n+    \"pe_layer.positional_encoding_gaussian_matrix\": \"shared_embedding.positional_embedding\",\n+    \"obj_ptr_tpos_proj\": \"temporal_positional_encoding_projection_layer\",\n+    \"no_obj_embed_spatial\": \"occlusion_spatial_embedding_parameter\",\n+    \"sam_prompt_encoder\": \"prompt_encoder\",\n+    \"sam_mask_decoder\": \"mask_decoder\",\n+    \"maskmem_tpos_enc\": \"memory_temporal_positional_encoding\",\n+    \"gamma\": \"scale\",\n+    \"image_encoder.neck\": \"vision_encoder.neck\",\n+    \"image_encoder\": \"vision_encoder.backbone\",\n+    \"neck.0\": \"neck.conv1\",\n+    \"neck.1\": \"neck.layer_norm1\",\n+    \"neck.2\": \"neck.conv2\",\n+    \"neck.3\": \"neck.layer_norm2\",\n+    \"pix_feat_proj\": \"feature_projection\",\n+    \"patch_embed.proj\": \"patch_embed.projection\",\n+    \"no_mem_embed\": \"no_memory_embedding\",\n+    \"no_mem_pos_enc\": \"no_memory_positional_encoding\",\n+    \"obj_ptr\": \"object_pointer\",\n+    \".norm\": \".layer_norm\",\n+    \"trunk.\": \"\",\n+    \"out_proj\": \"o_proj\",\n+}\n+\n+\n+def replace_keys(state_dict):\n+    model_state_dict = {}\n+    output_hypernetworks_mlps_pattern = r\".*.output_hypernetworks_mlps.(\\d+).layers.(\\d+).*\"\n+    output_mask_decoder_mlps_pattern = r\"mask_decoder.transformer.layers.(\\d+).mlp.layers.(\\d+).*\"\n+    output_mask_decoder_score_head_pattern = r\"mask_decoder.pred_obj_score_head.layers.(\\d+).*\"\n+    output_vision_encoder_mlps_pattern = r\"vision_encoder.backbone.blocks.(\\d+).mlp.layers.(\\d+).*\"\n+    output_vision_encoder_neck_pattern = r\"vision_encoder.neck.convs.(\\d+).conv\"\n+    output_memory_encoder_projection_pattern = r\"memory_encoder.o_proj.*\"\n+    output_object_pointer_proj_pattern = r\"object_pointer_proj.layers.(\\d+).*\"\n+\n+    # Stack the point embed module list:\n+    for key, value in state_dict.items():\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        # vision_encoder.blocks.0.mlp.layers.1.weight -> vision_encoder.blocks.0.mlp.proj_out.weight\n+        if re.match(output_vision_encoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_vision_encoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"proj_out\")\n+\n+        # mask_decoder.transformer.layers.0.mlp.layers.1.weight -> mask_decoder.transformer.layers.1.mlp.proj_out.weight\n+        if re.match(output_mask_decoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"mlp.layers.0\", \"mlp.proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"mlp.layers.1\", \"mlp.proj_out\")\n+\n+        # mask_decoder.pred_obj_score_head.layers.1.weight -> mask_decoder.pred_obj_score_head.proj_in.weight\n+        if re.match(output_mask_decoder_score_head_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_score_head_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        if re.match(output_hypernetworks_mlps_pattern, key):\n+            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        # vision_encoder.neck.convs.1.conv.bias -> vision_encoder.neck.convs.1.bias\n+        if re.match(output_vision_encoder_neck_pattern, key):\n+            key = key.replace(\".conv.\", \".\")\n+\n+        # memory_encoder.out_proj.weight -> memory_encoder.projection.weight\n+        if re.match(output_memory_encoder_projection_pattern, key):\n+            key = key.replace(\".o_proj.\", \".projection.\")\n+\n+        if re.match(output_object_pointer_proj_pattern, key):\n+            layer_nb = int(re.match(output_object_pointer_proj_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        model_state_dict[key] = value\n+\n+    model_state_dict[\"shared_image_embedding.positional_embedding\"] = model_state_dict[\n+        \"prompt_encoder.shared_embedding.positional_embedding\"\n+    ]\n+    model_state_dict[\"prompt_encoder.point_embed.weight\"] = torch.cat(\n+        [model_state_dict.pop(f\"prompt_encoder.point_embed.{i}.weight\") for i in range(4)],\n+        dim=0,\n+    )\n+\n+    return model_state_dict\n+\n+\n+def convert_sam2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub):\n+    config = get_config(model_name)\n+\n+    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"model\"]\n+    state_dict = replace_keys(state_dict)\n+\n+    image_processor = Sam2ImageProcessorFast()\n+    processor = Sam2Processor(image_processor=image_processor)\n+    hf_model = Sam2Model(config)\n+    hf_model.eval()\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    missing_keys, unexpected_keys = hf_model.load_state_dict(state_dict, strict=False)\n+    hf_model = hf_model.to(device)\n+    for pattern in Sam2Model._keys_to_ignore_on_load_unexpected:\n+        unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n+    if missing_keys or unexpected_keys:\n+        print(\"Missing keys:\", missing_keys)\n+        print(\"Unexpected keys:\", unexpected_keys)\n+        raise ValueError(\"Missing or unexpected keys in the state dict\")\n+\n+    img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+\n+    input_points = [[[[1000, 600]]]]\n+    input_labels = [[[1]]]\n+\n+    inputs = processor(\n+        images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+    ).to(device)\n+\n+    with torch.no_grad():\n+        output = hf_model(**inputs)\n+    scores = output.iou_scores.squeeze()\n+\n+    if model_name == \"sam2.1_hiera_tiny\":\n+        assert torch.allclose(scores, torch.tensor([0.0316, 0.9647, 0.1029]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_small\":\n+        assert torch.allclose(scores, torch.tensor([0.9664, 0.1494, 0.0456]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_base_plus\":\n+        assert torch.allclose(scores, torch.tensor([0.0361, 0.9775, 0.1307]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_large\":\n+        assert torch.allclose(scores, torch.tensor([0.9648, 0.0371, 0.1898]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_tiny\":\n+        assert torch.allclose(scores, torch.tensor([0.0439, 0.9567, 0.1415]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_small\":\n+        assert torch.allclose(scores, torch.tensor([0.9593, 0.1633, 0.0392]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_base_plus\":\n+        assert torch.allclose(scores, torch.tensor([0.0423, 0.9815, 0.0897]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_large\":\n+        assert torch.allclose(scores, torch.tensor([0.9514, 0.0535, 0.1787]).cuda(), atol=1e-2)\n+    else:\n+        raise ValueError(f\"Model {model_name} not supported\")\n+\n+    if pytorch_dump_folder is not None:\n+        processor.save_pretrained(pytorch_dump_folder)\n+        hf_model.save_pretrained(pytorch_dump_folder)\n+\n+    if push_to_hub:\n+        repo_id = f\"danelcsb/{pytorch_dump_folder.split('/')[-1]}\"\n+        processor.push_to_hub(repo_id)\n+        hf_model.push_to_hub(repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    choices = [\n+        \"sam2.1_hiera_tiny\",\n+        \"sam2.1_hiera_small\",\n+        \"sam2.1_hiera_base_plus\",\n+        \"sam2.1_hiera_large\",\n+        \"sam2_hiera_tiny\",\n+        \"sam2_hiera_small\",\n+        \"sam2_hiera_base_plus\",\n+        \"sam2_hiera_large\",\n+    ]\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"sam2.1_hiera_tiny\",\n+        choices=choices,\n+        type=str,\n+        help=\"Name of the original model to convert\",\n+    )\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        type=str,\n+        required=False,\n+        help=\"Path to the original checkpoint\",\n+    )\n+    parser.add_argument(\"--pytorch_dump_folder_path\", default=\"\", type=str, help=\"Path to the output PyTorch model.\")\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the model and processor to the hub after converting\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    hf_model_name = args.model_name.replace(\"_\", \"-\")\n+    checkpoint_path = (\n+        hf_hub_download(f\"facebook/{hf_model_name}\", f\"{args.model_name.lower()}.pt\")\n+        if args.checkpoint_path is None\n+        else args.checkpoint_path\n+    )\n+\n+    convert_sam2_checkpoint(args.model_name, checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "d68e41fc6d60e55a890af058867f039c6bfe09a8",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "added",
            "additions": 729,
            "deletions": 0,
            "changes": 729,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,729 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam2/modular_sam2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from copy import deepcopy\n+from itertools import product\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    pil_torch_interpolation_mapping,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.ops.boxes import batched_nms\n+elif is_torchvision_available():\n+    from torchvision.ops.boxes import batched_nms\n+\n+\n+class Sam2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    mask_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n+    \"\"\"\n+\n+    mask_size: Optional[dict[str, int]]\n+\n+\n+def _compute_stability_score(masks: \"torch.Tensor\", mask_threshold: float, stability_score_offset: int):\n+    # One mask is always contained inside the other.\n+    # Save memory by preventing unnecessary cast to torch.int64\n+    intersections = (\n+        (masks > (mask_threshold + stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    )\n+    unions = (masks > (mask_threshold - stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    stability_scores = intersections / unions\n+    return stability_scores\n+\n+\n+def _mask_to_rle(input_mask: \"torch.Tensor\"):\n+    \"\"\"\n+    Encodes masks the run-length encoding (RLE), in the format expected by pycoco tools.\n+    \"\"\"\n+    # Put in fortran order and flatten height and width\n+    batch_size, height, width = input_mask.shape\n+    input_mask = input_mask.permute(0, 2, 1).flatten(1)\n+\n+    # Compute change indices\n+    diff = input_mask[:, 1:] ^ input_mask[:, :-1]\n+    change_indices = diff.nonzero()\n+\n+    # Encode run length\n+    out = []\n+    for i in range(batch_size):\n+        cur_idxs = change_indices[change_indices[:, 0] == i, 1] + 1\n+        if len(cur_idxs) == 0:\n+            # No changes => either all 0 or all 1\n+            # If the entire mask is 0, RLE is [height*width] or if the entire mask is 1, RLE is [0, height*width].\n+            if input_mask[i, 0] == 0:\n+                out.append({\"size\": [height, width], \"counts\": [height * width]})\n+            else:\n+                out.append({\"size\": [height, width], \"counts\": [0, height * width]})\n+            continue\n+        btw_idxs = cur_idxs[1:] - cur_idxs[:-1]\n+        counts = [] if input_mask[i, 0] == 0 else [0]\n+        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1].item()]\n+        out.append({\"size\": [height, width], \"counts\": counts})\n+    return out\n+\n+\n+def _batched_mask_to_box(masks: \"torch.Tensor\"):\n+    \"\"\"\n+    Computes the bounding boxes around the given input masks. The bounding boxes are in the XYXY format which\n+    corresponds the following required indices:\n+        - LEFT: left hand side of the bounding box\n+        - TOP: top of the bounding box\n+        - RIGHT: right of the bounding box\n+        - BOTTOM: bottom of the bounding box\n+\n+    Return [0,0,0,0] for an empty mask. For input shape channel_1 x channel_2 x ... x height x width, the output shape\n+    is channel_1 x channel_2 x ... x 4.\n+\n+    Args:\n+        - masks (`torch.Tensor` of shape `(batch, nb_mask, height, width)`)\n+    \"\"\"\n+    # torch.max below raises an error on empty inputs, just skip in this case\n+\n+    if torch.numel(masks) == 0:\n+        return torch.zeros(*masks.shape[:-2], 4, device=masks.device)\n+\n+    # Normalize shape to Cxheightxwidth\n+    shape = masks.shape\n+    height, width = shape[-2:]\n+\n+    # Get top and bottom edges\n+    in_height, _ = torch.max(masks, dim=-1)\n+    in_height_coords = in_height * torch.arange(height, device=in_height.device)[None, :]\n+    bottom_edges, _ = torch.max(in_height_coords, dim=-1)\n+    in_height_coords = in_height_coords + height * (~in_height)\n+    top_edges, _ = torch.min(in_height_coords, dim=-1)\n+\n+    # Get left and right edges\n+    in_width, _ = torch.max(masks, dim=-2)\n+    in_width_coords = in_width * torch.arange(width, device=in_width.device)[None, :]\n+    right_edges, _ = torch.max(in_width_coords, dim=-1)\n+    in_width_coords = in_width_coords + width * (~in_width)\n+    left_edges, _ = torch.min(in_width_coords, dim=-1)\n+\n+    # If the mask is empty the right edge will be to the left of the left edge.\n+    # Replace these boxes with [0, 0, 0, 0]\n+    empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)\n+    out = torch.stack([left_edges, top_edges, right_edges, bottom_edges], dim=-1)\n+    out = out * (~empty_filter).unsqueeze(-1)\n+\n+    # Return to original shape\n+    out = out.reshape(*shape[:-2], 4)\n+    return out\n+\n+\n+def _is_box_near_crop_edge(boxes, crop_box, orig_box, atol=20.0):\n+    \"\"\"Filter masks at the edge of a crop, but not at the edge of the original image.\"\"\"\n+    crop_box_torch = torch.as_tensor(crop_box, dtype=torch.float, device=boxes.device)\n+    orig_box_torch = torch.as_tensor(orig_box, dtype=torch.float, device=boxes.device)\n+\n+    left, top, _, _ = crop_box\n+    offset = torch.tensor([[left, top, left, top]], device=boxes.device)\n+    # Check if boxes has a channel dimension\n+    if len(boxes.shape) == 3:\n+        offset = offset.unsqueeze(1)\n+    boxes = (boxes + offset).float()\n+\n+    near_crop_edge = torch.isclose(boxes, crop_box_torch[None, :], atol=atol, rtol=0)\n+    near_image_edge = torch.isclose(boxes, orig_box_torch[None, :], atol=atol, rtol=0)\n+    near_crop_edge = torch.logical_and(near_crop_edge, ~near_image_edge)\n+    return torch.any(near_crop_edge, dim=1)\n+\n+\n+def _pad_masks(masks, crop_box: list[int], orig_height: int, orig_width: int):\n+    left, top, right, bottom = crop_box\n+    if left == 0 and top == 0 and right == orig_width and bottom == orig_height:\n+        return masks\n+    # Coordinate transform masks\n+    pad_x, pad_y = orig_width - (right - left), orig_height - (bottom - top)\n+    pad = (left, pad_x - left, top, pad_y - top)\n+    return torch.nn.functional.pad(masks, pad, value=0)\n+\n+\n+def _generate_crop_boxes(\n+    image,\n+    target_size: int,  # Is it tuple here?\n+    crop_n_layers: int = 0,\n+    overlap_ratio: float = 512 / 1500,\n+    points_per_crop: Optional[int] = 32,\n+    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+) -> tuple[list[list[int]], list[int]]:\n+    \"\"\"\n+    Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+    Args:\n+        image (Union[`numpy.ndarray`, `PIL.Image`, `torch.Tensor`]):\n+            Image to generate crops for.\n+        target_size (`int`):\n+            Size of the smallest crop.\n+        crop_n_layers (`int`, *optional*):\n+            If `crops_n_layers>0`, mask prediction will be run again on crops of the image. Sets the number of layers\n+            to run, where each layer has 2**i_layer number of image crops.\n+        overlap_ratio (`int`, *optional*):\n+            Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the\n+            image length. Later layers with more crops scale down this overlap.\n+        points_per_crop (`int`, *optional*):\n+            Number of points to sam2ple per crop.\n+        crop_n_points_downscale_factor (`int`, *optional*):\n+            The number of points-per-side sam2pled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+        input_data_format (`str` or `ChannelDimension`, *optional*):\n+            The channel dimension format of the input image. If not provided, it will be inferred.\n+    \"\"\"\n+\n+    if isinstance(image, list):\n+        raise ValueError(\"Only one image is allowed for crop generation.\")\n+    original_size = image.shape[-2:]\n+\n+    points_grid = []\n+    for i in range(crop_n_layers + 1):\n+        n_points = int(points_per_crop / (crop_n_points_downscale_factor**i))\n+        points_grid.append(_build_point_grid(n_points))\n+\n+    crop_boxes, layer_idxs = _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size)\n+\n+    cropped_images, point_grid_per_crop = _generate_crop_images(\n+        crop_boxes, image, points_grid, layer_idxs, target_size, original_size\n+    )\n+    crop_boxes = torch.tensor(crop_boxes)\n+    crop_boxes = crop_boxes.float()\n+    points_per_crop = torch.stack(point_grid_per_crop)\n+    points_per_crop = points_per_crop.unsqueeze(0).permute(0, 2, 1, 3)\n+    cropped_images = torch.stack(cropped_images)\n+\n+    input_labels = torch.ones_like(points_per_crop[:, :, :, 0], dtype=torch.int64)\n+\n+    return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+\n+def _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size):\n+    \"\"\"\n+    Generates 2 ** (layers idx + 1) crops for each crop_n_layers. Crops are in the XYWH format : The XYWH format\n+    consists of the following required indices:\n+        - X: X coordinate of the top left of the bounding box\n+        - Y: Y coordinate of the top left of the bounding box\n+        - W: width of the bounding box\n+        - H: height of the bounding box\n+    \"\"\"\n+    crop_boxes, layer_idxs = [], []\n+    im_height, im_width = original_size\n+    short_side = min(im_height, im_width)\n+\n+    # Original image\n+    crop_boxes.append([0, 0, im_width, im_height])\n+    layer_idxs.append(0)\n+    for i_layer in range(crop_n_layers):\n+        n_crops_per_side = 2 ** (i_layer + 1)\n+        overlap = int(overlap_ratio * short_side * (2 / n_crops_per_side))\n+\n+        crop_width = int(math.ceil((overlap * (n_crops_per_side - 1) + im_width) / n_crops_per_side))\n+        crop_height = int(math.ceil((overlap * (n_crops_per_side - 1) + im_height) / n_crops_per_side))\n+\n+        crop_box_x0 = [int((crop_width - overlap) * i) for i in range(n_crops_per_side)]\n+        crop_box_y0 = [int((crop_height - overlap) * i) for i in range(n_crops_per_side)]\n+\n+        for left, top in product(crop_box_x0, crop_box_y0):\n+            box = [left, top, min(left + crop_width, im_width), min(top + crop_height, im_height)]\n+            crop_boxes.append(box)\n+            layer_idxs.append(i_layer + 1)\n+\n+    return crop_boxes, layer_idxs\n+\n+\n+def _build_point_grid(n_per_side: int) -> torch.Tensor:\n+    \"\"\"Generates a 2D grid of points evenly spaced in [0,1]x[0,1].\"\"\"\n+    offset = 1 / (2 * n_per_side)\n+    points_one_side = torch.linspace(offset, 1 - offset, n_per_side)\n+    points_x = torch.tile(points_one_side[None, :], (n_per_side, 1))\n+    points_y = torch.tile(points_one_side[:, None], (1, n_per_side))\n+    points = torch.stack([points_x, points_y], dim=-1).reshape(-1, 2)\n+    return points\n+\n+\n+def _generate_crop_images(\n+    crop_boxes, image, points_grid, layer_idxs, target_size, original_size, input_data_format=None\n+):\n+    \"\"\"\n+    Takes as an input bounding boxes that are used to crop the image. Based in the crops, the corresponding points are\n+    also passed.\n+    \"\"\"\n+    cropped_images = []\n+    total_points_per_crop = []\n+    for i, crop_box in enumerate(crop_boxes):\n+        left, top, right, bottom = crop_box\n+        cropped_im = image[:, top:bottom, left:right]\n+\n+        cropped_images.append(cropped_im)\n+\n+        cropped_im_size = cropped_im.shape[-2:]\n+        points_scale = torch.tensor(cropped_im_size).flip(dims=(0,)).unsqueeze(0)\n+\n+        points = points_grid[layer_idxs[i]] * points_scale\n+        normalized_points = _normalize_coordinates(target_size, points, original_size)\n+        total_points_per_crop.append(normalized_points)\n+\n+    return cropped_images, total_points_per_crop\n+\n+\n+def _normalize_coordinates(\n+    target_size: int, coords: torch.Tensor, original_size: tuple[int, int], is_bounding_box=False\n+) -> torch.Tensor:\n+    \"\"\"\n+    Expects a numpy array of length 2 in the final dimension. Requires the original image size in (height, width)\n+    format.\n+    \"\"\"\n+    old_height, old_width = original_size\n+\n+    scale = target_size * 1.0 / max(old_height, old_width)\n+    new_height, new_width = old_height * scale, old_width * scale\n+    new_width = int(new_width + 0.5)\n+    new_height = int(new_height + 0.5)\n+\n+    coords = deepcopy(coords).float()\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 2, 2)\n+\n+    coords[..., 0] = coords[..., 0] * (new_width / old_width)\n+    coords[..., 1] = coords[..., 1] * (new_height / old_height)\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 4)\n+\n+    return coords\n+\n+\n+def _rle_to_mask(rle: dict[str, Any]) -> torch.Tensor:\n+    \"\"\"Compute a binary mask from an uncompressed RLE.\"\"\"\n+    height, width = rle[\"size\"]\n+    mask = torch.empty(height * width, dtype=bool)\n+    idx = 0\n+    parity = False\n+    for count in rle[\"counts\"]:\n+        mask[idx : idx + count] = parity\n+        idx += count\n+        parity = not parity\n+    mask = mask.reshape(width, height)\n+    return mask.transpose(0, 1)  # Reshape to original shape\n+\n+\n+def _post_process_for_mask_generation(rle_masks, iou_scores, mask_boxes, amg_crops_nms_thresh=0.7):\n+    \"\"\"\n+    Perform NMS (Non Maximum Suppression) on the outputs.\n+\n+    Args:\n+            rle_masks (`torch.Tensor`):\n+                binary masks in the RLE format\n+            iou_scores (`torch.Tensor` of shape (nb_masks, 1)):\n+                iou_scores predicted by the model\n+            mask_boxes (`torch.Tensor`):\n+                The bounding boxes corresponding to segmentation masks\n+            amg_crops_nms_thresh (`float`, *optional*, defaults to 0.7):\n+                NMS threshold.\n+    \"\"\"\n+    keep_by_nms = batched_nms(\n+        boxes=mask_boxes.float(),\n+        scores=iou_scores,\n+        idxs=torch.zeros(mask_boxes.shape[0]),\n+        iou_threshold=amg_crops_nms_thresh,\n+    )\n+\n+    iou_scores = iou_scores[keep_by_nms]\n+    rle_masks = [rle_masks[i] for i in keep_by_nms]\n+    mask_boxes = mask_boxes[keep_by_nms]\n+    masks = [_rle_to_mask(rle) for rle in rle_masks]\n+\n+    return masks, iou_scores, rle_masks, mask_boxes\n+\n+\n+@auto_docstring\n+class Sam2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 1024, \"width\": 1024}\n+    mask_size = {\"height\": 256, \"width\": 256}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    valid_kwargs = Sam2FastImageProcessorKwargs\n+\n+    # modular artefacts\n+    do_pad = None\n+    pad_size = None\n+    mask_pad_size = None\n+\n+    def __init__(self, **kwargs: Unpack[Sam2FastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        mask_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if mask_size is not None:\n+            mask_size = SizeDict(**get_size_dict(mask_size, param_name=\"mask_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"mask_size\"] = mask_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[Sam2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[Sam2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        original_sizes = [image.shape[-2:] for image in images]\n+        images_kwargs = kwargs.copy()\n+        pixel_values = self._preprocess(images, **images_kwargs)\n+        reshaped_input_sizes = [image.shape[-2:] for image in images]\n+        data = {\n+            \"pixel_values\": pixel_values,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            )\n+            data[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        return super()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n+\n+    def generate_crop_boxes(\n+        self,\n+        image: \"torch.Tensor\",\n+        target_size,\n+        crop_n_layers: int = 0,\n+        overlap_ratio: float = 512 / 1500,\n+        points_per_crop: Optional[int] = 32,\n+        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        device: Optional[\"torch.device\"] = None,\n+    ):\n+        \"\"\"\n+        Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Input original image\n+            target_size (`int`):\n+                Target size of the resized image\n+            crop_n_layers (`int`, *optional*, defaults to 0):\n+                If >0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where\n+                each layer has 2**i_layer number of image crops.\n+            overlap_ratio (`float`, *optional*, defaults to 512/1500):\n+                Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of\n+                the image length. Later layers with more crops scale down this overlap.\n+            points_per_crop (`int`, *optional*, defaults to 32):\n+                Number of points to sam2ple from each crop.\n+            crop_n_points_downscale_factor (`list[int]`, *optional*, defaults to 1):\n+                The number of points-per-side sam2pled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+            device (`torch.device`, *optional*, defaults to None):\n+                Device to use for the computation. If None, cpu will be used.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+            return_tensors (`str`, *optional*, defaults to `pt`):\n+                If `pt`, returns `torch.Tensor`. If `tf`, returns `tf.Tensor`.\n+        \"\"\"\n+        image = self._process_image(image)\n+        crop_boxes, points_per_crop, cropped_images, input_labels = _generate_crop_boxes(\n+            image,\n+            target_size,\n+            crop_n_layers,\n+            overlap_ratio,\n+            points_per_crop,\n+            crop_n_points_downscale_factor,\n+        )\n+        if device is None:\n+            device = torch.device(\"cpu\")\n+        crop_boxes = crop_boxes.to(device)\n+        points_per_crop = points_per_crop.to(device)\n+        # cropped_images stays as torch.Tensor\n+        input_labels = input_labels.to(device)\n+\n+        return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+    def filter_masks(\n+        self,\n+        masks,\n+        iou_scores,\n+        original_size,\n+        cropped_box_image,\n+        pred_iou_thresh=0.88,\n+        stability_score_thresh=0.95,\n+        mask_threshold=0,\n+        stability_score_offset=1,\n+    ):\n+        \"\"\"\n+        Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being\n+        that the iou scores needs to be greater than `pred_iou_thresh`. The second criterion is that the stability\n+        score needs to be greater than `stability_score_thresh`. The method also converts the predicted masks to\n+        bounding boxes and pad the predicted masks if necessary.\n+\n+        Args:\n+            masks (`torch.Tensor`):\n+                Input masks.\n+            iou_scores (`torch.Tensor`):\n+                List of IoU scores.\n+            original_size (`tuple[int,int]`):\n+                Size of the original image.\n+            cropped_box_image (`torch.Tensor`):\n+                The cropped image.\n+            pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n+                The threshold for the iou scores.\n+            stability_score_thresh (`float`, *optional*, defaults to 0.95):\n+                The threshold for the stability score.\n+            mask_threshold (`float`, *optional*, defaults to 0):\n+                The threshold for the predicted masks.\n+            stability_score_offset (`float`, *optional*, defaults to 1):\n+                The offset for the stability score used in the `_compute_stability_score` method.\n+\n+        \"\"\"\n+        original_height, original_width = original_size\n+        iou_scores = iou_scores.flatten(0, 1)\n+        masks = masks.flatten(0, 1)\n+\n+        if masks.shape[0] != iou_scores.shape[0]:\n+            raise ValueError(\"masks and iou_scores must have the sam2e batch size.\")\n+\n+        if masks.device != iou_scores.device:\n+            iou_scores = iou_scores.to(masks.device)\n+\n+        batch_size = masks.shape[0]\n+\n+        keep_mask = torch.ones(batch_size, dtype=torch.bool, device=masks.device)\n+\n+        if pred_iou_thresh > 0.0:\n+            keep_mask = keep_mask & (iou_scores > pred_iou_thresh)\n+\n+        # compute stability score\n+        if stability_score_thresh > 0.0:\n+            stability_scores = _compute_stability_score(masks, mask_threshold, stability_score_offset)\n+            keep_mask = keep_mask & (stability_scores > stability_score_thresh)\n+\n+        scores = iou_scores[keep_mask]\n+        masks = masks[keep_mask]\n+\n+        # binarize masks\n+        masks = masks > mask_threshold\n+        converted_boxes = _batched_mask_to_box(masks)\n+\n+        keep_mask = ~_is_box_near_crop_edge(\n+            converted_boxes, cropped_box_image, [0, 0, original_width, original_height]\n+        )\n+\n+        scores = scores[keep_mask]\n+        masks = masks[keep_mask]\n+        converted_boxes = converted_boxes[keep_mask]\n+\n+        masks = _pad_masks(masks, cropped_box_image, original_height, original_width)\n+        # conversion to rle is necessary to run non-maximum suppression\n+        masks = _mask_to_rle(masks)\n+\n+        return masks, scores, converted_boxes\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[torch.Tensor, List[torch.Tensor], np.ndarray, List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n+            original_sizes = original_sizes.tolist()\n+        # TODO: add connected components kernel for postprocessing\n+        output_masks = []\n+        for i, original_size in enumerate(original_sizes):\n+            if isinstance(masks[i], np.ndarray):\n+                masks[i] = torch.from_numpy(masks[i])\n+            elif not isinstance(masks[i], torch.Tensor):\n+                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+            interpolated_mask = F.interpolate(masks[i], original_size, mode=\"bilinear\", align_corners=False)\n+            if apply_non_overlapping_constraints:\n+                interpolated_mask = self._apply_non_overlapping_constraints(interpolated_mask)\n+            if binarize:\n+                interpolated_mask = interpolated_mask > mask_threshold\n+            output_masks.append(interpolated_mask)\n+\n+        return output_masks\n+\n+    def post_process_for_mask_generation(self, all_masks, all_scores, all_boxes, crops_nms_thresh):\n+        \"\"\"\n+        Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.\n+\n+        Args:\n+            all_masks (`torch.Tensor`):\n+                List of all predicted segmentation masks\n+            all_scores (`torch.Tensor`):\n+                List of all predicted iou scores\n+            all_boxes (`torch.Tensor`):\n+                List of all bounding boxes of the predicted masks\n+            crops_nms_thresh (`float`):\n+                Threshold for NMS (Non Maximum Suppression) algorithm.\n+        \"\"\"\n+        return _post_process_for_mask_generation(all_masks, all_scores, all_boxes, crops_nms_thresh)\n+\n+    def _apply_non_overlapping_constraints(self, pred_masks: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Apply non-overlapping constraints to the object scores in pred_masks. Here we\n+        keep only the highest scoring object at each spatial location in pred_masks.\n+        \"\"\"\n+        batch_size = pred_masks.size(0)\n+        if batch_size == 1:\n+            return pred_masks\n+\n+        device = pred_masks.device\n+        # \"max_obj_inds\": object index of the object with the highest score at each location\n+        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)\n+        # \"batch_obj_inds\": object index of each object slice (along dim 0) in `pred_masks`\n+        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]\n+        keep = max_obj_inds == batch_obj_inds\n+        # suppress overlapping regions' scores below -10.0 so that the foreground regions\n+        # don't overlap (here sigmoid(-10.0)=4.5398e-05)\n+        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks\n+\n+\n+__all__ = [\"Sam2ImageProcessorFast\"]"
        },
        {
            "sha": "e78e991742f0f6162e8e8800fdc3712fd26d4efb",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "added",
            "additions": 1618,
            "deletions": 0,
            "changes": 1618,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,1618 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam2/modular_sam2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch import Tensor\n+\n+from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n+\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n+from ...utils import (\n+    ModelOutput,\n+    auto_docstring,\n+)\n+from ..auto import AutoModel\n+from .configuration_sam2 import (\n+    Sam2Config,\n+    Sam2HieraDetConfig,\n+    Sam2MaskDecoderConfig,\n+    Sam2PromptEncoderConfig,\n+    Sam2VisionConfig,\n+)\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the vision encoder's outputs.\")\n+class Sam2VisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, height, width, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    fpn_hidden_states (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Feature maps from the Feature Pyramid Network neck.\n+    fpn_position_encoding (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Positional encodings corresponding to the `fpn_hidden_states`.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`. Hidden-states of the\n+        model at the output of each stage.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    fpn_hidden_states: Optional[torch.FloatTensor] = None\n+    fpn_position_encoding: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the Sam2 model's output.\")\n+class Sam2ImageSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    iou_scores (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks)`):\n+        The Intersection over Union (IoU) scores of the predicted masks.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks, height, width)`):\n+        The predicted low-resolution masks. This is an alias for `low_res_masks`. These masks need to be post-processed\n+        by the processor to be brought to the original image size.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size, point_batch_size, 1)`):\n+        Logits for the object score, indicating if an object is present.\n+    image_embeddings (`tuple(torch.FloatTensor)`):\n+        The features from the FPN, which are used by the mask decoder. This is a tuple of `torch.FloatTensor` where each\n+        tensor has shape `(batch_size, channels, height, width)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`.\n+        Hidden-states of the vision model at the output of each stage.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the vision model.\n+    mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the mask decoder.\n+    \"\"\"\n+\n+    iou_scores: torch.FloatTensor = None\n+    pred_masks: torch.FloatTensor = None\n+    object_score_logits: torch.FloatTensor = None\n+    image_embeddings: tuple[torch.FloatTensor, ...] = None\n+    vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    mask_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class Sam2PatchEmbeddings(nn.Module):\n+    r\"\"\"\n+    Turns pixel values into patch embeddings for transformer consumption.\n+\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`Sam2ImageProcessorFast.__call__`] for details.\n+\n+    Returns:\n+        embeddings (`torch.FloatTensor`):\n+            Patch embeddings depend on image_size, patch_kernel_size, patch_stride and patch_padding\n+    \"\"\"\n+\n+    def __init__(self, config: Sam2HieraDetConfig):\n+        super().__init__()\n+        num_channels = config.num_channels\n+        hidden_size = config.hidden_size\n+\n+        self.projection = nn.Conv2d(\n+            num_channels,\n+            hidden_size,\n+            kernel_size=config.patch_kernel_size,\n+            stride=config.patch_stride,\n+            padding=config.patch_padding,\n+        )\n+\n+    def forward(self, pixel_values):\n+        _, num_channels, height, width = pixel_values.shape\n+        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        return embeddings\n+\n+\n+# copied and adapted from original implementation, also practically equal to DetrSinePositionEmbedding\n+class Sam2SinePositionEmbedding(nn.Module):\n+    \"\"\"\n+    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n+    need paper, generalized to work on images.\n+    \"\"\"\n+\n+    def __init__(\n+        self, num_pos_feats: int = 64, temperature: int = 10000, normalize: bool = False, scale: Optional[float] = None\n+    ):\n+        super().__init__()\n+        if scale is not None and normalize is False:\n+            raise ValueError(\"normalize should be True if scale is passed\")\n+        self.num_pos_feats = num_pos_feats\n+        self.temperature = temperature\n+        self.normalize = normalize\n+        self.scale = 2 * math.pi if scale is None else scale\n+\n+    @compile_compatible_method_lru_cache(maxsize=1)\n+    def forward(\n+        self,\n+        shape: torch.Size,\n+        device: Union[torch.device, str],\n+        dtype: torch.dtype,\n+        mask: Optional[Tensor] = None,\n+    ) -> Tensor:\n+        if mask is None:\n+            mask = torch.zeros((shape[0], shape[2], shape[3]), device=device, dtype=torch.bool)\n+        not_mask = (~mask).to(dtype)\n+        y_embed = not_mask.cumsum(1)\n+        x_embed = not_mask.cumsum(2)\n+        if self.normalize:\n+            eps = 1e-6\n+            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n+            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n+\n+        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=device).to(dtype)\n+        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.num_pos_feats)\n+\n+        pos_x = x_embed[:, :, :, None] / dim_t\n+        pos_y = y_embed[:, :, :, None] / dim_t\n+        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n+        return pos\n+\n+\n+class Sam2VisionNeck(nn.Module):\n+    def __init__(self, config: Sam2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.position_encoding = Sam2SinePositionEmbedding(num_pos_feats=config.fpn_hidden_size // 2, normalize=True)\n+        self.convs = nn.ModuleList()\n+        for in_channels in config.backbone_channel_list:\n+            self.convs.append(\n+                nn.Conv2d(\n+                    in_channels=in_channels,\n+                    out_channels=config.fpn_hidden_size,\n+                    kernel_size=config.fpn_kernel_size,\n+                    stride=config.fpn_stride,\n+                    padding=config.fpn_padding,\n+                ),\n+            )\n+        self.fpn_top_down_levels = config.fpn_top_down_levels\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[tuple[torch.Tensor, ...], tuple[torch.Tensor, ...]]:\n+        fpn_hidden_states = ()\n+        fpn_position_encoding = ()\n+\n+        # forward in top-down order (from low to high resolution)\n+        n = len(self.convs) - 1\n+        for i in range(n, -1, -1):\n+            lateral_features = hidden_states[i].permute(0, 3, 1, 2)\n+            lateral_features = self.convs[n - i](lateral_features)\n+            if i not in self.fpn_top_down_levels or i == n:\n+                prev_features = lateral_features\n+            else:\n+                top_down_features = F.interpolate(\n+                    prev_features.to(dtype=torch.float32),\n+                    scale_factor=2.0,\n+                    mode=\"nearest\",\n+                    align_corners=None,\n+                    antialias=False,\n+                ).to(lateral_features.dtype)\n+                prev_features = lateral_features + top_down_features\n+\n+            prev_position_encoding = self.position_encoding(\n+                prev_features.shape, prev_features.device, prev_features.dtype\n+            ).to(prev_features.dtype)\n+\n+            fpn_hidden_states += (prev_features,)\n+            fpn_position_encoding += (prev_position_encoding,)\n+\n+        return fpn_hidden_states, fpn_position_encoding\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def do_pool(x: torch.Tensor, query_stride: Optional[int] = None) -> torch.Tensor:\n+    if query_stride is None:\n+        return x\n+    # (B, H, W, C) -> (B, C, H, W)\n+    x = x.permute(0, 3, 1, 2)\n+    x = nn.functional.max_pool2d(x, kernel_size=query_stride, stride=query_stride, ceil_mode=False)\n+    # (B, C, H', W') -> (B, H', W', C)\n+    x = x.permute(0, 2, 3, 1)\n+    return x\n+\n+\n+class Sam2MultiScaleAttention(nn.Module):\n+    def __init__(\n+        self,\n+        config: Sam2HieraDetConfig,\n+        dim: int,\n+        dim_out: int,\n+        num_attention_heads: int,\n+        query_stride: Optional[tuple[int, int]] = None,\n+    ):\n+        super().__init__()\n+\n+        self.config = config\n+\n+        self.dim = dim\n+        self.dim_out = dim_out\n+        self.query_stride = query_stride\n+\n+        self.num_attention_heads = num_attention_heads\n+        head_dim = dim_out // num_attention_heads\n+        self.scale = head_dim**-0.5\n+        self.qkv = nn.Linear(dim, dim_out * 3)\n+        self.proj = nn.Linear(dim_out, dim_out)\n+\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor, **kwargs) -> torch.Tensor:\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (B, H * W, 3, nHead, C)\n+        qkv = self.qkv(hidden_states).reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+        # q, k, v with shape (B, H * W, nheads, C)\n+        query, key, value = torch.unbind(qkv, 2)\n+\n+        attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n+        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n+\n+        # Q pooling (for downsample at stage changes)\n+        if self.query_stride:\n+            query = do_pool(query.reshape(batch_size, height, width, -1), self.query_stride)\n+            height, width = query.shape[1:3]  # downsampled shape\n+            query = query.reshape(batch_size, height * width, self.num_attention_heads, -1)\n+\n+        # transpose query, key, value to (B, nHead, H * W, C)\n+        query = query.transpose(1, 2)\n+        key = key.transpose(1, 2)\n+        value = value.transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+        attn_output, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, height, width, -1)\n+\n+        attn_output = self.proj(attn_output)\n+\n+        return attn_output\n+\n+\n+class Sam2FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        input_dim: int,\n+        hidden_dim: int,\n+        output_dim: int,\n+        num_layers: int,\n+        activation: str = \"relu\",\n+        sigmoid_output: bool = False,\n+    ):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        self.activation = ACT2FN[activation]\n+        self.proj_in = nn.Linear(input_dim, hidden_dim)\n+        self.proj_out = nn.Linear(hidden_dim, output_dim)\n+        self.layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n+        self.sigmoid_output = sigmoid_output\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.proj_in(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        for layer in self.layers:\n+            hidden_states = self.activation(layer(hidden_states))\n+\n+        hidden_states = self.proj_out(hidden_states)\n+        if self.sigmoid_output:\n+            hidden_states = F.sigmoid(hidden_states)\n+        return hidden_states\n+\n+\n+def window_partition(hidden_state, window_size):\n+    \"\"\"\n+    Partition into non-overlapping windows with padding if needed.\n+\n+    Args:\n+        hidden_state (`torch.Tensor`):\n+            Input tokens with [batch_size, height, width, num_channels].\n+        window_size (`int`):\n+            Window size.\n+\n+    Returns:\n+        `tuple(torch.FloatTensor)` comprising various elements:\n+        - windows: windows after partition with [batch_size * num_windows, window_size, window_size, num_channels].\n+        - (padded_height, padded_width): padded height and width before partition\n+    \"\"\"\n+    batch_size, height, width, num_channels = hidden_state.shape\n+\n+    pad_height = (window_size - height % window_size) % window_size\n+    pad_width = (window_size - width % window_size) % window_size\n+\n+    # Noop in case pad_width == 0 and pad_height == 0.\n+    hidden_state = nn.functional.pad(hidden_state, (0, 0, 0, pad_width, 0, pad_height))\n+\n+    padded_height, padded_width = height + pad_height, width + pad_width\n+\n+    hidden_state = hidden_state.view(\n+        batch_size, padded_height // window_size, window_size, padded_width // window_size, window_size, num_channels\n+    )\n+    windows = hidden_state.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)\n+    return windows, (padded_height, padded_width)\n+\n+\n+def window_unpartition(windows, window_size, pad_height_width, height_width):\n+    \"\"\"\n+    Window unpartition into original sequences and removing padding.\n+\n+    Args:\n+        windows (`torch.Tensor`):\n+            Input tokens with [batch_size * num_windows, window_size, window_size, num_channels].\n+        window_size (`int`):\n+            Window size.\n+        pad_height_width (`tuple[int]`):\n+            Padded height and width (padded_height, padded_width).\n+        height_width (`tuple[int]`):\n+            Original height and width before padding.\n+\n+    Returns:\n+        hidden_state: unpartitioned sequences with [batch_size, height, width, num_channels].\n+    \"\"\"\n+    padded_height, padded_width = pad_height_width\n+    height, width = height_width\n+    batch_size = windows.shape[0] // (padded_height * padded_width // window_size // window_size)\n+    hidden_state = windows.view(\n+        batch_size, padded_height // window_size, padded_width // window_size, window_size, window_size, -1\n+    )\n+    hidden_state = hidden_state.permute(0, 1, 3, 2, 4, 5).contiguous()\n+    hidden_state = hidden_state.view(batch_size, padded_height, padded_width, -1)\n+\n+    # We always have height <= padded_height and width <= padded_width\n+    hidden_state = hidden_state[:, :height, :width, :].contiguous()\n+    return hidden_state\n+\n+\n+class Sam2MultiScaleBlock(GradientCheckpointingLayer):\n+    def __init__(\n+        self,\n+        config: Sam2HieraDetConfig,\n+        stage_idx: int,\n+        block_idx: int,\n+        total_block_idx: int,\n+    ):\n+        super().__init__()\n+\n+        # take embed dim from previous stage if first block of stage\n+        self.dim = (\n+            config.embed_dim_per_stage[stage_idx - 1]\n+            if stage_idx > 0 and block_idx == 0\n+            else config.embed_dim_per_stage[stage_idx]\n+        )\n+        self.dim_out = config.embed_dim_per_stage[stage_idx]\n+        self.layer_norm1 = nn.LayerNorm(self.dim, eps=config.layer_norm_eps)\n+        # take window size from previous stage if first block of stage\n+        self.window_size = (\n+            config.window_size_per_stage[stage_idx - 1]\n+            if stage_idx > 0 and block_idx == 0\n+            else config.window_size_per_stage[stage_idx]\n+        )\n+        self.window_size = 0 if total_block_idx in config.global_attention_blocks else self.window_size\n+        # use query stride for first block of stage if stage is a query pool stage\n+        self.query_stride = (\n+            config.query_stride if 0 < stage_idx <= config.num_query_pool_stages and block_idx == 0 else None\n+        )\n+\n+        self.attn = Sam2MultiScaleAttention(\n+            config,\n+            self.dim,\n+            self.dim_out,\n+            num_attention_heads=config.num_attention_heads_per_stage[stage_idx],\n+            query_stride=self.query_stride,\n+        )\n+        self.layer_norm2 = nn.LayerNorm(self.dim_out, eps=config.layer_norm_eps)\n+        self.mlp = Sam2FeedForward(\n+            self.dim_out,\n+            int(self.dim_out * config.mlp_ratio),\n+            self.dim_out,\n+            num_layers=2,\n+            activation=config.hidden_act,\n+        )\n+        if self.dim != self.dim_out:\n+            self.proj = nn.Linear(self.dim, self.dim_out)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states  # batch_size, height, width, channel\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+\n+        # Skip connection\n+        if self.dim != self.dim_out:\n+            residual = do_pool(self.proj(hidden_states), self.query_stride)\n+\n+        # Window partition\n+        window_size = self.window_size\n+        if self.window_size > 0:\n+            H, W = hidden_states.shape[1], hidden_states.shape[2]\n+            hidden_states, pad_hw = window_partition(hidden_states, window_size)\n+\n+        # Window Attention + Q Pooling (if stage change)\n+        attn_output = self.attn(\n+            hidden_states=hidden_states,\n+            **kwargs,\n+        )\n+        hidden_states = attn_output\n+        if self.query_stride:\n+            # Shapes have changed due to Q pooling\n+            window_size = self.window_size // self.query_stride[0]\n+            H, W = residual.shape[1:3]\n+\n+            pad_h = (window_size - H % window_size) % window_size\n+            pad_w = (window_size - W % window_size) % window_size\n+            pad_hw = (H + pad_h, W + pad_w)\n+\n+        # Reverse window partition\n+        if self.window_size > 0:\n+            hidden_states = window_unpartition(hidden_states, window_size, pad_hw, (H, W))\n+\n+        hidden_states = residual + hidden_states\n+        layernorm_output = self.layer_norm2(hidden_states)\n+        hidden_states = hidden_states + self.mlp(layernorm_output)\n+\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Hiera model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n+class Sam2HieraDetModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, height, width, hidden_size)`):\n+        hidden-states at the output of the last layer of the model.\n+    intermediate_hidden_states (`tuple[torch.FloatTensor]` of shape `(batch_size, height, width, hidden_size)`):\n+        Sequence of hidden-states at the output of the intermediate layers of the model.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class Sam2PreTrainedModel(PreTrainedModel):\n+    config_class = Sam2Config\n+    base_model_prefix = \"sam2\"\n+    main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (nn.LayerNorm, Sam2LayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        if isinstance(module, Sam2HieraDetModel):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n+            if module.pos_embed_window is not None:\n+                module.pos_embed_window.data.zero_()\n+        if isinstance(module, Sam2Model):\n+            if module.no_memory_embedding is not None:\n+                module.no_memory_embedding.data.zero_()\n+\n+\n+class Sam2HieraDetModel(Sam2PreTrainedModel):\n+    config_class = Sam2HieraDetConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Sam2MultiScaleBlock,\n+        \"attentions\": Sam2MultiScaleAttention,\n+    }\n+\n+    def __init__(self, config: Sam2HieraDetConfig):\n+        super().__init__(config)\n+\n+        self.patch_embed = Sam2PatchEmbeddings(config)\n+        # Windowed positional embedding (https://arxiv.org/abs/2311.05613)\n+        self.pos_embed = nn.Parameter(\n+            torch.zeros(1, config.hidden_size, *config.window_positional_embedding_background_size)\n+        )\n+        self.pos_embed_window = nn.Parameter(\n+            torch.zeros(1, config.hidden_size, config.window_size_per_stage[0], config.window_size_per_stage[0])\n+        )\n+        self.stage_ends = (np.cumsum(config.blocks_per_stage) - 1).tolist()\n+        self.blocks = nn.ModuleList()\n+        total_block_idx = 0\n+        for stage_idx, blocks_per_stage in enumerate(config.blocks_per_stage):\n+            for block_idx in range(blocks_per_stage):\n+                block = Sam2MultiScaleBlock(\n+                    config=config, stage_idx=stage_idx, block_idx=block_idx, total_block_idx=total_block_idx\n+                )\n+                self.blocks.append(block)\n+                total_block_idx += 1\n+\n+    def get_input_embeddings(self):\n+        return self.patch_embed\n+\n+    def _get_pos_embed(self, hw: tuple[int, int]) -> torch.Tensor:\n+        h, w = hw\n+        window_embed = self.pos_embed_window\n+        pos_embed = F.interpolate(self.pos_embed, size=(h, w), mode=\"bicubic\")\n+        pos_embed = pos_embed + window_embed.tile([x // y for x, y in zip(pos_embed.shape, window_embed.shape)])\n+        pos_embed = pos_embed.permute(0, 2, 3, 1)\n+        return pos_embed\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Sam2HieraDetModelOutput]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.patch_embed(pixel_values)\n+        hidden_states = hidden_states + self._get_pos_embed(hidden_states.shape[1:3])\n+\n+        intermediate_hidden_states = ()\n+        for i, block_module in enumerate(self.blocks):\n+            hidden_states = block_module(hidden_states, **kwargs)\n+\n+            if i in self.stage_ends:\n+                intermediate_hidden_states = intermediate_hidden_states + (hidden_states,)\n+\n+        return Sam2HieraDetModelOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate_hidden_states,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from Sam without any head or projection on top.\n+    \"\"\"\n+)\n+class Sam2VisionModel(Sam2PreTrainedModel):\n+    config_class = Sam2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Sam2MultiScaleBlock,\n+        \"attentions\": Sam2MultiScaleAttention,\n+    }\n+\n+    def __init__(self, config: Sam2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.backbone = AutoModel.from_config(config.backbone_config)\n+\n+        self.neck = Sam2VisionNeck(config)\n+        self.num_feature_levels = config.num_feature_levels\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.backbone.get_input_embeddings()\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Sam2VisionEncoderOutput]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        # Forward through backbone\n+        backbone_output = self.backbone(pixel_values, **kwargs)\n+        hidden_states = backbone_output.last_hidden_state\n+        intermediate_hidden_states = backbone_output.intermediate_hidden_states\n+\n+        fpn_hidden_states, fpn_position_encoding = self.neck(intermediate_hidden_states)\n+        # Select last `num_feature_levels` feature levels from FPN and reverse order to get features from high to low resolution\n+        fpn_hidden_states = fpn_hidden_states[-self.num_feature_levels :][::-1]\n+        fpn_position_encoding = fpn_position_encoding[-self.num_feature_levels :][::-1]\n+\n+        return Sam2VisionEncoderOutput(\n+            last_hidden_state=hidden_states,\n+            fpn_hidden_states=fpn_hidden_states,\n+            fpn_position_encoding=fpn_position_encoding,\n+        )\n+\n+\n+class Sam2PositionalEmbedding(nn.Module):\n+    def __init__(self, config: Sam2PromptEncoderConfig):\n+        super().__init__()\n+        self.scale = config.scale\n+        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n+        self.register_buffer(\"positional_embedding\", positional_embedding)\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+        coordinates.to(torch.float32)\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n+class Sam2MaskEmbedding(nn.Module):\n+    def __init__(self, config: Sam2PromptEncoderConfig):\n+        super().__init__()\n+        self.mask_input_channels = config.mask_input_channels // 4\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.conv1 = nn.Conv2d(1, self.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv2 = nn.Conv2d(self.mask_input_channels, config.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv3 = nn.Conv2d(config.mask_input_channels, config.hidden_size, kernel_size=1)\n+        self.layer_norm1 = Sam2LayerNorm(\n+            self.mask_input_channels, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+        self.layer_norm2 = Sam2LayerNorm(\n+            self.mask_input_channels * 4, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+\n+    def forward(self, masks):\n+        hidden_states = self.conv1(masks)\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        dense_embeddings = self.conv3(hidden_states)\n+        return dense_embeddings\n+\n+\n+class Sam2PromptEncoder(nn.Module):\n+    def __init__(self, config: Sam2PromptEncoderConfig):\n+        super().__init__()\n+        self.shared_embedding = Sam2PositionalEmbedding(config)\n+        self.mask_embed = Sam2MaskEmbedding(config)\n+        self.no_mask_embed = nn.Embedding(1, config.hidden_size)\n+\n+        self.image_embedding_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+        self.mask_input_size = (4 * config.image_size // config.patch_size, 4 * config.image_size // config.patch_size)\n+        self.input_image_size = config.image_size\n+\n+        self.point_embed = nn.Embedding(config.num_point_embeddings, config.hidden_size)\n+        self.hidden_size = config.hidden_size\n+        self.not_a_point_embed = nn.Embedding(1, config.hidden_size)\n+\n+    def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -> torch.Tensor:\n+        \"\"\"Embeds point prompts.\"\"\"\n+        points = points + 0.5  # Shift to center of pixel\n+        if pad:\n+            points = torch.nn.functional.pad(points, (0, 0, 0, 1), mode=\"constant\", value=0)\n+            labels = torch.nn.functional.pad(labels, (0, 1), mode=\"constant\", value=-1)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        point_embedding = self.shared_embedding(points, input_shape)\n+\n+        # torch.where and expanding the labels tensor is required by the ONNX export\n+        point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n+\n+        # This is required for the ONNX export. The dtype, device need to be explicitely\n+        # specificed as otherwise torch.onnx.export interprets as double\n+        point_embedding = torch.where(\n+            labels[..., None] != -10,\n+            point_embedding,\n+            torch.zeros_like(point_embedding),\n+        )\n+\n+        # Add point embeddings for labels >= 0\n+        point_embedding = point_embedding + self.point_embed(labels.clamp(min=0)) * (labels >= 0).unsqueeze(-1)\n+\n+        return point_embedding\n+\n+    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Embeds box prompts.\"\"\"\n+        boxes = boxes + 0.5  # Shift to center of pixel\n+        batch_size, nb_boxes = boxes.shape[:2]\n+        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        corner_embedding = self.shared_embedding(coords, input_shape)\n+        corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n+        corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        return corner_embedding\n+\n+    def forward(\n+        self,\n+        input_points: Optional[tuple[torch.Tensor, torch.Tensor]],\n+        input_labels: Optional[torch.Tensor],\n+        input_boxes: Optional[torch.Tensor],\n+        input_masks: Optional[torch.Tensor],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Embeds different types of prompts, returning both sparse and dense embeddings.\n+\n+        Args:\n+            points (`torch.Tensor`, *optional*):\n+                point coordinates and labels to embed.\n+            boxes (`torch.Tensor`, *optional*):\n+                boxes to embed\n+            masks (`torch.Tensor`, *optional*):\n+                masks to embed\n+        \"\"\"\n+        sparse_embeddings = None\n+        batch_size = 1\n+        if input_points is not None:\n+            batch_size = input_points.shape[0]\n+            if input_labels is None:\n+                raise ValueError(\"If points are provided, labels must also be provided.\")\n+            point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))\n+            sparse_embeddings = point_embeddings\n+        if input_boxes is not None:\n+            batch_size = input_boxes.shape[0]\n+            box_embeddings = self._embed_boxes(input_boxes)\n+            if sparse_embeddings is None:\n+                sparse_embeddings = box_embeddings\n+            else:\n+                sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=2)\n+        if input_masks is not None:\n+            dense_embeddings = self.mask_embed(input_masks)\n+        else:\n+            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n+                batch_size, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n+            )\n+\n+        return sparse_embeddings, dense_embeddings\n+\n+\n+class Sam2Attention(nn.Module):\n+    \"\"\"\n+    SAM2's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__()\n+        downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.internal_dim = config.hidden_size // downsample_rate\n+        self.num_attention_heads = config.num_attention_heads\n+        self.head_dim = self.internal_dim // config.num_attention_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.o_proj = nn.Linear(self.internal_dim, self.hidden_size)\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Input projections\n+        batch_size, point_batch_size = query.shape[:2]\n+        new_shape = (batch_size * point_batch_size, -1, self.num_attention_heads, self.head_dim)\n+\n+        query = self.q_proj(query).view(*new_shape).transpose(1, 2)\n+        key = self.k_proj(key).view(*new_shape).transpose(1, 2)\n+        value = self.v_proj(value).view(*new_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_similarity,\n+            dropout=0.0 if not self.training else self.dropout_p,\n+            scaling=self.scaling,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(\n+            batch_size, point_batch_size, -1, self.num_attention_heads * self.head_dim\n+        ).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Sam2TwoWayAttentionBlock(nn.Module):\n+    def __init__(self, config: Sam2MaskDecoderConfig, skip_first_layer_pe: bool = False):\n+        \"\"\"\n+        A transformer block with four layers:\n+            (1) self-attention of sparse inputs (2) cross attention of sparse inputs -> dense inputs (3) mlp block on\n+            sparse inputs (4) cross attention of dense inputs -> sparse inputs\n+\n+        Arguments:\n+            config (`Sam2MaskDecoderConfig`):\n+                The configuration file used to instantiate the block\n+            attention_downsample_rate (*optionalk*, int, defaults to 2):\n+                The downsample ratio of the block used to reduce the inner dim of the attention.\n+            skip_first_layer_pe (*optional*, bool, defaults to `False`):\n+                Whether or not to skip the addition of the query_point_embedding on the first layer.\n+        \"\"\"\n+        super().__init__()\n+        self.self_attn = Sam2Attention(config, downsample_rate=1)\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n+\n+        self.cross_attn_token_to_image = Sam2Attention(config)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n+\n+        self.mlp = Sam2FeedForward(\n+            config.hidden_size, config.mlp_dim, config.hidden_size, num_layers=config.num_hidden_layers\n+        )\n+        self.layer_norm3 = nn.LayerNorm(config.hidden_size)\n+\n+        self.layer_norm4 = nn.LayerNorm(config.hidden_size)\n+        self.cross_attn_image_to_token = Sam2Attention(config)\n+\n+        self.skip_first_layer_pe = skip_first_layer_pe\n+\n+    def forward(\n+        self,\n+        queries: Tensor,\n+        keys: Tensor,\n+        query_point_embedding: Tensor,\n+        key_point_embedding: Tensor,\n+        attention_similarity: Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        # Self attention block\n+        if self.skip_first_layer_pe:\n+            queries, _ = self.self_attn(query=queries, key=queries, value=queries)\n+        else:\n+            query = queries + query_point_embedding\n+            attn_out, _ = self.self_attn(query=query, key=query, value=queries)\n+            queries = queries + attn_out\n+        queries = self.layer_norm1(queries)\n+\n+        # Cross attention block, tokens attending to image embedding\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out, _ = self.cross_attn_token_to_image(\n+            query=query, key=key, value=keys, attention_similarity=attention_similarity\n+        )\n+        queries = queries + attn_out\n+\n+        queries = self.layer_norm2(queries)\n+\n+        # MLP block\n+        mlp_out = self.mlp(queries)\n+        queries = queries + mlp_out\n+        queries = self.layer_norm3(queries)\n+\n+        # Cross attention block, image embedding attending to tokens\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out, _ = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n+        keys = keys + attn_out\n+\n+        keys = self.layer_norm4(keys)\n+        return queries, keys, attn_out\n+\n+\n+class Sam2TwoWayTransformer(nn.Module):\n+    def __init__(self, config: Sam2MaskDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.layers = nn.ModuleList()\n+\n+        for i in range(self.num_hidden_layers):\n+            self.layers.append(Sam2TwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n+\n+        self.final_attn_token_to_image = Sam2Attention(config)\n+        self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        point_embeddings: Tensor,\n+        image_embeddings: Tensor,\n+        image_positional_embeddings: Tensor,\n+        attention_similarity: Tensor,\n+        target_embedding=None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        if image_embeddings is None:\n+            raise ValueError(\"You have to specify an image_embedding\")\n+\n+        image_embeddings = image_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+        image_positional_embeddings = image_positional_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+\n+        # Prepare queries\n+        queries = point_embeddings\n+        keys = image_embeddings\n+\n+        # Apply transformer blocks and final layernorm\n+        for layer in self.layers:\n+            if target_embedding is not None:\n+                queries += target_embedding\n+\n+            queries, keys, _ = layer(\n+                queries=queries,\n+                keys=keys,\n+                query_point_embedding=point_embeddings,\n+                key_point_embedding=image_positional_embeddings,\n+                attention_similarity=attention_similarity,\n+                **kwargs,\n+            )\n+        # Apply the final attention layer from the points to the image\n+        query = queries + point_embeddings\n+        key = keys + image_positional_embeddings\n+\n+        attn_out, _ = self.final_attn_token_to_image(query=query, key=key, value=keys)\n+\n+        queries = queries + attn_out\n+        queries = self.layer_norm_final_attn(queries)\n+        return queries, keys\n+\n+\n+class Sam2LayerNorm(nn.Module):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_first\"):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(normalized_shape))\n+        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n+        self.eps = eps\n+        self.data_format = data_format\n+        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {self.data_format}\")\n+        self.normalized_shape = (normalized_shape,)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        if self.data_format == \"channels_last\":\n+            x = torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        elif self.data_format == \"channels_first\":\n+            input_dtype = x.dtype\n+            x = x.float()\n+            u = x.mean(1, keepdim=True)\n+            s = (x - u).pow(2).mean(1, keepdim=True)\n+            x = (x - u) / torch.sqrt(s + self.eps)\n+            x = x.to(dtype=input_dtype)\n+            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n+        return x\n+\n+\n+class Sam2MaskDecoder(nn.Module):\n+    def __init__(self, config: Sam2MaskDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+\n+        self.num_multimask_outputs = config.num_multimask_outputs\n+        self.num_mask_tokens = config.num_multimask_outputs + 1\n+\n+        self.iou_token = nn.Embedding(1, self.hidden_size)\n+        self.mask_tokens = nn.Embedding(self.num_mask_tokens, self.hidden_size)\n+\n+        self.transformer = Sam2TwoWayTransformer(config)\n+\n+        # should we create a new class for this?\n+        self.upscale_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.upscale_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+        self.upscale_layer_norm = Sam2LayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.activation = nn.GELU()\n+\n+        mlps_list = []\n+        for _ in range(self.num_mask_tokens):\n+            mlps_list += [Sam2FeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)]\n+        self.output_hypernetworks_mlps = nn.ModuleList(mlps_list)\n+        self.iou_prediction_head = Sam2FeedForward(\n+            self.hidden_size,\n+            config.iou_head_hidden_dim,\n+            self.num_mask_tokens,\n+            config.iou_head_depth,\n+            sigmoid_output=True,\n+        )\n+\n+        self.conv_s0 = nn.Conv2d(config.hidden_size, config.hidden_size // 8, kernel_size=1, stride=1)\n+        self.conv_s1 = nn.Conv2d(config.hidden_size, config.hidden_size // 4, kernel_size=1, stride=1)\n+\n+        self.obj_score_token = nn.Embedding(1, self.hidden_size)\n+        self.pred_obj_score_head = Sam2FeedForward(self.hidden_size, self.hidden_size, 1, 3)\n+\n+        self.dynamic_multimask_via_stability = config.dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = config.dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = config.dynamic_multimask_stability_thresh\n+\n+    def forward(\n+        self,\n+        image_embeddings: torch.Tensor,\n+        image_positional_embeddings: torch.Tensor,\n+        sparse_prompt_embeddings: torch.Tensor,\n+        dense_prompt_embeddings: torch.Tensor,\n+        multimask_output: bool,\n+        high_resolution_features: list[torch.Tensor],\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Predict masks given image and prompt embeddings.\n+\n+        Args:\n+            image_embeddings (`torch.Tensor`):\n+                The embeddings from the image encoder.\n+            image_positional_embeddings (`torch.Tensor`):\n+                Positional encoding with the shape of image_embeddings.\n+            sparse_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the points and boxes.\n+            dense_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the mask inputs.\n+            multimask_output (`bool`):\n+                Whether to return multiple masks or a single mask.\n+            high_resolution_features (`list[torch.Tensor]`, *optional*):\n+                The high-resolution features from the vision encoder.\n+            attention_similarity (`torch.Tensor`, *optional*):\n+                The attention similarity tensor.\n+            target_embedding (`torch.Tensor`, *optional*):\n+                The target embedding.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image_embeddings.shape\n+        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        # Concatenate output tokens\n+        output_tokens = torch.cat(\n+            [\n+                self.obj_score_token.weight,\n+                self.iou_token.weight,\n+                self.mask_tokens.weight,\n+            ],\n+            dim=0,\n+        )\n+        output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n+\n+        if sparse_prompt_embeddings.shape[0] != 0:\n+            tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)\n+        else:\n+            tokens = output_tokens\n+        point_embeddings = tokens.to(self.iou_token.weight.dtype)\n+\n+        # Expand per-image data in batch direction to be per-mask\n+        image_embeddings = image_embeddings + dense_prompt_embeddings\n+        image_embeddings = image_embeddings.repeat_interleave(point_batch_size, dim=0)\n+        image_positional_embeddings = image_positional_embeddings.repeat_interleave(point_batch_size, 0)\n+        # Run the transformer\n+        point_embeddings, image_embeddings = self.transformer(\n+            point_embeddings=point_embeddings,\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+        iou_token_out = point_embeddings[:, :, 1, :]\n+        mask_tokens_out = point_embeddings[:, :, 2 : (2 + self.num_mask_tokens), :]\n+\n+        # Upscale mask embeddings and predict masks using the mask tokens\n+        image_embeddings = image_embeddings.transpose(2, 3).view(\n+            batch_size * point_batch_size, num_channels, height, width\n+        )\n+\n+        feat_s0, feat_s1 = high_resolution_features\n+        feat_s0 = feat_s0.repeat_interleave(point_batch_size, dim=0)\n+        feat_s1 = feat_s1.repeat_interleave(point_batch_size, dim=0)\n+        upscaled_embedding = self.upscale_conv1(image_embeddings) + feat_s1\n+        upscaled_embedding = self.activation(self.upscale_layer_norm(upscaled_embedding))\n+        upscaled_embedding = self.activation(self.upscale_conv2(upscaled_embedding) + feat_s0)\n+\n+        hyper_in_list: list[torch.Tensor] = []\n+        for i in range(self.num_mask_tokens):\n+            current_mlp = self.output_hypernetworks_mlps[i]\n+            hyper_in_list += [current_mlp(mask_tokens_out[:, :, i, :])]\n+        hyper_in = torch.stack(hyper_in_list, dim=2)\n+\n+        _, num_channels, height, width = upscaled_embedding.shape\n+        upscaled_embedding = upscaled_embedding.view(batch_size, point_batch_size, num_channels, height * width)\n+        masks = (hyper_in @ upscaled_embedding).view(batch_size, point_batch_size, -1, height, width)\n+\n+        # Generate mask quality predictions\n+        iou_pred = self.iou_prediction_head(iou_token_out)\n+        object_score_logits = self.pred_obj_score_head(point_embeddings[:, :, 0, :])\n+\n+        # Select the correct mask or masks for output\n+        if multimask_output:\n+            mask_slice = slice(1, None)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+        elif self.dynamic_multimask_via_stability and not self.training:\n+            mask_slice = slice(0, 1)\n+            masks, iou_pred = self._dynamic_multimask_via_stability(masks, iou_pred)\n+        else:\n+            mask_slice = slice(0, 1)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+\n+        sam_tokens_out = mask_tokens_out[:, :, mask_slice]  # [b, 3, c] shape\n+\n+        return masks, iou_pred, sam_tokens_out, object_score_logits\n+\n+    def _get_stability_scores(self, mask_logits):\n+        \"\"\"\n+        Compute stability scores of the mask logits based on the IoU between upper and\n+        lower thresholds.\n+        \"\"\"\n+        mask_logits = mask_logits.flatten(-2)\n+        stability_delta = self.dynamic_multimask_stability_delta\n+        area_i = torch.sum(mask_logits > stability_delta, dim=-1).float()\n+        area_u = torch.sum(mask_logits > -stability_delta, dim=-1).float()\n+        stability_scores = torch.where(area_u > 0, area_i / area_u, 1.0)\n+        return stability_scores\n+\n+    def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n+        \"\"\"\n+        When outputting a single mask, if the stability score from the current single-mask\n+        output (based on output token 0) falls below a threshold, we instead select from\n+        multi-mask outputs (based on output token 1~3) the mask with the highest predicted\n+        IoU score. This is intended to ensure a valid mask for both clicking and tracking.\n+        \"\"\"\n+        # The best mask from multimask output tokens (1~3)\n+        multimask_logits = all_mask_logits[:, :, 1:, :, :]\n+        multimask_iou_scores = all_iou_scores[:, :, 1:]\n+        best_scores_inds = torch.argmax(multimask_iou_scores, dim=-1)  # [B, P]\n+        best_scores_inds_expanded = best_scores_inds.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n+        best_scores_inds_expanded = best_scores_inds_expanded.expand(\n+            -1, -1, 1, multimask_logits.size(-2), multimask_logits.size(-1)\n+        )\n+        best_multimask_logits = torch.gather(multimask_logits, 2, best_scores_inds_expanded)  # [B, P, 1, H, W]\n+        best_multimask_iou_scores = torch.gather(multimask_iou_scores, 2, best_scores_inds.unsqueeze(-1))  # [B, P, 1]\n+\n+        # The mask from singlemask output token 0 and its stability score\n+        singlemask_logits = all_mask_logits[:, :, 0:1, :, :]\n+        singlemask_iou_scores = all_iou_scores[:, :, 0:1]\n+        stability_scores = self._get_stability_scores(singlemask_logits)\n+        is_stable = stability_scores >= self.dynamic_multimask_stability_thresh\n+\n+        # Dynamically fall back to best multimask output upon low stability scores.\n+        mask_logits_out = torch.where(\n+            is_stable[..., None, None].expand_as(singlemask_logits),\n+            singlemask_logits,\n+            best_multimask_logits,\n+        )\n+        iou_scores_out = torch.where(\n+            is_stable.expand_as(singlemask_iou_scores),\n+            singlemask_iou_scores,\n+            best_multimask_iou_scores,\n+        )\n+        return mask_logits_out, iou_scores_out\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Segment Anything Model 2 (SAM 2) for generating segmentation masks, given an input image and\n+    input points and labels, boxes, or masks.\n+    \"\"\"\n+)\n+class Sam2Model(Sam2PreTrainedModel):\n+    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+    # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n+    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+    _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam2TwoWayAttentionBlock, index=2)}\n+    _keys_to_ignore_on_load_unexpected = [\n+        r\"^memory_.*\",\n+        r\"^mask_downsample.*\",\n+        r\"^object_pointer_proj.*\",\n+        r\"^temporal_positional_encoding_projection_layer.*\",\n+        \"no_memory_positional_encoding\",\n+        \"no_object_pointer\",\n+        \"occlusion_spatial_embedding_parameter\",\n+    ]\n+\n+    def __init__(self, config: Sam2Config):\n+        super().__init__(config)\n+        self.shared_image_embedding = Sam2PositionalEmbedding(config.prompt_encoder_config)\n+        self.vision_encoder = AutoModel.from_config(config.vision_config)\n+        self.prompt_encoder = Sam2PromptEncoder(config.prompt_encoder_config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n+        self.mask_decoder = Sam2MaskDecoder(config.mask_decoder_config)\n+\n+        self.num_feature_levels = config.vision_config.num_feature_levels\n+        self.backbone_feature_sizes = config.vision_config.backbone_feature_sizes\n+        # a single token to indicate no memory embedding from previous frames\n+        self.hidden_dim = config.vision_config.fpn_hidden_size\n+        self.no_memory_embedding = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n+\n+        self.post_init()\n+\n+    def _tie_weights(self):\n+        self.prompt_encoder.shared_embedding.positional_embedding.data = (\n+            self.shared_image_embedding.positional_embedding.data\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.vision_encoder.get_input_embeddings()\n+\n+    def get_image_wide_positional_embeddings(self) -> torch.Tensor:\n+        size = self.prompt_encoder.image_embedding_size\n+        target_device = self.shared_image_embedding.positional_embedding.device\n+        target_dtype = self.shared_image_embedding.positional_embedding.dtype\n+        grid = torch.ones(size, device=target_device, dtype=target_dtype)\n+        y_embed = grid.cumsum(dim=0) - 0.5\n+        x_embed = grid.cumsum(dim=1) - 0.5\n+        y_embed = y_embed / size[0]\n+        x_embed = x_embed / size[1]\n+\n+        positional_embedding = self.shared_image_embedding(torch.stack([x_embed, y_embed], dim=-1))\n+        return positional_embedding.permute(2, 0, 1).unsqueeze(0)  # channel x height x width\n+\n+    @torch.no_grad()\n+    def get_image_embeddings(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> list[torch.Tensor]:\n+        r\"\"\"\n+        Returns the image embeddings by passing the pixel values through the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Input pixel values\n+        \"\"\"\n+        batch_size = pixel_values.shape[0]\n+        feature_maps, _, _, _ = self.get_image_features(pixel_values, **kwargs)\n+\n+        # add no memory embedding to the last feature map\n+        feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+        # reshape feature maps to the same shape as the backbone feature sizes\n+        image_embeddings = [\n+            feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+            for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+        ]\n+\n+        return image_embeddings\n+\n+    @torch.no_grad()\n+    def get_prompt_embeddings(\n+        self,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+    ):\n+        r\"\"\"\n+        Returns the prompt embeddings by passing the input points, labels, boxes and masks through the prompt encoder.\n+\n+        Args:\n+            input_points (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_points_per_image, 2)`):\n+                Optional input points for the prompt encoder. The padding of the point is automatically done by the\n+                processor. `point_batch_size` refers to the number of masks that we want the model to predict per\n+                point. The model will output `point_batch_size` times 3 masks in total.\n+            input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points_per_image)`):\n+                Optional input labels for the prompt encoder. The padding of the labels is automatically done by the\n+                processor, or can be fed by the user.\n+            input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes_per_image, 4)`):\n+                Optional input boxes for the prompt encoder. The padding of the boxes is automatically done by the\n+                processor. users can also pass manually the input boxes.\n+            input_masks (`torch.LongTensor` of shape `(batch_size, image_size, image_size)`):\n+                Optional input masks for the prompt encoder.\n+        \"\"\"\n+        prompt_output = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        return prompt_output\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+        image_embeddings: Optional[torch.FloatTensor] = None,\n+        multimask_output: bool = True,\n+        attention_similarity: Optional[torch.FloatTensor] = None,\n+        target_embedding: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Sam2ImageSegmentationOutput:\n+        r\"\"\"\n+        input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+            Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+            better results. The points can be obtained by passing a list of list of list to the processor that will\n+            create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+            second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+            per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+            multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+            coordinates of the point. If a different number of points is passed either for each image, or for each\n+            mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+            computation of the embedding will be skipped for these points using the labels.\n+        input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+            Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+            official implementation, there are 3 types of labels\n+\n+            - `1`: the point is a point that contains the object of interest\n+            - `0`: the point is a point that does not contain the object of interest\n+            - `-1`: the point corresponds to the background\n+\n+            We added the label:\n+\n+            - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+            The padding labels should be automatically done by the processor.\n+        input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+            Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+            much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+            that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+            - `x1`: the x coordinate of the top left point of the input box\n+            - `y1`: the y coordinate of the top left point of the input box\n+            - `x2`: the x coordinate of the bottom right point of the input box\n+            - `y2`: the y coordinate of the bottom right point of the input box\n+        input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+            SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+            generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+            manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+            Image embeddings, this is used by the mask decoder to generate masks and iou scores. For more memory\n+            efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+            method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+        multimask_output (`bool`, *optional*):\n+            In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+            bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+            \"best\" mask, by specifying `multimask_output=False`.\n+        attention_similarity (`torch.FloatTensor`, *optional*):\n+            Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+            model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+        target_embedding (`torch.FloatTensor`, *optional*):\n+            Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+            the model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoModel, AutoProcessor\n+\n+        >>> model = AutoModel.from_pretrained(\"danelcsb/sam2.1_hiera_tiny\")\n+        >>> processor = AutoProcessor.from_pretrained(\"danelcsb/sam2.1_hiera_tiny\")\n+\n+        >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n+        >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        >>> input_points = [[[400, 650]]]  # 2D location of a window on the car\n+        >>> inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\")\n+\n+        >>> # Get segmentation mask\n+        >>> outputs = model(**inputs)\n+\n+        >>> # Postprocess masks\n+        >>> masks = processor.post_process_masks(\n+        ...     outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n+        ... )\n+        ```\n+        \"\"\"\n+        if not ((pixel_values is None) ^ (image_embeddings is None)):\n+            raise ValueError(\"Exactly one of pixel_values or image_embeddings must be provided.\")\n+        if input_points is not None and input_boxes is not None:\n+            if input_points.shape[1] != input_boxes.shape[1]:\n+                raise ValueError(\n+                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n+                        input_points.shape[1], input_boxes.shape[1]\n+                    )\n+                )\n+\n+        image_positional_embeddings = self.get_image_wide_positional_embeddings()\n+        # repeat with batch size\n+        batch_size = pixel_values.shape[0] if pixel_values is not None else image_embeddings[-1].shape[0]\n+        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n+\n+        vision_attentions = None\n+        vision_hidden_states = None\n+\n+        if pixel_values is not None:\n+            feature_maps, _, vision_hidden_states, vision_attentions = self.get_image_features(\n+                pixel_values,\n+                **kwargs,\n+            )\n+\n+            # add no memory embedding to the last feature map\n+            feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+            # reshape feature maps to the same shape as the backbone feature sizes\n+            image_embeddings = [\n+                feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+                for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+            ]\n+\n+        if input_points is not None and input_labels is None:\n+            input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n+\n+        if input_points is None and input_boxes is None:\n+            # If no points are provide, pad with an empty point (with label -1)\n+            input_points = torch.zeros(\n+                batch_size, 1, 1, 2, dtype=image_embeddings[-1].dtype, device=image_embeddings[-1].device\n+            )\n+            input_labels = -torch.ones(batch_size, 1, 1, dtype=torch.int32, device=image_embeddings[-1].device)\n+\n+        if input_masks is not None:\n+            # If mask_inputs is provided, downsize it into low-res mask input if needed\n+            # and feed it as a dense mask prompt into the SAM mask encoder\n+            if input_masks.shape[-2:] != self.prompt_encoder.mask_input_size:\n+                input_masks = F.interpolate(\n+                    input_masks.float(),\n+                    size=self.prompt_encoder.mask_input_size,\n+                    align_corners=False,\n+                    mode=\"bilinear\",\n+                    antialias=True,  # use antialias for downsampling\n+                ).to(input_masks.dtype)\n+\n+        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        low_res_multimasks, iou_scores, _, object_score_logits = self.mask_decoder(\n+            image_embeddings=image_embeddings[-1],\n+            image_positional_embeddings=image_positional_embeddings,\n+            sparse_prompt_embeddings=sparse_embeddings,\n+            dense_prompt_embeddings=dense_embeddings,\n+            multimask_output=multimask_output,\n+            high_resolution_features=image_embeddings[:-1],\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+\n+        return Sam2ImageSegmentationOutput(\n+            iou_scores=iou_scores,\n+            pred_masks=low_res_multimasks,\n+            object_score_logits=object_score_logits,\n+            image_embeddings=image_embeddings,\n+            vision_hidden_states=vision_hidden_states,\n+            vision_attentions=vision_attentions,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[\n+        list[torch.Tensor],\n+        list[torch.Tensor],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+    ]:\n+        r\"\"\"\n+        Extract and preprocess image features using the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Input pixel values of shape `(batch_size, num_channels, height, width)`.\n+\n+        Returns:\n+            `tuple`: A tuple containing:\n+                - feature_maps (`list[torch.Tensor]`): List of feature maps from different levels.\n+                - feature_maps_position_embeddings (`list[torch.Tensor]`): List of positional embeddings for each feature level.\n+                - vision_hidden_states (`tuple[torch.FloatTensor]`, *optional*): Hidden states from the vision encoder.\n+                - vision_attentions (`tuple[torch.FloatTensor]`, *optional*): Attention weights from the vision encoder.\n+        \"\"\"\n+        vision_outputs: Sam2VisionEncoderOutput = self.vision_encoder(\n+            pixel_values,\n+            **kwargs,\n+        )\n+\n+        feature_maps = vision_outputs.fpn_hidden_states\n+        feature_maps_position_embeddings = vision_outputs.fpn_position_encoding\n+\n+        # precompute projected level 0 and level 1 features in SAM decoder\n+        # to avoid running it again on every SAM click\n+        feature_maps = list(feature_maps)\n+        feature_maps[0] = self.mask_decoder.conv_s0(feature_maps[0])\n+        feature_maps[1] = self.mask_decoder.conv_s1(feature_maps[1])\n+\n+        # flatten NxCxHxW to HWxNxC\n+        feature_maps = [feature_map.flatten(2).permute(2, 0, 1) for feature_map in feature_maps]\n+        feature_maps_position_embeddings = [\n+            feature_map_position_embedding.flatten(2).permute(2, 0, 1)\n+            for feature_map_position_embedding in feature_maps_position_embeddings\n+        ]\n+\n+        return feature_maps, feature_maps_position_embeddings, vision_outputs.hidden_states, vision_outputs.attentions\n+\n+\n+__all__ = [\"Sam2Model\", \"Sam2VisionModel\", \"Sam2PreTrainedModel\", \"Sam2HieraDetModel\"]"
        },
        {
            "sha": "8976ba0098b0b29119d8c8723ee860422bf19ee1",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "added",
            "additions": 1474,
            "deletions": 0,
            "changes": 1474,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,1474 @@\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch SAM 2 model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+\n+from transformers.models.maskformer.modeling_maskformer import MaskFormerSinePositionEmbedding\n+from transformers.models.sam.image_processing_sam_fast import SamImageProcessorFast\n+from transformers.models.sam.modeling_sam import (\n+    SamLayerNorm,\n+    SamMaskDecoder,\n+    SamMaskEmbedding,\n+    SamModel,\n+    SamPromptEncoder,\n+    SamTwoWayAttentionBlock,\n+    SamTwoWayTransformer,\n+    eager_attention_forward,\n+)\n+from transformers.models.vitdet.modeling_vitdet import window_partition, window_unpartition\n+from transformers.utils.generic import TransformersKwargs, check_model_inputs\n+\n+from ...activations import ACT2FN\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    pil_torch_interpolation_mapping,\n+)\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ..auto import AutoModel\n+from .configuration_sam2 import (\n+    Sam2Config,\n+    Sam2HieraDetConfig,\n+    Sam2MaskDecoderConfig,\n+    Sam2PromptEncoderConfig,\n+    Sam2VisionConfig,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch.nn import functional as F\n+\n+if is_torchvision_v2_available():\n+    pass\n+elif is_torchvision_available():\n+    pass\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Sam2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    mask_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n+    \"\"\"\n+\n+    mask_size: Optional[dict[str, int]]\n+\n+\n+@auto_docstring\n+class Sam2ImageProcessorFast(SamImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 1024, \"width\": 1024}\n+    mask_size = {\"height\": 256, \"width\": 256}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    valid_kwargs = Sam2FastImageProcessorKwargs\n+\n+    # modular artefacts\n+    do_pad = None\n+    pad_size = None\n+    mask_pad_size = None\n+\n+    def __init__(self, **kwargs: Unpack[Sam2FastImageProcessorKwargs]):\n+        SamImageProcessorFast().__init__(**kwargs)\n+\n+    def pad_image():\n+        raise NotImplementedError(\"No pad_image for SAM 2.\")\n+\n+    def _get_preprocess_shape():\n+        raise NotImplementedError(\"No _get_preprocess_shape for SAM 2.\")\n+\n+    def resize():\n+        raise NotImplementedError(\"No need to override resize for SAM 2.\")\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        return SamImageProcessorFast()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[Sam2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        original_sizes = [image.shape[-2:] for image in images]\n+        images_kwargs = kwargs.copy()\n+        pixel_values = self._preprocess(images, **images_kwargs)\n+        reshaped_input_sizes = [image.shape[-2:] for image in images]\n+        data = {\n+            \"pixel_values\": pixel_values,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            )\n+            data[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        mask_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if mask_size is not None:\n+            mask_size = SizeDict(**get_size_dict(mask_size, param_name=\"mask_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"mask_size\"] = mask_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+    def _apply_non_overlapping_constraints(self, pred_masks: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Apply non-overlapping constraints to the object scores in pred_masks. Here we\n+        keep only the highest scoring object at each spatial location in pred_masks.\n+        \"\"\"\n+        batch_size = pred_masks.size(0)\n+        if batch_size == 1:\n+            return pred_masks\n+\n+        device = pred_masks.device\n+        # \"max_obj_inds\": object index of the object with the highest score at each location\n+        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)\n+        # \"batch_obj_inds\": object index of each object slice (along dim 0) in `pred_masks`\n+        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]\n+        keep = max_obj_inds == batch_obj_inds\n+        # suppress overlapping regions' scores below -10.0 so that the foreground regions\n+        # don't overlap (here sigmoid(-10.0)=4.5398e-05)\n+        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[torch.Tensor, List[torch.Tensor], np.ndarray, List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n+            original_sizes = original_sizes.tolist()\n+        # TODO: add connected components kernel for postprocessing\n+        output_masks = []\n+        for i, original_size in enumerate(original_sizes):\n+            if isinstance(masks[i], np.ndarray):\n+                masks[i] = torch.from_numpy(masks[i])\n+            elif not isinstance(masks[i], torch.Tensor):\n+                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+            interpolated_mask = F.interpolate(masks[i], original_size, mode=\"bilinear\", align_corners=False)\n+            if apply_non_overlapping_constraints:\n+                interpolated_mask = self._apply_non_overlapping_constraints(interpolated_mask)\n+            if binarize:\n+                interpolated_mask = interpolated_mask > mask_threshold\n+            output_masks.append(interpolated_mask)\n+\n+        return output_masks\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the vision encoder's outputs.\")\n+class Sam2VisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, height, width, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    fpn_hidden_states (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Feature maps from the Feature Pyramid Network neck.\n+    fpn_position_encoding (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Positional encodings corresponding to the `fpn_hidden_states`.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`. Hidden-states of the\n+        model at the output of each stage.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    fpn_hidden_states: Optional[torch.FloatTensor] = None\n+    fpn_position_encoding: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the Sam2 model's output.\")\n+class Sam2ImageSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    iou_scores (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks)`):\n+        The Intersection over Union (IoU) scores of the predicted masks.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks, height, width)`):\n+        The predicted low-resolution masks. This is an alias for `low_res_masks`. These masks need to be post-processed\n+        by the processor to be brought to the original image size.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size, point_batch_size, 1)`):\n+        Logits for the object score, indicating if an object is present.\n+    image_embeddings (`tuple(torch.FloatTensor)`):\n+        The features from the FPN, which are used by the mask decoder. This is a tuple of `torch.FloatTensor` where each\n+        tensor has shape `(batch_size, channels, height, width)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`.\n+        Hidden-states of the vision model at the output of each stage.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the vision model.\n+    mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the mask decoder.\n+    \"\"\"\n+\n+    iou_scores: torch.FloatTensor = None\n+    pred_masks: torch.FloatTensor = None\n+    object_score_logits: torch.FloatTensor = None\n+    image_embeddings: tuple[torch.FloatTensor, ...] = None\n+    vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    mask_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class Sam2PatchEmbeddings(nn.Module):\n+    r\"\"\"\n+    Turns pixel values into patch embeddings for transformer consumption.\n+\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`Sam2ImageProcessorFast.__call__`] for details.\n+\n+    Returns:\n+        embeddings (`torch.FloatTensor`):\n+            Patch embeddings depend on image_size, patch_kernel_size, patch_stride and patch_padding\n+    \"\"\"\n+\n+    def __init__(self, config: Sam2HieraDetConfig):\n+        super().__init__()\n+        num_channels = config.num_channels\n+        hidden_size = config.hidden_size\n+\n+        self.projection = nn.Conv2d(\n+            num_channels,\n+            hidden_size,\n+            kernel_size=config.patch_kernel_size,\n+            stride=config.patch_stride,\n+            padding=config.patch_padding,\n+        )\n+\n+    def forward(self, pixel_values):\n+        _, num_channels, height, width = pixel_values.shape\n+        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        return embeddings\n+\n+\n+class Sam2SinePositionEmbedding(MaskFormerSinePositionEmbedding):\n+    pass\n+\n+\n+class Sam2VisionNeck(nn.Module):\n+    def __init__(self, config: Sam2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.position_encoding = Sam2SinePositionEmbedding(num_pos_feats=config.fpn_hidden_size // 2, normalize=True)\n+        self.convs = nn.ModuleList()\n+        for in_channels in config.backbone_channel_list:\n+            self.convs.append(\n+                nn.Conv2d(\n+                    in_channels=in_channels,\n+                    out_channels=config.fpn_hidden_size,\n+                    kernel_size=config.fpn_kernel_size,\n+                    stride=config.fpn_stride,\n+                    padding=config.fpn_padding,\n+                ),\n+            )\n+        self.fpn_top_down_levels = config.fpn_top_down_levels\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[tuple[torch.Tensor, ...], tuple[torch.Tensor, ...]]:\n+        fpn_hidden_states = ()\n+        fpn_position_encoding = ()\n+\n+        # forward in top-down order (from low to high resolution)\n+        n = len(self.convs) - 1\n+        for i in range(n, -1, -1):\n+            lateral_features = hidden_states[i].permute(0, 3, 1, 2)\n+            lateral_features = self.convs[n - i](lateral_features)\n+            if i not in self.fpn_top_down_levels or i == n:\n+                prev_features = lateral_features\n+            else:\n+                top_down_features = F.interpolate(\n+                    prev_features.to(dtype=torch.float32),\n+                    scale_factor=2.0,\n+                    mode=\"nearest\",\n+                    align_corners=None,\n+                    antialias=False,\n+                ).to(lateral_features.dtype)\n+                prev_features = lateral_features + top_down_features\n+\n+            prev_position_encoding = self.position_encoding(\n+                prev_features.shape, prev_features.device, prev_features.dtype\n+            ).to(prev_features.dtype)\n+\n+            fpn_hidden_states += (prev_features,)\n+            fpn_position_encoding += (prev_position_encoding,)\n+\n+        return fpn_hidden_states, fpn_position_encoding\n+\n+\n+def do_pool(x: torch.Tensor, query_stride: Optional[int] = None) -> torch.Tensor:\n+    if query_stride is None:\n+        return x\n+    # (B, H, W, C) -> (B, C, H, W)\n+    x = x.permute(0, 3, 1, 2)\n+    x = nn.functional.max_pool2d(x, kernel_size=query_stride, stride=query_stride, ceil_mode=False)\n+    # (B, C, H', W') -> (B, H', W', C)\n+    x = x.permute(0, 2, 3, 1)\n+    return x\n+\n+\n+class Sam2MultiScaleAttention(nn.Module):\n+    def __init__(\n+        self,\n+        config: Sam2HieraDetConfig,\n+        dim: int,\n+        dim_out: int,\n+        num_attention_heads: int,\n+        query_stride: Optional[tuple[int, int]] = None,\n+    ):\n+        super().__init__()\n+\n+        self.config = config\n+\n+        self.dim = dim\n+        self.dim_out = dim_out\n+        self.query_stride = query_stride\n+\n+        self.num_attention_heads = num_attention_heads\n+        head_dim = dim_out // num_attention_heads\n+        self.scale = head_dim**-0.5\n+        self.qkv = nn.Linear(dim, dim_out * 3)\n+        self.proj = nn.Linear(dim_out, dim_out)\n+\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor, **kwargs) -> torch.Tensor:\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (B, H * W, 3, nHead, C)\n+        qkv = self.qkv(hidden_states).reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+        # q, k, v with shape (B, H * W, nheads, C)\n+        query, key, value = torch.unbind(qkv, 2)\n+\n+        attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n+        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n+\n+        # Q pooling (for downsample at stage changes)\n+        if self.query_stride:\n+            query = do_pool(query.reshape(batch_size, height, width, -1), self.query_stride)\n+            height, width = query.shape[1:3]  # downsampled shape\n+            query = query.reshape(batch_size, height * width, self.num_attention_heads, -1)\n+\n+        # transpose query, key, value to (B, nHead, H * W, C)\n+        query = query.transpose(1, 2)\n+        key = key.transpose(1, 2)\n+        value = value.transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+        attn_output, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, height, width, -1)\n+\n+        attn_output = self.proj(attn_output)\n+\n+        return attn_output\n+\n+\n+class Sam2FeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        input_dim: int,\n+        hidden_dim: int,\n+        output_dim: int,\n+        num_layers: int,\n+        activation: str = \"relu\",\n+        sigmoid_output: bool = False,\n+    ):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        self.activation = ACT2FN[activation]\n+        self.proj_in = nn.Linear(input_dim, hidden_dim)\n+        self.proj_out = nn.Linear(hidden_dim, output_dim)\n+        self.layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n+        self.sigmoid_output = sigmoid_output\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.proj_in(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        for layer in self.layers:\n+            hidden_states = self.activation(layer(hidden_states))\n+\n+        hidden_states = self.proj_out(hidden_states)\n+        if self.sigmoid_output:\n+            hidden_states = F.sigmoid(hidden_states)\n+        return hidden_states\n+\n+\n+class Sam2MultiScaleBlock(GradientCheckpointingLayer):\n+    def __init__(\n+        self,\n+        config: Sam2HieraDetConfig,\n+        stage_idx: int,\n+        block_idx: int,\n+        total_block_idx: int,\n+    ):\n+        super().__init__()\n+\n+        # take embed dim from previous stage if first block of stage\n+        self.dim = (\n+            config.embed_dim_per_stage[stage_idx - 1]\n+            if stage_idx > 0 and block_idx == 0\n+            else config.embed_dim_per_stage[stage_idx]\n+        )\n+        self.dim_out = config.embed_dim_per_stage[stage_idx]\n+        self.layer_norm1 = nn.LayerNorm(self.dim, eps=config.layer_norm_eps)\n+        # take window size from previous stage if first block of stage\n+        self.window_size = (\n+            config.window_size_per_stage[stage_idx - 1]\n+            if stage_idx > 0 and block_idx == 0\n+            else config.window_size_per_stage[stage_idx]\n+        )\n+        self.window_size = 0 if total_block_idx in config.global_attention_blocks else self.window_size\n+        # use query stride for first block of stage if stage is a query pool stage\n+        self.query_stride = (\n+            config.query_stride if 0 < stage_idx <= config.num_query_pool_stages and block_idx == 0 else None\n+        )\n+\n+        self.attn = Sam2MultiScaleAttention(\n+            config,\n+            self.dim,\n+            self.dim_out,\n+            num_attention_heads=config.num_attention_heads_per_stage[stage_idx],\n+            query_stride=self.query_stride,\n+        )\n+        self.layer_norm2 = nn.LayerNorm(self.dim_out, eps=config.layer_norm_eps)\n+        self.mlp = Sam2FeedForward(\n+            self.dim_out,\n+            int(self.dim_out * config.mlp_ratio),\n+            self.dim_out,\n+            num_layers=2,\n+            activation=config.hidden_act,\n+        )\n+        if self.dim != self.dim_out:\n+            self.proj = nn.Linear(self.dim, self.dim_out)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states  # batch_size, height, width, channel\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+\n+        # Skip connection\n+        if self.dim != self.dim_out:\n+            residual = do_pool(self.proj(hidden_states), self.query_stride)\n+\n+        # Window partition\n+        window_size = self.window_size\n+        if self.window_size > 0:\n+            H, W = hidden_states.shape[1], hidden_states.shape[2]\n+            hidden_states, pad_hw = window_partition(hidden_states, window_size)\n+\n+        # Window Attention + Q Pooling (if stage change)\n+        attn_output = self.attn(\n+            hidden_states=hidden_states,\n+            **kwargs,\n+        )\n+        hidden_states = attn_output\n+        if self.query_stride:\n+            # Shapes have changed due to Q pooling\n+            window_size = self.window_size // self.query_stride[0]\n+            H, W = residual.shape[1:3]\n+\n+            pad_h = (window_size - H % window_size) % window_size\n+            pad_w = (window_size - W % window_size) % window_size\n+            pad_hw = (H + pad_h, W + pad_w)\n+\n+        # Reverse window partition\n+        if self.window_size > 0:\n+            hidden_states = window_unpartition(hidden_states, window_size, pad_hw, (H, W))\n+\n+        hidden_states = residual + hidden_states\n+        layernorm_output = self.layer_norm2(hidden_states)\n+        hidden_states = hidden_states + self.mlp(layernorm_output)\n+\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Hiera model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n+class Sam2HieraDetModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, height, width, hidden_size)`):\n+        hidden-states at the output of the last layer of the model.\n+    intermediate_hidden_states (`tuple[torch.FloatTensor]` of shape `(batch_size, height, width, hidden_size)`):\n+        Sequence of hidden-states at the output of the intermediate layers of the model.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class Sam2PreTrainedModel(PreTrainedModel):\n+    config_class = Sam2Config\n+    base_model_prefix = \"sam2\"\n+    main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (nn.LayerNorm, Sam2LayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        if isinstance(module, Sam2HieraDetModel):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n+            if module.pos_embed_window is not None:\n+                module.pos_embed_window.data.zero_()\n+        if isinstance(module, Sam2Model):\n+            if module.no_memory_embedding is not None:\n+                module.no_memory_embedding.data.zero_()\n+\n+\n+class Sam2HieraDetModel(Sam2PreTrainedModel):\n+    config_class = Sam2HieraDetConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Sam2MultiScaleBlock,\n+        \"attentions\": Sam2MultiScaleAttention,\n+    }\n+\n+    def __init__(self, config: Sam2HieraDetConfig):\n+        super().__init__(config)\n+\n+        self.patch_embed = Sam2PatchEmbeddings(config)\n+        # Windowed positional embedding (https://arxiv.org/abs/2311.05613)\n+        self.pos_embed = nn.Parameter(\n+            torch.zeros(1, config.hidden_size, *config.window_positional_embedding_background_size)\n+        )\n+        self.pos_embed_window = nn.Parameter(\n+            torch.zeros(1, config.hidden_size, config.window_size_per_stage[0], config.window_size_per_stage[0])\n+        )\n+        self.stage_ends = (np.cumsum(config.blocks_per_stage) - 1).tolist()\n+        self.blocks = nn.ModuleList()\n+        total_block_idx = 0\n+        for stage_idx, blocks_per_stage in enumerate(config.blocks_per_stage):\n+            for block_idx in range(blocks_per_stage):\n+                block = Sam2MultiScaleBlock(\n+                    config=config, stage_idx=stage_idx, block_idx=block_idx, total_block_idx=total_block_idx\n+                )\n+                self.blocks.append(block)\n+                total_block_idx += 1\n+\n+    def get_input_embeddings(self):\n+        return self.patch_embed\n+\n+    def _get_pos_embed(self, hw: tuple[int, int]) -> torch.Tensor:\n+        h, w = hw\n+        window_embed = self.pos_embed_window\n+        pos_embed = F.interpolate(self.pos_embed, size=(h, w), mode=\"bicubic\")\n+        pos_embed = pos_embed + window_embed.tile([x // y for x, y in zip(pos_embed.shape, window_embed.shape)])\n+        pos_embed = pos_embed.permute(0, 2, 3, 1)\n+        return pos_embed\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Sam2HieraDetModelOutput]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.patch_embed(pixel_values)\n+        hidden_states = hidden_states + self._get_pos_embed(hidden_states.shape[1:3])\n+\n+        intermediate_hidden_states = ()\n+        for i, block_module in enumerate(self.blocks):\n+            hidden_states = block_module(hidden_states, **kwargs)\n+\n+            if i in self.stage_ends:\n+                intermediate_hidden_states = intermediate_hidden_states + (hidden_states,)\n+\n+        return Sam2HieraDetModelOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate_hidden_states,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from Sam without any head or projection on top.\n+    \"\"\"\n+)\n+class Sam2VisionModel(Sam2PreTrainedModel):\n+    config_class = Sam2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Sam2MultiScaleBlock,\n+        \"attentions\": Sam2MultiScaleAttention,\n+    }\n+\n+    def __init__(self, config: Sam2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.backbone = AutoModel.from_config(config.backbone_config)\n+\n+        self.neck = Sam2VisionNeck(config)\n+        self.num_feature_levels = config.num_feature_levels\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.backbone.get_input_embeddings()\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Sam2VisionEncoderOutput]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        # Forward through backbone\n+        backbone_output = self.backbone(pixel_values, **kwargs)\n+        hidden_states = backbone_output.last_hidden_state\n+        intermediate_hidden_states = backbone_output.intermediate_hidden_states\n+\n+        fpn_hidden_states, fpn_position_encoding = self.neck(intermediate_hidden_states)\n+        # Select last `num_feature_levels` feature levels from FPN and reverse order to get features from high to low resolution\n+        fpn_hidden_states = fpn_hidden_states[-self.num_feature_levels :][::-1]\n+        fpn_position_encoding = fpn_position_encoding[-self.num_feature_levels :][::-1]\n+\n+        return Sam2VisionEncoderOutput(\n+            last_hidden_state=hidden_states,\n+            fpn_hidden_states=fpn_hidden_states,\n+            fpn_position_encoding=fpn_position_encoding,\n+        )\n+\n+\n+class Sam2PositionalEmbedding(nn.Module):\n+    def __init__(self, config: Sam2PromptEncoderConfig):\n+        super().__init__()\n+        self.scale = config.scale\n+        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n+        self.register_buffer(\"positional_embedding\", positional_embedding)\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+        coordinates.to(torch.float32)\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n+class Sam2MaskEmbedding(SamMaskEmbedding):\n+    pass\n+\n+\n+class Sam2PromptEncoder(SamPromptEncoder):\n+    def __init__(self, config: Sam2PromptEncoderConfig):\n+        SamPromptEncoder().__init__()\n+        self.shared_embedding = Sam2PositionalEmbedding(config)\n+        self.mask_embed = Sam2MaskEmbedding(config)\n+        self.no_mask_embed = nn.Embedding(1, config.hidden_size)\n+\n+        self.image_embedding_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+        self.mask_input_size = (4 * config.image_size // config.patch_size, 4 * config.image_size // config.patch_size)\n+        self.input_image_size = config.image_size\n+\n+        self.point_embed = nn.Embedding(config.num_point_embeddings, config.hidden_size)\n+        self.hidden_size = config.hidden_size\n+        self.not_a_point_embed = nn.Embedding(1, config.hidden_size)\n+\n+    def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -> torch.Tensor:\n+        \"\"\"Embeds point prompts.\"\"\"\n+        points = points + 0.5  # Shift to center of pixel\n+        if pad:\n+            points = torch.nn.functional.pad(points, (0, 0, 0, 1), mode=\"constant\", value=0)\n+            labels = torch.nn.functional.pad(labels, (0, 1), mode=\"constant\", value=-1)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        point_embedding = self.shared_embedding(points, input_shape)\n+\n+        # torch.where and expanding the labels tensor is required by the ONNX export\n+        point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n+\n+        # This is required for the ONNX export. The dtype, device need to be explicitely\n+        # specificed as otherwise torch.onnx.export interprets as double\n+        point_embedding = torch.where(\n+            labels[..., None] != -10,\n+            point_embedding,\n+            torch.zeros_like(point_embedding),\n+        )\n+\n+        # Add point embeddings for labels >= 0\n+        point_embedding = point_embedding + self.point_embed(labels.clamp(min=0)) * (labels >= 0).unsqueeze(-1)\n+\n+        return point_embedding\n+\n+    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Embeds box prompts.\"\"\"\n+        boxes = boxes + 0.5  # Shift to center of pixel\n+        batch_size, nb_boxes = boxes.shape[:2]\n+        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        corner_embedding = self.shared_embedding(coords, input_shape)\n+        corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n+        corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        return corner_embedding\n+\n+\n+class Sam2Attention(nn.Module):\n+    \"\"\"\n+    SAM2's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__()\n+        downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.internal_dim = config.hidden_size // downsample_rate\n+        self.num_attention_heads = config.num_attention_heads\n+        self.head_dim = self.internal_dim // config.num_attention_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.o_proj = nn.Linear(self.internal_dim, self.hidden_size)\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Input projections\n+        batch_size, point_batch_size = query.shape[:2]\n+        new_shape = (batch_size * point_batch_size, -1, self.num_attention_heads, self.head_dim)\n+\n+        query = self.q_proj(query).view(*new_shape).transpose(1, 2)\n+        key = self.k_proj(key).view(*new_shape).transpose(1, 2)\n+        value = self.v_proj(value).view(*new_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_similarity,\n+            dropout=0.0 if not self.training else self.dropout_p,\n+            scaling=self.scaling,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(\n+            batch_size, point_batch_size, -1, self.num_attention_heads * self.head_dim\n+        ).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Sam2TwoWayAttentionBlock(SamTwoWayAttentionBlock, GradientCheckpointingLayer):\n+    def __init__(self, config: Sam2MaskDecoderConfig, skip_first_layer_pe: bool = False):\n+        SamTwoWayAttentionBlock().__init__()\n+        self.self_attn = Sam2Attention(config, downsample_rate=1)\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n+\n+        self.cross_attn_token_to_image = Sam2Attention(config)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n+\n+        self.mlp = Sam2FeedForward(\n+            config.hidden_size, config.mlp_dim, config.hidden_size, num_layers=config.num_hidden_layers\n+        )\n+        self.layer_norm3 = nn.LayerNorm(config.hidden_size)\n+\n+        self.layer_norm4 = nn.LayerNorm(config.hidden_size)\n+        self.cross_attn_image_to_token = Sam2Attention(config)\n+\n+        self.skip_first_layer_pe = skip_first_layer_pe\n+\n+\n+class Sam2TwoWayTransformer(SamTwoWayTransformer):\n+    pass\n+\n+\n+class Sam2LayerNorm(SamLayerNorm):\n+    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_first\"):\n+        super().__init__()\n+\n+\n+class Sam2MaskDecoder(SamMaskDecoder):\n+    def __init__(self, config: Sam2MaskDecoderConfig):\n+        super().__init__(config)\n+        del self.iou_prediction_head\n+        self.iou_prediction_head = Sam2FeedForward(\n+            self.hidden_size,\n+            config.iou_head_hidden_dim,\n+            self.num_mask_tokens,\n+            config.iou_head_depth,\n+            sigmoid_output=True,\n+        )\n+\n+        self.conv_s0 = nn.Conv2d(config.hidden_size, config.hidden_size // 8, kernel_size=1, stride=1)\n+        self.conv_s1 = nn.Conv2d(config.hidden_size, config.hidden_size // 4, kernel_size=1, stride=1)\n+\n+        self.obj_score_token = nn.Embedding(1, self.hidden_size)\n+        self.pred_obj_score_head = Sam2FeedForward(self.hidden_size, self.hidden_size, 1, 3)\n+\n+        self.dynamic_multimask_via_stability = config.dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = config.dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = config.dynamic_multimask_stability_thresh\n+\n+    def _get_stability_scores(self, mask_logits):\n+        \"\"\"\n+        Compute stability scores of the mask logits based on the IoU between upper and\n+        lower thresholds.\n+        \"\"\"\n+        mask_logits = mask_logits.flatten(-2)\n+        stability_delta = self.dynamic_multimask_stability_delta\n+        area_i = torch.sum(mask_logits > stability_delta, dim=-1).float()\n+        area_u = torch.sum(mask_logits > -stability_delta, dim=-1).float()\n+        stability_scores = torch.where(area_u > 0, area_i / area_u, 1.0)\n+        return stability_scores\n+\n+    def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n+        \"\"\"\n+        When outputting a single mask, if the stability score from the current single-mask\n+        output (based on output token 0) falls below a threshold, we instead select from\n+        multi-mask outputs (based on output token 1~3) the mask with the highest predicted\n+        IoU score. This is intended to ensure a valid mask for both clicking and tracking.\n+        \"\"\"\n+        # The best mask from multimask output tokens (1~3)\n+        multimask_logits = all_mask_logits[:, :, 1:, :, :]\n+        multimask_iou_scores = all_iou_scores[:, :, 1:]\n+        best_scores_inds = torch.argmax(multimask_iou_scores, dim=-1)  # [B, P]\n+        best_scores_inds_expanded = best_scores_inds.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n+        best_scores_inds_expanded = best_scores_inds_expanded.expand(\n+            -1, -1, 1, multimask_logits.size(-2), multimask_logits.size(-1)\n+        )\n+        best_multimask_logits = torch.gather(multimask_logits, 2, best_scores_inds_expanded)  # [B, P, 1, H, W]\n+        best_multimask_iou_scores = torch.gather(multimask_iou_scores, 2, best_scores_inds.unsqueeze(-1))  # [B, P, 1]\n+\n+        # The mask from singlemask output token 0 and its stability score\n+        singlemask_logits = all_mask_logits[:, :, 0:1, :, :]\n+        singlemask_iou_scores = all_iou_scores[:, :, 0:1]\n+        stability_scores = self._get_stability_scores(singlemask_logits)\n+        is_stable = stability_scores >= self.dynamic_multimask_stability_thresh\n+\n+        # Dynamically fall back to best multimask output upon low stability scores.\n+        mask_logits_out = torch.where(\n+            is_stable[..., None, None].expand_as(singlemask_logits),\n+            singlemask_logits,\n+            best_multimask_logits,\n+        )\n+        iou_scores_out = torch.where(\n+            is_stable.expand_as(singlemask_iou_scores),\n+            singlemask_iou_scores,\n+            best_multimask_iou_scores,\n+        )\n+        return mask_logits_out, iou_scores_out\n+\n+    def forward(\n+        self,\n+        image_embeddings: torch.Tensor,\n+        image_positional_embeddings: torch.Tensor,\n+        sparse_prompt_embeddings: torch.Tensor,\n+        dense_prompt_embeddings: torch.Tensor,\n+        multimask_output: bool,\n+        high_resolution_features: list[torch.Tensor],\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Predict masks given image and prompt embeddings.\n+\n+        Args:\n+            image_embeddings (`torch.Tensor`):\n+                The embeddings from the image encoder.\n+            image_positional_embeddings (`torch.Tensor`):\n+                Positional encoding with the shape of image_embeddings.\n+            sparse_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the points and boxes.\n+            dense_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the mask inputs.\n+            multimask_output (`bool`):\n+                Whether to return multiple masks or a single mask.\n+            high_resolution_features (`list[torch.Tensor]`, *optional*):\n+                The high-resolution features from the vision encoder.\n+            attention_similarity (`torch.Tensor`, *optional*):\n+                The attention similarity tensor.\n+            target_embedding (`torch.Tensor`, *optional*):\n+                The target embedding.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image_embeddings.shape\n+        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        # Concatenate output tokens\n+        output_tokens = torch.cat(\n+            [\n+                self.obj_score_token.weight,\n+                self.iou_token.weight,\n+                self.mask_tokens.weight,\n+            ],\n+            dim=0,\n+        )\n+        output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n+\n+        if sparse_prompt_embeddings.shape[0] != 0:\n+            tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)\n+        else:\n+            tokens = output_tokens\n+        point_embeddings = tokens.to(self.iou_token.weight.dtype)\n+\n+        # Expand per-image data in batch direction to be per-mask\n+        image_embeddings = image_embeddings + dense_prompt_embeddings\n+        image_embeddings = image_embeddings.repeat_interleave(point_batch_size, dim=0)\n+        image_positional_embeddings = image_positional_embeddings.repeat_interleave(point_batch_size, 0)\n+        # Run the transformer\n+        point_embeddings, image_embeddings = self.transformer(\n+            point_embeddings=point_embeddings,\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+        iou_token_out = point_embeddings[:, :, 1, :]\n+        mask_tokens_out = point_embeddings[:, :, 2 : (2 + self.num_mask_tokens), :]\n+\n+        # Upscale mask embeddings and predict masks using the mask tokens\n+        image_embeddings = image_embeddings.transpose(2, 3).view(\n+            batch_size * point_batch_size, num_channels, height, width\n+        )\n+\n+        feat_s0, feat_s1 = high_resolution_features\n+        feat_s0 = feat_s0.repeat_interleave(point_batch_size, dim=0)\n+        feat_s1 = feat_s1.repeat_interleave(point_batch_size, dim=0)\n+        upscaled_embedding = self.upscale_conv1(image_embeddings) + feat_s1\n+        upscaled_embedding = self.activation(self.upscale_layer_norm(upscaled_embedding))\n+        upscaled_embedding = self.activation(self.upscale_conv2(upscaled_embedding) + feat_s0)\n+\n+        hyper_in_list: list[torch.Tensor] = []\n+        for i in range(self.num_mask_tokens):\n+            current_mlp = self.output_hypernetworks_mlps[i]\n+            hyper_in_list += [current_mlp(mask_tokens_out[:, :, i, :])]\n+        hyper_in = torch.stack(hyper_in_list, dim=2)\n+\n+        _, num_channels, height, width = upscaled_embedding.shape\n+        upscaled_embedding = upscaled_embedding.view(batch_size, point_batch_size, num_channels, height * width)\n+        masks = (hyper_in @ upscaled_embedding).view(batch_size, point_batch_size, -1, height, width)\n+\n+        # Generate mask quality predictions\n+        iou_pred = self.iou_prediction_head(iou_token_out)\n+        object_score_logits = self.pred_obj_score_head(point_embeddings[:, :, 0, :])\n+\n+        # Select the correct mask or masks for output\n+        if multimask_output:\n+            mask_slice = slice(1, None)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+        elif self.dynamic_multimask_via_stability and not self.training:\n+            mask_slice = slice(0, 1)\n+            masks, iou_pred = self._dynamic_multimask_via_stability(masks, iou_pred)\n+        else:\n+            mask_slice = slice(0, 1)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+\n+        sam_tokens_out = mask_tokens_out[:, :, mask_slice]  # [b, 3, c] shape\n+\n+        return masks, iou_pred, sam_tokens_out, object_score_logits\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Segment Anything Model 2 (SAM 2) for generating segmentation masks, given an input image and\n+    input points and labels, boxes, or masks.\n+    \"\"\"\n+)\n+class Sam2Model(SamModel):\n+    _keys_to_ignore_on_load_unexpected = [\n+        r\"^memory_.*\",\n+        r\"^mask_downsample.*\",\n+        r\"^object_pointer_proj.*\",\n+        r\"^temporal_positional_encoding_projection_layer.*\",\n+        \"no_memory_positional_encoding\",\n+        \"no_object_pointer\",\n+        \"occlusion_spatial_embedding_parameter\",\n+    ]\n+\n+    def __init__(self, config: Sam2Config):\n+        SamModel().__init__(config)\n+        self.shared_image_embedding = Sam2PositionalEmbedding(config.prompt_encoder_config)\n+        self.vision_encoder = AutoModel.from_config(config.vision_config)\n+        self.prompt_encoder = Sam2PromptEncoder(config.prompt_encoder_config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n+        self.mask_decoder = Sam2MaskDecoder(config.mask_decoder_config)\n+\n+        self.num_feature_levels = config.vision_config.num_feature_levels\n+        self.backbone_feature_sizes = config.vision_config.backbone_feature_sizes\n+        # a single token to indicate no memory embedding from previous frames\n+        self.hidden_dim = config.vision_config.fpn_hidden_size\n+        self.no_memory_embedding = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n+\n+        self.post_init()\n+\n+    def get_image_wide_positional_embeddings(self) -> torch.Tensor:\n+        size = self.prompt_encoder.image_embedding_size\n+        target_device = self.shared_image_embedding.positional_embedding.device\n+        target_dtype = self.shared_image_embedding.positional_embedding.dtype\n+        grid = torch.ones(size, device=target_device, dtype=target_dtype)\n+        y_embed = grid.cumsum(dim=0) - 0.5\n+        x_embed = grid.cumsum(dim=1) - 0.5\n+        y_embed = y_embed / size[0]\n+        x_embed = x_embed / size[1]\n+\n+        positional_embedding = self.shared_image_embedding(torch.stack([x_embed, y_embed], dim=-1))\n+        return positional_embedding.permute(2, 0, 1).unsqueeze(0)  # channel x height x width\n+\n+    @torch.no_grad()\n+    def get_image_embeddings(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> list[torch.Tensor]:\n+        r\"\"\"\n+        Returns the image embeddings by passing the pixel values through the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Input pixel values\n+        \"\"\"\n+        batch_size = pixel_values.shape[0]\n+        feature_maps, _, _, _ = self.get_image_features(pixel_values, **kwargs)\n+\n+        # add no memory embedding to the last feature map\n+        feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+        # reshape feature maps to the same shape as the backbone feature sizes\n+        image_embeddings = [\n+            feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+            for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+        ]\n+\n+        return image_embeddings\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[\n+        list[torch.Tensor],\n+        list[torch.Tensor],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+    ]:\n+        r\"\"\"\n+        Extract and preprocess image features using the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Input pixel values of shape `(batch_size, num_channels, height, width)`.\n+\n+        Returns:\n+            `tuple`: A tuple containing:\n+                - feature_maps (`list[torch.Tensor]`): List of feature maps from different levels.\n+                - feature_maps_position_embeddings (`list[torch.Tensor]`): List of positional embeddings for each feature level.\n+                - vision_hidden_states (`tuple[torch.FloatTensor]`, *optional*): Hidden states from the vision encoder.\n+                - vision_attentions (`tuple[torch.FloatTensor]`, *optional*): Attention weights from the vision encoder.\n+        \"\"\"\n+        vision_outputs: Sam2VisionEncoderOutput = self.vision_encoder(\n+            pixel_values,\n+            **kwargs,\n+        )\n+\n+        feature_maps = vision_outputs.fpn_hidden_states\n+        feature_maps_position_embeddings = vision_outputs.fpn_position_encoding\n+\n+        # precompute projected level 0 and level 1 features in SAM decoder\n+        # to avoid running it again on every SAM click\n+        feature_maps = list(feature_maps)\n+        feature_maps[0] = self.mask_decoder.conv_s0(feature_maps[0])\n+        feature_maps[1] = self.mask_decoder.conv_s1(feature_maps[1])\n+\n+        # flatten NxCxHxW to HWxNxC\n+        feature_maps = [feature_map.flatten(2).permute(2, 0, 1) for feature_map in feature_maps]\n+        feature_maps_position_embeddings = [\n+            feature_map_position_embedding.flatten(2).permute(2, 0, 1)\n+            for feature_map_position_embedding in feature_maps_position_embeddings\n+        ]\n+\n+        return feature_maps, feature_maps_position_embeddings, vision_outputs.hidden_states, vision_outputs.attentions\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+        image_embeddings: Optional[torch.FloatTensor] = None,\n+        multimask_output: bool = True,\n+        attention_similarity: Optional[torch.FloatTensor] = None,\n+        target_embedding: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Sam2ImageSegmentationOutput:\n+        r\"\"\"\n+        input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+            Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+            better results. The points can be obtained by passing a list of list of list to the processor that will\n+            create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+            second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+            per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+            multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+            coordinates of the point. If a different number of points is passed either for each image, or for each\n+            mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+            computation of the embedding will be skipped for these points using the labels.\n+        input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+            Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+            official implementation, there are 3 types of labels\n+\n+            - `1`: the point is a point that contains the object of interest\n+            - `0`: the point is a point that does not contain the object of interest\n+            - `-1`: the point corresponds to the background\n+\n+            We added the label:\n+\n+            - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+            The padding labels should be automatically done by the processor.\n+        input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+            Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+            much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+            that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+            - `x1`: the x coordinate of the top left point of the input box\n+            - `y1`: the y coordinate of the top left point of the input box\n+            - `x2`: the x coordinate of the bottom right point of the input box\n+            - `y2`: the y coordinate of the bottom right point of the input box\n+        input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+            SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+            generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+            manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+            Image embeddings, this is used by the mask decoder to generate masks and iou scores. For more memory\n+            efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+            method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+        multimask_output (`bool`, *optional*):\n+            In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+            bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+            \"best\" mask, by specifying `multimask_output=False`.\n+        attention_similarity (`torch.FloatTensor`, *optional*):\n+            Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+            model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+        target_embedding (`torch.FloatTensor`, *optional*):\n+            Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+            the model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoModel, AutoProcessor\n+\n+        >>> model = AutoModel.from_pretrained(\"danelcsb/sam2.1_hiera_tiny\")\n+        >>> processor = AutoProcessor.from_pretrained(\"danelcsb/sam2.1_hiera_tiny\")\n+\n+        >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n+        >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        >>> input_points = [[[400, 650]]]  # 2D location of a window on the car\n+        >>> inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\")\n+\n+        >>> # Get segmentation mask\n+        >>> outputs = model(**inputs)\n+\n+        >>> # Postprocess masks\n+        >>> masks = processor.post_process_masks(\n+        ...     outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n+        ... )\n+        ```\n+        \"\"\"\n+        if not ((pixel_values is None) ^ (image_embeddings is None)):\n+            raise ValueError(\"Exactly one of pixel_values or image_embeddings must be provided.\")\n+        if input_points is not None and input_boxes is not None:\n+            if input_points.shape[1] != input_boxes.shape[1]:\n+                raise ValueError(\n+                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n+                        input_points.shape[1], input_boxes.shape[1]\n+                    )\n+                )\n+\n+        image_positional_embeddings = self.get_image_wide_positional_embeddings()\n+        # repeat with batch size\n+        batch_size = pixel_values.shape[0] if pixel_values is not None else image_embeddings[-1].shape[0]\n+        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n+\n+        vision_attentions = None\n+        vision_hidden_states = None\n+\n+        if pixel_values is not None:\n+            feature_maps, _, vision_hidden_states, vision_attentions = self.get_image_features(\n+                pixel_values,\n+                **kwargs,\n+            )\n+\n+            # add no memory embedding to the last feature map\n+            feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+            # reshape feature maps to the same shape as the backbone feature sizes\n+            image_embeddings = [\n+                feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+                for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+            ]\n+\n+        if input_points is not None and input_labels is None:\n+            input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n+\n+        if input_points is None and input_boxes is None:\n+            # If no points are provide, pad with an empty point (with label -1)\n+            input_points = torch.zeros(\n+                batch_size, 1, 1, 2, dtype=image_embeddings[-1].dtype, device=image_embeddings[-1].device\n+            )\n+            input_labels = -torch.ones(batch_size, 1, 1, dtype=torch.int32, device=image_embeddings[-1].device)\n+\n+        if input_masks is not None:\n+            # If mask_inputs is provided, downsize it into low-res mask input if needed\n+            # and feed it as a dense mask prompt into the SAM mask encoder\n+            if input_masks.shape[-2:] != self.prompt_encoder.mask_input_size:\n+                input_masks = F.interpolate(\n+                    input_masks.float(),\n+                    size=self.prompt_encoder.mask_input_size,\n+                    align_corners=False,\n+                    mode=\"bilinear\",\n+                    antialias=True,  # use antialias for downsampling\n+                ).to(input_masks.dtype)\n+\n+        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        low_res_multimasks, iou_scores, _, object_score_logits = self.mask_decoder(\n+            image_embeddings=image_embeddings[-1],\n+            image_positional_embeddings=image_positional_embeddings,\n+            sparse_prompt_embeddings=sparse_embeddings,\n+            dense_prompt_embeddings=dense_embeddings,\n+            multimask_output=multimask_output,\n+            high_resolution_features=image_embeddings[:-1],\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+\n+        return Sam2ImageSegmentationOutput(\n+            iou_scores=iou_scores,\n+            pred_masks=low_res_multimasks,\n+            object_score_logits=object_score_logits,\n+            image_embeddings=image_embeddings,\n+            vision_hidden_states=vision_hidden_states,\n+            vision_attentions=vision_attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"Sam2Model\",\n+    \"Sam2VisionModel\",\n+    \"Sam2PreTrainedModel\",\n+    \"Sam2ImageProcessorFast\",\n+    \"Sam2HieraDetModel\",\n+]"
        },
        {
            "sha": "71664acdf03d9c3428ddc1da40cd79c36b5d1c2f",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "added",
            "additions": 526,
            "deletions": 0,
            "changes": 526,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,526 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for SAM2.\n+\"\"\"\n+\n+from copy import deepcopy\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import TensorType, is_torch_available, logging\n+from ...utils.import_utils import requires\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`Sam2Processor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n+    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam2ImageProcessorFast`):\n+            An instance of [`Sam2ImageProcessorFast`].\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\"]\n+    image_processor_class = \"Sam2ImageProcessorFast\"\n+\n+    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+        super().__init__(image_processor, **kwargs)\n+        self.point_pad_value = point_pad_value\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        segmentation_maps: ImageInput = None,\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam2ImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n+        points and bounding boxes for the model if they are provided.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `reshaped_input_sizes` (`torch.Tensor`): The reshaped input sizes of the images.\n+            - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n+            - `input_points` (`torch.Tensor`): The processed points.\n+            - `input_labels` (`torch.Tensor`): The processed labels.\n+            - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        # pop arguments that are not used in the foward but used nevertheless\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+\n+        # Process input points, labels, and boxes if provided\n+        if input_points is not None or input_labels is not None or input_boxes is not None:\n+            # Validate and convert inputs to standardized format\n+            processed_points = self._validate_single_input(\n+                input_points,\n+                expected_depth=4,\n+                input_name=\"points\",\n+                expected_format=\"[image level, object level, point level, point coordinates]\",\n+                expected_coord_size=2,\n+            )\n+            processed_labels = self._validate_single_input(\n+                input_labels,\n+                expected_depth=3,\n+                input_name=\"labels\",\n+                expected_format=\"[image level, object level, point level]\",\n+            )\n+            processed_boxes = self._validate_single_input(\n+                input_boxes,\n+                expected_depth=3,\n+                input_name=\"boxes\",\n+                expected_format=\"[image level, box level, box coordinates]\",\n+                expected_coord_size=4,\n+            )\n+\n+            # Get padding requirements for all inputs\n+            if processed_points is not None:\n+                points_max_dims = self._get_nested_dimensions(processed_points)[:3]\n+            if processed_labels is not None:\n+                labels_max_dims = self._get_nested_dimensions(processed_labels)[:3]\n+            if processed_boxes is not None:\n+                boxes_max_dims = self._get_nested_dimensions(processed_boxes)[:2]\n+\n+            # Ensure points and labels have consistent dimensions\n+            if processed_points is not None and processed_labels is not None:\n+                if points_max_dims != labels_max_dims:\n+                    raise ValueError(\n+                        \"Input points and labels have inconsistent dimensions. Please ensure they have the same dimensions.\"\n+                    )\n+\n+            # Check that boxes don't need padding (model limitation)\n+            if processed_boxes is not None and len(processed_boxes) >= 2:\n+                if any(len(img_boxes) < boxes_max_dims[1] for img_boxes in processed_boxes):\n+                    raise ValueError(\n+                        \"Input boxes have inconsistent dimensions that would require padding, \"\n+                        \"but boxes cannot be padded due to model limitations. \"\n+                        \"Please ensure all images have the same number of boxes.\"\n+                    )\n+\n+            # Pad and normalize all inputs to final tensor format\n+            if processed_points is not None:\n+                padded_points = self._pad_nested_list(processed_points, points_max_dims + [2])\n+                final_points = torch.tensor(padded_points, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_points, original_sizes, preserve_padding=True)\n+                encoding_image_processor.update({\"input_points\": final_points})\n+\n+            if processed_labels is not None:\n+                padded_labels = self._pad_nested_list(processed_labels, labels_max_dims)\n+                final_labels = torch.tensor(padded_labels, dtype=torch.int64)\n+                encoding_image_processor.update({\"input_labels\": final_labels})\n+\n+            if processed_boxes is not None:\n+                final_boxes = torch.tensor(processed_boxes, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_boxes, original_sizes, is_bounding_box=True)\n+                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+\n+        return encoding_image_processor\n+\n+    def _normalize_coordinates(\n+        self, target_size: int, coords: \"torch.Tensor\", original_size, is_bounding_box=False\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H, W) format.\n+\n+        Args:\n+            target_size (`int`):\n+                The target size of the image.\n+            coords (`torch.Tensor`):\n+                The coordinates to be normalized.\n+            original_size (`tuple`):\n+                The original size of the image.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether the coordinates are bounding boxes.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        new_h, new_w = target_size, target_size\n+        coords = deepcopy(coords).float()\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n+        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _convert_to_nested_list(self, data, expected_depth, current_depth=0):\n+        \"\"\"\n+        Recursively convert various input formats (tensors, numpy arrays, lists) to nested lists.\n+\n+        Args:\n+            data: Input data in any format\n+            expected_depth: Expected nesting depth\n+            current_depth: Current depth in recursion\n+\n+        Returns:\n+            Nested list representation of the data\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Convert tensor/numpy to list if we're at a leaf level or if it's a multi-dimensional array\n+        if isinstance(data, torch.Tensor):  # PyTorch tensor\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small tensor\n+                return data.numpy().tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, np.ndarray):  # NumPy array\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small array\n+                return data.tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, list):\n+            if current_depth == expected_depth:\n+                # We've reached the expected depth, return as is\n+                return data\n+            else:\n+                # Continue recursion\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, (int, float)):\n+            return data\n+        else:\n+            raise ValueError(f\"Unsupported data type: {type(data)}\")\n+\n+    def _get_nested_dimensions(self, nested_list, max_dims=None):\n+        \"\"\"\n+        Get the maximum dimensions at each level of nesting.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list structure.\n+            max_dims (`list`, *optional*):\n+                Current maximum dimensions (for recursion).\n+\n+        Returns:\n+            `list`: A list of maximum dimensions for each nesting level.\n+        \"\"\"\n+        if max_dims is None:\n+            max_dims = []\n+\n+        if not isinstance(nested_list, list):\n+            return max_dims\n+\n+        if len(max_dims) == 0:\n+            max_dims.append(len(nested_list))\n+        else:\n+            max_dims[0] = max(max_dims[0], len(nested_list))\n+\n+        if len(nested_list) > 0:\n+            for item in nested_list:\n+                if isinstance(item, list):\n+                    sub_dims = self._get_nested_dimensions(item)\n+                    # Merge sub_dims into max_dims\n+                    for i, dim in enumerate(sub_dims):\n+                        if i + 1 >= len(max_dims):\n+                            max_dims.append(dim)\n+                        else:\n+                            max_dims[i + 1] = max(max_dims[i + 1], dim)\n+\n+        return max_dims\n+\n+    def _pad_nested_list(self, nested_list, target_dims, current_level=0, pad_value=None):\n+        \"\"\"\n+        Recursively pad a nested list to match target dimensions.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list to pad.\n+            target_dims (`list`):\n+                Target dimensions for each level.\n+            current_level (`int`, *optional*, defaults to 0):\n+                Current nesting level.\n+            pad_value (`int`, *optional*):\n+                Value to use for padding.\n+\n+        Returns:\n+            `list`: The padded nested list.\n+        \"\"\"\n+        if pad_value is None:\n+            pad_value = self.point_pad_value\n+\n+        if current_level >= len(target_dims):\n+            return nested_list\n+\n+        # Ensure we have a list\n+        if not isinstance(nested_list, list):\n+            nested_list = [nested_list]\n+\n+        # Pad current level\n+        current_size = len(nested_list)\n+        target_size = target_dims[current_level]\n+\n+        # Pad with appropriate values\n+        if current_level == len(target_dims) - 1:\n+            # At the coordinate level, pad with pad_value\n+            nested_list.extend([pad_value] * (target_size - current_size))\n+        else:\n+            # At higher levels, pad with nested structures\n+            if current_size > 0:\n+                # Create appropriately sized template\n+                if current_level < len(target_dims) - 2:\n+                    # For non-coordinate levels, create empty nested structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    template = self._create_empty_nested_structure(template_dims, pad_value)\n+                else:\n+                    # For coordinate level, create list of pad_values\n+                    template = [pad_value] * target_dims[current_level + 1]\n+\n+                nested_list.extend([deepcopy(template) for _ in range(target_size - current_size)])\n+            else:\n+                # Create from scratch\n+                template_dims = target_dims[current_level + 1 :]\n+                template = self._create_empty_nested_structure(template_dims, pad_value)\n+                nested_list.extend([deepcopy(template) for _ in range(target_size)])\n+\n+        # Recursively pad sublists\n+        if current_level < len(target_dims) - 1:\n+            for i in range(len(nested_list)):\n+                if isinstance(nested_list[i], list):\n+                    nested_list[i] = self._pad_nested_list(nested_list[i], target_dims, current_level + 1, pad_value)\n+\n+        return nested_list\n+\n+    def _create_empty_nested_structure(self, dims, pad_value):\n+        \"\"\"\n+        Create an empty nested structure with given dimensions filled with pad_value.\n+\n+        Args:\n+            dims (`list`):\n+                The dimensions of the nested structure.\n+            pad_value (`int`):\n+                The value to fill the structure with.\n+        \"\"\"\n+        if len(dims) == 1:\n+            return [pad_value] * dims[0]\n+        else:\n+            return [self._create_empty_nested_structure(dims[1:], pad_value) for _ in range(dims[0])]\n+\n+    def _get_nesting_level(self, input_list):\n+        \"\"\"\n+        Get the nesting level of a list structure.\n+\n+        Args:\n+            input_list (`list`):\n+                The list to get the nesting level of.\n+        \"\"\"\n+        if isinstance(input_list, list):\n+            if len(input_list) == 0:\n+                return 1\n+            return 1 + self._get_nesting_level(input_list[0])\n+        elif isinstance(input_list, (np.ndarray, torch.Tensor)):\n+            # For arrays/tensors, the nesting level is the number of dimensions\n+            return len(input_list.shape)\n+        return 0\n+\n+    def _validate_single_input(\n+        self,\n+        data: Union[torch.Tensor, np.ndarray, list],\n+        expected_depth: int,\n+        input_name: str,\n+        expected_format: str,\n+        expected_coord_size: Optional[int] = None,\n+    ) -> list:\n+        \"\"\"\n+                Validate a single input by ensuring proper nesting and raising an error if the input is not valid.\n+\n+                Args:\n+                    data (`torch.Tensor`, `np.ndarray`, or `list`):\n+                        Input data to process.\n+                    expected_depth (`int`):\n+                        Expected nesting depth.\n+                    input_name (`str`):\n+                        Name of the input for error messages.\n+                    expected_format (`str`):\n+                        The expected format of the input.\n+                    expected_coord_size (`int`, *optional*):\n+                        Expected coordinate size (2 for points, 4 for boxes, None for labels).\n+        .\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Handle tensors and numpy arrays first\n+        if isinstance(data, (torch.Tensor, np.ndarray)):\n+            # For tensors/arrays, we can directly check the number of dimensions\n+            if data.ndim != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a tensor/array with {expected_depth} dimensions. The expected nesting format is {expected_format}. Got {data.ndim} dimensions.\"\n+                )\n+            elif expected_coord_size is not None:\n+                if data.shape[-1] != expected_coord_size:\n+                    raise ValueError(\n+                        f\"Input {input_name} must be a tensor/array with {expected_coord_size} as the last dimension, got {data.shape[-1]}.\"\n+                    )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+        # Handle nested lists\n+        if isinstance(data, list):\n+            current_depth = self._get_nesting_level(data)\n+            if current_depth != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a nested list with {expected_depth} levels. The expected nesting format is {expected_format}. Got {current_depth} levels.\"\n+                )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+    def _normalize_tensor_coordinates(self, tensor, original_sizes, is_bounding_box=False, preserve_padding=False):\n+        \"\"\"\n+        Helper method to normalize coordinates in a tensor across multiple images.\n+\n+        Args:\n+            tensor (`torch.Tensor`):\n+                Input tensor with coordinates.\n+            original_sizes (`list`):\n+                Original image sizes.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether coordinates are bounding boxes.\n+            preserve_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to preserve padding values (for points).\n+        \"\"\"\n+        if preserve_padding:\n+            # For points: avoid normalizing pad values\n+            mask = tensor != self.point_pad_value\n+            coord_mask = mask.all(dim=-1, keepdim=True)\n+\n+        for img_idx in range(len(original_sizes)):\n+            if img_idx < tensor.shape[0]:\n+                original_size = original_sizes[img_idx] if img_idx < len(original_sizes) else original_sizes[0]\n+                normalized_coords = self._normalize_coordinates(\n+                    self.target_size, tensor[img_idx], original_size, is_bounding_box=is_bounding_box\n+                )\n+\n+                if preserve_padding:\n+                    # Only update non-padded values\n+                    img_mask = coord_mask[img_idx]\n+                    tensor[img_idx] = torch.where(\n+                        img_mask.expand_as(tensor[img_idx]), normalized_coords, tensor[img_idx]\n+                    )\n+                else:\n+                    tensor[img_idx] = normalized_coords\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        return self.image_processor.post_process_masks(\n+            masks,\n+            original_sizes,\n+            mask_threshold,\n+            binarize,\n+            max_hole_area,\n+            max_sprinkle_area,\n+            apply_non_overlapping_constraints,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Sam2Processor\"]"
        },
        {
            "sha": "565e8bcaf4d0d327ff1dbf21343008b24e0e844b",
            "filename": "src/transformers/models/sam2_video/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2F__init__.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam2_video import *\n+    from .modeling_sam2_video import *\n+    from .processing_sam2_video import *\n+    from .video_processing_sam2_video import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "a47858c6340e4a71f0d528bf1427c5ba7cd06126",
            "filename": "src/transformers/models/sam2_video/configuration_sam2_video.py",
            "status": "added",
            "additions": 393,
            "deletions": 0,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,393 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam2_video/modular_sam2_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam2_video.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Sam2VideoPromptEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2VideoPromptEncoder`]. The [`Sam2VideoPromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1024,\n+        patch_size=16,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.scale = scale\n+\n+\n+class Sam2VideoMaskDecoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam2VideoMaskDecoder`]. It is used to instantiate a SAM2_VIDEO\n+    memory encoder according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the SAM2_VIDEO mask decoder.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            The dimension of the MLP in the two-way transformer.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            The number of hidden layers in the two-way transformer.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads in the two-way transformer.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsample rate for the attention layers.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of multimask outputs.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The depth of the IoU head.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the IoU head.\n+        dynamic_multimask_via_stability (`bool`, *optional*, defaults to `True`):\n+            Whether to use dynamic multimask via stability.\n+        dynamic_multimask_stability_delta (`float`, *optional*, defaults to 0.05):\n+            The stability delta for the dynamic multimask.\n+        dynamic_multimask_stability_thresh (`float`, *optional*, defaults to 0.98):\n+            The stability threshold for the dynamic multimask.\n+\n+    \"\"\"\n+\n+    base_config_key = \"mask_decoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        hidden_act=\"gelu\",\n+        mlp_dim=2048,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=256,\n+        dynamic_multimask_via_stability=True,\n+        dynamic_multimask_stability_delta=0.05,\n+        dynamic_multimask_stability_thresh=0.98,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.hidden_act = hidden_act\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.dynamic_multimask_via_stability = dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = dynamic_multimask_stability_thresh\n+\n+        # TwoWayTransformer configuration\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_dim = mlp_dim\n+        self.attention_downsample_rate = attention_downsample_rate\n+\n+\n+class Sam2VideoConfig(PretrainedConfig):\n+    r\"\"\"\n+    [`Sam2Config`] is the configuration class to store the configuration of a [`Sam2Model`]. It is used to instantiate a\n+    SAM2 model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n+    configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n+    [facebook/sam2.1-hiera-tiny](https://huggingface.co/facebook/sam2.1-hiera-tiny) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `Sam2VisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2VisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `Sam2PromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2PromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `Sam2MaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam2MaskDecoderConfig`].\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation for parameter initialization.\n+        num_maskmem (`int`, *optional*, defaults to 7):\n+            The number of memory slots for the mask memory.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            The size of the input images.\n+        sigmoid_scale_for_mem_enc (`float`, *optional*, defaults to 20.0):\n+            Scale factor for the sigmoid function in the memory encoder.\n+        sigmoid_bias_for_mem_enc (`float`, *optional*, defaults to -10.0):\n+            Bias for the sigmoid function in the memory encoder.\n+        enable_occlusion_spatial_embedding (`bool`, *optional*, defaults to `True`):\n+            Whether to enable spatial embedding for occlusions.\n+        multimask_output_in_sam (`bool`, *optional*, defaults to `True`):\n+            Whether to output multiple masks from the SAM head.\n+        multimask_min_pt_num (`int`, *optional*, defaults to 0):\n+            The minimum number of points to trigger multimask output.\n+        multimask_max_pt_num (`int`, *optional*, defaults to 1):\n+            The maximum number of points to trigger multimask output.\n+        multimask_output_for_tracking (`bool`, *optional*, defaults to `True`):\n+            Whether to use multimask output for tracking.\n+        max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n+            The maximum number of object pointers in the encoder.\n+        enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n+            Whether to enable temporal positional encoding for object pointers.\n+        memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory attention hidden states.\n+        memory_attention_num_layers (`int`, *optional*, defaults to 4):\n+            The number of layers in the memory attention module.\n+        memory_attention_num_attention_heads (`int`, *optional*, defaults to 1):\n+            Number of attention heads for each attention layer in the memory attention.\n+        memory_attention_downsample_rate (`int`, *optional*, defaults to 1):\n+            The downsample rate for the attention layers.\n+        memory_attention_feed_forward_hidden_size (`int`, *optional*, defaults to 2048):\n+            The dimension of the feedforward network in the memory attention module.\n+        memory_attention_feed_forward_hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in the feedforward network in the memory attention module.\n+        memory_attention_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout rate for the memory attention module.\n+        memory_attention_rope_theta (`float`, *optional*, defaults to 10000):\n+            The Rope theta parameter.\n+        memory_attention_rope_feat_sizes (`list[int]`, *optional*, defaults to `[64, 64]`):\n+            The feature sizes for the Rope positional encoding.\n+        memory_attention_rope_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout rate for the Rope positional encoding.\n+        memory_encoder_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory encoder hidden states.\n+        memory_encoder_output_channels (`int`, *optional*, defaults to 64):\n+            The number of output channels for the memory encoder.\n+        mask_downsampler_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the mask downsampler embedding.\n+        mask_downsampler_kernel_size (`int`, *optional*, defaults to 3):\n+            The kernel size for the mask downsampler.\n+        mask_downsampler_stride (`int`, *optional*, defaults to 2):\n+            The stride for the mask downsampler.\n+        mask_downsampler_padding (`int`, *optional*, defaults to 1):\n+            The padding for the mask downsampler.\n+        mask_downsampler_total_stride (`int`, *optional*, defaults to 16):\n+            The total stride for the mask downsampler.\n+        mask_downsampler_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the mask downsampler.\n+        memory_fuser_num_layers (`int`, *optional*, defaults to 2):\n+            The number of layers in the memory fuser.\n+        memory_fuser_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the embedding layer in the memory fuser.\n+        memory_fuser_intermediate_dim (`int`, *optional*, defaults to 1024):\n+            The dimension of the intermediate layer in the memory fuser.\n+        memory_fuser_kernel_size (`int`, *optional*, defaults to 7):\n+            The kernel size for the memory fuser.\n+        memory_fuser_padding (`int`, *optional*, defaults to 3):\n+            The padding for the memory fuser.\n+        memory_fuser_layer_scale_init_value (`float`, *optional*, defaults to 1e-06):\n+            The initial value for the layer scale in the memory fuser.\n+        memory_fuser_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the memory fuser.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     Sam2VisionConfig,\n+    ...     Sam2PromptEncoderConfig,\n+    ...     Sam2MaskDecoderConfig,\n+    ...     Sam2Model,\n+    ... )\n+\n+    >>> # Initializing a Sam2Config with `\"facebook/sam2.1_hiera_tiny\"` style configuration\n+    >>> configuration = Sam2config()\n+\n+    >>> # Initializing a Sam2Model (with random weights) from the `\"facebook/sam2.1_hiera_tiny\"` style configuration\n+    >>> model = Sam2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Sam2Config from a Sam2VisionConfig, Sam2PromptEncoderConfig, and Sam2MaskDecoderConfig\n+\n+    >>> # Initializing SAM2 vision encoder, memory attention, and memory encoder configurations\n+    >>> vision_config = Sam2VisionConfig()\n+    >>> prompt_encoder_config = Sam2PromptEncoderConfig()\n+    >>> mask_decoder_config = Sam2MaskDecoderConfig()\n+\n+    >>> config = Sam2Config(vision_config, prompt_encoder_config, mask_decoder_config)\n+    ```\"\"\"\n+\n+    model_type = \"sam2_video\"\n+    sub_configs = {\n+        \"vision_config\": AutoConfig,\n+        \"prompt_encoder_config\": Sam2VideoPromptEncoderConfig,\n+        \"mask_decoder_config\": Sam2VideoMaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        num_maskmem=7,\n+        image_size=1024,\n+        sigmoid_scale_for_mem_enc=20.0,\n+        sigmoid_bias_for_mem_enc=-10.0,\n+        enable_occlusion_spatial_embedding=True,\n+        multimask_output_in_sam=True,\n+        multimask_min_pt_num=0,\n+        multimask_max_pt_num=1,\n+        multimask_output_for_tracking=True,\n+        max_object_pointers_in_encoder=16,\n+        enable_temporal_pos_encoding_for_object_pointers=True,\n+        # memory attention\n+        memory_attention_hidden_size=256,\n+        memory_attention_num_layers=4,\n+        memory_attention_num_attention_heads=1,\n+        memory_attention_downsample_rate=1,\n+        memory_attention_feed_forward_hidden_size=2048,\n+        memory_attention_feed_forward_hidden_act=\"relu\",\n+        memory_attention_dropout=0.1,\n+        memory_attention_rope_theta=10000,\n+        memory_attention_rope_feat_sizes=None,\n+        memory_attention_rope_dropout=0.1,\n+        # memory encoder\n+        memory_encoder_hidden_size=256,\n+        memory_encoder_output_channels=64,\n+        mask_downsampler_embed_dim=256,\n+        mask_downsampler_kernel_size=3,\n+        mask_downsampler_stride=2,\n+        mask_downsampler_padding=1,\n+        mask_downsampler_total_stride=16,\n+        mask_downsampler_hidden_act=\"gelu\",\n+        memory_fuser_num_layers=2,\n+        memory_fuser_embed_dim=256,\n+        memory_fuser_intermediate_dim=1024,\n+        memory_fuser_kernel_size=7,\n+        memory_fuser_padding=3,\n+        memory_fuser_layer_scale_init_value=1e-6,\n+        memory_fuser_hidden_act=\"gelu\",\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        vision_config = vision_config if vision_config is not None else {}\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+        memory_attention_rope_feat_sizes = (\n+            [64, 64] if memory_attention_rope_feat_sizes is None else memory_attention_rope_feat_sizes\n+        )\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam2_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif isinstance(vision_config, PretrainedConfig):\n+            vision_config = vision_config\n+        if isinstance(prompt_encoder_config, Sam2VideoPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam2VideoMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam2VideoPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam2VideoMaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+        self.num_maskmem = num_maskmem  # default 1 input frame + 6 previous frames\n+        self.image_size = image_size\n+        self.sigmoid_scale_for_mem_enc = sigmoid_scale_for_mem_enc\n+        self.sigmoid_bias_for_mem_enc = sigmoid_bias_for_mem_enc\n+        self.multimask_output_in_sam = multimask_output_in_sam\n+        self.multimask_min_pt_num = multimask_min_pt_num\n+        self.multimask_max_pt_num = multimask_max_pt_num\n+        self.multimask_output_for_tracking = multimask_output_for_tracking\n+        self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        # The next 4 are True for sam2.1 and False for sam2\n+        self.enable_occlusion_spatial_embedding = enable_occlusion_spatial_embedding\n+        self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n+\n+        # memory attention\n+        self.memory_attention_hidden_size = memory_attention_hidden_size\n+        self.memory_attention_num_layers = memory_attention_num_layers\n+        self.memory_attention_num_attention_heads = memory_attention_num_attention_heads\n+        self.memory_attention_downsample_rate = memory_attention_downsample_rate\n+        self.memory_attention_feed_forward_hidden_size = memory_attention_feed_forward_hidden_size\n+        self.memory_attention_feed_forward_hidden_act = memory_attention_feed_forward_hidden_act\n+        self.memory_attention_dropout = memory_attention_dropout\n+        self.memory_attention_rope_theta = memory_attention_rope_theta\n+        self.memory_attention_rope_feat_sizes = memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_dropout = memory_attention_rope_dropout\n+\n+        # memory encoder\n+        self.memory_encoder_hidden_size = memory_encoder_hidden_size\n+        self.memory_encoder_output_channels = memory_encoder_output_channels\n+        self.mask_downsampler_embed_dim = mask_downsampler_embed_dim\n+        self.mask_downsampler_kernel_size = mask_downsampler_kernel_size\n+        self.mask_downsampler_stride = mask_downsampler_stride\n+        self.mask_downsampler_padding = mask_downsampler_padding\n+        self.mask_downsampler_total_stride = mask_downsampler_total_stride\n+        self.mask_downsampler_hidden_act = mask_downsampler_hidden_act\n+        self.memory_fuser_num_layers = memory_fuser_num_layers\n+        self.memory_fuser_embed_dim = memory_fuser_embed_dim\n+        self.memory_fuser_intermediate_dim = memory_fuser_intermediate_dim\n+        self.memory_fuser_kernel_size = memory_fuser_kernel_size\n+        self.memory_fuser_padding = memory_fuser_padding\n+        self.memory_fuser_layer_scale_init_value = memory_fuser_layer_scale_init_value\n+        self.memory_fuser_hidden_act = memory_fuser_hidden_act\n+\n+\n+__all__ = [\"Sam2VideoMaskDecoderConfig\", \"Sam2VideoPromptEncoderConfig\", \"Sam2VideoConfig\"]"
        },
        {
            "sha": "322aa5507978a168f73372d09d345d1417ea5165",
            "filename": "src/transformers/models/sam2_video/convert_sam2_video_to_hf.py",
            "status": "added",
            "additions": 330,
            "deletions": 0,
            "changes": 330,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconvert_sam2_video_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconvert_sam2_video_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconvert_sam2_video_to_hf.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,330 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Convert SAM checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/segment-anything-2.\n+\"\"\"\n+\n+import argparse\n+import re\n+\n+import numpy as np\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+\n+from transformers import (\n+    Sam2HieraDetConfig,\n+    Sam2ImageProcessorFast,\n+    Sam2VideoConfig,\n+    Sam2VideoMaskDecoderConfig,\n+    Sam2VideoModel,\n+    Sam2VideoProcessor,\n+    Sam2VideoPromptEncoderConfig,\n+    Sam2VideoVideoProcessor,\n+    Sam2VisionConfig,\n+)\n+\n+\n+def get_config(model_name):\n+    if \"hiera_tiny\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig()\n+        vision_config = Sam2VisionConfig(backbone_config=hiera_det_config)\n+    elif \"hiera_small\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(blocks_per_stage=[1, 2, 11, 2], global_attention_blocks=[7, 10, 13])\n+        vision_config = Sam2VisionConfig(backbone_config=hiera_det_config)\n+    elif \"hiera_base_plus\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(\n+            hidden_size=112,\n+            embed_dim_per_stage=[112, 224, 448, 896],\n+            num_attention_heads_per_stage=[2, 4, 8, 16],\n+            blocks_per_stage=[2, 3, 16, 3],\n+            global_attention_blocks=[12, 16, 20],\n+            window_positional_embedding_background_size=(14, 14),\n+        )\n+        vision_config = Sam2VisionConfig(\n+            backbone_config=hiera_det_config,\n+            backbone_channel_list=[896, 448, 224, 112],\n+        )\n+    elif \"hiera_large\" in model_name:\n+        hiera_det_config = Sam2HieraDetConfig(\n+            hidden_size=144,\n+            embed_dim_per_stage=[144, 288, 576, 1152],\n+            num_attention_heads_per_stage=[2, 4, 8, 16],\n+            blocks_per_stage=[2, 6, 36, 4],\n+            global_attention_blocks=[23, 33, 43],\n+            window_positional_embedding_background_size=(7, 7),\n+            window_size_per_stage=[8, 4, 16, 8],\n+        )\n+        vision_config = Sam2VisionConfig(\n+            backbone_config=hiera_det_config,\n+            backbone_channel_list=[1152, 576, 288, 144],\n+        )\n+    prompt_encoder_config = Sam2VideoPromptEncoderConfig()\n+    mask_decoder_config = Sam2VideoMaskDecoderConfig()\n+\n+    if \"sam2.1\" in model_name:\n+        enable_temporal_pos_encoding_for_object_pointers = True\n+        enable_occlusion_spatial_embedding = True\n+    else:\n+        enable_temporal_pos_encoding_for_object_pointers = False\n+        enable_occlusion_spatial_embedding = False\n+\n+    config = Sam2VideoConfig(\n+        vision_config=vision_config,\n+        prompt_encoder_config=prompt_encoder_config,\n+        mask_decoder_config=mask_decoder_config,\n+        enable_temporal_pos_encoding_for_object_pointers=enable_temporal_pos_encoding_for_object_pointers,\n+        enable_occlusion_spatial_embedding=enable_occlusion_spatial_embedding,\n+    )\n+\n+    return config\n+\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"iou_prediction_head.layers.0\": \"iou_prediction_head.proj_in\",\n+    \"iou_prediction_head.layers.1\": \"iou_prediction_head.layers.0\",\n+    \"iou_prediction_head.layers.2\": \"iou_prediction_head.proj_out\",\n+    \"mask_decoder.output_upscaling.0\": \"mask_decoder.upscale_conv1\",\n+    \"mask_decoder.output_upscaling.1\": \"mask_decoder.upscale_layer_norm\",\n+    \"mask_decoder.output_upscaling.3\": \"mask_decoder.upscale_conv2\",\n+    \"mask_downscaling.0\": \"mask_embed.conv1\",\n+    \"mask_downscaling.1\": \"mask_embed.layer_norm1\",\n+    \"mask_downscaling.3\": \"mask_embed.conv2\",\n+    \"mask_downscaling.4\": \"mask_embed.layer_norm2\",\n+    \"mask_downscaling.6\": \"mask_embed.conv3\",\n+    \"dwconv\": \"depthwise_conv\",\n+    \"pwconv\": \"pointwise_conv\",\n+    \"fuser\": \"memory_fuser\",\n+    \"point_embeddings\": \"point_embed\",\n+    \"pe_layer.positional_encoding_gaussian_matrix\": \"shared_embedding.positional_embedding\",\n+    \"obj_ptr_tpos_proj\": \"temporal_positional_encoding_projection_layer\",\n+    \"no_obj_embed_spatial\": \"occlusion_spatial_embedding_parameter\",\n+    \"sam_prompt_encoder\": \"prompt_encoder\",\n+    \"sam_mask_decoder\": \"mask_decoder\",\n+    \"maskmem_tpos_enc\": \"memory_temporal_positional_encoding\",\n+    \"gamma\": \"scale\",\n+    \"image_encoder.neck\": \"vision_encoder.neck\",\n+    \"image_encoder\": \"vision_encoder.backbone\",\n+    \"neck.0\": \"neck.conv1\",\n+    \"neck.1\": \"neck.layer_norm1\",\n+    \"neck.2\": \"neck.conv2\",\n+    \"neck.3\": \"neck.layer_norm2\",\n+    \"pix_feat_proj\": \"feature_projection\",\n+    \"patch_embed.proj\": \"patch_embed.projection\",\n+    \"no_mem_embed\": \"no_memory_embedding\",\n+    \"no_mem_pos_enc\": \"no_memory_positional_encoding\",\n+    \"obj_ptr\": \"object_pointer\",\n+    \".norm\": \".layer_norm\",\n+    \"trunk.\": \"\",\n+    \"out_proj\": \"o_proj\",\n+}\n+\n+\n+def replace_keys(state_dict, config):\n+    model_state_dict = {}\n+    output_hypernetworks_mlps_pattern = r\".*.output_hypernetworks_mlps.(\\d+).layers.(\\d+).*\"\n+    output_mask_decoder_mlps_pattern = r\"mask_decoder.transformer.layers.(\\d+).mlp.layers.(\\d+).*\"\n+    output_mask_decoder_score_head_pattern = r\"mask_decoder.pred_obj_score_head.layers.(\\d+).*\"\n+    output_vision_encoder_mlps_pattern = r\"vision_encoder.backbone.blocks.(\\d+).mlp.layers.(\\d+).*\"\n+    output_vision_encoder_neck_pattern = r\"vision_encoder.neck.convs.(\\d+).conv\"\n+    output_memory_encoder_projection_pattern = r\"memory_encoder.o_proj.*\"\n+    output_object_pointer_proj_pattern = r\"object_pointer_proj.layers.(\\d+).*\"\n+    output_memory_encoder_mask_downsampler_pattern = r\"memory_encoder.mask_downsampler.encoder.(\\d+).*\"\n+\n+    for key, value in state_dict.items():\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        # vision_encoder.blocks.0.mlp.layers.1.weight -> vision_encoder.blocks.0.mlp.proj_out.weight\n+        if re.match(output_vision_encoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_vision_encoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"proj_out\")\n+\n+        # mask_decoder.transformer.layers.0.mlp.layers.1.weight -> mask_decoder.transformer.layers.1.mlp.proj_out.weight\n+        if re.match(output_mask_decoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"mlp.layers.0\", \"mlp.proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"mlp.layers.1\", \"mlp.proj_out\")\n+\n+        # mask_decoder.pred_obj_score_head.layers.1.weight -> mask_decoder.pred_obj_score_head.proj_in.weight\n+        if re.match(output_mask_decoder_score_head_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_score_head_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        if re.match(output_hypernetworks_mlps_pattern, key):\n+            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        # vision_encoder.neck.convs.1.conv.bias -> vision_encoder.neck.convs.1.bias\n+        if re.match(output_vision_encoder_neck_pattern, key):\n+            key = key.replace(\".conv.\", \".\")\n+\n+        # memory_encoder.out_proj.weight -> memory_encoder.projection.weight\n+        if re.match(output_memory_encoder_projection_pattern, key):\n+            key = key.replace(\".o_proj.\", \".projection.\")\n+\n+        if re.match(output_object_pointer_proj_pattern, key):\n+            layer_nb = int(re.match(output_object_pointer_proj_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        if re.match(output_memory_encoder_mask_downsampler_pattern, key):\n+            layer_nb = int(re.match(output_memory_encoder_mask_downsampler_pattern, key).group(1))\n+            if layer_nb == 12:\n+                key = key.replace(f\"encoder.{layer_nb}\", \"final_conv\")\n+            elif layer_nb % 3 == 0:\n+                key = key.replace(f\"encoder.{layer_nb}\", f\"layers.{layer_nb // 3}.conv\")\n+            elif layer_nb % 3 == 1:\n+                key = key.replace(f\"encoder.{layer_nb}\", f\"layers.{layer_nb // 3}.layer_norm\")\n+\n+        model_state_dict[key] = value\n+\n+    model_state_dict[\"shared_image_embedding.positional_embedding\"] = model_state_dict[\n+        \"prompt_encoder.shared_embedding.positional_embedding\"\n+    ]\n+    model_state_dict[\"prompt_encoder.point_embed.weight\"] = torch.cat(\n+        [model_state_dict.pop(f\"prompt_encoder.point_embed.{i}.weight\") for i in range(4)],\n+        dim=0,\n+    )\n+    return model_state_dict\n+\n+\n+def convert_sam2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub):\n+    config = get_config(model_name)\n+\n+    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"model\"]\n+    state_dict = replace_keys(state_dict, config)\n+\n+    image_processor = Sam2ImageProcessorFast()\n+    video_processor = Sam2VideoVideoProcessor()\n+    processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)\n+    hf_model = Sam2VideoModel(config)\n+    hf_model.eval()\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    missing_keys, unexpected_keys = hf_model.load_state_dict(state_dict, strict=True)\n+    hf_model = hf_model.to(device)\n+    print(\"Missing keys:\", missing_keys)\n+    print(\"Unexpected keys:\", unexpected_keys)\n+\n+    img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+\n+    input_points = [[[[1000, 600]]]]\n+    input_labels = [[[1]]]\n+\n+    inputs = processor(\n+        images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+    ).to(device)\n+\n+    with torch.no_grad():\n+        output = hf_model._single_frame_forward(**inputs)\n+    scores = output.iou_scores.squeeze()\n+\n+    if model_name == \"sam2.1_hiera_tiny\":\n+        assert torch.allclose(scores, torch.tensor([0.0316, 0.9647, 0.1029]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_small\":\n+        assert torch.allclose(scores, torch.tensor([0.9664, 0.1494, 0.0456]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_base_plus\":\n+        assert torch.allclose(scores, torch.tensor([0.0361, 0.9775, 0.1307]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2.1_hiera_large\":\n+        assert torch.allclose(scores, torch.tensor([0.9648, 0.0371, 0.1898]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_tiny\":\n+        assert torch.allclose(scores, torch.tensor([0.0439, 0.9567, 0.1415]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_small\":\n+        assert torch.allclose(scores, torch.tensor([0.9593, 0.1633, 0.0392]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_base_plus\":\n+        assert torch.allclose(scores, torch.tensor([0.0423, 0.9815, 0.0897]).cuda(), atol=1e-2)\n+    elif model_name == \"sam2_hiera_large\":\n+        assert torch.allclose(scores, torch.tensor([0.9514, 0.0535, 0.1787]).cuda(), atol=1e-2)\n+    else:\n+        raise ValueError(f\"Model {model_name} not supported\")\n+\n+    if pytorch_dump_folder is not None:\n+        processor.save_pretrained(pytorch_dump_folder)\n+        hf_model.save_pretrained(pytorch_dump_folder)\n+\n+    if push_to_hub:\n+        repo_id = f\"yonigozlan/{pytorch_dump_folder.split('/')[-1]}\"\n+        processor.push_to_hub(repo_id)\n+        hf_model.push_to_hub(repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    choices = [\n+        \"sam2.1_hiera_tiny\",\n+        \"sam2.1_hiera_small\",\n+        \"sam2.1_hiera_base_plus\",\n+        \"sam2.1_hiera_large\",\n+        \"sam2_hiera_tiny\",\n+        \"sam2_hiera_small\",\n+        \"sam2_hiera_base_plus\",\n+        \"sam2_hiera_large\",\n+    ]\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"sam2.1_hiera_tiny\",\n+        choices=choices,\n+        type=str,\n+        help=\"Name of the original model to convert\",\n+    )\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        type=str,\n+        required=False,\n+        help=\"Path to the original checkpoint\",\n+    )\n+    parser.add_argument(\"--pytorch_dump_folder_path\", default=\"\", type=str, help=\"Path to the output PyTorch model.\")\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the model and processor to the hub after converting\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    hf_model_name = args.model_name.replace(\"_\", \"-\")\n+    checkpoint_path = (\n+        hf_hub_download(f\"facebook/{hf_model_name}\", f\"{args.model_name.lower()}.pt\")\n+        if args.checkpoint_path is None\n+        else args.checkpoint_path\n+    )\n+\n+    convert_sam2_checkpoint(args.model_name, checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "62cb5f3ef610d81ad50d3b2188005e8ada7c0db4",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "added",
            "additions": 2555,
            "deletions": 0,
            "changes": 2555,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0"
        },
        {
            "sha": "d00e791b6c3a82a304c0effe455586245e4ac8be",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "added",
            "additions": 2355,
            "deletions": 0,
            "changes": 2355,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0"
        },
        {
            "sha": "31cf3168e3f584e12cdc3a9fa029548aa2e63c77",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "added",
            "additions": 819,
            "deletions": 0,
            "changes": 819,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,819 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam2_video/modular_sam2_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam2_video.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from copy import deepcopy\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import TensorType\n+from ...utils.import_utils import requires\n+from ...video_utils import VideoInput\n+from .modeling_sam2_video import Sam2VideoInferenceSession\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam2VideoProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`Sam2VideoProcessor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n+    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam2ImageProcessorFast`):\n+            An instance of [`Sam2ImageProcessorFast`].\n+        video_processor (`Sam2VideoVideoProcessor`):\n+            An instance of [`Sam2VideoVideoProcessor`].\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"video_processor\"]\n+    image_processor_class = \"Sam2ImageProcessorFast\"\n+    video_processor_class = \"Sam2VideoVideoProcessor\"\n+\n+    def __init__(\n+        self, image_processor, video_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n+    ):\n+        super().__init__(image_processor, video_processor, **kwargs)\n+        self.point_pad_value = point_pad_value\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        segmentation_maps: ImageInput = None,\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam2VideoImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n+        points and bounding boxes for the model if they are provided.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `reshaped_input_sizes` (`torch.Tensor`): The reshaped input sizes of the images.\n+            - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n+            - `input_points` (`torch.Tensor`): The processed points.\n+            - `input_labels` (`torch.Tensor`): The processed labels.\n+            - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        # pop arguments that are not used in the foward but used nevertheless\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+\n+        # Process input points, labels, and boxes if provided\n+        if input_points is not None or input_labels is not None or input_boxes is not None:\n+            # Validate and convert inputs to standardized format\n+            processed_points = self._validate_single_input(\n+                input_points,\n+                expected_depth=4,\n+                input_name=\"points\",\n+                expected_format=\"[image level, object level, point level, point coordinates]\",\n+                expected_coord_size=2,\n+            )\n+            processed_labels = self._validate_single_input(\n+                input_labels,\n+                expected_depth=3,\n+                input_name=\"labels\",\n+                expected_format=\"[image level, object level, point level]\",\n+            )\n+            processed_boxes = self._validate_single_input(\n+                input_boxes,\n+                expected_depth=3,\n+                input_name=\"boxes\",\n+                expected_format=\"[image level, box level, box coordinates]\",\n+                expected_coord_size=4,\n+            )\n+\n+            # Get padding requirements for all inputs\n+            if processed_points is not None:\n+                points_max_dims = self._get_nested_dimensions(processed_points)[:3]\n+            if processed_labels is not None:\n+                labels_max_dims = self._get_nested_dimensions(processed_labels)[:3]\n+            if processed_boxes is not None:\n+                boxes_max_dims = self._get_nested_dimensions(processed_boxes)[:2]\n+\n+            # Ensure points and labels have consistent dimensions\n+            if processed_points is not None and processed_labels is not None:\n+                if points_max_dims != labels_max_dims:\n+                    raise ValueError(\n+                        \"Input points and labels have inconsistent dimensions. Please ensure they have the same dimensions.\"\n+                    )\n+\n+            # Check that boxes don't need padding (model limitation)\n+            if processed_boxes is not None and len(processed_boxes) >= 2:\n+                if any(len(img_boxes) < boxes_max_dims[1] for img_boxes in processed_boxes):\n+                    raise ValueError(\n+                        \"Input boxes have inconsistent dimensions that would require padding, \"\n+                        \"but boxes cannot be padded due to model limitations. \"\n+                        \"Please ensure all images have the same number of boxes.\"\n+                    )\n+\n+            # Pad and normalize all inputs to final tensor format\n+            if processed_points is not None:\n+                padded_points = self._pad_nested_list(processed_points, points_max_dims + [2])\n+                final_points = torch.tensor(padded_points, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_points, original_sizes, preserve_padding=True)\n+                encoding_image_processor.update({\"input_points\": final_points})\n+\n+            if processed_labels is not None:\n+                padded_labels = self._pad_nested_list(processed_labels, labels_max_dims)\n+                final_labels = torch.tensor(padded_labels, dtype=torch.int64)\n+                encoding_image_processor.update({\"input_labels\": final_labels})\n+\n+            if processed_boxes is not None:\n+                final_boxes = torch.tensor(processed_boxes, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_boxes, original_sizes, is_bounding_box=True)\n+                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+\n+        return encoding_image_processor\n+\n+    def _normalize_coordinates(\n+        self, target_size: int, coords: \"torch.Tensor\", original_size, is_bounding_box=False\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H, W) format.\n+\n+        Args:\n+            target_size (`int`):\n+                The target size of the image.\n+            coords (`torch.Tensor`):\n+                The coordinates to be normalized.\n+            original_size (`tuple`):\n+                The original size of the image.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether the coordinates are bounding boxes.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        new_h, new_w = target_size, target_size\n+        coords = deepcopy(coords).float()\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n+        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _convert_to_nested_list(self, data, expected_depth, current_depth=0):\n+        \"\"\"\n+        Recursively convert various input formats (tensors, numpy arrays, lists) to nested lists.\n+\n+        Args:\n+            data: Input data in any format\n+            expected_depth: Expected nesting depth\n+            current_depth: Current depth in recursion\n+\n+        Returns:\n+            Nested list representation of the data\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Convert tensor/numpy to list if we're at a leaf level or if it's a multi-dimensional array\n+        if isinstance(data, torch.Tensor):  # PyTorch tensor\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small tensor\n+                return data.numpy().tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, np.ndarray):  # NumPy array\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small array\n+                return data.tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, list):\n+            if current_depth == expected_depth:\n+                # We've reached the expected depth, return as is\n+                return data\n+            else:\n+                # Continue recursion\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, (int, float)):\n+            return data\n+        else:\n+            raise ValueError(f\"Unsupported data type: {type(data)}\")\n+\n+    def _get_nested_dimensions(self, nested_list, max_dims=None):\n+        \"\"\"\n+        Get the maximum dimensions at each level of nesting.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list structure.\n+            max_dims (`list`, *optional*):\n+                Current maximum dimensions (for recursion).\n+\n+        Returns:\n+            `list`: A list of maximum dimensions for each nesting level.\n+        \"\"\"\n+        if max_dims is None:\n+            max_dims = []\n+\n+        if not isinstance(nested_list, list):\n+            return max_dims\n+\n+        if len(max_dims) == 0:\n+            max_dims.append(len(nested_list))\n+        else:\n+            max_dims[0] = max(max_dims[0], len(nested_list))\n+\n+        if len(nested_list) > 0:\n+            for item in nested_list:\n+                if isinstance(item, list):\n+                    sub_dims = self._get_nested_dimensions(item)\n+                    # Merge sub_dims into max_dims\n+                    for i, dim in enumerate(sub_dims):\n+                        if i + 1 >= len(max_dims):\n+                            max_dims.append(dim)\n+                        else:\n+                            max_dims[i + 1] = max(max_dims[i + 1], dim)\n+\n+        return max_dims\n+\n+    def _pad_nested_list(self, nested_list, target_dims, current_level=0, pad_value=None):\n+        \"\"\"\n+        Recursively pad a nested list to match target dimensions.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list to pad.\n+            target_dims (`list`):\n+                Target dimensions for each level.\n+            current_level (`int`, *optional*, defaults to 0):\n+                Current nesting level.\n+            pad_value (`int`, *optional*):\n+                Value to use for padding.\n+\n+        Returns:\n+            `list`: The padded nested list.\n+        \"\"\"\n+        if pad_value is None:\n+            pad_value = self.point_pad_value\n+\n+        if current_level >= len(target_dims):\n+            return nested_list\n+\n+        # Ensure we have a list\n+        if not isinstance(nested_list, list):\n+            nested_list = [nested_list]\n+\n+        # Pad current level\n+        current_size = len(nested_list)\n+        target_size = target_dims[current_level]\n+\n+        # Pad with appropriate values\n+        if current_level == len(target_dims) - 1:\n+            # At the coordinate level, pad with pad_value\n+            nested_list.extend([pad_value] * (target_size - current_size))\n+        else:\n+            # At higher levels, pad with nested structures\n+            if current_size > 0:\n+                # Create appropriately sized template\n+                if current_level < len(target_dims) - 2:\n+                    # For non-coordinate levels, create empty nested structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    template = self._create_empty_nested_structure(template_dims, pad_value)\n+                else:\n+                    # For coordinate level, create list of pad_values\n+                    template = [pad_value] * target_dims[current_level + 1]\n+\n+                nested_list.extend([deepcopy(template) for _ in range(target_size - current_size)])\n+            else:\n+                # Create from scratch\n+                template_dims = target_dims[current_level + 1 :]\n+                template = self._create_empty_nested_structure(template_dims, pad_value)\n+                nested_list.extend([deepcopy(template) for _ in range(target_size)])\n+\n+        # Recursively pad sublists\n+        if current_level < len(target_dims) - 1:\n+            for i in range(len(nested_list)):\n+                if isinstance(nested_list[i], list):\n+                    nested_list[i] = self._pad_nested_list(nested_list[i], target_dims, current_level + 1, pad_value)\n+\n+        return nested_list\n+\n+    def _create_empty_nested_structure(self, dims, pad_value):\n+        \"\"\"\n+        Create an empty nested structure with given dimensions filled with pad_value.\n+\n+        Args:\n+            dims (`list`):\n+                The dimensions of the nested structure.\n+            pad_value (`int`):\n+                The value to fill the structure with.\n+        \"\"\"\n+        if len(dims) == 1:\n+            return [pad_value] * dims[0]\n+        else:\n+            return [self._create_empty_nested_structure(dims[1:], pad_value) for _ in range(dims[0])]\n+\n+    def _get_nesting_level(self, input_list):\n+        \"\"\"\n+        Get the nesting level of a list structure.\n+\n+        Args:\n+            input_list (`list`):\n+                The list to get the nesting level of.\n+        \"\"\"\n+        if isinstance(input_list, list):\n+            if len(input_list) == 0:\n+                return 1\n+            return 1 + self._get_nesting_level(input_list[0])\n+        elif isinstance(input_list, (np.ndarray, torch.Tensor)):\n+            # For arrays/tensors, the nesting level is the number of dimensions\n+            return len(input_list.shape)\n+        return 0\n+\n+    def _validate_single_input(\n+        self,\n+        data: Union[torch.Tensor, np.ndarray, list],\n+        expected_depth: int,\n+        input_name: str,\n+        expected_format: str,\n+        expected_coord_size: Optional[int] = None,\n+    ) -> list:\n+        \"\"\"\n+                Validate a single input by ensuring proper nesting and raising an error if the input is not valid.\n+\n+                Args:\n+                    data (`torch.Tensor`, `np.ndarray`, or `list`):\n+                        Input data to process.\n+                    expected_depth (`int`):\n+                        Expected nesting depth.\n+                    input_name (`str`):\n+                        Name of the input for error messages.\n+                    expected_format (`str`):\n+                        The expected format of the input.\n+                    expected_coord_size (`int`, *optional*):\n+                        Expected coordinate size (2 for points, 4 for boxes, None for labels).\n+        .\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Handle tensors and numpy arrays first\n+        if isinstance(data, (torch.Tensor, np.ndarray)):\n+            # For tensors/arrays, we can directly check the number of dimensions\n+            if data.ndim != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a tensor/array with {expected_depth} dimensions. The expected nesting format is {expected_format}. Got {data.ndim} dimensions.\"\n+                )\n+            elif expected_coord_size is not None:\n+                if data.shape[-1] != expected_coord_size:\n+                    raise ValueError(\n+                        f\"Input {input_name} must be a tensor/array with {expected_coord_size} as the last dimension, got {data.shape[-1]}.\"\n+                    )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+        # Handle nested lists\n+        if isinstance(data, list):\n+            current_depth = self._get_nesting_level(data)\n+            if current_depth != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a nested list with {expected_depth} levels. The expected nesting format is {expected_format}. Got {current_depth} levels.\"\n+                )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+    def _normalize_tensor_coordinates(self, tensor, original_sizes, is_bounding_box=False, preserve_padding=False):\n+        \"\"\"\n+        Helper method to normalize coordinates in a tensor across multiple images.\n+\n+        Args:\n+            tensor (`torch.Tensor`):\n+                Input tensor with coordinates.\n+            original_sizes (`list`):\n+                Original image sizes.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether coordinates are bounding boxes.\n+            preserve_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to preserve padding values (for points).\n+        \"\"\"\n+        if preserve_padding:\n+            # For points: avoid normalizing pad values\n+            mask = tensor != self.point_pad_value\n+            coord_mask = mask.all(dim=-1, keepdim=True)\n+\n+        for img_idx in range(len(original_sizes)):\n+            if img_idx < tensor.shape[0]:\n+                original_size = original_sizes[img_idx] if img_idx < len(original_sizes) else original_sizes[0]\n+                normalized_coords = self._normalize_coordinates(\n+                    self.target_size, tensor[img_idx], original_size, is_bounding_box=is_bounding_box\n+                )\n+\n+                if preserve_padding:\n+                    # Only update non-padded values\n+                    img_mask = coord_mask[img_idx]\n+                    tensor[img_idx] = torch.where(\n+                        img_mask.expand_as(tensor[img_idx]), normalized_coords, tensor[img_idx]\n+                    )\n+                else:\n+                    tensor[img_idx] = normalized_coords\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        return self.image_processor.post_process_masks(\n+            masks,\n+            original_sizes,\n+            mask_threshold,\n+            binarize,\n+            max_hole_area,\n+            max_sprinkle_area,\n+            apply_non_overlapping_constraints,\n+            **kwargs,\n+        )\n+\n+    def init_video_session(\n+        self,\n+        video: Optional[VideoInput] = None,\n+        inference_device: Union[str, \"torch.device\"] = \"cpu\",\n+        inference_state_device: Union[str, \"torch.device\"] = None,\n+        processing_device: Union[str, \"torch.device\"] = None,\n+        video_storage_device: Union[str, \"torch.device\"] = None,\n+        max_vision_features_cache_size: int = 1,\n+        torch_dtype: torch.dtype = torch.float32,\n+    ):\n+        \"\"\"\n+        Initializes a video session for inference.\n+        If a video is provided (async inference), the video will be processed and stored on the `video_storage_device`.\n+\n+        Args:\n+            video (`VideoInput`, *optional*):\n+                The video to process. No need to provide when streaming.\n+            inference_device (`str` or `torch.device`, *optional*, defaults to \"cpu\"):\n+                The device to use for inference.\n+            inference_state_device (`str` or `torch.device`, *optional*):\n+                The device to store the inference state on.\n+            processing_device (`str` or `torch.device`, *optional*):\n+                The device to use for video processing.\n+            video_storage_device (`str` or `torch.device`, *optional*):\n+                The device to store the processed video frames on.\n+            max_vision_features_cache_size (`int`, *optional*, defaults to 1):\n+                The maximum number of vision features to cache.\n+            torch_dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n+                The torch dtype to use for the whole session.\n+        \"\"\"\n+        video_storage_device = video_storage_device if video_storage_device is not None else inference_device\n+        inference_state_device = inference_state_device if inference_state_device is not None else inference_device\n+        processing_device = processing_device if processing_device is not None else inference_device\n+        pixel_values_video = None\n+        video_height = None\n+        video_width = None\n+        if video is not None:\n+            processed_video = self.video_processor(videos=video, device=processing_device, return_tensors=\"pt\")\n+            pixel_values_video = processed_video.pixel_values_videos[0]\n+            video_height = processed_video.original_sizes[0][0]\n+            video_width = processed_video.original_sizes[0][1]\n+        inference_session = Sam2VideoInferenceSession(\n+            video=pixel_values_video,\n+            video_height=video_height,\n+            video_width=video_width,\n+            inference_device=inference_device,\n+            video_storage_device=video_storage_device,\n+            inference_state_device=inference_state_device,\n+            torch_dtype=torch_dtype,\n+            max_vision_features_cache_size=max_vision_features_cache_size,\n+        )\n+        return inference_session\n+\n+    def add_inputs_to_inference_session(\n+        self,\n+        inference_session: Sam2VideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: Union[list[int], int],\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        input_masks: Optional[Union[np.ndarray, torch.Tensor, list[np.ndarray], list[torch.Tensor]]] = None,\n+        original_size: Optional[tuple[int, int]] = None,\n+        clear_old_inputs: bool = True,\n+    ) -> Sam2VideoInferenceSession:\n+        \"\"\"\n+        Process new points, boxes, or masks for a video frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam2VideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]` or `int`):\n+                The object ID(s) to associate with the points or box.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            input_masks (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, or `list[torch.Tensor]`, *optional*):\n+                The mask(s) to add to the frame.\n+            original_size (`tuple[int, int]`, *optional*):\n+                The original size of the video. Provide when streaming.\n+            clear_old_inputs (`bool`, *optional*, defaults to `True`):\n+                Whether to clear old inputs for the object.\n+        \"\"\"\n+\n+        if isinstance(obj_ids, int):\n+            obj_ids = [obj_ids]\n+\n+        # Validate inputs\n+        if (input_points is not None) != (input_labels is not None):\n+            raise ValueError(\"points and labels must be provided together\")\n+        if input_points is None and input_boxes is None and input_masks is None:\n+            raise ValueError(\"at least one of points, boxes, or masks must be provided as input\")\n+        if input_masks is not None and (input_points is not None or input_boxes is not None):\n+            raise ValueError(\"masks cannot be provided together with points or boxes\")\n+\n+        if input_masks is not None:\n+            return self.process_new_mask_for_video_frame(inference_session, frame_idx, obj_ids, input_masks)\n+        else:\n+            return self.process_new_points_or_boxes_for_video_frame(\n+                inference_session,\n+                frame_idx,\n+                obj_ids,\n+                input_points,\n+                input_labels,\n+                input_boxes,\n+                original_size,\n+                clear_old_inputs,\n+            )\n+\n+    def process_new_points_or_boxes_for_video_frame(\n+        self,\n+        inference_session: Sam2VideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: list[int],\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_size: Optional[tuple[int, int]] = None,\n+        clear_old_inputs: bool = True,\n+    ) -> Sam2VideoInferenceSession:\n+        \"\"\"\n+        Process new points or boxes for a video frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam2VideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]`):\n+                The object ID(s) to associate with the points or box.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_size (`tuple[int, int]`, *optional*):\n+                The original size of the video. Provide when streaming.\n+            clear_old_inputs (`bool`, *optional*, defaults to `True`):\n+                Whether to clear old inputs for the object.\n+        \"\"\"\n+        if original_size is not None:\n+            inference_session.video_height = original_size[0]\n+            inference_session.video_width = original_size[1]\n+        elif inference_session.video_height is None or inference_session.video_width is None:\n+            raise ValueError(\"original_size must be provided when adding points or boxes on a first streamed frame\")\n+\n+        original_sizes = [[inference_session.video_height, inference_session.video_width]]\n+\n+        encoded_inputs = self(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            original_sizes=original_sizes,\n+            return_tensors=\"pt\",\n+        )\n+        input_points = encoded_inputs.get(\"input_points\", None)\n+        input_labels = encoded_inputs.get(\"input_labels\", None)\n+        input_boxes = encoded_inputs.get(\"input_boxes\", None)\n+\n+        if input_points is not None:\n+            if input_points.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of points ({input_points.shape[1]})\"\n+                )\n+        else:\n+            input_points = torch.zeros(1, len(obj_ids), 0, 2, dtype=torch.float32)\n+        if input_labels is not None:\n+            if input_labels.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of labels ({input_labels.shape[1]})\"\n+                )\n+        else:\n+            input_labels = torch.zeros(1, len(obj_ids), 0, dtype=torch.int32)\n+        if input_boxes is not None:\n+            if input_boxes.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of boxes ({input_boxes.shape[1]})\"\n+                )\n+\n+        if input_boxes is not None:\n+            if not clear_old_inputs:\n+                raise ValueError(\n+                    \"cannot add box without clearing old points, since \"\n+                    \"box prompt must be provided before any point prompt \"\n+                    \"(please use clear_old_points=True instead)\"\n+                )\n+            box_coords = input_boxes.reshape(1, -1, 2, 2)\n+            box_labels = torch.tensor([2, 3], dtype=torch.int32)\n+            box_labels = box_labels.reshape(1, -1, 2)\n+            input_points = torch.cat([box_coords, input_points], dim=2)\n+            input_labels = torch.cat([box_labels, input_labels], dim=2)\n+\n+        for obj_id, idx in zip(obj_ids, range(len(obj_ids))):\n+            obj_idx = inference_session.obj_id_to_idx(obj_id)\n+            input_points_for_obj = input_points[:, idx, :, :].unsqueeze(1)\n+            input_labels_for_obj = input_labels[:, idx, :].unsqueeze(1)\n+            # Handle existing points\n+            if not clear_old_inputs:\n+                existing_points = inference_session.point_inputs_per_obj[obj_idx].get(frame_idx, None)\n+                if existing_points is not None:\n+                    # Concatenate with existing points\n+                    input_points_for_obj = torch.cat(\n+                        [existing_points[\"point_coords\"].to(input_points_for_obj.device), input_points_for_obj], dim=2\n+                    )\n+                    input_labels_for_obj = torch.cat(\n+                        [existing_points[\"point_labels\"].to(input_labels_for_obj.device), input_labels_for_obj], dim=2\n+                    )\n+            point_inputs = {\n+                \"point_coords\": input_points_for_obj,\n+                \"point_labels\": input_labels_for_obj,\n+            }\n+\n+            inference_session.add_point_inputs(obj_idx, frame_idx, point_inputs)\n+            inference_session.remove_mask_inputs(obj_idx, frame_idx)  # Clear any mask inputs\n+\n+        inference_session.obj_with_new_inputs = obj_ids\n+\n+    def process_new_mask_for_video_frame(\n+        self,\n+        inference_session: Sam2VideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: list[int],\n+        input_masks: Union[np.ndarray, torch.Tensor, list[np.ndarray], list[torch.Tensor]],\n+    ):\n+        \"\"\"\n+        Add new mask to a frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam2VideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]`):\n+                The object ID(s) to associate with the mask.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_masks (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, or `list[torch.Tensor]`):\n+                The mask(s) to add to the frame.\n+        \"\"\"\n+        if not isinstance(input_masks, list):\n+            input_masks = [input_masks]\n+        if len(input_masks) != len(obj_ids):\n+            raise ValueError(\n+                f\"Number of object ids ({len(obj_ids)}) does not match number of masks ({len(input_masks)})\"\n+            )\n+\n+        for obj_id, mask in zip(obj_ids, input_masks):\n+            obj_idx = inference_session.obj_id_to_idx(obj_id)\n+\n+            device = inference_session.inference_device\n+\n+            # Process mask\n+            if not isinstance(mask, torch.Tensor):\n+                mask = torch.tensor(mask, dtype=torch.bool)\n+            nb_dim = mask.dim()\n+            if nb_dim > 4 or nb_dim < 2:\n+                raise ValueError(f\"Mask has an unsupported number of dimensions: {nb_dim}\")\n+            for i in range(4 - nb_dim):\n+                mask = mask.unsqueeze(0)\n+\n+            mask_H, mask_W = mask.shape[-2:]\n+            mask_inputs_orig = mask.to(device)\n+            mask_inputs_orig = mask_inputs_orig.float().to(device)\n+\n+            # Resize mask if needed\n+            if mask_H != self.target_size or mask_W != self.target_size:\n+                mask_inputs = torch.nn.functional.interpolate(\n+                    mask_inputs_orig,\n+                    size=(self.target_size, self.target_size),\n+                    align_corners=False,\n+                    mode=\"bilinear\",\n+                    antialias=True,\n+                )\n+                mask_inputs = (mask_inputs >= 0.5).float()\n+            else:\n+                mask_inputs = mask_inputs_orig\n+\n+            inference_session.add_mask_inputs(obj_idx, frame_idx, mask_inputs)\n+            inference_session.remove_point_inputs(obj_idx, frame_idx)  # Clear any point inputs\n+\n+        inference_session.obj_with_new_inputs = obj_ids\n+\n+\n+__all__ = [\"Sam2VideoProcessor\"]"
        },
        {
            "sha": "e304243051107357b1f3ebc85ceaf4d55fc4e846",
            "filename": "src/transformers/models/sam2_video/video_processing_sam2_video.py",
            "status": "added",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fvideo_processing_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,123 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for SAM2.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from ...utils.import_utils import requires\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch.nn import functional as F_t\n+\n+\n+if is_vision_available():\n+    from ...image_utils import PILImageResampling\n+\n+\n+@requires(backends=(\"torchvision\",))\n+class Sam2VideoVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 1024, \"width\": 1024}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    def _preprocess(\n+        self,\n+        videos: list[\"torch.Tensor\"],\n+        size: SizeDict,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        original_sizes = [video.shape[-2:] for video in videos]\n+        reshaped_input_sizes = [(size.height, size.width) for _ in range(len(videos))]\n+        batch_feature = super()._preprocess(videos, size=size, return_tensors=return_tensors, **kwargs)\n+        batch_feature = BatchFeature(\n+            data={\n+                \"original_sizes\": original_sizes,\n+                \"reshaped_input_sizes\": reshaped_input_sizes,\n+                **batch_feature.data,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+        return batch_feature\n+\n+    def post_process_masks(\n+        self, masks, original_sizes, reshaped_input_sizes, mask_threshold=0.0, binarize=True, pad_size=None\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            reshaped_input_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                The threshold to use for binarizing the masks.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            pad_size (`int`, *optional*, defaults to `self.pad_size`):\n+                The target size the images were padded to before being passed to the model. If None, the target size is\n+                assumed to be the processor's `pad_size`.\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        pad_size = self.size if pad_size is None else pad_size\n+        target_image_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n+            original_sizes = original_sizes.tolist()\n+        if isinstance(reshaped_input_sizes, (torch.Tensor, np.ndarray)):\n+            reshaped_input_sizes = reshaped_input_sizes.tolist()\n+        output_masks = []\n+        for i, original_size in enumerate(original_sizes):\n+            if isinstance(masks[i], np.ndarray):\n+                masks[i] = torch.from_numpy(masks[i])\n+            elif not isinstance(masks[i], torch.Tensor):\n+                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+            interpolated_mask = F_t.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n+            interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n+            interpolated_mask = F_t.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n+            if binarize:\n+                interpolated_mask = interpolated_mask > mask_threshold\n+            output_masks.append(interpolated_mask)\n+\n+        return output_masks\n+\n+\n+__all__ = [\"Sam2VideoVideoProcessor\"]"
        },
        {
            "sha": "62a40da18bd54aa349b90e5192e4564a73bd92ee",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -814,7 +814,7 @@ def forward(\n                 attention_similarity=attention_similarity,\n                 **kwargs,\n             )\n-        # Apply the final attenion layer from the points to the image\n+        # Apply the final attention layer from the points to the image\n         query = queries + point_embeddings\n         key = keys + image_positional_embeddings\n "
        },
        {
            "sha": "e19a048fd2bf62233dc930c2ac8c45e50adb9237",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 26,
            "deletions": 2,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -125,6 +125,10 @@ def _sanitize_parameters(self, **kwargs):\n             forward_params[\"mask_threshold\"] = kwargs[\"mask_threshold\"]\n         if \"stability_score_thresh\" in kwargs:\n             forward_params[\"stability_score_thresh\"] = kwargs[\"stability_score_thresh\"]\n+        if \"max_hole_area\" in kwargs:\n+            forward_params[\"max_hole_area\"] = kwargs[\"max_hole_area\"]\n+        if \"max_sprinkle_area\" in kwargs:\n+            forward_params[\"max_sprinkle_area\"] = kwargs[\"max_sprinkle_area\"]\n         if \"crops_nms_thresh\" in kwargs:\n             postprocess_kwargs[\"crops_nms_thresh\"] = kwargs[\"crops_nms_thresh\"]\n         if \"output_rle_mask\" in kwargs:\n@@ -196,7 +200,7 @@ def preprocess(\n         timeout: Optional[float] = None,\n     ):\n         image = load_image(image, timeout=timeout)\n-        target_size = self.image_processor.size[\"longest_edge\"]\n+        target_size = self.image_processor.size.get(\"longest_edge\", self.image_processor.size.get(\"height\"))\n         crop_boxes, grid_points, cropped_images, input_labels = self.image_processor.generate_crop_boxes(\n             image, target_size, crops_n_layers, crop_overlap_ratio, points_per_crop, crop_n_points_downscale_factor\n         )\n@@ -250,6 +254,8 @@ def _forward(\n         stability_score_thresh=0.95,\n         mask_threshold=0,\n         stability_score_offset=1,\n+        max_hole_area=None,\n+        max_sprinkle_area=None,\n     ):\n         input_boxes = model_inputs.pop(\"input_boxes\")\n         is_last = model_inputs.pop(\"is_last\")\n@@ -260,8 +266,26 @@ def _forward(\n \n         # post processing happens here in order to avoid CPU GPU copies of ALL the masks\n         low_resolution_masks = model_outputs[\"pred_masks\"]\n+        postprocess_kwargs = {}\n+        if max_hole_area is not None:\n+            postprocess_kwargs[\"max_hole_area\"] = max_hole_area\n+        if max_sprinkle_area is not None and max_sprinkle_area > 0:\n+            postprocess_kwargs[\"max_sprinkle_area\"] = max_sprinkle_area\n+        if postprocess_kwargs:\n+            low_resolution_masks = self.image_processor.post_process_masks(\n+                low_resolution_masks,\n+                original_sizes,\n+                mask_threshold=mask_threshold,\n+                reshaped_input_sizes=reshaped_input_sizes,\n+                binarize=False,\n+                **postprocess_kwargs,\n+            )\n         masks = self.image_processor.post_process_masks(\n-            low_resolution_masks, original_sizes, reshaped_input_sizes, mask_threshold, binarize=False\n+            low_resolution_masks,\n+            original_sizes,\n+            mask_threshold=mask_threshold,\n+            reshaped_input_sizes=reshaped_input_sizes,\n+            binarize=False,\n         )\n         iou_scores = model_outputs[\"iou_scores\"]\n         masks, iou_scores, boxes = self.image_processor.filter_masks("
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2F__init__.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0"
        },
        {
            "sha": "3818946d031325e3f7b74ffdf7a519a471317faf",
            "filename": "tests/models/sam2/test_image_processing_sam2.py",
            "status": "added",
            "additions": 243,
            "deletions": 0,
            "changes": 243,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_image_processing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_image_processing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_image_processing_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,243 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from datasets import load_dataset\n+\n+from transformers.file_utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available() and is_torchvision_available():\n+    from transformers import Sam2ImageProcessorFast\n+\n+\n+class Sam2ImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        mask_size=None,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+    ):\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        mask_size = mask_size if mask_size is not None else {\"height\": 12, \"width\": 12}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.mask_size = mask_size\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"mask_size\": self.mask_size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n+@require_torch\n+@require_vision\n+class SamImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    fast_image_processing_class = Sam2ImageProcessorFast if is_torchvision_available() else None\n+    test_slow_image_processor = False\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Sam2ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"mask_size\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing_class = image_processing_class(**self.image_processor_dict)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    def test_call_segmentation_maps(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processor(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.mask_size[\"height\"],\n+                    self.image_processor_tester.mask_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processor(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.mask_size[\"height\"],\n+                    self.image_processor_tester.mask_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processor(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.mask_size[\"height\"],\n+                    self.image_processor_tester.mask_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processor(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.mask_size[\"height\"],\n+                    self.image_processor_tester.mask_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)"
        },
        {
            "sha": "f697450f462311749e47d78590a3edd84a4ee9b2",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "added",
            "additions": 1013,
            "deletions": 0,
            "changes": 1013,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,1013 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM2 model.\"\"\"\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    Sam2Config,\n+    Sam2HieraDetConfig,\n+    Sam2MaskDecoderConfig,\n+    Sam2Processor,\n+    Sam2PromptEncoderConfig,\n+    Sam2VisionConfig,\n+    pipeline,\n+)\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+from transformers.video_utils import load_video\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import Sam2Model, Sam2Processor, Sam2VisionModel\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class Sam2VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=12,\n+        embed_dim_per_stage=[12, 24, 48, 96],\n+        num_attention_heads_per_stage=[1, 2, 4, 8],\n+        num_channels=3,\n+        image_size=128,\n+        patch_kernel_size=7,\n+        patch_stride=4,\n+        patch_padding=3,\n+        batch_size=2,\n+        blocks_per_stage=[1, 2, 7, 2],\n+        backbone_channel_list=[96, 48, 24, 12],\n+        backbone_feature_sizes=[[32, 32], [16, 16], [8, 8]],\n+        fpn_hidden_size=32,\n+        is_training=False,\n+    ):\n+        self.parent = parent\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.num_channels = num_channels\n+        self.patch_kernel_size = patch_kernel_size\n+        self.patch_stride = patch_stride\n+        self.patch_padding = patch_padding\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.blocks_per_stage = blocks_per_stage\n+        self.embed_dim_per_stage = embed_dim_per_stage\n+        self.num_attention_heads_per_stage = num_attention_heads_per_stage\n+        self.backbone_channel_list = backbone_channel_list\n+        self.backbone_feature_sizes = backbone_feature_sizes\n+        self.fpn_hidden_size = fpn_hidden_size\n+\n+    def get_config(self):\n+        backbone_config = Sam2HieraDetConfig(\n+            hidden_size=self.hidden_size,\n+            num_channels=self.num_channels,\n+            image_size=self.image_size,\n+            patch_stride=self.patch_stride,\n+            patch_kernel_size=self.patch_kernel_size,\n+            patch_padding=self.patch_padding,\n+            blocks_per_stage=self.blocks_per_stage,\n+            embed_dim_per_stage=self.embed_dim_per_stage,\n+            num_attention_heads_per_stage=self.num_attention_heads_per_stage,\n+        )\n+        return Sam2VisionConfig(\n+            backbone_config=backbone_config,\n+            backbone_channel_list=self.backbone_channel_list,\n+            backbone_feature_sizes=self.backbone_feature_sizes,\n+            fpn_hidden_size=self.fpn_hidden_size,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Sam2VisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        output_size = self.image_size // self.patch_stride // (2 * len(self.blocks_per_stage))\n+        output_channels = self.hidden_size * 2 * len(self.blocks_per_stage)\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, output_size, output_size, output_channels)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Sam2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (Sam2VisionModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = Sam2VisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Sam2VisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Overriding as attention shape depends on window_size\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            expected_num_attentions = sum(self.model_tester.blocks_per_stage)\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            window_size = config.backbone_config.window_size_per_stage[0]\n+            out_dim = config.backbone_config.hidden_size\n+            patch_stride = config.backbone_config.patch_stride\n+            num_windows = (\n+                self.model_tester.batch_size * (config.backbone_config.image_size // (window_size * patch_stride)) ** 2\n+            )\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                [num_windows, window_size, window_size, out_dim],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                [num_windows, window_size, window_size, out_dim],\n+            )\n+\n+    # Overriding as attention shape depends on window_size\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class, image_size):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.hidden_states\n+\n+            expected_num_layers = sum(self.model_tester.blocks_per_stage) + 1\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-4:]),\n+                [\n+                    self.model_tester.batch_size,\n+                    self.model_tester.image_size // self.model_tester.patch_stride,\n+                    self.model_tester.image_size // self.model_tester.patch_stride,\n+                    self.model_tester.hidden_size,\n+                ],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        image_size = self.model_tester.image_size\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class, image_size)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class, image_size)\n+\n+    # Override as diffence slightly higher than the threshold\n+    def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n+\n+\n+class Sam2PromptEncoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        input_image_size=128,\n+        patch_size=16,\n+        mask_input_channels=8,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+    ):\n+        self.hidden_size = hidden_size\n+        self.input_image_size = input_image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+\n+    def get_config(self):\n+        return Sam2PromptEncoderConfig(\n+            image_size=self.input_image_size,\n+            patch_size=self.patch_size,\n+            mask_input_channels=self.mask_input_channels,\n+            hidden_size=self.hidden_size,\n+            num_point_embeddings=self.num_point_embeddings,\n+            hidden_act=self.hidden_act,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        dummy_points = floats_tensor([self.batch_size, 3, 2])\n+        config = self.get_config()\n+\n+        return config, dummy_points\n+\n+\n+class Sam2MaskDecoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        hidden_act=\"relu\",\n+        mlp_dim=64,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=32,\n+    ):\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.mlp_dim = mlp_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_downsample_rate = attention_downsample_rate\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+\n+    def get_config(self):\n+        return Sam2MaskDecoderConfig(\n+            hidden_size=self.hidden_size,\n+            hidden_act=self.hidden_act,\n+            mlp_dim=self.mlp_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            attention_downsample_rate=self.attention_downsample_rate,\n+            num_multimask_outputs=self.num_multimask_outputs,\n+            iou_head_depth=self.iou_head_depth,\n+            iou_head_hidden_dim=self.iou_head_hidden_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        dummy_inputs = {\n+            \"image_embedding\": floats_tensor([self.batch_size, self.hidden_size]),\n+        }\n+\n+        return config, dummy_inputs\n+\n+\n+class Sam2ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        num_channels=3,\n+        image_size=128,\n+        hidden_size=12,\n+        patch_kernel_size=7,\n+        patch_stride=4,\n+        patch_padding=3,\n+        blocks_per_stage=[1, 2, 7, 2],\n+        embed_dim_per_stage=[12, 24, 48, 96],\n+        backbone_channel_list=[96, 48, 24, 12],\n+        backbone_feature_sizes=[[32, 32], [16, 16], [8, 8]],\n+        fpn_hidden_size=32,\n+        memory_encoder_hidden_size=32,\n+        batch_size=2,\n+        is_training=False,\n+    ):\n+        self.parent = parent\n+        self.image_size = image_size\n+        self.hidden_size = hidden_size\n+        self.patch_kernel_size = patch_kernel_size\n+        self.patch_stride = patch_stride\n+        self.patch_padding = patch_padding\n+        self.blocks_per_stage = blocks_per_stage\n+        self.embed_dim_per_stage = embed_dim_per_stage\n+        self.backbone_channel_list = backbone_channel_list\n+        self.backbone_feature_sizes = backbone_feature_sizes\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.memory_encoder_hidden_size = memory_encoder_hidden_size\n+\n+        self.prompt_encoder_tester = Sam2PromptEncoderTester()\n+        self.mask_decoder_tester = Sam2MaskDecoderTester()\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        backbone_config = Sam2HieraDetConfig(\n+            hidden_size=self.hidden_size,\n+            num_channels=self.num_channels,\n+            image_size=self.image_size,\n+            patch_stride=self.patch_stride,\n+            patch_kernel_size=self.patch_kernel_size,\n+            patch_padding=self.patch_padding,\n+            blocks_per_stage=self.blocks_per_stage,\n+            embed_dim_per_stage=self.embed_dim_per_stage,\n+        )\n+        vision_config = Sam2VisionConfig(\n+            backbone_config=backbone_config,\n+            backbone_channel_list=self.backbone_channel_list,\n+            backbone_feature_sizes=self.backbone_feature_sizes,\n+            fpn_hidden_size=self.fpn_hidden_size,\n+        )\n+\n+        prompt_encoder_config = self.prompt_encoder_tester.get_config()\n+\n+        mask_decoder_config = self.mask_decoder_tester.get_config()\n+\n+        return Sam2Config(\n+            vision_config=vision_config,\n+            prompt_encoder_config=prompt_encoder_config,\n+            mask_decoder_config=mask_decoder_config,\n+            memory_attention_hidden_size=self.hidden_size,\n+            memory_encoder_hidden_size=self.memory_encoder_hidden_size,\n+            image_size=self.image_size,\n+            mask_downsampler_embed_dim=32,\n+            memory_fuser_embed_dim=32,\n+            memory_attention_num_layers=1,\n+            memory_attention_feed_forward_hidden_size=32,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Sam2Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        self.parent.assertEqual(result.iou_scores.shape, (self.batch_size, 1, 3))\n+        self.parent.assertEqual(result.pred_masks.shape[:3], (self.batch_size, 1, 3))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Sam2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (Sam2Model,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": Sam2Model, \"mask-generation\": Sam2Model} if is_torch_available() else {}\n+    )\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Sam2ModelTester(self)\n+        common_properties = [\"initializer_range\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Sam2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Overriding as attention shape depends on window_size\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            expected_num_attentions = sum(self.model_tester.blocks_per_stage)\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.mask_decoder_config.output_attentions = True\n+            config.vision_config.output_attentions = True\n+            config.output_attentions = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            window_size = config.vision_config.backbone_config.window_size_per_stage[0]\n+            out_dim = self.model_tester.hidden_size\n+            patch_stride = self.model_tester.patch_stride\n+            num_windows = (\n+                self.model_tester.batch_size * (self.model_tester.image_size // (window_size * patch_stride)) ** 2\n+            )\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                [num_windows, window_size, window_size, out_dim],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                [num_windows, window_size, window_size, out_dim],\n+            )\n+\n+    # Override as Sam2Model has different sub-modules\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                vision_encoder_sdpa = getattr(model_sdpa, \"vision_encoder\")\n+                mask_decoder_sdpa = getattr(model_sdpa, \"mask_decoder\")\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(mask_decoder_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(vision_encoder_sdpa.config._attn_implementation == \"sdpa\")\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(getattr(model_eager, \"mask_decoder\").config._attn_implementation == \"eager\")\n+                self.assertTrue(getattr(model_eager, \"vision_encoder\").config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+    # Override as Sam2Model doesn't have hidden states\n+    def flash_attn_inference_equivalence(self, attn_implementation: str, padding_side: str):\n+        r\"\"\"\n+        Tests the equivalence between the eager and flash attention implementations.\n+        This test is only for inference and runs with `torch_dtype=torch.bfloat16`.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_model_classes:\n+            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n+                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n+            ):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model.main_input_name][:1]\n+                if dummy_input.dtype in [torch.float32, torch.float16]:\n+                    dummy_input = dummy_input.to(torch.bfloat16)\n+\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+\n+                if dummy_attention_mask is not None:\n+                    dummy_attention_mask = dummy_attention_mask[:1]\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[:, 1:] = 1\n+                        dummy_attention_mask[:, :1] = 0\n+                    else:\n+                        dummy_attention_mask[:, :-1] = 1\n+                        dummy_attention_mask[:, -1:] = 0\n+                if model.config.is_encoder_decoder:\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n+\n+                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                    outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                else:\n+                    outputs = model(dummy_input, output_hidden_states=True)\n+                    outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.vision_hidden_states[-1]\n+                logits_fa = outputs_fa.vision_hidden_states[-1]\n+\n+                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+\n+                if model.config.is_encoder_decoder:\n+                    other_inputs = {\n+                        \"decoder_input_ids\": decoder_input_ids,\n+                        \"decoder_attention_mask\": dummy_attention_mask,\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                    outputs = model(dummy_input, **other_inputs)\n+                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+                else:\n+                    other_inputs = {\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                    outputs = model(dummy_input, **other_inputs)\n+                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+\n+                logits = outputs.vision_hidden_states[-1]\n+                logits_fa = outputs_fa.vision_hidden_states[-1]\n+\n+                if padding_side == \"left\":\n+                    assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n+\n+                    # check with inference + dropout\n+                    model.train()\n+                    _ = model_fa(dummy_input, **other_inputs)\n+                else:\n+                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n+\n+    # Override as diffence slightly higher than the threshold\n+    def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    @unittest.skip(reason=\"Sam2Model does not support training\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in sub modules tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"yonigozlan/sam2.1_hiera_tiny_hf\"\n+        model = Sam2Model.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM2 model can't be compiled dynamic yet\")\n+\n+\n+def prepare_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_groceries_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_dog_img():\n+    img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_video():\n+    video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+    raw_video, _ = load_video(video_url)\n+    return raw_video\n+\n+\n+@slow\n+class Sam2ModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # fill_hole area is set to 0 to avoid running the `get_connected_components` cuda kernel\n+        self.model = Sam2Model.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\", fill_hole_area=0).to(torch.float32)\n+        self.processor = Sam2Processor.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\")\n+        self.model.to(torch_device)\n+        self.model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_mask_generation_one_point_multimask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+\n+        inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 3))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 3, 256, 256))\n+        sorted_indices = torch.argsort(outputs.iou_scores.squeeze(), descending=True)\n+        scores = outputs.iou_scores.squeeze()[sorted_indices]\n+        masks_logits = outputs.pred_masks.squeeze()[sorted_indices][0, :3, :3]\n+        torch.testing.assert_close(\n+            scores, torch.tensor([0.9547, 0.4932, 0.0427]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor(\n+                [[-24.9288, -41.7466, -31.0128], [-34.5113, -31.1054, -36.5913], [-25.2597, -37.5912, -33.4030]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_one_point_no_multimask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+\n+        inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 256, 256))\n+        scores = outputs.iou_scores.squeeze((0, 1))\n+        masks_logits = outputs.pred_masks.squeeze((0, 1))[0, :3, :3]\n+        torch.testing.assert_close(scores, torch.tensor([0.9364]).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor(\n+                [[-7.0462, -13.3857, -9.6419], [-10.4565, -9.7174, -12.3528], [-7.3704, -12.4391, -10.5539]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_batched_images_multi_points(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_dog_img()\n+        input_points = [[[[500, 375]]], [[[770, 200], [730, 120]]]]\n+        input_labels = [[[1]], [[1, 0]]]\n+\n+        inputs = self.processor(\n+            images=[raw_image1, raw_image2], input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 1, 3))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 1, 3, 256, 256))\n+\n+        sorted_indices = torch.argsort(outputs.iou_scores[0].squeeze(), descending=True)\n+        scores1 = outputs.iou_scores[0].squeeze()[sorted_indices]\n+        masks_logits1 = outputs.pred_masks[0].squeeze()[sorted_indices][0, :3, :3]\n+        sorted_indices = torch.argsort(outputs.iou_scores[1].squeeze(), descending=True)\n+        scores2 = outputs.iou_scores[1].squeeze()[sorted_indices]\n+        masks_logits2 = outputs.pred_masks[1].squeeze()[sorted_indices][0, :3, :3]\n+        torch.testing.assert_close(\n+            scores1, torch.tensor([0.9586, 0.4913, 0.0448]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            masks_logits1,\n+            torch.tensor(\n+                [[-22.2555, -37.9250, -27.8928], [-30.8681, -27.9519, -32.8032], [-22.4133, -33.9966, -29.7111]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        torch.testing.assert_close(\n+            scores2, torch.tensor([0.9504, 0.8117, 0.7426]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            masks_logits2,\n+            torch.tensor(\n+                [[-13.1182, -17.3217, -14.9651], [-16.2372, -12.7739, -17.6346], [-13.5013, -17.1549, -15.6614]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_batched_images_batched_points_multi_points(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_groceries_image()\n+        input_points = [[[[500, 375]], [[650, 750]]], [[[400, 300]], [[630, 300], [550, 300]]]]\n+        input_labels = [[[1], [1]], [[1], [1, 1]]]\n+        inputs = self.processor(\n+            images=[raw_image1, raw_image2], input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 2, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 2, 1, 256, 256))\n+        torch.testing.assert_close(\n+            outputs.iou_scores,\n+            torch.tensor([[[0.9500], [0.9718]], [[0.9568], [0.9114]]]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-5.8131, -11.3020], [-8.6487, -8.0690]]], [[[-4.7731, -8.7606], [-6.2399, -7.0738]]]],\n+                    [[[[-13.8661, -19.1254], [-20.2477, -14.1636]]], [[[-8.8229, -10.2760], [-11.3797, -8.7189]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_batched_images_batched_boxes(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_groceries_image()\n+        input_boxes = [\n+            [[75, 275, 1725, 850], [425, 600, 700, 875], [1375, 550, 1650, 800], [1240, 675, 1400, 750]],\n+            [[450, 170, 520, 350], [350, 190, 450, 350], [500, 170, 580, 350], [580, 170, 640, 350]],\n+        ]\n+        inputs = self.processor(images=[raw_image1, raw_image2], input_boxes=input_boxes, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 4, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 4, 1, 256, 256))\n+        torch.testing.assert_close(\n+            outputs.iou_scores,\n+            torch.tensor([[[0.9873], [0.9264], [0.9496], [0.9208]], [[0.9445], [0.9496], [0.9497], [0.9481]]]).to(\n+                torch_device\n+            ),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [\n+                        [[[-7.6204, -11.9286], [-8.7747, -10.5662]]],\n+                        [[[-17.1070, -23.4025], [-20.9608, -19.5600]]],\n+                        [[[-20.5766, -29.4410], [-26.0739, -24.3225]]],\n+                        [[[-19.7201, -29.0836], [-24.4915, -23.6377]]],\n+                    ],\n+                    [\n+                        [[[-18.5259, -23.5202], [-25.1906, -17.2518]]],\n+                        [[[-20.1214, -25.4215], [-25.7877, -19.1169]]],\n+                        [[[-21.0878, -24.7938], [-27.5625, -19.2650]]],\n+                        [[[-20.5210, -22.5343], [-26.0968, -17.7544]]],\n+                    ],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_from_existing_points_and_mask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+        original_inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(**original_inputs)\n+\n+        # best mask to use as input for new points\n+        mask_input = outputs.pred_masks[:, :, torch.argmax(outputs.iou_scores)]\n+\n+        new_input_points = [[[[500, 375], [1125, 625]]]]\n+        new_input_labels = [[[1, 1]]]\n+        inputs = self.processor(\n+            input_points=new_input_points,\n+            input_labels=new_input_labels,\n+            original_sizes=original_inputs[\"original_sizes\"],\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(\n+                **inputs,\n+                input_masks=mask_input,\n+                image_embeddings=outputs.image_embeddings,\n+                multimask_output=False,\n+            )\n+\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 256, 256))\n+        scores = outputs.iou_scores.squeeze((0, 1))\n+        masks_logits = outputs.pred_masks.squeeze((0, 1))[0, :3, :3]\n+        torch.testing.assert_close(scores, torch.tensor([0.9738]).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor([[-5.3899, -9.7908, -8.4931], [-5.5144, -8.8731, -8.3000], [-5.5976, -9.9249, -9.0761]]).to(\n+                torch_device\n+            ),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # with negative point\n+        new_input_points = [[[[500, 375], [1125, 625]]]]\n+        new_input_labels = [[[1, 0]]]\n+        inputs = self.processor(\n+            input_points=new_input_points,\n+            input_labels=new_input_labels,\n+            original_sizes=original_inputs[\"original_sizes\"],\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(\n+                **inputs,\n+                input_masks=mask_input,\n+                image_embeddings=outputs.image_embeddings,\n+                multimask_output=False,\n+            )\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 256, 256))\n+        scores = outputs.iou_scores.squeeze((0, 1))\n+        masks_logits = outputs.pred_masks.squeeze((0, 1))[0, :3, :3]\n+        torch.testing.assert_close(scores, torch.tensor([0.9719]).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor(\n+                [[-15.5081, -21.8641, -18.0479], [-17.4401, -17.4754, -23.6469], [-14.3975, -19.4346, -18.5884]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_dummy_pipeline_generation(self):\n+        generator = pipeline(\"mask-generation\", model=\"yonigozlan/sam2.1_hiera_tiny_hf\", device=torch_device)\n+        raw_image = prepare_image()\n+\n+        _ = generator(raw_image, points_per_batch=64)"
        },
        {
            "sha": "d0b099c77698c9c1caa41774b4e2a70d81ddaaa7",
            "filename": "tests/models/sam2/test_processor_sam2.py",
            "status": "added",
            "additions": 147,
            "deletions": 0,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_processor_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2%2Ftest_processor_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_processor_sam2.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,147 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torchvision,\n+    require_vision,\n+)\n+from transformers.utils import is_tf_available, is_torch_available, is_vision_available\n+\n+\n+if is_vision_available():\n+    from transformers import AutoProcessor, Sam2ImageProcessorFast, Sam2Processor\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_tf_available():\n+    pass\n+\n+\n+@require_vision\n+@require_torchvision\n+class Sam2ProcessorTest(unittest.TestCase):\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Sam2ImageProcessorFast()\n+        processor = Sam2Processor(image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        image_inputs = torch.randint(0, 256, size=(1, 3, 30, 400), dtype=torch.uint8)\n+        # image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+        return image_inputs\n+\n+    def prepare_mask_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        mask_inputs = torch.randint(0, 256, size=(1, 30, 400), dtype=torch.uint8)\n+        # mask_inputs = [Image.fromarray(x) for x in mask_inputs]\n+        return mask_inputs\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = Sam2Processor(image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n+\n+        processor = Sam2Processor.from_pretrained(self.tmpdirname, do_normalize=False, padding_value=1.0)\n+\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.image_processor, Sam2ImageProcessorFast)\n+\n+    def test_image_processor_no_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = Sam2Processor(image_processor=image_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_feat_extract = image_processor(image_input)\n+        input_processor = processor(images=image_input)\n+\n+        for key in input_feat_extract.keys():\n+            if key == \"pixel_values\":\n+                for input_feat_extract_item, input_processor_item in zip(\n+                    input_feat_extract[key], input_processor[key]\n+                ):\n+                    np.testing.assert_array_equal(input_feat_extract_item, input_processor_item)\n+            else:\n+                self.assertEqual(input_feat_extract[key], input_processor[key])\n+\n+        for image in input_feat_extract.pixel_values:\n+            self.assertEqual(image.shape, (3, 1024, 1024))\n+\n+        for original_size in input_feat_extract.original_sizes:\n+            np.testing.assert_array_equal(original_size, np.array([30, 400]))\n+\n+    def test_image_processor_with_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = Sam2Processor(image_processor=image_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+        mask_input = self.prepare_mask_inputs()\n+\n+        input_feat_extract = image_processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+        for label in input_feat_extract.labels:\n+            self.assertEqual(label.shape, (256, 256))\n+\n+    @require_torch\n+    def test_post_process_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = Sam2Processor(image_processor=image_processor)\n+        dummy_masks = [torch.ones((1, 3, 5, 5))]\n+\n+        original_sizes = [[1764, 2646]]\n+\n+        masks = processor.post_process_masks(dummy_masks, original_sizes)\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        masks = processor.post_process_masks(dummy_masks, torch.tensor(original_sizes))\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        # should also work with np\n+        dummy_masks = [np.ones((1, 3, 5, 5))]\n+        masks = processor.post_process_masks(dummy_masks, np.array(original_sizes))\n+\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        dummy_masks = [[1, 0], [0, 1]]\n+        with self.assertRaises(ValueError):\n+            masks = processor.post_process_masks(dummy_masks, np.array(original_sizes))"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam2_video/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2_video%2F__init__.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0"
        },
        {
            "sha": "771fd5e6ade92a52fd3a8e32690d86c1de891eaf",
            "filename": "tests/models/sam2_video/test_modeling_sam2_video.py",
            "status": "added",
            "additions": 508,
            "deletions": 0,
            "changes": 508,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,508 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM2 model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import requests\n+\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+from transformers.video_utils import load_video\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Sam2VideoModel, Sam2VideoProcessor\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+def prepare_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_groceries_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_dog_img():\n+    img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_video():\n+    video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+    raw_video, _ = load_video(video_url)\n+    return raw_video\n+\n+\n+@slow\n+class Sam2VideoModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # fill_hole area is set to 0 to avoid running the `get_connected_components` cuda kernel\n+        self.video_model = Sam2VideoModel.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\", fill_hole_area=0).to(\n+            torch.float32\n+        )\n+        self.processor = Sam2VideoProcessor.from_pretrained(\"yonigozlan/sam2.1_hiera_tiny_hf\")\n+        self.video_model.to(torch_device)\n+        self.video_model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_mask_generation_video_one_point(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350]]]],\n+            input_labels=[[[1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        self.assertEqual(low_res_masks.shape, (1, 1, 256, 256))\n+        video_res_masks = self.processor.post_process_masks([low_res_masks], [raw_video.shape[-3:-1]], binarize=False)[\n+            0\n+        ]\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-21.4113, -21.4113, -22.9687], [-23.3090, -23.3090, -24.2606], [-27.5705, -27.5705, -27.1616]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-21.4113, -21.4113], [-23.3090, -23.3090]]]],\n+                    [[[[-20.1003, -20.1003], [-21.2294, -21.2294]]]],\n+                    [[[[-19.9619, -19.9619], [-21.3060, -21.3060]]]],\n+                ],\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_video_one_point_propagate_in_video_directly(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350]]]],\n+            input_labels=[[[1]]],\n+        )\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-21.4113, -21.4113], [-23.3090, -23.3090]]]],\n+                    [[[[-20.1003, -20.1003], [-21.2294, -21.2294]]]],\n+                    [[[[-19.9619, -19.9619], [-21.3060, -21.3060]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_video_multi_points(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350], [250, 220]]]],\n+            input_labels=[[[1, 1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 256, 256))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-11.1487, -11.1487, -11.4202], [-11.6522, -11.6522, -11.8057], [-12.7829, -12.7829, -12.6715]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-11.1487, -11.1487], [-11.6522, -11.6522]]]],\n+                    [[[[-15.3821, -15.3821], [-16.0333, -16.0333]]]],\n+                    [[[[-15.4855, -15.4855], [-16.4230, -16.4230]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_one_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_boxes=[[[300, 0, 500, 400]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 256, 256))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-13.1427, -13.1427, -13.6418], [-13.7753, -13.7753, -14.1144], [-15.1957, -15.1957, -15.1757]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-13.1427, -13.1427], [-13.7753, -13.7753]]]],\n+                    [[[[-14.9998, -14.9998], [-15.7086, -15.7086]]]],\n+                    [[[[-15.4558, -15.4558], [-16.1649, -16.1649]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_one_point_one_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_boxes=[[[300, 0, 500, 400]]],\n+            input_points=[[[[460, 60]]]],\n+            input_labels=[[[1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 256, 256))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-12.3525, -12.3525, -12.8907], [-13.0608, -13.0608, -13.4079], [-14.6511, -14.6511, -14.5694]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-12.3525, -12.3525], [-13.0608, -13.0608]]]],\n+                    [[[[-15.8181, -15.8181], [-16.4163, -16.4163]]]],\n+                    [[[[-15.8900, -15.8900], [-16.5953, -16.5953]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_multi_objects_multi_points(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_ids = [2, 3]  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_ids,\n+            input_points=[[[[200, 300], [230, 250], [275, 175]], [[400, 150]]]],\n+            input_labels=[[[1, 1, 0], [1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (2, 1, 256, 256))\n+        self.assertEqual(video_res_masks.shape, (2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[:, 0, :2, :2],  # first object\n+            torch.tensor(\n+                [[[-12.6294, -12.6294], [-13.3659, -13.3659]], [[-20.3319, -20.3319], [-22.0491, -22.0491]]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-12.6294, -12.6294], [-13.3659, -13.3659]]], [[[-20.3319, -20.3319], [-22.0491, -22.0491]]]],\n+                    [[[[-18.5249, -18.5249], [-19.5830, -19.5830]]], [[[-17.5537, -17.5537], [-19.2259, -19.2259]]]],\n+                    [[[[-14.2722, -14.2722], [-15.4622, -15.4622]]], [[[-18.3185, -18.3185], [-20.0314, -20.0314]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_propagate_video_from_mask_input(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        # get input_mask\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350], [250, 220]]]],\n+            input_labels=[[[1, 1]]],\n+        )\n+        sam2_video_output = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+\n+        # set mask as input\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_masks=self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0],\n+        )\n+        sam2_video_output = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = sam2_video_output.pred_masks\n+        self.assertEqual(low_res_masks.shape, (1, 1, 256, 256))\n+        video_res_masks = self.processor.post_process_masks(\n+            [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-10.0000, -10.0000, -10.0000], [-10.0000, -10.0000, -10.0000], [-10.0000, -10.0000, -10.0000]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-10.0000, -10.0000], [-10.0000, -10.0000]]]],\n+                    [[[[-18.4807, -18.4807], [-19.1966, -19.1966]]]],\n+                    [[[[-20.0512, -20.0512], [-20.9110, -20.9110]]]],\n+                ],\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_propagate_on_streamed_video(self):\n+        raw_video = prepare_video()\n+\n+        inference_session = self.processor.init_video_session(inference_device=torch_device)\n+        video_res_masks = []\n+        max_frame_num_to_track = 3\n+        for frame_idx, frame in enumerate(raw_video):\n+            if frame_idx >= max_frame_num_to_track:\n+                break\n+            inputs = self.processor(images=frame, device=torch_device, return_tensors=\"pt\")\n+            if frame_idx == 0:\n+                self.processor.add_inputs_to_inference_session(\n+                    inference_session,\n+                    frame_idx=0,\n+                    obj_ids=1,\n+                    input_points=[[[[210, 350], [250, 220]]]],\n+                    input_labels=[[[1, 1]]],\n+                    original_size=inputs.original_sizes[0],\n+                )\n+            sam2_video_output = self.video_model(inference_session=inference_session, frame=inputs.pixel_values[0])\n+            video_res_masks.append(\n+                self.processor.post_process_masks(\n+                    [sam2_video_output.pred_masks], inputs.original_sizes, binarize=False\n+                )[0]\n+            )\n+\n+        video_res_masks = torch.stack(video_res_masks, dim=0)\n+        self.assertEqual(\n+            video_res_masks.shape, (max_frame_num_to_track, 1, 1, raw_video.shape[-3], raw_video.shape[-2])\n+        )\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            video_res_masks[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-11.1487, -11.1487], [-11.6522, -11.6522]]]],\n+                    [[[[-15.3821, -15.3821], [-16.0333, -16.0333]]]],\n+                    [[[[-15.4855, -15.4855], [-16.4230, -16.4230]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )"
        },
        {
            "sha": "0e359e716b9dc16c0d8bc089939c6c8ef99581d0",
            "filename": "tests/models/sam2_video/test_processor_sam2_video.py",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2Ftest_processor_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Fmodels%2Fsam2_video%2Ftest_processor_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2_video%2Ftest_processor_sam2_video.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -0,0 +1,156 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torchvision,\n+    require_vision,\n+)\n+from transformers.utils import is_tf_available, is_torch_available, is_vision_available\n+\n+\n+if is_vision_available():\n+    from transformers import AutoProcessor, Sam2ImageProcessorFast, Sam2VideoProcessor, Sam2VideoVideoProcessor\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_tf_available():\n+    pass\n+\n+\n+@require_vision\n+@require_torchvision\n+class Sam2ProcessorTest(unittest.TestCase):\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Sam2ImageProcessorFast()\n+        video_processor = Sam2VideoVideoProcessor()\n+        processor = Sam2VideoProcessor(image_processor, video_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        image_inputs = torch.randint(0, 256, size=(1, 3, 30, 400), dtype=torch.uint8)\n+        # image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+        return image_inputs\n+\n+    def prepare_mask_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        mask_inputs = torch.randint(0, 256, size=(1, 30, 400), dtype=torch.uint8)\n+        # mask_inputs = [Image.fromarray(x) for x in mask_inputs]\n+        return mask_inputs\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n+\n+        processor = Sam2VideoProcessor.from_pretrained(self.tmpdirname, do_normalize=False, padding_value=1.0)\n+\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.image_processor, Sam2ImageProcessorFast)\n+        self.assertIsInstance(processor.video_processor, Sam2VideoVideoProcessor)\n+\n+    def test_image_processor_no_masks(self):\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_feat_extract = image_processor(image_input)\n+        input_processor = processor(images=image_input)\n+\n+        for key in input_feat_extract.keys():\n+            if key == \"pixel_values\":\n+                for input_feat_extract_item, input_processor_item in zip(\n+                    input_feat_extract[key], input_processor[key]\n+                ):\n+                    np.testing.assert_array_equal(input_feat_extract_item, input_processor_item)\n+            else:\n+                self.assertEqual(input_feat_extract[key], input_processor[key])\n+\n+        for image in input_feat_extract.pixel_values:\n+            self.assertEqual(image.shape, (3, 1024, 1024))\n+\n+        for original_size in input_feat_extract.original_sizes:\n+            np.testing.assert_array_equal(original_size, np.array([30, 400]))\n+\n+    def test_image_processor_with_masks(self):\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+        mask_input = self.prepare_mask_inputs()\n+\n+        input_feat_extract = image_processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+        for label in input_feat_extract.labels:\n+            self.assertEqual(label.shape, (256, 256))\n+\n+    @require_torch\n+    def test_post_process_masks(self):\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)\n+        dummy_masks = [torch.ones((1, 3, 5, 5))]\n+\n+        original_sizes = [[1764, 2646]]\n+\n+        masks = processor.post_process_masks(dummy_masks, original_sizes)\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        masks = processor.post_process_masks(dummy_masks, torch.tensor(original_sizes))\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        # should also work with np\n+        dummy_masks = [np.ones((1, 3, 5, 5))]\n+        masks = processor.post_process_masks(dummy_masks, np.array(original_sizes))\n+\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        dummy_masks = [[1, 0], [0, 1]]\n+        with self.assertRaises(ValueError):\n+            masks = processor.post_process_masks(dummy_masks, np.array(original_sizes))"
        },
        {
            "sha": "ef8a1712530e5603336d77ce3071645e5742fb2e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -3825,6 +3825,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 \"sam_hq\",\n                 \"zamba2\",\n                 \"sam_vision_model\",\n+                \"sam2_vision_model\",\n                 \"sam_hq_vision_model\",\n             ]:\n                 self.skipTest("
        },
        {
            "sha": "8d82b29360478f610212d05dd513ffc5758c19e3",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a13cd4a65d0624a5b87827c6e0709a882613f0/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a13cd4a65d0624a5b87827c6e0709a882613f0/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=68a13cd4a65d0624a5b87827c6e0709a882613f0",
            "patch": "@@ -139,6 +139,8 @@\n         \"BridgeTowerVisionModel\",  # No need to test it as it is tested by BridgeTowerModel model.\n         \"BarkCausalModel\",  # Building part of bigger (tested) model.\n         \"BarkModel\",  # Does not have a forward signature - generation tested with integration tests.\n+        \"Sam2HieraDetModel\",  # Building part of bigger (tested) model.\n+        \"Sam2VideoModel\",  # inherit from Sam2Model (tested).\n         \"SeamlessM4TTextToUnitModel\",  # Building part of bigger (tested) model.\n         \"SeamlessM4TCodeHifiGan\",  # Building part of bigger (tested) model.\n         \"SeamlessM4TTextToUnitForConditionalGeneration\",  # Building part of bigger (tested) model.\n@@ -193,6 +195,7 @@\n     \"models/bark/test_modeling_bark.py\",\n     \"models/shieldgemma2/test_modeling_shieldgemma2.py\",\n     \"models/llama4/test_modeling_llama4.py\",\n+    \"models/sam2_video/test_modeling_sam2_video.py\",\n ]\n \n # Update this list for models that are not in any of the auto MODEL_XXX_MAPPING. Being in this list is an exception and\n@@ -245,6 +248,8 @@\n     \"JukeboxVQVAE\",\n     \"JukeboxPrior\",\n     \"SamModel\",\n+    \"Sam2Model\",\n+    \"Sam2VideoModel\",\n     \"SamHQModel\",\n     \"DPTForDepthEstimation\",\n     \"DecisionTransformerGPT2Model\","
        }
    ],
    "stats": {
        "total": 14582,
        "additions": 14563,
        "deletions": 19
    }
}