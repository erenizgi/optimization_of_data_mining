{
    "author": "HofitBata",
    "message": "Fix dt proj bias reassigned (#33314)\n\n* When we set self.dt_proj.bias = None, it removes the bias parameter from the model. When we later tried to assign a tensor to self.dt_proj.bias, it caused a TypeError because PyTorch expects a Parameter object.\r\n\r\n* When we set self.dt_proj.bias = None, it removes the bias parameter from the model. When we later tried to assign a tensor to self.dt_proj.bias, it caused a TypeError because PyTorch expects a Parameter object.\r\n\r\n* When we set self.dt_proj.bias = None, it removes the bias parameter from the model. When we later tried to assign a tensor to self.dt_proj.bias, it caused a TypeError because PyTorch expects a Parameter object.",
    "sha": "dc8156fdd8af3bc79f98a5af1bcabde4cf131d54",
    "files": [
        {
            "sha": "5ff9b13020cf71617bd1fd3884307c0536007c4a",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc8156fdd8af3bc79f98a5af1bcabde4cf131d54/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc8156fdd8af3bc79f98a5af1bcabde4cf131d54/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=dc8156fdd8af3bc79f98a5af1bcabde4cf131d54",
            "patch": "@@ -713,11 +713,14 @@ def cuda_kernels_forward(\n         # This is a hack to apply dt_proj while still using the forward pass of `torch.nn.Linear`, which is needed\n         # in order to make quantization work. Quantization code replaces `torch.nn.Linear` layers with quantized\n         # linear layers, and requires to call the forward pass directly.\n-        # The original code here was: ```discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)```\n-        time_proj_bias = self.dt_proj.bias\n-        self.dt_proj.bias = None\n+        # Quantized model can't work with the original code:\n+        # ```discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)```\n+        time_proj_bias = self.dt_proj.bias.data\n+        with torch.no_grad():\n+            self.dt_proj.bias.data = torch.zeros_like(self.dt_proj.bias.data)\n         discrete_time_step = self.dt_proj(time_step).transpose(1, 2)\n-        self.dt_proj.bias = time_proj_bias\n+        with torch.no_grad():\n+            self.dt_proj.bias.data = time_proj_bias\n \n         A = -torch.exp(self.A_log.float())\n         # 3.c perform the recurrence y ‚Üê SSM(A, B, C)(x)"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 7,
        "deletions": 4
    }
}