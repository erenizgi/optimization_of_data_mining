{
    "author": "ydshieh",
    "message": "fix `aria` tests (#39277)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "fe5f3c85d292e34bed52e02a53edd5fa2acfc010",
    "files": [
        {
            "sha": "c3211beb6f5e900afa272259658fb2e7982381fa",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe5f3c85d292e34bed52e02a53edd5fa2acfc010/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe5f3c85d292e34bed52e02a53edd5fa2acfc010/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=fe5f3c85d292e34bed52e02a53edd5fa2acfc010",
            "patch": "@@ -633,6 +633,10 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": AriaTextDecoderLayer,\n+        \"attentions\": AriaTextAttention,\n+    }\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "e2f1c9742150f0d35463b88648b447f308578df2",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe5f3c85d292e34bed52e02a53edd5fa2acfc010/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe5f3c85d292e34bed52e02a53edd5fa2acfc010/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=fe5f3c85d292e34bed52e02a53edd5fa2acfc010",
            "patch": "@@ -42,6 +42,7 @@\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n+    LlamaAttention,\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n     LlamaMLP,\n@@ -1250,6 +1251,13 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return output + shared_expert_output\n \n \n+class AriaTextAttention(LlamaAttention):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: AriaTextConfig, layer_idx: int):\n+        super().__init__()\n+\n+\n class AriaTextDecoderLayer(LlamaDecoderLayer):\n     \"\"\"\n     Aria Text Decoder Layer.\n@@ -1279,6 +1287,10 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": AriaTextDecoderLayer,\n+        \"attentions\": AriaTextAttention,\n+    }\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 16,
        "deletions": 0
    }
}