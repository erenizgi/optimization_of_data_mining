{
    "author": "SunMarc",
    "message": "Fix trainer tests (#40823)\n\n* fix liger\n\n* fix\n\n* more\n\n* fix\n\n* fix hp\n\n* fix\n\n---------\n\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>",
    "sha": "c5553b41201fe4f83ba020c88740fbb1d073ab22",
    "files": [
        {
            "sha": "9f3bb17505971001ea8acf88a71bbb2ec57d21cc",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c5553b41201fe4f83ba020c88740fbb1d073ab22/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c5553b41201fe4f83ba020c88740fbb1d073ab22/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=c5553b41201fe4f83ba020c88740fbb1d073ab22",
            "patch": "@@ -308,7 +308,7 @@ def run(self):\n extras[\"sigopt\"] = deps_list(\"sigopt\")\n extras[\"hub-kernels\"] = deps_list(\"kernels\")\n \n-extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n+extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"]\n \n extras[\"serving\"] = deps_list(\"openai\", \"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\") + extras[\"torch\"]\n extras[\"audio\"] = deps_list("
        },
        {
            "sha": "4d011033186abad79a3d22eef45eb9b6ce26b1e4",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 15,
            "deletions": 18,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c5553b41201fe4f83ba020c88740fbb1d073ab22/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c5553b41201fe4f83ba020c88740fbb1d073ab22/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=c5553b41201fe4f83ba020c88740fbb1d073ab22",
            "patch": "@@ -1895,7 +1895,7 @@ def test_get_eval_dataloader_with_persistent_workers(self):\n     def test_use_liger_kernel_patching(self):\n         # Ensure any monkey patching is cleaned up for subsequent tests\n         with patch(\"transformers.models.llama.modeling_llama\"):\n-            from liger_kernel.transformers import LigerRMSNorm, liger_rotary_pos_emb\n+            from liger_kernel.transformers import liger_rotary_pos_emb\n \n             from transformers.models.llama import modeling_llama\n \n@@ -1904,7 +1904,7 @@ def test_use_liger_kernel_patching(self):\n \n             # Spot check that modeling code and model instance variables are not yet patched\n             self.assertNotEqual(modeling_llama.apply_rotary_pos_emb, liger_rotary_pos_emb)\n-            self.assertFalse(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n+            self.assertFalse(\"LigerRMSNorm\" in tiny_llama.model.norm.__repr__())\n \n             args = TrainingArguments(\n                 self.get_auto_remove_tmp_dir(),\n@@ -1914,7 +1914,7 @@ def test_use_liger_kernel_patching(self):\n \n             # Spot check that modeling code and model instance variables are patched\n             self.assertEqual(modeling_llama.apply_rotary_pos_emb, liger_rotary_pos_emb)\n-            self.assertTrue(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n+            self.assertTrue(\"LigerRMSNorm\" in tiny_llama.model.norm.__repr__())\n \n     @require_liger_kernel\n     def test_use_liger_kernel_custom_config_patching(self):\n@@ -3231,7 +3231,7 @@ def test_run_seq2seq_double_train_wrap_once(self):\n         model_wrapped_after = trainer.model_wrapped\n         self.assertIs(model_wrapped_before, model_wrapped_after, \"should be not wrapped twice\")\n \n-    @require_torch_up_to_2_accelerators\n+    @require_torch_non_multi_accelerator\n     def test_can_resume_training(self):\n         # This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of\n         # save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model\n@@ -3532,16 +3532,17 @@ def test_auto_batch_size_with_resume_from_checkpoint(self):\n         )\n         trainer = Trainer(model, args, train_dataset=train_dataset, callbacks=[MockCudaOOMCallback()])\n         trainer.train()\n-        # After `auto_find_batch_size` is ran we should now be at 16*0.9=14\n-        self.assertEqual(trainer._train_batch_size, 14)\n+        previous_batch_size = trainer._train_batch_size\n+        # Depends on the number of gpus so it is easier to just check that the batch_size decreased as expected\n+        self.assertEqual(trainer._train_batch_size < 16, True)\n \n         # We can then make a new Trainer\n         trainer = Trainer(model, args, train_dataset=train_dataset)\n         # Check we are at 16 to start\n         self.assertEqual(trainer._train_batch_size, 16 * max(trainer.args.n_gpu, 1))\n         trainer.train(resume_from_checkpoint=True)\n         # We should be back to 14 again, picking up based upon the last ran Trainer\n-        self.assertEqual(trainer._train_batch_size, 14)\n+        self.assertEqual(trainer._train_batch_size, previous_batch_size)\n \n     # regression for this issue: https://github.com/huggingface/transformers/issues/12970\n     def test_training_with_resume_from_checkpoint_false(self):\n@@ -5147,11 +5148,7 @@ def test_trainer_works_without_model_config(self):\n \n         with tempfile.TemporaryDirectory() as tmpdir:\n             training_args = TrainingArguments(\n-                output_dir=tmpdir,\n-                report_to=\"none\",\n-                max_steps=5,\n-                per_device_train_batch_size=1,\n-                remove_unused_columns=False,\n+                output_dir=tmpdir, report_to=\"none\", max_steps=5, per_device_train_batch_size=1, use_cpu=True\n             )\n             trainer = Trainer(\n                 model=model,\n@@ -5387,7 +5384,7 @@ def model_init(trial):\n                 b = 0\n             config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(config)\n+            return RegressionPreTrainedModel(config).to(torch_device)\n \n         def hp_name(trial):\n             return MyTrialShortNamer.shortname(trial.params)\n@@ -5433,7 +5430,7 @@ def model_init(trial):\n                 b = 0\n             config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(config)\n+            return RegressionPreTrainedModel(config).to(torch_device)\n \n         def hp_name(trial):\n             return MyTrialShortNamer.shortname(trial.params)\n@@ -5481,7 +5478,7 @@ def model_init(trial):\n                 b = 0\n             config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(config)\n+            return RegressionPreTrainedModel(config).to(torch_device)\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(\n@@ -5526,7 +5523,7 @@ def model_init(config):\n                 b = config[\"b\"]\n             model_config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(model_config)\n+            return RegressionPreTrainedModel(model_config).to(torch_device)\n \n         def hp_name(params):\n             return MyTrialShortNamer.shortname(params)\n@@ -5589,7 +5586,7 @@ def model_init(trial):\n                 b = 0\n             config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(config)\n+            return RegressionPreTrainedModel(config).to(torch_device)\n \n         def hp_name(trial):\n             return MyTrialShortNamer.shortname(trial.assignments)\n@@ -6168,7 +6165,7 @@ def model_init(config):\n                 b = config[\"b\"]\n             model_config = RegressionModelConfig(a=a, b=b, double_output=False)\n \n-            return RegressionPreTrainedModel(model_config)\n+            return RegressionPreTrainedModel(model_config).to(torch_device)\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer("
        }
    ],
    "stats": {
        "total": 35,
        "additions": 16,
        "deletions": 19
    }
}