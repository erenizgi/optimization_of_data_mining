{
    "author": "Uvi-12",
    "message": "Fixed typos in Audio Classification Documentation (#35263)\n\n* Fixed typos in Audio Classification Documentation\r\n\r\n* removed space in '8000 kHZ'\r\n\r\n* Changes made as per review",
    "sha": "e94083bf90b4592ced8bc1bd9039e5f5a272a96b",
    "files": [
        {
            "sha": "2a6b6fd7a22c9833582903f9e774a63dd7a383ea",
            "filename": "docs/source/en/tasks/audio_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e94083bf90b4592ced8bc1bd9039e5f5a272a96b/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e94083bf90b4592ced8bc1bd9039e5f5a272a96b/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md?ref=e94083bf90b4592ced8bc1bd9039e5f5a272a96b",
            "patch": "@@ -128,7 +128,7 @@ The next step is to load a Wav2Vec2 feature extractor to process the audio signa\n >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n ```\n \n-The MInDS-14 dataset has a sampling rate of 8000khz (you can find this information in it's [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2 model:\n+The MInDS-14 dataset has a sampling rate of 8kHz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16kHz to use the pretrained Wav2Vec2 model:\n \n ```py\n >>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n@@ -208,7 +208,7 @@ You're ready to start training your model now! Load Wav2Vec2 with [`AutoModelFor\n \n At this point, only three steps remain:\n \n-1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n+1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir`, which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n 2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n 3. Call [`~Trainer.train`] to finetune your model.\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}