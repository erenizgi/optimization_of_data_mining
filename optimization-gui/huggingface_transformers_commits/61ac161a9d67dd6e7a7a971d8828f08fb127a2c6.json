{
    "author": "yonigozlan",
    "message": "Add support for custom inputs and batched inputs in ProcessorTesterMixin (#33711)\n\n* add support for custom inputs and batched inputs in ProcessorTesterMixin\r\n\r\n* Fix batch_size behavior ProcessorTesterMixin\r\n\r\n* Change format prepare inputs batched\r\n\r\n* Remove override test pixtral processor\r\n\r\n* Remove unnecessary tests and cleanup after new prepare_inputs functions\r\n\r\n* Fix instructBlipVideo image processor",
    "sha": "61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
    "files": [
        {
            "sha": "b83df54785fa14cc00f0fc3ba5347f124a296e1d",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -57,8 +57,11 @@ def make_batched_videos(videos) -> List[VideoInput]:\n         elif len(videos[0].shape) == 4:\n             return [list(video) for video in videos]\n \n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n+    elif is_valid_image(videos):\n+        if isinstance(videos, PIL.Image.Image):\n+            return [[videos]]\n+        elif len(videos.shape) == 4:\n+            return [list(videos)]\n \n     raise ValueError(f\"Could not make batched video from {videos}\")\n "
        },
        {
            "sha": "39a47293040bdd320ff3d6b09727b72310e788e8",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -190,7 +190,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n \n@@ -218,7 +218,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n \n@@ -237,7 +237,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n \n@@ -264,7 +264,7 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n \n@@ -290,7 +290,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n         inputs = processor(\n@@ -315,7 +315,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\", \"upper older longer string\"]\n+        input_str = self.prepare_text_inputs(batch_size=2)\n         # Fuyu uses tokenizer kwargs only when image is None.\n         image_input = None\n         inputs = processor("
        },
        {
            "sha": "a53109b02b695113e879e9f2e25d6edf85b4c875",
            "filename": "tests/models/idefics3/test_processing_idefics3.py",
            "status": "modified",
            "additions": 28,
            "deletions": 42,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -17,6 +17,7 @@\n import tempfile\n import unittest\n from io import BytesIO\n+from typing import Optional\n \n import numpy as np\n import requests\n@@ -284,44 +285,29 @@ def test_apply_chat_template(self):\n         )\n         self.assertEqual(rendered, expected_rendered)\n \n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+    # Override as Idefics3Processor needs image tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <image>\"\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer <image>\"\n-        image_input = self.prepare_image_inputs()\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n \n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 3)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 364)  # crop size doesn't affect our image processor\n-\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\n-            \"image_processor\", max_image_size={\"longest_edge\": 32}, size={\"longest_edge\": 32}\n+        if batch_size == 1:\n+            return [\"lower newer <image>\"]\n+        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n+            batch_size - 2\n         )\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor, image_seq_len=2)\n-        self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer <image>\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 3)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 32)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+    # Override as Idefics3Processor needs nested images to work properly with batched inputs\n+    @require_vision\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        if batch_size is None:\n+            return super().prepare_image_inputs()\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+        return [[super().prepare_image_inputs()]] * batch_size\n \n     @require_vision\n     @require_torch\n@@ -333,7 +319,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer<image>\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=30)\n@@ -350,7 +336,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer<image>\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -378,7 +364,7 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer<image>\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -402,7 +388,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer<image>\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n@@ -419,11 +405,11 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"<image>lower newer\", \"<image>upper older longer string\"]\n-        image_input = self.prepare_image_inputs()\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n-            images=[image_input, image_input],\n+            images=image_input,\n             return_tensors=\"pt\",\n             padding=\"longest\",\n             max_length=76,\n@@ -446,7 +432,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer<image>\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,"
        },
        {
            "sha": "8874c7d1d30e0311bb0ad0a1303a883f7d1e2056",
            "filename": "tests/models/kosmos2/test_processor_kosmos2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -499,7 +499,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # set image input to None\n         image_input = None\n \n@@ -525,7 +525,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -551,7 +551,7 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -574,7 +574,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # set image input to None\n         image_input = None\n \n@@ -593,7 +593,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         # set image input to None\n         image_input = None\n         inputs = processor(\n@@ -618,7 +618,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\", \"upper older longer string\"]\n+        input_str = self.prepare_text_inputs(batch_size=2)\n         # set image input to None\n         image_input = None\n         inputs = processor("
        },
        {
            "sha": "52e1926e50b22f37f3367500763328542ddcb853",
            "filename": "tests/models/omdet_turbo/test_processor_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 165,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -17,7 +17,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import AutoProcessor, CLIPTokenizerFast, OmDetTurboProcessor\n@@ -36,15 +35,14 @@\n     from transformers.models.omdet_turbo.modeling_omdet_turbo import OmDetTurboObjectDetectionOutput\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import DetrImageProcessor\n \n \n @require_torch\n @require_vision\n class OmDetTurboProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = OmDetTurboProcessor\n+    text_input_name = \"classes_input_ids\"\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n@@ -77,17 +75,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def get_fake_omdet_turbo_output(self):\n         torch.manual_seed(42)\n         return OmDetTurboObjectDetectionOutput(\n@@ -210,154 +197,3 @@ def test_model_input_names(self):\n         inputs = processor(images=image_input, text=input_classes, task=input_tasks, return_tensors=\"pt\")\n \n         self.assertListEqual(list(inputs.keys()), self.input_keys)\n-\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(images=image_input, text=[input_str], task=input_str, return_tensors=\"pt\")\n-\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 117)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 117)\n-\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(images=image_input, text=[input_str], task=input_str, return_tensors=\"pt\", max_length=112)\n-\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 112)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 112)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            images=image_input,\n-            text=[input_str],\n-            task=input_str,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            images=image_input,\n-            text=[input_str],\n-            task=input_str,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 6)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 6)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76, \"task\": input_str},\n-        }\n-\n-        inputs = processor(images=image_input, text=[input_str], **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        # Rewrite as OmDet-Turbo processor outputs \"input_ids\" for both tasks and classes.\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76, \"task\": input_str},\n-        }\n-\n-        inputs = processor(images=image_input, text=[input_str], **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"tasks_input_ids\"][0]), 76)\n-        self.assertEqual(len(inputs[\"classes_input_ids\"][0]), 76)"
        },
        {
            "sha": "f832ffd2d64f7cbe11288d0d70a6c03574d9559e",
            "filename": "tests/models/pix2struct/test_processor_pix2struct.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -96,7 +96,7 @@ def test_tokenizer(self):\n \n         processor = Pix2StructProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n \n         encoded_processor = processor(text=input_str)\n \n@@ -111,7 +111,7 @@ def test_processor(self):\n \n         processor = Pix2StructProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input)\n@@ -130,7 +130,7 @@ def test_processor_max_patches(self):\n \n         processor = Pix2StructProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input)\n@@ -168,7 +168,7 @@ def test_model_input_names(self):\n \n         processor = Pix2StructProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input)\n@@ -195,7 +195,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input)\n@@ -213,7 +213,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, max_patches=1024)\n@@ -231,7 +231,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -257,8 +257,8 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n             images=image_input,\n@@ -284,7 +284,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -313,7 +313,7 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality"
        },
        {
            "sha": "8cdbf93c6476b884ab378e83b10669005ac0084d",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 24,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -14,6 +14,7 @@\n import shutil\n import tempfile\n import unittest\n+from typing import Optional\n \n import requests\n import torch\n@@ -246,27 +247,11 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         # fmt: on\n \n     # Override as PixtralProcessor needs nested images to work properly with batched inputs\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = [self.prepare_image_inputs()] * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            do_rescale=True,\n-            rescale_factor=-1,\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n-        self.assertTrue(\n-            len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n-            and len(inputs[self.text_input_name][1]) < 76\n-        )\n+    @require_vision\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        if batch_size is None:\n+            return super().prepare_image_inputs()\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+        return [[super().prepare_image_inputs()]] * batch_size"
        },
        {
            "sha": "187cf50c733cb63acc93ba3ddc6507c755b0e01b",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 29,
            "deletions": 13,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61ac161a9d67dd6e7a7a971d8828f08fb127a2c6/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=61ac161a9d67dd6e7a7a971d8828f08fb127a2c6",
            "patch": "@@ -17,6 +17,7 @@\n import inspect\n import json\n import tempfile\n+from typing import Optional\n \n import numpy as np\n \n@@ -86,10 +87,25 @@ def get_processor(self):\n         processor = self.processor_class(**components, **self.prepare_processor_dict())\n         return processor\n \n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer\"]\n+        return [\"lower newer\", \"upper older longer string\"] + [\"lower newer\"] * (batch_size - 2)\n+\n     @require_vision\n-    def prepare_image_inputs(self):\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n-        return prepare_image_inputs()\n+        if batch_size is None:\n+            return prepare_image_inputs()[0]\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+        return prepare_image_inputs() * batch_size\n \n     @require_vision\n     def prepare_video_inputs(self):\n@@ -148,7 +164,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n@@ -170,7 +186,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n@@ -184,7 +200,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n@@ -203,7 +219,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n@@ -216,7 +232,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -238,8 +254,8 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n             images=image_input,\n@@ -263,7 +279,7 @@ def test_doubly_passed_kwargs(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [\"lower newer\"]\n+        input_str = [self.prepare_text_inputs()]\n         image_input = self.prepare_image_inputs()\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -281,7 +297,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -303,7 +319,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         processor_components = self.prepare_components()\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -326,7 +342,7 @@ def test_overlapping_text_kwargs_handling(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = \"lower newer\"\n+        input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n \n         with self.assertRaises(ValueError):"
        }
    ],
    "stats": {
        "total": 364,
        "additions": 95,
        "deletions": 269
    }
}