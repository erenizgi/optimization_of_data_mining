{
    "author": "Cyrilvallez",
    "message": "Remove random flag (#40629)\n\nremove flag",
    "sha": "ed49376a423ce71ced042c7624271f886a6cd37a",
    "files": [
        {
            "sha": "f5e337e52ebd81464a63ddc13f214c590e3853fb",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -524,10 +524,8 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = BambaRMSNormGated(self.intermediate_size, eps=self.layer_norm_epsilon)\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n "
        },
        {
            "sha": "aec09861de81621c68572944cd796e87e2cf8257",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -282,10 +282,8 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = BambaRMSNormGated(self.intermediate_size, eps=self.layer_norm_epsilon)\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n "
        },
        {
            "sha": "da16bbbd03276a532cc8def3b3658da48c8a4932",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -555,7 +555,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.mamba_rms_norm = config.mamba_rms_norm\n \n         if self.mamba_rms_norm:\n@@ -566,7 +565,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n                 norm_before_gate=config.mamba_norm_before_gate,\n             )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, config.hidden_size, bias=config.projectors_bias)\n "
        },
        {
            "sha": "34193212a99c02fbe790462852599449ade2e55a",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -360,7 +360,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.mamba_rms_norm = config.mamba_rms_norm\n \n         if self.mamba_rms_norm:\n@@ -371,7 +370,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n                 norm_before_gate=config.mamba_norm_before_gate,\n             )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, config.hidden_size, bias=config.projectors_bias)\n "
        },
        {
            "sha": "7b61d2bdefd99c5b8865c9222f7875d3b06f0986",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -562,8 +562,6 @@ def _init_weights(self, module):\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.copy_(torch.log(A))\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n             module.D.data.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale"
        },
        {
            "sha": "c0efccf4b5bbc78345d719e5de0b42436b2d8064",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -451,10 +451,8 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = GraniteMoeHybridRMSNormGated(self.intermediate_size, eps=self.layer_norm_epsilon)\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n "
        },
        {
            "sha": "3f39c2d8490ba0a6e013a6ea2904ce20d79f5442",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -495,8 +495,6 @@ def _init_weights(self, module):\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             module.A_log.copy_(torch.log(A))\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n             module.D.data.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale"
        },
        {
            "sha": "85cf026e49d0e9364046cef00e58ef9c0ff201e7",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -279,10 +279,8 @@ def __init__(self, config: Mamba2Config, layer_idx: int):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = MambaRMSNormGated(self.intermediate_size, eps=self.layer_norm_epsilon)\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n@@ -727,8 +725,6 @@ def _init_weights(self, module):\n             # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n             A = torch.arange(1, self.config.num_heads + 1)\n             module.A_log.copy_(torch.log(A))\n-            module.A_log._no_weight_decay = True\n-            module.D._no_weight_decay = True\n             module.D.data.fill_(1.0)\n \n             dt = torch.exp("
        },
        {
            "sha": "ddd4f6f69079a72859c40e731d3d1700561634da",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -554,12 +554,10 @@ def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = Zamba2RMSNormGated(\n             self.intermediate_size, group_size=self.intermediate_size // self.n_groups, eps=1e-5\n         )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.add_bias_linear)\n "
        },
        {
            "sha": "ea305ba7a5b6c7bcb8e7ffea9bc9e3b057b4a66e",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed49376a423ce71ced042c7624271f886a6cd37a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=ed49376a423ce71ced042c7624271f886a6cd37a",
            "patch": "@@ -338,12 +338,10 @@ def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n         # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n-        self.A_log._no_weight_decay = True\n         self.norm = Zamba2RMSNormGated(\n             self.intermediate_size, group_size=self.intermediate_size // self.n_groups, eps=1e-5\n         )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n-        self.D._no_weight_decay = True\n \n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.add_bias_linear)\n "
        }
    ],
    "stats": {
        "total": 22,
        "additions": 0,
        "deletions": 22
    }
}