{
    "author": "null-pointer-access",
    "message": "Drop unnecessary tokens in GPT2Model generation (#39016)\n\nDrop unnecessary tokens in GPT2Model generation.\n\nCo-authored-by: Yi Pan <conlesspan@outlook.com>",
    "sha": "7b3807387b5b24a98fc66101268972ac8e25d7ed",
    "files": [
        {
            "sha": "13523539205b972582ddb9805f491fd35c210216",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b3807387b5b24a98fc66101268972ac8e25d7ed/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b3807387b5b24a98fc66101268972ac8e25d7ed/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=7b3807387b5b24a98fc66101268972ac8e25d7ed",
            "patch": "@@ -1163,6 +1163,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1208,25 +1209,26 @@ def forward(\n             torch.cuda.set_device(self.transformer.first_device)\n             hidden_states = hidden_states.to(self.lm_head.weight.device)\n \n-        lm_logits = self.lm_head(hidden_states)\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n             # Flatten the tokens\n             loss = self.loss_function(\n-                lm_logits,\n+                logits,\n                 labels,\n                 vocab_size=self.config.vocab_size,\n                 **kwargs,\n             )\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}