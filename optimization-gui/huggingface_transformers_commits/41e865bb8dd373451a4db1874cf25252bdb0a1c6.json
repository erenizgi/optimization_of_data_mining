{
    "author": "ydshieh",
    "message": "fix some flaky tests in `tests/generation/test_utils.py` (#39254)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "41e865bb8dd373451a4db1874cf25252bdb0a1c6",
    "files": [
        {
            "sha": "c6fd59f68bdc2ea0f4e297a6890fa4bcfa9eb6b7",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/41e865bb8dd373451a4db1874cf25252bdb0a1c6/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41e865bb8dd373451a4db1874cf25252bdb0a1c6/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=41e865bb8dd373451a4db1874cf25252bdb0a1c6",
            "patch": "@@ -1694,6 +1694,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n         \"\"\"Tests that we can generate from `inputs_embeds` instead of `input_ids` in LLMs, VLMs, etc\"\"\"\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n+        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n@@ -1703,8 +1704,10 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 continue\n             config.is_decoder = True\n \n+            set_config_for_less_flaky_test(config)\n             # Skip models without explicit support\n             model = model_class(config).to(torch_device).eval()\n+            set_model_for_less_flaky_test(model)\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 continue\n \n@@ -2298,13 +2301,14 @@ def _test_attention_implementation(self, attn_implementation):\n         NOTE: despite the test logic being the same, different implementations actually need different decorators, hence\n         this separate function.\n         \"\"\"\n-        max_new_tokens = 30\n+        max_new_tokens = 3\n         support_flag = {\n             \"sdpa\": \"_supports_sdpa\",\n             \"flash_attention_2\": \"_supports_flash_attn_2\",\n             \"flash_attention_3\": \"_supports_flash_attn_3\",\n         }\n \n+        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             if not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n@@ -2330,6 +2334,7 @@ def _test_attention_implementation(self, attn_implementation):\n             if hasattr(config, \"max_position_embeddings\"):\n                 config.max_position_embeddings = max_new_tokens + main_input.shape[1] + 1\n \n+            set_config_for_less_flaky_test(config)\n             model = model_class(config)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -2350,6 +2355,7 @@ def _test_attention_implementation(self, attn_implementation):\n                     torch_dtype=torch.float16,\n                     attn_implementation=\"eager\",\n                 ).to(torch_device)\n+                set_model_for_less_flaky_test(model_eager)\n                 res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n                 del model_eager\n                 gc.collect()\n@@ -2359,6 +2365,7 @@ def _test_attention_implementation(self, attn_implementation):\n                     torch_dtype=torch.float16,\n                     attn_implementation=attn_implementation,\n                 ).to(torch_device)\n+                set_model_for_less_flaky_test(model_attn)\n                 res_attn = model_attn.generate(**inputs_dict, **generate_kwargs)\n                 del model_attn\n                 gc.collect()"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}