{
    "author": "gante",
    "message": "üö® [v5] remove deprecated entry point (#40997)\n\n* remove old entry point\n\n* update references to transformers-cli",
    "sha": "dfc230389c2cfcbd23718594363d097b89233e25",
    "files": [
        {
            "sha": "0f4f91d30a67cf100dadde590bc7419d3ceeb536",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -23,7 +23,7 @@ Text generation is the most popular application for large language models (LLMs)\n In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities. This guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n \n > [!TIP]\n-> You can also chat with a model directly from the command line. ([reference](./conversations.md#transformers-cli))\n+> You can also chat with a model directly from the command line. ([reference](./conversations.md#transformers))\n > ```shell\n > transformers chat Qwen/Qwen2.5-0.5B-Instruct\n > ```"
        },
        {
            "sha": "893162083dd8ff3fd85bf041c25c9da5324592af",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -72,7 +72,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n <hfoption id=\"transformers CLI\">\n ```bash\n-echo \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model ibm-ai-platform/Bamba-9B-v2 --device 0\n+echo \"Plants create energy through a process known as\" | transformers run --task text-generation --model ibm-ai-platform/Bamba-9B-v2 --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "d1eeafb82b23a7511bb279399ec6731f8d2b2228",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -79,7 +79,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model facebook/bart-large --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model facebook/bart-large --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "6488e197d212a588cf48ee9a42a63477daef8fc9",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -81,7 +81,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model vinai/bertweet-base --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model vinai/bertweet-base --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "5e431c6883d03034a81e2d75c7bd8573e525e354",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -79,7 +79,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-!echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model google/bigbird-roberta-base --device 0\n+!echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model google/bigbird-roberta-base --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "fe3241ed7ab61363ef4b8b4e4c09cf0d8ca890fa",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -78,10 +78,10 @@ output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers\">\n \n ```bash\n-echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers-cli run --task summarization --model google/bigbird-pegasus-large-arxiv --device 0\n+echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers run --task summarization --model google/bigbird-pegasus-large-arxiv --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "4676a440c751e2ceb9efff3584892f560099e0a9",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -71,7 +71,7 @@ inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     generated_ids = model.generate(**inputs, max_length=50)\n-    \n+\n output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n print(output)\n ```\n@@ -80,7 +80,7 @@ print(output)\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Ibuprofen is best used for\" | transformers-cli run --task text-generation --model microsoft/biogpt --device 0\n+echo -e \"Ibuprofen is best used for\" | transformers run --task text-generation --model microsoft/biogpt --device 0\n ```\n \n </hfoption>\n@@ -103,7 +103,7 @@ bnb_config = BitsAndBytesConfig(\n \n tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"microsoft/BioGPT-Large\", \n+    \"microsoft/BioGPT-Large\",\n     quantization_config=bnb_config,\n     dtype=torch.bfloat16,\n     device_map=\"auto\"\n@@ -112,7 +112,7 @@ model = AutoModelForCausalLM.from_pretrained(\n input_text = \"Ibuprofen is best used for\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n with torch.no_grad():\n-    generated_ids = model.generate(**inputs, max_length=50)    \n+    generated_ids = model.generate(**inputs, max_length=50)\n output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n print(output)\n ```\n@@ -125,7 +125,7 @@ print(output)\n \n    ```py\n    from transformers import AutoModelForCausalLM\n-   \n+\n    model = AutoModelForCausalLM.from_pretrained(\n       \"microsoft/biogpt\",\n       attn_implementation=\"eager\"\n@@ -163,4 +163,4 @@ print(output)\n ## BioGptForSequenceClassification\n \n [[autodoc]] BioGptForSequenceClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "1c9239e4892ad60d58a8f096bae9694021408be5",
            "filename": "docs/source/en/model_doc/byt5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -70,10 +70,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers\">\n \n ```bash\n-echo -e \"translate English to French: Life is beautiful.\" | transformers-cli run --task text2text-generation --model google/byt5-small --device 0\n+echo -e \"translate English to French: Life is beautiful.\" | transformers run --task text2text-generation --model google/byt5-small --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "4e46e943c8e9e4d04bdfa4cfe1ef2fc416092e5b",
            "filename": "docs/source/en/model_doc/canine.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"feature-extraction\",\n     model=\"google/canine-c\",\n-    device=0,               \n+    device=0,\n )\n \n pipeline(\"Plant create energy through a process known as photosynthesis.\")\n@@ -60,7 +60,7 @@ model = AutoModel.from_pretrained(\"google/canine-c\")\n text = \"Plant create energy through a process known as photosynthesis.\"\n input_ids = torch.tensor([[ord(char) for char in text]])\n \n-outputs = model(input_ids)  \n+outputs = model(input_ids)\n pooled_output = outputs.pooler_output\n sequence_output = outputs.last_hidden_state\n ```\n@@ -69,7 +69,7 @@ sequence_output = outputs.last_hidden_state\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plant create energy through a process known as photosynthesis.\" | transformers-cli run --task feature-extraction --model google/canine-c --device 0\n+echo -e \"Plant create energy through a process known as photosynthesis.\" | transformers run --task feature-extraction --model google/canine-c --device 0\n ```\n \n </hfoption>\n@@ -81,7 +81,7 @@ echo -e \"Plant create energy through a process known as photosynthesis.\" | trans\n \n     ```py\n     from transformers import AutoTokenizer, AutoModel\n-    \n+\n     tokenizer = AutoTokenizer(\"google/canine-c\")\n     inputs = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\"]\n     encoding = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")"
        },
        {
            "sha": "b1edcf8c8517d68ec8c3268ce07b163abe549af8",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -45,7 +45,7 @@ import torch\n from transformers import pipeline\n \n pipeline = pipeline(\n-    task=\"text-generation\", \n+    task=\"text-generation\",\n     model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n     dtype=torch.float16,\n     device_map=0\n@@ -66,9 +66,9 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"CohereLabs/c4ai-command-r7b-12-2024\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"CohereLabs/c4ai-command-r7b-12-2024\", \n-    dtype=torch.float16, \n-    device_map=\"auto\", \n+    \"CohereLabs/c4ai-command-r7b-12-2024\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n \n@@ -90,7 +90,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers-cli chat CohereLabs/c4ai-command-r7b-12-2024 --dtype auto --attn_implementation flash_attention_2\n+transformers chat CohereLabs/c4ai-command-r7b-12-2024 --dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -107,10 +107,10 @@ from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"CohereLabs/c4ai-command-r7b-12-2024\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"CohereLabs/c4ai-command-r7b-12-2024\", \n-    dtype=torch.float16, \n-    device_map=\"auto\", \n-    quantization_config=bnb_config, \n+    \"CohereLabs/c4ai-command-r7b-12-2024\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n+    quantization_config=bnb_config,\n     attn_implementation=\"sdpa\"\n )\n \n@@ -141,5 +141,3 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] Cohere2ForCausalLM\n     - forward\n-\n-"
        },
        {
            "sha": "7c92cd6cb9d33a2abccc02fcc44ca4371b4b4ac7",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -84,7 +84,7 @@ print(f\"Predicted label: {predicted_label}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"DeBERTa-v2 is great at understanding context!\" | transformers-cli run --task fill-mask --model microsoft/deberta-v2-xlarge-mnli --device 0\n+echo -e \"DeBERTa-v2 is great at understanding context!\" | transformers run --task fill-mask --model microsoft/deberta-v2-xlarge-mnli --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "58361f55eb173bae58c898d39f4acb8ac34b4aad",
            "filename": "docs/source/en/model_doc/encoder-decoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -71,7 +71,7 @@ print(tokenizer.decode(summary[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\" | transformers-cli run --task summarization --model \"patrickvonplaten/bert2bert-cnn_dailymail-fp16\" --device 0\n+echo -e \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\" | transformers run --task summarization --model \"patrickvonplaten/bert2bert-cnn_dailymail-fp16\" --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "49ad1b2552704901a507c3b8c3597854c9a589fb",
            "filename": "docs/source/en/model_doc/flex_olmo.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n \n # FlexOlmo\n \n-[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. \n+[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets.\n \n You can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\n \n@@ -49,7 +49,7 @@ pipe = pipeline(\n     dtype=torch.bfloat16,\n     device=0,\n )\n-    \n+\n result = pipe(\"Plants create energy through a process known as\")\n print(result)\n ```\n@@ -81,7 +81,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/FlexOlmo-7x7B-1T --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model allenai/FlexOlmo-7x7B-1T --device 0\n ```\n \n </hfoption>\n@@ -136,4 +136,4 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ## FlexOlmoPreTrainedModel\n \n [[autodoc]] FlexOlmoPreTrainedModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "de48bce65085dd4ebc9b52a595dc6ec4ea5da3af",
            "filename": "docs/source/en/model_doc/gpt_neo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -65,7 +65,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Hello, I'm a language model\" | transformers-cli run --task text-generation --model EleutherAI/gpt-neo-1.3B --device 0\n+echo -e \"Hello, I'm a language model\" | transformers run --task text-generation --model EleutherAI/gpt-neo-1.3B --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "fce23a3c3493dbc8eb296be3a60c42eb5d5775ec",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -59,8 +59,8 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-base\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"ibm-granite/granite-3.3-2b-base\",                                          \n-    dtype=torch.bfloat16, \n+    \"ibm-granite/granite-3.3-2b-base\",\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -73,7 +73,7 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```python\n-echo -e \"Explain quantum computing simply.\" | transformers-cli run --task text-generation --model ibm-granite/granite-3.3-8b-instruct --device 0\n+echo -e \"Explain quantum computing simply.\" | transformers run --task text-generation --model ibm-granite/granite-3.3-8b-instruct --device 0\n ```\n </hfoption>\n </hfoptions>\n@@ -110,7 +110,7 @@ outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n-  \n+\n ## GraniteConfig\n \n [[autodoc]] GraniteConfig"
        },
        {
            "sha": "4acc6a639797ce838f0156e1549816f5914f0052",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -84,10 +84,10 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers\">\n \n ```bash\n-!echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers-cli run --task summarization --model allenai/led-base-16384 --device 0\n+!echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers run --task summarization --model allenai/led-base-16384 --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "547e959634e394f3fcd1882e71d4b60d9020a6cf",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -52,22 +52,22 @@ pipeline(\"Plants create energy through a process known as\")\n <hfoption id=\"AutoModel\">\n \n ```python\n-import torch  \n-from transformers import AutoModelForCausalLM, AutoTokenizer  \n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")  \n-input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)  \n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n-output = model.generate(**input_ids)  \n+output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model mistralai/Mamba-Codestral-7B-v0.1 --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model mistralai/Mamba-Codestral-7B-v0.1 --device 0\n ```\n \n </hfoption>\n@@ -97,14 +97,14 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n   - `cuda_kernels_forward` uses the original CUDA kernels if they're available in your environment. It is slower during prefill because it requires a \"warmup run\" due to the higher CPU overhead (see [these](https://github.com/state-spaces/mamba/issues/389#issuecomment-2171755306) [comments](https://github.com/state-spaces/mamba/issues/355#issuecomment-2147597457) for more details).\n \n - There are no positional embeddings in this model, but there is an `attention_mask` and a specific logic to mask out hidden states in two places in the case of batched generation (see this [comment](https://github.com/state-spaces/mamba/issues/66#issuecomment-1863563829) for more details). This (and the addition of the reimplemented Mamba 2 kernels) results in a slight discrepancy between batched and cached generation.\n- \n-- The SSM algorithm heavily relies on tensor contractions, which have matmul equivalents but the order of operations is slightly different. This makes the difference greater at smaller precisions. \n+\n+- The SSM algorithm heavily relies on tensor contractions, which have matmul equivalents but the order of operations is slightly different. This makes the difference greater at smaller precisions.\n \n - Hidden states that correspond to padding tokens is shutdown in 2 places and is mostly tested with left-padding. Right-padding propagates noise down the line and is not guaranteed to yield satisfactory results. `tokenizer.padding_side = \"left\"` ensures you are using the correct padding side.\n \n - The example below demonstrates how to fine-tune Mamba 2 with [PEFT](https://huggingface.co/docs/peft).\n \n-```python \n+```python\n from datasets import load_dataset\n from peft import LoraConfig\n from trl import SFTConfig, SFTTrainer"
        },
        {
            "sha": "bf582bc2ef54684579c838495b714295ca5001b3",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -46,7 +46,7 @@ pipe = pipeline(\n     dtype=torch.float16,\n     device=0,\n )\n-    \n+\n result = pipe(\"Plants create energy through a process known as\")\n print(result)\n ```\n@@ -78,7 +78,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/OLMo-2-0425-1B --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model allenai/OLMo-2-0425-1B --device 0\n ```\n \n </hfoption>\n@@ -121,11 +121,11 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n - OLMo2 uses RMSNorm instead of standard layer norm. The RMSNorm is applied to attention queries and keys, and it is applied after the attention and feedforward layers rather than before.\n - OLMo2 requires Transformers v4.48 or higher.\n-- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`]. \n+- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`].\n \n     ```py\n     from transformers import AutoModelForCausalLM\n-    \n+\n     model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\", revision=\"stage1-step140000-tokens294B\")\n     ```\n "
        },
        {
            "sha": "ecf384ee7cc0748f819cfb4c0a568923e9583b54",
            "filename": "docs/source/en/model_doc/olmo3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -79,7 +79,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/TBA --device 0\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model allenai/TBA --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "94b42eb9e7f9d9dc14a62466f5925ff451c61ecd",
            "filename": "docs/source/en/model_doc/pegasus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -82,7 +82,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants are remarkable organisms that produce their own food using a method called photosynthesis. This process involves converting sunlight, carbon dioxide, and water into glucose, which provides energy for growth. Plants play a crucial role in sustaining life on Earth by generating oxygen and serving as the foundation of most ecosystems.\" | transformers-cli run --task summarization --model google/pegasus-xsum --device 0\n+echo -e \"Plants are remarkable organisms that produce their own food using a method called photosynthesis. This process involves converting sunlight, carbon dioxide, and water into glucose, which provides energy for growth. Plants play a crucial role in sustaining life on Earth by generating oxygen and serving as the foundation of most ecosystems.\" | transformers run --task summarization --model google/pegasus-xsum --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "4f048e5496cb2f1830878dd15f067e2b4b512c99",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -79,10 +79,10 @@ output = model.generate(**input_ids, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers\">\n \n ```bash\n-echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers-cli run --task summarization --model google/pegasus-x-large --device 0\n+echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers run --task summarization --model google/pegasus-x-large --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "580ff09e72c965760cf4b2f047144f5474e29118",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -83,7 +83,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model FacebookAI/roberta-base --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model FacebookAI/roberta-base --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "3430f3369076e68d7b57a718778819200bef3c61",
            "filename": "docs/source/en/model_doc/roc_bert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -29,7 +29,7 @@ You can find all the original RoCBert checkpoints under the [weiweishi](https://\n \n > [!TIP]\n > This model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n-> \n+>\n > Click on the RoCBert models in the right sidebar for more examples of how to apply RoCBert to different Chinese language tasks.\n \n The example below demonstrates how to predict the [MASK] token with [`Pipeline`], [`AutoModel`], and from the command line.\n@@ -82,7 +82,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"ÈÄôÂÆ∂È§êÂª≥ÁöÑÊãâÈ∫µÊòØÊàë[MASK]ÈÅéÁöÑÊúÄÂ•ΩÁöÑÊãâÈ∫µ‰πã\" | transformers-cli run --task fill-mask --model weiweishi/roc-bert-base-zh --device 0\n+echo -e \"ÈÄôÂÆ∂È§êÂª≥ÁöÑÊãâÈ∫µÊòØÊàë[MASK]ÈÅéÁöÑÊúÄÂ•ΩÁöÑÊãâÈ∫µ‰πã\" | transformers run --task fill-mask --model weiweishi/roc-bert-base-zh --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "c892988e449e6b68b7623433491a84f381462fdd",
            "filename": "docs/source/en/model_doc/roformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -75,7 +75,7 @@ print(decoded)\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Ê∞¥Âú®Èõ∂Â∫¶Êó∂‰ºö[MASK]\" | transformers-cli run --task fill-mask --model junnyu/roformer_chinese_base --device 0\n+echo -e \"Ê∞¥Âú®Èõ∂Â∫¶Êó∂‰ºö[MASK]\" | transformers run --task fill-mask --model junnyu/roformer_chinese_base --device 0\n ```\n \n </hfoption>"
        },
        {
            "sha": "8ae33e8b286a1b458895bb474143edd0ed3ec62d",
            "filename": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -37,43 +37,43 @@ The example below demonstrates how to predict the `<mask>` token with [`Pipeline\n <hfoption id=\"Pipeline\">\n \n ```python\n-import torch  \n-from transformers import pipeline  \n-\n-pipeline = pipeline(  \n-    task=\"fill-mask\",  \n-    model=\"facebook/xlm-roberta-xl\",  \n-    dtype=torch.float16,  \n-    device=0  \n-)  \n-pipeline(\"Bonjour, je suis un mod√®le <mask>.\")  \n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"fill-mask\",\n+    model=\"facebook/xlm-roberta-xl\",\n+    dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"Bonjour, je suis un mod√®le <mask>.\")\n ```\n \n </hfoption>\n <hfoption id=\"AutoModel\">\n \n ```python\n-import torch  \n-from transformers import AutoModelForMaskedLM, AutoTokenizer  \n-\n-tokenizer = AutoTokenizer.from_pretrained(  \n-    \"facebook/xlm-roberta-xl\",  \n-)  \n-model = AutoModelForMaskedLM.from_pretrained(  \n-    \"facebook/xlm-roberta-xl\",  \n-    dtype=torch.float16,  \n-    device_map=\"auto\",  \n-    attn_implementation=\"sdpa\"  \n-)  \n-inputs = tokenizer(\"Bonjour, je suis un mod√®le <mask>.\", return_tensors=\"pt\").to(model.device)  \n-\n-with torch.no_grad():  \n-    outputs = model(**inputs)  \n-    predictions = outputs.logits  \n-\n-masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]  \n-predicted_token_id = predictions[0, masked_index].argmax(dim=-1)  \n-predicted_token = tokenizer.decode(predicted_token_id)  \n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"facebook/xlm-roberta-xl\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"facebook/xlm-roberta-xl\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+inputs = tokenizer(\"Bonjour, je suis un mod√®le <mask>.\", return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    predictions = outputs.logits\n+\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)\n+predicted_token = tokenizer.decode(predicted_token_id)\n \n print(f\"The predicted token is: {predicted_token}\")\n ```\n@@ -82,7 +82,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model facebook/xlm-roberta-xl --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model facebook/xlm-roberta-xl --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "65468a786a07d4e9789b41f247cc3029715c8151",
            "filename": "docs/source/en/model_doc/xlm-roberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -85,7 +85,7 @@ print(f\"The predicted token is: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model FacebookAI/xlm-roberta-base --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model FacebookAI/xlm-roberta-base --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "b4d84c791f5a9426628399cc7b785364acbe9527",
            "filename": "docs/source/en/model_doc/xlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -77,7 +77,7 @@ print(f\"Predicted token: {predicted_token}\")\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model FacebookAI/xlm-mlm-en-2048 --device 0\n+echo -e \"Plants create <mask> through a process known as photosynthesis.\" | transformers run --task fill-mask --model FacebookAI/xlm-mlm-en-2048 --device 0\n ```\n </hfoption>\n </hfoptions>"
        },
        {
            "sha": "482e70627048d11483273a30880cfdda8acf95b2",
            "filename": "docs/source/zh/model_doc/bert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -81,10 +81,10 @@ print(f\"The predicted token is: {predicted_token}\")\n ```\n \n </hfoption>\n-<hfoption id=\"transformers-cli\">\n+<hfoption id=\"transformers\">\n \n ```bash\n-echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model google-bert/bert-base-uncased --device 0\n+echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers run --task fill-mask --model google-bert/bert-base-uncased --device 0\n ```\n \n </hfoption>\n@@ -157,4 +157,4 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n ## Bert specific outputs\n \n-[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput\n\\ No newline at end of file\n+[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput"
        },
        {
            "sha": "10bac3c7707f0f383dc20521cb7519618477b8f5",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -439,7 +439,6 @@ def run(self):\n     entry_points={\n         \"console_scripts\": [\n             \"transformers=transformers.commands.transformers_cli:main\",\n-            \"transformers-cli=transformers.commands.transformers_cli:main_cli\",\n         ]\n     },\n     python_requires=\">=3.9.0\","
        },
        {
            "sha": "7d5f3b6fb3837e75b8ec8cf2dcd33500a338781f",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfc230389c2cfcbd23718594363d097b89233e25/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfc230389c2cfcbd23718594363d097b89233e25/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=dfc230389c2cfcbd23718594363d097b89233e25",
            "patch": "@@ -12,7 +12,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n \n from transformers import HfArgumentParser\n from transformers.commands.add_fast_image_processor import AddFastImageProcessorCommand\n@@ -24,14 +23,6 @@\n from transformers.commands.serving import ServeCommand\n \n \n-def main_cli():\n-    warnings.warn(\n-        \"`transformers-cli` is deprecated in favour of `transformers` directly and will be removed in v5.\",\n-        DeprecationWarning,\n-    )\n-    main()\n-\n-\n def main():\n     parser = HfArgumentParser(prog=\"Transformers CLI tool\", usage=\"transformers <command> [<args>]\")\n     commands_parser = parser.add_subparsers(help=\"transformers command helpers\")"
        }
    ],
    "stats": {
        "total": 210,
        "additions": 99,
        "deletions": 111
    }
}