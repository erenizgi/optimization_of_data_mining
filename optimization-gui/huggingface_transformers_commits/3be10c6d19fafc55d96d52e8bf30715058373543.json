{
    "author": "Cyrilvallez",
    "message": "Fix consistency and a few docstrings warnings (#39314)\n\n* Update modeling_deepseek_v2.py\n\n* fix docstrings\n\n* fix\n\n* fix",
    "sha": "3be10c6d19fafc55d96d52e8bf30715058373543",
    "files": [
        {
            "sha": "b19cde98fcd904ba731f2401fe5a2fda597ea3f8",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -1674,7 +1674,9 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DFineObjectDetectionOutput]:\n-        \"\"\"\n+        r\"\"\"\n+        Example:\n+\n         ```python\n         >>> import torch\n         >>> from transformers.image_utils import load_image"
        },
        {
            "sha": "4b5a9a2b33f80d94bdc7d4b718a16aac0f59925d",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -917,7 +917,9 @@ def __init__(self, config: DFineConfig):\n         self.post_init()\n \n     def forward(**super_kwargs):\n-        \"\"\"\n+        r\"\"\"\n+        Example:\n+\n         ```python\n         >>> import torch\n         >>> from transformers.image_utils import load_image"
        },
        {
            "sha": "47862f30c7efdac9610e8634b1f891caf50c1789",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -456,6 +456,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DeepseekV2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "7d37f00daa91303519362e438d023a5db6532ccf",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -747,7 +747,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        \"\"\"\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "dd15ee75b8725158f68f26b26c7583ddd081b983",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -412,7 +412,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        \"\"\"\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "f43473370a20873ebbf49ae81c18d4b41863a744",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -1502,6 +1502,13 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n+        qformer_input_ids (`torch.LongTensor` of shape (batch_size, sequence_length)):\n+            The sequence used as a prompt to be fed to the Q-Former module.\n+        qformer_attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n+            Mask to avoid performing attention on padding token indices.\n+\n+        Examples:\n+\n         ```python\n         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration\n         >>> import torch"
        },
        {
            "sha": "02b6bd5d0689833616e337cc26976c67e86bcb46",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -391,6 +391,13 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n+        qformer_input_ids (`torch.LongTensor` of shape (batch_size, sequence_length)):\n+            The sequence used as a prompt to be fed to the Q-Former module.\n+        qformer_attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n+            Mask to avoid performing attention on padding token indices.\n+\n+        Examples:\n+\n         ```python\n         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration\n         >>> import torch"
        },
        {
            "sha": "af303825d2efe0c12b5f8e5646b0a55fdbeae6b7",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -172,8 +172,8 @@ class MimiEncoderOutput(ModelOutput):\n \n         If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n         have their past key value states given to this model).\n-    padding_cache (<fill_type>):\n-        <fill_docstring>\n+    padding_cache (`MimiConv1dPaddingCache`, *optional*):\n+        Padding cache for MimiConv1d causal convolutions in order to support streaming via cache padding.\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "7053db9b171b31da266c73074037049af0cd2843",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -876,6 +876,15 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SmolVLMCausalLMOutputWithPast]:\n         r\"\"\"\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n+            Mask to avoid performing attention on padding pixel indices.\n+        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+            The hidden states of the image encoder after modality projection.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or `model.image_token_id`. Tokens with indices set to `model.image_token_id` are\n+            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n         Example:\n \n         ```python"
        },
        {
            "sha": "e85074efb88ad692b2cb9079ecd6d7ca5feeff24",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3be10c6d19fafc55d96d52e8bf30715058373543/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=3be10c6d19fafc55d96d52e8bf30715058373543",
            "patch": "@@ -358,6 +358,15 @@ def __init__(self, config):\n \n     def forward(self, **super_kwargs):\n         r\"\"\"\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n+            Mask to avoid performing attention on padding pixel indices.\n+        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+            The hidden states of the image encoder after modality projection.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or `model.image_token_id`. Tokens with indices set to `model.image_token_id` are\n+            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n         Example:\n \n         ```python"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 43,
        "deletions": 6
    }
}