{
    "author": "MekkCyber",
    "message": "Fix Qwen3 tp plan with FP8 (#37871)\n\n* update for qwen 3\n\n* fix style\n\n* rm print",
    "sha": "36e2e33bbe0e54df38033755d39bd75c23d45271",
    "files": [
        {
            "sha": "22d1e9e30ade8a587dcece7f79aea31b058e284d",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/36e2e33bbe0e54df38033755d39bd75c23d45271/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36e2e33bbe0e54df38033755d39bd75c23d45271/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=36e2e33bbe0e54df38033755d39bd75c23d45271",
            "patch": "@@ -333,12 +333,8 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n             return F.linear(input, self.weight, self.bias)\n         else:\n             # Context manager used to switch among the available cuda devices\n-            # with torch.cuda.device(input.device):\n-            qinput, scale = act_quant(input, self.block_size[1])\n-            # Blocks the CPU until all CUDA operations on the specified device are complete. It is used to ensure that the results of the\n-            # preceding operations are ready before proceeding\n-            # torch.cuda.synchronize(device=self.weight.device)\n             with torch.cuda.device(input.device):\n+                qinput, scale = act_quant(input, self.block_size[1])\n                 output = w8a8_block_fp8_matmul_triton(\n                     qinput,\n                     self.weight,\n@@ -347,6 +343,8 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n                     self.block_size,\n                     output_dtype=input.dtype,\n                 )\n+            # Blocks the CPU until all CUDA operations on the specified device are complete. It is used to ensure that the results of the\n+            # preceding operations are ready before proceeding\n             torch.cuda.synchronize()\n             if self.bias is not None:\n                 output = output + self.bias"
        },
        {
            "sha": "880d573a319296dec7481a780a76607283c48f0d",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/36e2e33bbe0e54df38033755d39bd75c23d45271/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36e2e33bbe0e54df38033755d39bd75c23d45271/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=36e2e33bbe0e54df38033755d39bd75c23d45271",
            "patch": "@@ -194,6 +194,31 @@ def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> Li\n                         not_missing_keys.append(missing)\n         return [k for k in missing_keys if k not in not_missing_keys]\n \n+    def update_tp_plan(self, config):\n+        if \"Qwen3\" in config.__class__.__name__:\n+            text_plan = {\n+                \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.q_proj.weight_scale_inv\": \"local_colwise\",\n+                \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.k_proj.weight_scale_inv\": \"local_colwise\",\n+                \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.v_proj.weight_scale_inv\": \"local_colwise\",\n+                \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n+                \"layers.*.self_attn.o_proj.weight_scale_inv\": \"local_rowwise\",\n+                \"layers.*.self_attn\": \"gather\",\n+                \"layers.*.mlp.gate_proj.weight\": \"local_colwise\",\n+                \"layers.*.mlp.gate_proj.weight_scale_inv\": \"local_colwise\",\n+                \"layers.*.mlp.up_proj.weight\": \"local_colwise\",\n+                \"layers.*.mlp.up_proj.weight_scale_inv\": \"local_colwise\",\n+                \"layers.*.mlp.down_proj.weight\": \"local_rowwise\",\n+                \"layers.*.mlp.down_proj.weight_scale_inv\": \"local_rowwise\",\n+                \"layers.*.mlp\": \"gather\",\n+            }\n+\n+            config.base_model_tp_plan = text_plan\n+\n+            return config\n+\n     def is_serializable(self, safe_serialization=None):\n         return True\n "
        }
    ],
    "stats": {
        "total": 33,
        "additions": 28,
        "deletions": 5
    }
}