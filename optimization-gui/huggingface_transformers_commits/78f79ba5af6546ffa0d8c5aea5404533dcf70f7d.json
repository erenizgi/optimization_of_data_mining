{
    "author": "zRzRzRzRzRzRzR",
    "message": "Update GLM-4.6 doc (#41471)\n\nUpdate glm4_moe.md",
    "sha": "78f79ba5af6546ffa0d8c5aea5404533dcf70f7d",
    "files": [
        {
            "sha": "1cc3d2824298818f5939a7586e023dfd023db3e0",
            "filename": "docs/source/en/model_doc/glm4_moe.md",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f79ba5af6546ffa0d8c5aea5404533dcf70f7d/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f79ba5af6546ffa0d8c5aea5404533dcf70f7d/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md?ref=78f79ba5af6546ffa0d8c5aea5404533dcf70f7d",
            "patch": "@@ -19,6 +19,27 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n+Both **GLM-4.6** and **GLM-4.5** language model use this class. The implementation in transformers does not include an MTP layer.\n+\n+### GLM-4.6\n+\n+Compared with GLM-4.5, **GLM-4.6**  brings several key improvements:\n+\n+* **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n+* **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n+* **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n+* **More capable agents:** GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n+* **Refined writing:** Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n+\n+We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as **DeepSeek-V3.1-Terminus** and **Claude Sonnet 4**.\n+\n+![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench_glm46.png)\n+\n+For more eval results, show cases, and technical details, please visit our [technical blog](https://z.ai/blog/glm-4.6).\n+\n+\n+### GLM-4.5\n+\n The [**GLM-4.5**](https://huggingface.co/papers/2508.06471) series models are foundation models designed for intelligent agents, MoE variants are documented here as Glm4Moe.\n \n GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications."
        }
    ],
    "stats": {
        "total": 21,
        "additions": 21,
        "deletions": 0
    }
}