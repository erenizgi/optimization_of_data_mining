{
    "author": "astefanutti",
    "message": "Fix BitsAndBytesConfig JSON serialization in TrainingArguments (#37520)\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "4541c2cdefe5e7aedbcdbf8b62936e95fcb2cff9",
    "files": [
        {
            "sha": "65cb2705cba3ba09541001c94661a17f3791025e",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4541c2cdefe5e7aedbcdbf8b62936e95fcb2cff9/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4541c2cdefe5e7aedbcdbf8b62936e95fcb2cff9/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=4541c2cdefe5e7aedbcdbf8b62936e95fcb2cff9",
            "patch": "@@ -2498,6 +2498,11 @@ def to_dict(self):\n             # Handle the accelerator_config if passed\n             if is_accelerate_available() and isinstance(v, AcceleratorConfig):\n                 d[k] = v.to_dict()\n+            # Handle the quantization_config if passed\n+            if k == \"model_init_kwargs\" and isinstance(v, dict) and \"quantization_config\" in v:\n+                quantization_config = v.get(\"quantization_config\")\n+                if quantization_config and not isinstance(quantization_config, dict):\n+                    d[k][\"quantization_config\"] = quantization_config.to_dict()\n         self._dict_torch_dtype_to_str(d)\n \n         return d"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}