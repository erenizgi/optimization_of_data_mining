{
    "author": "ydshieh",
    "message": "Remove the head masking block in some vision models (#41620)\n\n* old\n\n* new\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "da382917aade28bb73eecd64e47e2af8e3210e4e",
    "files": [
        {
            "sha": "c700c320490443c91c41b42a6327b8c0f4ffde04",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -117,10 +117,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "329ef32d48c4f34d0d4189455d2768b324f44703",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -182,10 +182,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "b0e74df05d199640823ee2441dbf51a603e9ae97",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -170,10 +170,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "65e219fcfdd465c06a91a2fc921a7bec717866f7",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -190,10 +190,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "1fa962dd807d4f0d6695db920427118ad27775e9",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -208,10 +208,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "1f8bb5d86ac961d2b67b3ebf924b9cd56d1a464b",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -288,10 +288,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "10c7803ce10fe53b6dedc1829bc9d3ad65ecdee3",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -161,10 +161,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "d0193387c58982f7e21c28492fdedfb3b1ab14a7",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -199,10 +199,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "7c3eb386ee3b036d86839747712223533d160762",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -187,10 +187,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "62a6f302afd39faf12a85e067e1d84a4def47f43",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -347,10 +347,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "a14e72ec6c0b01b51bdd9dc668b853450419acfc",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -184,10 +184,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "f7baf2cc8dada162f6269ff9609d319a7fc37477",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -116,10 +116,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "8bb155e34f1a029bd6d72738dfe95f35d056614b",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -177,10 +177,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "2683762529287cc01c4cee45502ae4130b145faa",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -169,10 +169,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "55c896aefb7e33d314c5a62c78e7cb6906a0fd3d",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da382917aade28bb73eecd64e47e2af8e3210e4e/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=da382917aade28bb73eecd64e47e2af8e3210e4e",
            "patch": "@@ -232,10 +232,6 @@ def eager_attention_forward(\n     # seem a bit unusual, but is taken from the original Transformer paper.\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-    # Mask heads if we want to\n-    if attention_mask is not None:\n-        attn_weights = attn_weights * attention_mask\n-\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        }
    ],
    "stats": {
        "total": 60,
        "additions": 0,
        "deletions": 60
    }
}