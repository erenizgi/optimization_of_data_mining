{
    "author": "gante",
    "message": "[docs] remove TF references from `/en/model_doc` (#40344)\n\n* models up to F\n\n* models up to M\n\n* all models",
    "sha": "c031f6f99427ebeacaea33d5842fb623845ef766",
    "files": [
        {
            "sha": "af5d7f3b0613037553808f2452a83316956f13db",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 48,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -31,7 +30,7 @@ You can find all the original BLIP checkpoints under the [BLIP](https://huggingf\n \n > [!TIP]\n > This model was contributed by [ybelkada](https://huggingface.co/ybelkada).\n-> \n+>\n > Click on the BLIP models in the right sidebar for more examples of how to apply BLIP to different vision language tasks.\n \n The example below demonstrates how to visual question answering with [`Pipeline`] or the [`AutoModel`] class.\n@@ -64,7 +63,7 @@ from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n \n processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n model = AutoModelForVisualQuestionAnswering.from_pretrained(\n-    \"Salesforce/blip-vqa-base\", \n+    \"Salesforce/blip-vqa-base\",\n     torch_dtype=torch.float16,\n     device_map=\"auto\"\n )\n@@ -113,9 +112,6 @@ Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/exam\n [[autodoc]] BlipImageProcessorFast\n     - preprocess\n \n-<frameworkcontent>\n-<pt>\n-\n ## BlipModel\n \n `BlipModel` is going to be deprecated in future versions, please use `BlipForConditionalGeneration`, `BlipForImageTextRetrieval` or `BlipForQuestionAnswering` depending on your usecase.\n@@ -154,45 +150,3 @@ Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/exam\n \n [[autodoc]] BlipForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFBlipModel\n-\n-[[autodoc]] TFBlipModel\n-    - call\n-    - get_text_features\n-    - get_image_features\n-\n-## TFBlipTextModel\n-\n-[[autodoc]] TFBlipTextModel\n-    - call\n-\n-## TFBlipTextLMHeadModel\n-\n-[[autodoc]] TFBlipTextLMHeadModel\n-- forward\n-\n-## TFBlipVisionModel\n-\n-[[autodoc]] TFBlipVisionModel\n-    - call\n-\n-## TFBlipForConditionalGeneration\n-\n-[[autodoc]] TFBlipForConditionalGeneration\n-    - call\n-\n-## TFBlipForImageTextRetrieval\n-\n-[[autodoc]] TFBlipForImageTextRetrieval\n-    - call\n-\n-## TFBlipForQuestionAnswering\n-\n-[[autodoc]] TFBlipForQuestionAnswering\n-    - call\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "67c5d0da706b3be89b98cde6ad3ad910b5b5ef81",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 6,
            "deletions": 44,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,8 +18,7 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n \t<div class=\"flex flex-wrap space-x-1\">\n \t\t<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-\t\t<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-    <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n \t</div>\n </div>\n \n@@ -51,7 +50,7 @@ from transformers import pipeline\n pipeline = pipeline(\"fill-mask\", model=\"camembert-base\", torch_dtype=torch.float16, device=0)\n pipeline(\"Le camembert est un dÃ©licieux fromage <mask>.\")\n ```\n-</hfoption> \n+</hfoption>\n \n <hfoption id=\"AutoModel\">\n \n@@ -73,23 +72,23 @@ predicted_token = tokenizer.decode(predicted_token_id)\n \n print(f\"The predicted token is: {predicted_token}\")\n ```\n-</hfoption> \n+</hfoption>\n \n <hfoption id=\"transformers CLI\">\n \n ```bash\n echo -e \"Le camembert est un dÃ©licieux fromage <mask>.\" | transformers run --task fill-mask --model camembert-base --device 0\n ```\n \n-</hfoption> \n+</hfoption>\n \n-</hfoptions> \n+</hfoptions>\n \n \n Quantization reduces the memory burden of large models by representing weights in lower precision. Refer to the [Quantization](../quantization/overview) overview for available options.\n \n The example below uses [bitsandbytes](../quantization/bitsandbytes) quantization to quantize the weights to 8-bits.\n-  \n+\n ```python\n from transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\n import torch\n@@ -131,9 +130,6 @@ print(f\"The predicted token is: {predicted_token}\")\n \n [[autodoc]] CamembertTokenizerFast\n \n-<frameworkcontent>\n-<pt>\n-\n ## CamembertModel\n \n [[autodoc]] CamembertModel\n@@ -161,37 +157,3 @@ print(f\"The predicted token is: {predicted_token}\")\n ## CamembertForQuestionAnswering\n \n [[autodoc]] CamembertForQuestionAnswering\n-\n-</pt>\n-<tf>\n-\n-## TFCamembertModel\n-\n-[[autodoc]] TFCamembertModel\n-\n-## TFCamembertForCausalLM\n-\n-[[autodoc]] TFCamembertForCausalLM\n-\n-## TFCamembertForMaskedLM\n-\n-[[autodoc]] TFCamembertForMaskedLM\n-\n-## TFCamembertForSequenceClassification\n-\n-[[autodoc]] TFCamembertForSequenceClassification\n-\n-## TFCamembertForMultipleChoice\n-\n-[[autodoc]] TFCamembertForMultipleChoice\n-\n-## TFCamembertForTokenClassification\n-\n-[[autodoc]] TFCamembertForTokenClassification\n-\n-## TFCamembertForQuestionAnswering\n-\n-[[autodoc]] TFCamembertForQuestionAnswering\n-\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "cabeb7140f0b1bc5649161848cc8ae04484d6668",
            "filename": "docs/source/en/model_doc/convbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -72,9 +71,6 @@ ConvBERT training tips are similar to those of BERT. For usage tips refer to [BE\n \n [[autodoc]] ConvBertTokenizerFast\n \n-<frameworkcontent>\n-<pt>\n-\n ## ConvBertModel\n \n [[autodoc]] ConvBertModel\n@@ -104,39 +100,3 @@ ConvBERT training tips are similar to those of BERT. For usage tips refer to [BE\n \n [[autodoc]] ConvBertForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFConvBertModel\n-\n-[[autodoc]] TFConvBertModel\n-    - call\n-\n-## TFConvBertForMaskedLM\n-\n-[[autodoc]] TFConvBertForMaskedLM\n-    - call\n-\n-## TFConvBertForSequenceClassification\n-\n-[[autodoc]] TFConvBertForSequenceClassification\n-    - call\n-\n-## TFConvBertForMultipleChoice\n-\n-[[autodoc]] TFConvBertForMultipleChoice\n-    - call\n-\n-## TFConvBertForTokenClassification\n-\n-[[autodoc]] TFConvBertForTokenClassification\n-    - call\n-\n-## TFConvBertForQuestionAnswering\n-\n-[[autodoc]] TFConvBertForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "ecede09b0166ca3f543993c8132725629a3c3e4d",
            "filename": "docs/source/en/model_doc/convnext.md",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -43,8 +42,7 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> ConvNeXT architecture. Taken from the <a href=\"https://huggingface.co/papers/2201.03545\">original paper</a>.</small>\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [ariG23498](https://github.com/ariG23498),\n-[gante](https://github.com/gante), and [sayakpaul](https://github.com/sayakpaul) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt).\n+This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt).\n \n ## Resources\n \n@@ -75,9 +73,6 @@ If you're interested in submitting a resource to be included here, please feel f\n [[autodoc]] ConvNextImageProcessorFast\n     - preprocess\n \n-<frameworkcontent>\n-<pt>\n-\n ## ConvNextModel\n \n [[autodoc]] ConvNextModel\n@@ -87,19 +82,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] ConvNextForImageClassification\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFConvNextModel\n-\n-[[autodoc]] TFConvNextModel\n-    - call\n-\n-## TFConvNextForImageClassification\n-\n-[[autodoc]] TFConvNextForImageClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "ea782411ea0a72745b5faaade569ebbfafdb4ebf",
            "filename": "docs/source/en/model_doc/convnextv2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -61,14 +60,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] ConvNextV2ForImageClassification\n     - forward\n-\n-## TFConvNextV2Model\n-\n-[[autodoc]] TFConvNextV2Model\n-    - call\n-\n-\n-## TFConvNextV2ForImageClassification\n-\n-[[autodoc]] TFConvNextV2ForImageClassification\n-    - call"
        },
        {
            "sha": "e5b48d638b68119cf5edd156d8ff9fc94f578aaa",
            "filename": "docs/source/en/model_doc/ctrl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -52,7 +51,7 @@ This model was contributed by [keskarnitishr](https://huggingface.co/keskarnitis\n   token in a sequence. Leveraging this feature allows CTRL to generate syntactically coherent text as it can be\n   observed in the *run_generation.py* example script.\n - The PyTorch models can take the `past_key_values` as input, which is the previously computed key/value attention pairs.\n-  TensorFlow models accepts `past` as input. Using the `past_key_values` value prevents the model from re-computing\n+  Using the `past_key_values` value prevents the model from re-computing\n   pre-computed values in the context of text generation. See the [`forward`](model_doc/ctrl#transformers.CTRLModel.forward)\n   method for more information on the usage of this argument.\n \n@@ -71,9 +70,6 @@ This model was contributed by [keskarnitishr](https://huggingface.co/keskarnitis\n [[autodoc]] CTRLTokenizer\n     - save_vocabulary\n \n-<frameworkcontent>\n-<pt>\n-\n ## CTRLModel\n \n [[autodoc]] CTRLModel\n@@ -88,24 +84,3 @@ This model was contributed by [keskarnitishr](https://huggingface.co/keskarnitis\n \n [[autodoc]] CTRLForSequenceClassification\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFCTRLModel\n-\n-[[autodoc]] TFCTRLModel\n-    - call\n-\n-## TFCTRLLMHeadModel\n-\n-[[autodoc]] TFCTRLLMHeadModel\n-    - call\n-\n-## TFCTRLForSequenceClassification\n-\n-[[autodoc]] TFCTRLForSequenceClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "f54f809774f369c6efb375d475200be1ef455cda",
            "filename": "docs/source/en/model_doc/cvt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 22,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -30,7 +29,7 @@ You can find all the CvT checkpoints under the [Microsoft](https://huggingface.c\n \n > [!TIP]\n > This model was contributed by [anujunj](https://huggingface.co/anugunj).\n-> \n+>\n > Click on the CvT models in the right sidebar for more examples of how to apply CvT to different computer vision tasks.\n \n The example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n@@ -46,7 +45,7 @@ pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"microsoft/cvt-13\",\n     torch_dtype=torch.float16,\n-    device=0 \n+    device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n ```\n@@ -91,9 +90,6 @@ Refer to this set of ViT [notebooks](https://github.com/NielsRogge/Transformers-\n \n [[autodoc]] CvtConfig\n \n-<frameworkcontent>\n-<pt>\n-\n ## CvtModel\n \n [[autodoc]] CvtModel\n@@ -103,19 +99,3 @@ Refer to this set of ViT [notebooks](https://github.com/NielsRogge/Transformers-\n \n [[autodoc]] CvtForImageClassification\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFCvtModel\n-\n-[[autodoc]] TFCvtModel\n-    - call\n-\n-## TFCvtForImageClassification\n-\n-[[autodoc]] TFCvtForImageClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "b913be8ceb63eacb233663b43cce070bff28d50a",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 5,
            "deletions": 31,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -43,7 +43,6 @@ natural language understanding demonstrate a new state of the art or competitive\n Models and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec.*\n \n This model was contributed by [edugp](https://huggingface.co/edugp) and [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n-[sayakpaul](https://github.com/sayakpaul) and [Rocketknight1](https://github.com/Rocketknight1) contributed Data2Vec for vision in TensorFlow.\n \n The original code (for NLP and Speech) can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/data2vec).\n The original code for vision can be found [here](https://github.com/facebookresearch/data2vec_vision/tree/main/beit).\n@@ -54,17 +53,17 @@ The original code for vision can be found [here](https://github.com/facebookrese\n - For Data2VecAudio, preprocessing is identical to [`Wav2Vec2Model`], including feature extraction\n - For Data2VecText, preprocessing is identical to [`RobertaModel`], including tokenization.\n - For Data2VecVision, preprocessing is identical to [`BeitModel`], including feature extraction.\n-- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n+- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n The SDPA implementation is currently available for the Data2VecAudio and Data2VecVision models.\n@@ -103,7 +102,6 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n <PipelineTag pipeline=\"image-classification\"/>\n \n - [`Data2VecVisionForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- To fine-tune [`TFData2VecVisionForImageClassification`] on a custom dataset, see [this notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb).\n \n **Data2VecText documentation resources**\n - [Text classification task guide](../tasks/sequence_classification)\n@@ -135,9 +133,6 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] Data2VecVisionConfig\n \n-<frameworkcontent>\n-<pt>\n-\n ## Data2VecAudioModel\n \n [[autodoc]] Data2VecAudioModel\n@@ -212,24 +207,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] Data2VecVisionForSemanticSegmentation\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFData2VecVisionModel\n-\n-[[autodoc]] TFData2VecVisionModel\n-    - call\n-\n-## TFData2VecVisionForImageClassification\n-\n-[[autodoc]] TFData2VecVisionForImageClassification\n-    - call\n-\n-## TFData2VecVisionForSemanticSegmentation\n-\n-[[autodoc]] TFData2VecVisionForSemanticSegmentation\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "0ae37740397ba7839154efc66ecf6102747f1147",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n-           <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -139,9 +138,6 @@ print(f\"Predicted label: {predicted_label}\")\n     - build_inputs_with_special_tokens\n     - create_token_type_ids_from_sequences\n \n-<frameworkcontent>\n-<pt>\n-\n ## DebertaV2Model\n \n [[autodoc]] DebertaV2Model\n@@ -176,44 +172,3 @@ print(f\"Predicted label: {predicted_label}\")\n \n [[autodoc]] DebertaV2ForMultipleChoice\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFDebertaV2Model\n-\n-[[autodoc]] TFDebertaV2Model\n-    - call\n-\n-## TFDebertaV2PreTrainedModel\n-\n-[[autodoc]] TFDebertaV2PreTrainedModel\n-    - call\n-\n-## TFDebertaV2ForMaskedLM\n-\n-[[autodoc]] TFDebertaV2ForMaskedLM\n-    - call\n-\n-## TFDebertaV2ForSequenceClassification\n-\n-[[autodoc]] TFDebertaV2ForSequenceClassification\n-    - call\n-\n-## TFDebertaV2ForTokenClassification\n-\n-[[autodoc]] TFDebertaV2ForTokenClassification\n-    - call\n-\n-## TFDebertaV2ForQuestionAnswering\n-\n-[[autodoc]] TFDebertaV2ForQuestionAnswering\n-    - call\n-\n-## TFDebertaV2ForMultipleChoice\n-\n-[[autodoc]] TFDebertaV2ForMultipleChoice\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "e938ffd5b5f1b6bc76433cc785c0898cfc43a453",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,8 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-</div>\n     </div>\n </div>\n \n@@ -117,9 +115,6 @@ echo -e '{\"text\": \"A soccer game with multiple people playing.\", \"text_pair\": \"S\n     - build_inputs_with_special_tokens\n     - create_token_type_ids_from_sequences\n \n-<frameworkcontent>\n-<pt>\n-\n ## DebertaModel\n \n [[autodoc]] DebertaModel\n@@ -148,40 +143,3 @@ echo -e '{\"text\": \"A soccer game with multiple people playing.\", \"text_pair\": \"S\n \n [[autodoc]] DebertaForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFDebertaModel\n-\n-[[autodoc]] TFDebertaModel\n-    - call\n-\n-## TFDebertaPreTrainedModel\n-\n-[[autodoc]] TFDebertaPreTrainedModel\n-    - call\n-\n-## TFDebertaForMaskedLM\n-\n-[[autodoc]] TFDebertaForMaskedLM\n-    - call\n-\n-## TFDebertaForSequenceClassification\n-\n-[[autodoc]] TFDebertaForSequenceClassification\n-    - call\n-\n-## TFDebertaForTokenClassification\n-\n-[[autodoc]] TFDebertaForTokenClassification\n-    - call\n-\n-## TFDebertaForQuestionAnswering\n-\n-[[autodoc]] TFDebertaForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-"
        },
        {
            "sha": "923daeb74104ed21b020b39bded51e058d94604c",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 35,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n@@ -46,7 +45,7 @@ distillation, especially when using a convnet as a teacher. This leads us to rep\n for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and\n models.*\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version of this model was added by [amyeroberts](https://huggingface.co/amyeroberts).\n+This model was contributed by [nielsr](https://huggingface.co/nielsr).\n \n ## Usage tips\n \n@@ -78,13 +77,13 @@ This model was contributed by [nielsr](https://huggingface.co/nielsr). The Tenso\n \n ### Using Scaled Dot Product Attention (SDPA)\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n page for more information.\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n ```\n@@ -138,9 +137,6 @@ If you're interested in submitting a resource to be included here, please feel f\n [[autodoc]] DeiTImageProcessorFast\n     - preprocess\n \n-<frameworkcontent>\n-<pt>\n-\n ## DeiTModel\n \n [[autodoc]] DeiTModel\n@@ -160,29 +156,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] DeiTForImageClassificationWithTeacher\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFDeiTModel\n-\n-[[autodoc]] TFDeiTModel\n-    - call\n-\n-## TFDeiTForMaskedImageModeling\n-\n-[[autodoc]] TFDeiTForMaskedImageModeling\n-    - call\n-\n-## TFDeiTForImageClassification\n-\n-[[autodoc]] TFDeiTForImageClassification\n-    - call\n-\n-## TFDeiTForImageClassificationWithTeacher\n-\n-[[autodoc]] TFDeiTForImageClassificationWithTeacher\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "5fe48bc47e7bc481aba8a265336ec9721d83d83a",
            "filename": "docs/source/en/model_doc/dpr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n@@ -85,9 +84,6 @@ This model was contributed by [lhoestq](https://huggingface.co/lhoestq). The ori\n \n [[autodoc]] models.dpr.modeling_dpr.DPRReaderOutput\n \n-<frameworkcontent>\n-<pt>\n-\n ## DPRContextEncoder\n \n [[autodoc]] DPRContextEncoder\n@@ -102,25 +98,3 @@ This model was contributed by [lhoestq](https://huggingface.co/lhoestq). The ori\n \n [[autodoc]] DPRReader\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFDPRContextEncoder\n-\n-[[autodoc]] TFDPRContextEncoder\n-    - call\n-\n-## TFDPRQuestionEncoder\n-\n-[[autodoc]] TFDPRQuestionEncoder\n-    - call\n-\n-## TFDPRReader\n-\n-[[autodoc]] TFDPRReader\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-"
        },
        {
            "sha": "f25460976d0f82ae9a975f4477f0494eb320c0e4",
            "filename": "docs/source/en/model_doc/efficientformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n <Tip warning={true}>\n@@ -55,7 +54,7 @@ EfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work pr\n reach extremely low latency on mobile devices while maintaining high performance.*\n \n This model was contributed by [novice03](https://huggingface.co/novice03) and [Bearnardd](https://huggingface.co/Bearnardd).\n-The original code can be found [here](https://github.com/snap-research/EfficientFormer). The TensorFlow version of this model was added by [D-Roberts](https://huggingface.co/D-Roberts).\n+The original code can be found [here](https://github.com/snap-research/EfficientFormer).\n \n ## Documentation resources\n \n@@ -70,9 +69,6 @@ The original code can be found [here](https://github.com/snap-research/Efficient\n [[autodoc]] EfficientFormerImageProcessor\n     - preprocess\n \n-<frameworkcontent>\n-<pt>\n-\n ## EfficientFormerModel\n \n [[autodoc]] EfficientFormerModel\n@@ -87,24 +83,3 @@ The original code can be found [here](https://github.com/snap-research/Efficient\n \n [[autodoc]] EfficientFormerForImageClassificationWithTeacher\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFEfficientFormerModel\n-\n-[[autodoc]] TFEfficientFormerModel\n-    - call\n-\n-## TFEfficientFormerForImageClassification\n-\n-[[autodoc]] TFEfficientFormerForImageClassification\n-    - call\n-\n-## TFEfficientFormerForImageClassificationWithTeacher\n-\n-[[autodoc]] TFEfficientFormerForImageClassificationWithTeacher\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "e83e2d5aa1da527ed36c996e2a3de0315d54afdb",
            "filename": "docs/source/en/model_doc/esm.md",
            "status": "modified",
            "additions": 5,
            "deletions": 35,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fesm.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,16 +19,15 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n \n-This page provides code and pre-trained weights for Transformer protein language models from Meta AI's Fundamental \n+This page provides code and pre-trained weights for Transformer protein language models from Meta AI's Fundamental\n AI Research Team, providing the state-of-the-art ESMFold and ESM-2, and the previously released ESM-1b and ESM-1v.\n Transformer protein language models were introduced in the paper [Biological structure and function emerge from scaling\n-unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by \n-Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, \n+unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by\n+Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,\n C. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\n The first version of this paper was [preprinted in 2019](https://www.biorxiv.org/content/10.1101/622803v1?versioned=true).\n \n@@ -46,8 +45,8 @@ they do not require a database of known protein sequences and structures with as\n to make predictions, and are much faster as a result.\n \n \n-The abstract from \n-\"Biological structure and function emerge from scaling unsupervised learning to 250 \n+The abstract from\n+\"Biological structure and function emerge from scaling unsupervised learning to 250\n million protein sequences\" is\n \n \n@@ -113,9 +112,6 @@ help throughout the process!\n     - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n-<frameworkcontent>\n-<pt>\n-\n ## EsmModel\n \n [[autodoc]] EsmModel\n@@ -140,29 +136,3 @@ help throughout the process!\n \n [[autodoc]] EsmForProteinFolding\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFEsmModel\n-\n-[[autodoc]] TFEsmModel\n-    - call\n-\n-## TFEsmForMaskedLM\n-\n-[[autodoc]] TFEsmForMaskedLM\n-    - call\n-\n-## TFEsmForSequenceClassification\n-\n-[[autodoc]] TFEsmForSequenceClassification\n-    - call\n-\n-## TFEsmForTokenClassification\n-\n-[[autodoc]] TFEsmForTokenClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "dd3ce34336dca8e7872b3e87f0aa32fdb5290812",
            "filename": "docs/source/en/model_doc/flaubert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -63,9 +62,6 @@ Tips:\n \n [[autodoc]] FlaubertTokenizer\n \n-<frameworkcontent>\n-<pt>\n-\n ## FlaubertModel\n \n [[autodoc]] FlaubertModel\n@@ -100,42 +96,3 @@ Tips:\n \n [[autodoc]] FlaubertForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFFlaubertModel\n-\n-[[autodoc]] TFFlaubertModel\n-    - call\n-\n-## TFFlaubertWithLMHeadModel\n-\n-[[autodoc]] TFFlaubertWithLMHeadModel\n-    - call\n-\n-## TFFlaubertForSequenceClassification\n-\n-[[autodoc]] TFFlaubertForSequenceClassification\n-    - call\n-\n-## TFFlaubertForMultipleChoice\n-\n-[[autodoc]] TFFlaubertForMultipleChoice\n-    - call\n-\n-## TFFlaubertForTokenClassification\n-\n-[[autodoc]] TFFlaubertForTokenClassification\n-    - call\n-\n-## TFFlaubertForQuestionAnsweringSimple\n-\n-[[autodoc]] TFFlaubertForQuestionAnsweringSimple\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-\n-\n-"
        },
        {
            "sha": "611e17fba8ce1424078468b72bd66aa1b67d8a3d",
            "filename": "docs/source/en/model_doc/funnel.md",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -89,11 +88,6 @@ This model was contributed by [sgugger](https://huggingface.co/sgugger). The ori\n \n [[autodoc]] models.funnel.modeling_funnel.FunnelForPreTrainingOutput\n \n-[[autodoc]] models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## FunnelBaseModel\n \n [[autodoc]] FunnelBaseModel\n@@ -133,49 +127,3 @@ This model was contributed by [sgugger](https://huggingface.co/sgugger). The ori\n \n [[autodoc]] FunnelForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFFunnelBaseModel\n-\n-[[autodoc]] TFFunnelBaseModel\n-    - call\n-\n-## TFFunnelModel\n-\n-[[autodoc]] TFFunnelModel\n-    - call\n-\n-## TFFunnelModelForPreTraining\n-\n-[[autodoc]] TFFunnelForPreTraining\n-    - call\n-\n-## TFFunnelForMaskedLM\n-\n-[[autodoc]] TFFunnelForMaskedLM\n-    - call\n-\n-## TFFunnelForSequenceClassification\n-\n-[[autodoc]] TFFunnelForSequenceClassification\n-    - call\n-\n-## TFFunnelForMultipleChoice\n-\n-[[autodoc]] TFFunnelForMultipleChoice\n-    - call\n-\n-## TFFunnelForTokenClassification\n-\n-[[autodoc]] TFFunnelForTokenClassification\n-    - call\n-\n-## TFFunnelForQuestionAnswering\n-\n-[[autodoc]] TFFunnelForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "f5569d72398fa02b88f6e7023de9602dc5169380",
            "filename": "docs/source/en/model_doc/groupvit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 32,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -31,19 +30,18 @@ The abstract from the paper is the following:\n \n *Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*\n \n-This model was contributed by [xvjiarui](https://huggingface.co/xvjiarui). The TensorFlow version was contributed by [ariG23498](https://huggingface.co/ariG23498) with the help of [Yih-Dar SHIEH](https://huggingface.co/ydshieh), [Amy Roberts](https://huggingface.co/amyeroberts), and [Joao Gante](https://huggingface.co/joaogante).\n-The original code can be found [here](https://github.com/NVlabs/GroupViT).\n+This model was contributed by [xvjiarui](https://huggingface.co/xvjiarui). The original code can be found [here](https://github.com/NVlabs/GroupViT).\n \n ## Usage tips\n- \n-- You may specify `output_segmentation=True` in the forward of `GroupViTModel` to get the segmentation logits of input texts. \n+\n+- You may specify `output_segmentation=True` in the forward of `GroupViTModel` to get the segmentation logits of input texts.\n \n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with GroupViT.\n \n - The quickest way to get started with GroupViT is by checking the [example notebooks](https://github.com/xvjiarui/GroupViT/blob/main/demo/GroupViT_hf_inference_notebook.ipynb) (which showcase zero-shot segmentation inference).\n-- One can also check out the [HuggingFace Spaces demo](https://huggingface.co/spaces/xvjiarui/GroupViT) to play with GroupViT. \n+- One can also check out the [HuggingFace Spaces demo](https://huggingface.co/spaces/xvjiarui/GroupViT) to play with GroupViT.\n \n ## GroupViTConfig\n \n@@ -58,9 +56,6 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] GroupViTVisionConfig\n \n-<frameworkcontent>\n-<pt>\n-\n ## GroupViTModel\n \n [[autodoc]] GroupViTModel\n@@ -77,26 +72,3 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] GroupViTVisionModel\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFGroupViTModel\n-\n-[[autodoc]] TFGroupViTModel\n-    - call\n-    - get_text_features\n-    - get_image_features\n-\n-## TFGroupViTTextModel\n-\n-[[autodoc]] TFGroupViTTextModel\n-    - call\n-\n-## TFGroupViTVisionModel\n-\n-[[autodoc]] TFGroupViTVisionModel\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "c770f0b9fab5ad621582bee2ea50f5a96d87b81a",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n     </div>\n@@ -125,9 +124,6 @@ print(transcription[0])\n [[autodoc]] HubertConfig\n     - all\n \n-<frameworkcontent>\n-<pt>\n-\n ## HubertModel\n \n [[autodoc]] HubertModel\n@@ -142,19 +138,3 @@ print(transcription[0])\n \n [[autodoc]] HubertForSequenceClassification\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFHubertModel\n-\n-[[autodoc]] TFHubertModel\n-    - call\n-\n-## TFHubertForCTC\n-\n-[[autodoc]] TFHubertForCTC\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "6296e72266043e6c5f0295b8ee4c7b1280e9432d",
            "filename": "docs/source/en/model_doc/idefics.md",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n@@ -59,16 +58,6 @@ To train a new IDEFICS model from scratch use the m4 codebase (a link will be pr\n [[autodoc]] IdeficsForVisionText2Text\n     - forward\n \n-## TFIdeficsModel\n-\n-[[autodoc]] TFIdeficsModel\n-    - call\n-\n-## TFIdeficsForVisionText2Text\n-\n-[[autodoc]] TFIdeficsForVisionText2Text\n-    - call\n-\n ## IdeficsImageProcessor\n \n [[autodoc]] IdeficsImageProcessor"
        },
        {
            "sha": "cadc77e7b88ff14550293538d9da600e71fbd678",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 45,
            "deletions": 75,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -31,57 +30,57 @@ You can find all the original LayoutLM checkpoints under the [LayoutLM](https://\n > [!TIP]\n > Click on the LayoutLM models in the right sidebar for more examples of how to apply LayoutLM to different vision and language tasks.\n \n-The example below demonstrates question answering with the [`AutoModel`] class. \n+The example below demonstrates question answering with the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">\n <hfoption id=\"AutoModel\">\n \n ```py\n-import torch  \n-from datasets import load_dataset  \n-from transformers import AutoTokenizer, LayoutLMForQuestionAnswering  \n-\n-tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)  \n-model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", torch_dtype=torch.float16)  \n-\n-dataset = load_dataset(\"nielsr/funsd\", split=\"train\")  \n-example = dataset[0]  \n-question = \"what's his name?\"  \n-words = example[\"words\"]  \n-boxes = example[\"bboxes\"]  \n-\n-encoding = tokenizer(  \n-    question.split(), \n-    words, \n-    is_split_into_words=True, \n-    return_token_type_ids=True, \n-    return_tensors=\"pt\"  \n-)  \n-bbox = []  \n-for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):  \n-    if s == 1:  \n-        bbox.append(boxes[w])  \n-    elif i == tokenizer.sep_token_id:  \n-        bbox.append([1000] * 4)  \n-    else:  \n-        bbox.append([0] * 4)  \n-encoding[\"bbox\"] = torch.tensor([bbox])  \n-\n-word_ids = encoding.word_ids(0)  \n-outputs = model(**encoding)  \n-loss = outputs.loss  \n-start_scores = outputs.start_logits  \n-end_scores = outputs.end_logits  \n-start, end = word_ids[start_scores.argmax(-1)], word_ids[end_scores.argmax(-1)]  \n-print(\" \".join(words[start : end + 1]))  \n+import torch\n+from datasets import load_dataset\n+from transformers import AutoTokenizer, LayoutLMForQuestionAnswering\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n+model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", torch_dtype=torch.float16)\n+\n+dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n+example = dataset[0]\n+question = \"what's his name?\"\n+words = example[\"words\"]\n+boxes = example[\"bboxes\"]\n+\n+encoding = tokenizer(\n+    question.split(),\n+    words,\n+    is_split_into_words=True,\n+    return_token_type_ids=True,\n+    return_tensors=\"pt\"\n+)\n+bbox = []\n+for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):\n+    if s == 1:\n+        bbox.append(boxes[w])\n+    elif i == tokenizer.sep_token_id:\n+        bbox.append([1000] * 4)\n+    else:\n+        bbox.append([0] * 4)\n+encoding[\"bbox\"] = torch.tensor([bbox])\n+\n+word_ids = encoding.word_ids(0)\n+outputs = model(**encoding)\n+loss = outputs.loss\n+start_scores = outputs.start_logits\n+end_scores = outputs.end_logits\n+start, end = word_ids[start_scores.argmax(-1)], word_ids[end_scores.argmax(-1)]\n+print(\" \".join(words[start : end + 1]))\n ```\n \n </hfoption>\n </hfoptions>\n \n ## Notes\n \n-- The original LayoutLM was not designed with a unified processing workflow. Instead, it expects preprocessed text (`words`) and bounding boxes (`boxes`) from an external OCR engine (like [Pytesseract](https://pypi.org/project/pytesseract/)) and provide them as additional inputs to the tokenizer. \n+- The original LayoutLM was not designed with a unified processing workflow. Instead, it expects preprocessed text (`words`) and bounding boxes (`boxes`) from an external OCR engine (like [Pytesseract](https://pypi.org/project/pytesseract/)) and provide them as additional inputs to the tokenizer.\n \n - The [`~LayoutLMModel.forward`] method expects the input `bbox` (bounding boxes of the input tokens). Each bounding box should be in the format `(x0, y0, x1, y1)`.  `(x0, y0)` corresponds to the upper left corner of the bounding box and `{x1, y1)` corresponds to the lower right corner. The bounding boxes need to be normalized on a 0-1000 scale as shown below.\n \n@@ -110,12 +109,12 @@ width, height = image.size\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with LayoutLM. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n-- Read [fine-tuning LayoutLM for document-understanding using Keras & Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras) to learn more.  \n-- Read [fine-tune LayoutLM for document-understanding using only Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm) for more information.  \n-- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb) for a practical example of how to fine-tune LayoutLM.  \n-- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) for an example of how to fine-tune LayoutLM for sequence classification.  \n-- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) for an example of how to fine-tune LayoutLM for token classification.  \n-- Read [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm) to learn how to deploy LayoutLM.  \n+- Read [fine-tuning LayoutLM for document-understanding using Keras & Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras) to learn more.\n+- Read [fine-tune LayoutLM for document-understanding using only Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm) for more information.\n+- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb) for a practical example of how to fine-tune LayoutLM.\n+- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) for an example of how to fine-tune LayoutLM for sequence classification.\n+- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) for an example of how to fine-tune LayoutLM for token classification.\n+- Read [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm) to learn how to deploy LayoutLM.\n \n \n ## LayoutLMConfig\n@@ -132,9 +131,6 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] LayoutLMTokenizerFast\n     - __call__\n \n-<frameworkcontent>\n-<pt>\n-\n ## LayoutLMModel\n \n [[autodoc]] LayoutLMModel\n@@ -154,29 +150,3 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## LayoutLMForQuestionAnswering\n \n [[autodoc]] LayoutLMForQuestionAnswering\n-\n-</pt>\n-<tf>\n-\n-## TFLayoutLMModel\n-\n-[[autodoc]] TFLayoutLMModel\n-\n-## TFLayoutLMForMaskedLM\n-\n-[[autodoc]] TFLayoutLMForMaskedLM\n-\n-## TFLayoutLMForSequenceClassification\n-\n-[[autodoc]] TFLayoutLMForSequenceClassification\n-\n-## TFLayoutLMForTokenClassification\n-\n-[[autodoc]] TFLayoutLMForTokenClassification\n-\n-## TFLayoutLMForQuestionAnswering\n-\n-[[autodoc]] TFLayoutLMForQuestionAnswering\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "9bb75e7772b77632bc2143c6d4d92dde3e2af0b4",
            "filename": "docs/source/en/model_doc/layoutlmv3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -32,7 +32,7 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> LayoutLMv3 architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.08387\">original paper</a>. </small>\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version of this model was added by [chriskoo](https://huggingface.co/chriskoo), [tokec](https://huggingface.co/tokec), and [lre](https://huggingface.co/lre). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/layoutlmv3).\n+This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/layoutlmv3).\n \n ## Usage tips\n \n@@ -110,9 +110,6 @@ LayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2\n [[autodoc]] LayoutLMv3Processor\n     - __call__\n \n-<frameworkcontent>\n-<pt>\n-\n ## LayoutLMv3Model\n \n [[autodoc]] LayoutLMv3Model\n@@ -132,29 +129,3 @@ LayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2\n \n [[autodoc]] LayoutLMv3ForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFLayoutLMv3Model\n-\n-[[autodoc]] TFLayoutLMv3Model\n-    - call\n-\n-## TFLayoutLMv3ForSequenceClassification\n-\n-[[autodoc]] TFLayoutLMv3ForSequenceClassification\n-    - call\n-\n-## TFLayoutLMv3ForTokenClassification\n-\n-[[autodoc]] TFLayoutLMv3ForTokenClassification\n-    - call\n-\n-## TFLayoutLMv3ForQuestionAnswering\n-\n-[[autodoc]] TFLayoutLMv3ForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "02a6667e11cdbb2372e4fa56b382625d3f02dea9",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-            <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -173,15 +172,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput\n \n-[[autodoc]] models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput\n-\n-[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput\n-\n-[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## LEDModel\n \n [[autodoc]] LEDModel\n@@ -201,22 +191,3 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] LEDForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFLEDModel\n-\n-[[autodoc]] TFLEDModel\n-    - call\n-\n-## TFLEDForConditionalGeneration\n-\n-[[autodoc]] TFLEDForConditionalGeneration\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-\n-\n-"
        },
        {
            "sha": "44c78198779f6fe31ff3965d9ac0d46682fbea09",
            "filename": "docs/source/en/model_doc/longformer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -16,7 +16,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -127,20 +126,6 @@ echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of t\n \n [[autodoc]] models.longformer.modeling_longformer.LongformerTokenClassifierOutput\n \n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutput\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput\n-\n-[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput\n-\n ## LongformerModel\n \n [[autodoc]] LongformerModel\n@@ -170,33 +155,3 @@ echo -e \"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of t\n \n [[autodoc]] LongformerForQuestionAnswering\n     - forward\n-\n-## TFLongformerModel\n-\n-[[autodoc]] TFLongformerModel\n-    - call\n-\n-## TFLongformerForMaskedLM\n-\n-[[autodoc]] TFLongformerForMaskedLM\n-    - call\n-\n-## TFLongformerForQuestionAnswering\n-\n-[[autodoc]] TFLongformerForQuestionAnswering\n-    - call\n-\n-## TFLongformerForSequenceClassification\n-\n-[[autodoc]] TFLongformerForSequenceClassification\n-    - call\n-\n-## TFLongformerForTokenClassification\n-\n-[[autodoc]] TFLongformerForTokenClassification\n-    - call\n-\n-## TFLongformerForMultipleChoice\n-\n-[[autodoc]] TFLongformerForMultipleChoice\n-    - call"
        },
        {
            "sha": "071cfdcba7bdd91df138b8bbe7e463db1a1a3ed6",
            "filename": "docs/source/en/model_doc/lxmert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -85,13 +84,6 @@ This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). T\n \n [[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\n \n-[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\n-\n-[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## LxmertModel\n \n [[autodoc]] LxmertModel\n@@ -106,19 +98,3 @@ This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). T\n \n [[autodoc]] LxmertForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFLxmertModel\n-\n-[[autodoc]] TFLxmertModel\n-    - call\n-\n-## TFLxmertForPreTraining\n-\n-[[autodoc]] TFLxmertForPreTraining\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "599251fc9c419d90c07d32231fa131692c79a3f3",
            "filename": "docs/source/en/model_doc/metaclip_2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-07-31.*\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-20.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n@@ -129,6 +129,3 @@ print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_\n \n [[autodoc]] MetaClip2ForImageClassification\n     - forward\n-\n-</pt>\n-<tf>"
        },
        {
            "sha": "1696e73b238102d0a4894a5f7c58ba2bcd548dc1",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -107,11 +106,6 @@ echo -e \"The capital of France is [MASK].\" | transformers run --task fill-mask -\n \n [[autodoc]] models.mobilebert.modeling_mobilebert.MobileBertForPreTrainingOutput\n \n-[[autodoc]] models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTrainingOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## MobileBertModel\n \n [[autodoc]] MobileBertModel\n@@ -151,49 +145,3 @@ echo -e \"The capital of France is [MASK].\" | transformers run --task fill-mask -\n \n [[autodoc]] MobileBertForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFMobileBertModel\n-\n-[[autodoc]] TFMobileBertModel\n-    - call\n-\n-## TFMobileBertForPreTraining\n-\n-[[autodoc]] TFMobileBertForPreTraining\n-    - call\n-\n-## TFMobileBertForMaskedLM\n-\n-[[autodoc]] TFMobileBertForMaskedLM\n-    - call\n-\n-## TFMobileBertForNextSentencePrediction\n-\n-[[autodoc]] TFMobileBertForNextSentencePrediction\n-    - call\n-\n-## TFMobileBertForSequenceClassification\n-\n-[[autodoc]] TFMobileBertForSequenceClassification\n-    - call\n-\n-## TFMobileBertForMultipleChoice\n-\n-[[autodoc]] TFMobileBertForMultipleChoice\n-    - call\n-\n-## TFMobileBertForTokenClassification\n-\n-[[autodoc]] TFMobileBertForTokenClassification\n-    - call\n-\n-## TFMobileBertForQuestionAnswering\n-\n-[[autodoc]] TFMobileBertForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "7e58d5b0013e9702259b87a73f05f79e74689782",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 31,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ Unless required by applicable law or agreed to in writing, software distributed\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-2\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -35,7 +34,7 @@ You can find all the original MobileViT checkpoints under the [Apple](https://hu\n \n \n > [!TIP]\n-> - This model was contributed by [matthijs](https://huggingface.co/Matthijs) and the TensorFlow version was contributed by [sayakpaul](https://huggingface.co/sayakpaul).\n+> - This model was contributed by [matthijs](https://huggingface.co/Matthijs).\n >\n > Click on the MobileViT models in the right sidebar for more examples of how to apply MobileViT to different vision tasks.\n \n@@ -94,8 +93,6 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n </hfoptions>\n \n \n-\n-\n ## Notes\n \n - Does **not** operate on sequential data, it's purely designed for image tasks.\n@@ -104,12 +101,9 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n - If using custom preprocessing, ensure that images are in **BGR** format (not RGB), as expected by the pretrained weights.\n - The classification models are pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k).\n - The segmentation models use a [DeepLabV3](https://huggingface.co/papers/1706.05587) head and are pretrained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n-- TensorFlow versions are compatible with TensorFlow Lite, making them ideal for edge/mobile deployment.\n-\n \n \n \n-  \n ## MobileViTConfig\n \n [[autodoc]] MobileViTConfig\n@@ -132,9 +126,6 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n     - preprocess\n     - post_process_semantic_segmentation\n \n-<frameworkcontent>\n-<pt>\n-\n ## MobileViTModel\n \n [[autodoc]] MobileViTModel\n@@ -149,24 +140,3 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n \n [[autodoc]] MobileViTForSemanticSegmentation\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFMobileViTModel\n-\n-[[autodoc]] TFMobileViTModel\n-    - call\n-\n-## TFMobileViTForImageClassification\n-\n-[[autodoc]] TFMobileViTForImageClassification\n-    - call\n-\n-## TFMobileViTForSemanticSegmentation\n-\n-[[autodoc]] TFMobileViTForSemanticSegmentation\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "08a150146518b048031a2d4ed0ec55fa18835950",
            "filename": "docs/source/en/model_doc/mpnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 41,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -47,7 +46,7 @@ The original code can be found [here](https://github.com/microsoft/MPNet).\n \n ## Usage tips\n \n-MPNet doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just \n+MPNet doesn't have `token_type_ids`, you don't need to indicate which token belongs to which segment. Just\n separate your segments with the separation token `tokenizer.sep_token` (or `[sep]`).\n \n ## Resources\n@@ -74,9 +73,6 @@ separate your segments with the separation token `tokenizer.sep_token` (or `[sep\n \n [[autodoc]] MPNetTokenizerFast\n \n-<frameworkcontent>\n-<pt>\n-\n ## MPNetModel\n \n [[autodoc]] MPNetModel\n@@ -106,39 +102,3 @@ separate your segments with the separation token `tokenizer.sep_token` (or `[sep\n \n [[autodoc]] MPNetForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFMPNetModel\n-\n-[[autodoc]] TFMPNetModel\n-    - call\n-\n-## TFMPNetForMaskedLM\n-\n-[[autodoc]] TFMPNetForMaskedLM\n-    - call\n-\n-## TFMPNetForSequenceClassification\n-\n-[[autodoc]] TFMPNetForSequenceClassification\n-    - call\n-\n-## TFMPNetForMultipleChoice\n-\n-[[autodoc]] TFMPNetForMultipleChoice\n-    - call\n-\n-## TFMPNetForTokenClassification\n-\n-[[autodoc]] TFMPNetForTokenClassification\n-    - call\n-\n-## TFMPNetForQuestionAnswering\n-\n-[[autodoc]] TFMPNetForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "80ce63bcbcfd375424d4baa6fcf7fb78316c6b27",
            "filename": "docs/source/en/model_doc/rag.md",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -20,7 +20,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n   <div class=\"flex flex-wrap space-x-1\">\n     <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-    <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n   </div>\n </div>\n@@ -105,9 +104,6 @@ print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0])\n \n [[autodoc]] RagRetriever\n \n-<frameworkcontent>\n-<pt>\n-\n ## RagModel\n \n [[autodoc]] RagModel\n@@ -124,26 +120,3 @@ print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0])\n [[autodoc]] RagTokenForGeneration\n     - forward\n     - generate\n-\n-</pt>\n-<tf>\n-\n-## TFRagModel\n-\n-[[autodoc]] TFRagModel\n-    - call\n-\n-## TFRagSequenceForGeneration\n-\n-[[autodoc]] TFRagSequenceForGeneration\n-    - call\n-    - generate\n-\n-## TFRagTokenForGeneration\n-\n-[[autodoc]] TFRagTokenForGeneration\n-    - call\n-    - generate\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "0d56840436c246f6b509d89c1e0af8ae1a155238",
            "filename": "docs/source/en/model_doc/regnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -31,9 +31,7 @@ The abstract from the paper is the following:\n \n *In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.*\n \n-This model was contributed by [Francesco](https://huggingface.co/Francesco). The TensorFlow version of the model\n-was contributed by [sayakpaul](https://huggingface.co/sayakpaul) and [ariG23498](https://huggingface.co/ariG23498).\n-The original code can be found [here](https://github.com/facebookresearch/pycls).\n+This model was contributed by [Francesco](https://huggingface.co/Francesco). The original code can be found [here](https://github.com/facebookresearch/pycls).\n \n The huge 10B model from [Self-supervised Pretraining of Visual Features in the Wild](https://huggingface.co/papers/2103.01988),\n trained on  one billion Instagram images, is available on the [hub](https://huggingface.co/facebook/regnet-y-10b-seer)"
        },
        {
            "sha": "7a1f9930d1f1e5f45a7e07e503f1fa9aaa8f385b",
            "filename": "docs/source/en/model_doc/rembert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -76,9 +75,6 @@ also similar to the Albert one rather than the BERT one.\n     - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n-<frameworkcontent>\n-<pt>\n-\n ## RemBertModel\n \n [[autodoc]] RemBertModel\n@@ -113,44 +109,3 @@ also similar to the Albert one rather than the BERT one.\n \n [[autodoc]] RemBertForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFRemBertModel\n-\n-[[autodoc]] TFRemBertModel\n-    - call\n-\n-## TFRemBertForMaskedLM\n-\n-[[autodoc]] TFRemBertForMaskedLM\n-    - call\n-\n-## TFRemBertForCausalLM\n-\n-[[autodoc]] TFRemBertForCausalLM\n-    - call\n-\n-## TFRemBertForSequenceClassification\n-\n-[[autodoc]] TFRemBertForSequenceClassification\n-    - call\n-\n-## TFRemBertForMultipleChoice\n-\n-[[autodoc]] TFRemBertForMultipleChoice\n-    - call\n-\n-## TFRemBertForTokenClassification\n-\n-[[autodoc]] TFRemBertForTokenClassification\n-    - call\n-\n-## TFRemBertForQuestionAnswering\n-\n-[[autodoc]] TFRemBertForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "8b44ac0bc65a5546fdfc350c7b442ec36663f85e",
            "filename": "docs/source/en/model_doc/resnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -36,7 +36,7 @@ The figure below illustrates the architecture of ResNet. Taken from the [origina\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png\"/>\n \n-This model was contributed by [Francesco](https://huggingface.co/Francesco). The TensorFlow version of this model was added by [amyeroberts](https://huggingface.co/amyeroberts). The original code can be found [here](https://github.com/KaimingHe/deep-residual-networks).\n+This model was contributed by [Francesco](https://huggingface.co/Francesco). The original code can be found [here](https://github.com/KaimingHe/deep-residual-networks).\n \n ## Resources\n "
        },
        {
            "sha": "045917b1c8d84e8e74fe9c763e109e4b609027f2",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -139,41 +138,24 @@ alt=\"drawing\" width=\"900\"/>\n \n [[autodoc]] SamPromptEncoderConfig\n \n-\n ## SamProcessor\n \n [[autodoc]] SamProcessor\n \n-\n ## SamImageProcessor\n \n [[autodoc]] SamImageProcessor\n \n-\n ## SamImageProcessorFast\n \n [[autodoc]] SamImageProcessorFast\n \n-\n ## SamVisionModel\n \n [[autodoc]] SamVisionModel\n     - forward\n \n-\n ## SamModel\n \n [[autodoc]] SamModel\n     - forward\n-\n-\n-## TFSamVisionModel\n-\n-[[autodoc]] TFSamVisionModel\n-    - call\n-\n-\n-## TFSamModel\n-\n-[[autodoc]] TFSamModel\n-    - call"
        },
        {
            "sha": "6af5d8d2886a37afe556d7d743e475ed9d324978",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -46,8 +45,7 @@ The figure below illustrates the architecture of SegFormer. Taken from the [orig\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/segformer_architecture.png\"/>\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version\n-of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/NVlabs/SegFormer).\n+This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/NVlabs/SegFormer).\n \n ## Usage tips\n \n@@ -62,7 +60,6 @@ of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). T\n   found on the [hub](https://huggingface.co/models?other=segformer).\n - The quickest way to get started with SegFormer is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer) (which showcase both inference and\n   fine-tuning on custom data). One can also check out the [blog post](https://huggingface.co/blog/fine-tune-segformer) introducing SegFormer and illustrating how it can be fine-tuned on custom data.\n-- TensorFlow users should refer to [this repository](https://github.com/deep-diver/segformer-tf-transformers) that shows off-the-shelf inference and fine-tuning.\n - One can also check out [this interactive demo on Hugging Face Spaces](https://huggingface.co/spaces/chansung/segformer-tf-transformers)\n   to try out a SegFormer model on custom images.\n - SegFormer works on any input size, as it pads the input to be divisible by `config.patch_sizes`.\n@@ -108,7 +105,6 @@ Semantic segmentation:\n - [`SegformerForSemanticSegmentation`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation).\n - A blog on fine-tuning SegFormer on a custom dataset can be found [here](https://huggingface.co/blog/fine-tune-segformer).\n - More demo notebooks on SegFormer (both inference + fine-tuning on a custom dataset) can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SegFormer).\n-- [`TFSegformerForSemanticSegmentation`] is supported by this [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb).\n - [Semantic segmentation task guide](../tasks/semantic_segmentation)\n \n If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n@@ -135,9 +131,6 @@ If you're interested in submitting a resource to be included here, please feel f\n     - preprocess\n     - post_process_semantic_segmentation\n \n-<frameworkcontent>\n-<pt>\n-\n ## SegformerModel\n \n [[autodoc]] SegformerModel\n@@ -157,29 +150,3 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] SegformerForSemanticSegmentation\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFSegformerDecodeHead\n-\n-[[autodoc]] TFSegformerDecodeHead\n-    - call\n-\n-## TFSegformerModel\n-\n-[[autodoc]] TFSegformerModel\n-    - call\n-\n-## TFSegformerForImageClassification\n-\n-[[autodoc]] TFSegformerForImageClassification\n-    - call\n-\n-## TFSegformerForSemanticSegmentation\n-\n-[[autodoc]] TFSegformerForSemanticSegmentation\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "5b802ca558698078c89bb76aeb75d3b4d01071dd",
            "filename": "docs/source/en/model_doc/speech_to_text.md",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -128,9 +127,6 @@ See the [model hub](https://huggingface.co/models?filter=speech_to_text) to look\n     - batch_decode\n     - decode\n \n-<frameworkcontent>\n-<pt>\n-\n ## Speech2TextModel\n \n [[autodoc]] Speech2TextModel\n@@ -140,19 +136,3 @@ See the [model hub](https://huggingface.co/models?filter=speech_to_text) to look\n \n [[autodoc]] Speech2TextForConditionalGeneration\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFSpeech2TextModel\n-\n-[[autodoc]] TFSpeech2TextModel\n-    - call\n-\n-## TFSpeech2TextForConditionalGeneration\n-\n-[[autodoc]] TFSpeech2TextForConditionalGeneration\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "b86f7fd4aa77e3cb237e553f742516e4e79c3445",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -139,12 +139,7 @@ processed_outputs = processor.post_process_keypoint_detection(outputs, [image_si\n - preprocess\n - post_process_keypoint_detection\n \n-<frameworkcontent>\n-<pt>\n ## SuperPointForKeypointDetection\n \n [[autodoc]] SuperPointForKeypointDetection\n-\n - forward\n-\n-</pt>"
        },
        {
            "sha": "ccffc893a7e295872936303422c3cbeb83ec5136",
            "filename": "docs/source/en/model_doc/swiftformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -32,8 +31,7 @@ The abstract from the paper is the following:\n \n *Self-attention has become a defacto choice for capturing global context in various vision applications. However, its quadratic computational complexity with respect to image resolution limits its use in real-time applications, especially for deployment on resource-constrained mobile devices. Although hybrid approaches have been proposed to combine the advantages of convolutions and self-attention for a better speed-accuracy trade-off, the expensive matrix multiplication operations in self-attention remain a bottleneck. In this work, we introduce a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations with linear element-wise multiplications. Our design shows that the key-value interaction can be replaced with a linear layer without sacrificing any accuracy. Unlike previous state-of-the-art methods, our efficient formulation of self-attention enables its usage at all stages of the network. Using our proposed efficient additive attention, we build a series of models called \"SwiftFormer\" which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster compared to MobileViT-v2.*\n \n-This model was contributed by [shehan97](https://huggingface.co/shehan97). The TensorFlow version was contributed by [joaocmd](https://huggingface.co/joaocmd).\n-The original code can be found [here](https://github.com/Amshaker/SwiftFormer).\n+This model was contributed by [shehan97](https://huggingface.co/shehan97). The original code can be found [here](https://github.com/Amshaker/SwiftFormer).\n \n ## SwiftFormerConfig\n \n@@ -48,13 +46,3 @@ The original code can be found [here](https://github.com/Amshaker/SwiftFormer).\n \n [[autodoc]] SwiftFormerForImageClassification\n     - forward\n-\n-## TFSwiftFormerModel\n-\n-[[autodoc]] TFSwiftFormerModel\n-    - call\n-\n-## TFSwiftFormerForImageClassification\n-\n-[[autodoc]] TFSwiftFormerForImageClassification\n-    - call"
        },
        {
            "sha": "7f86a11e8c5dc9fbabb195fd4d1a9fb22c6fc0be",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -92,9 +91,6 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n [[autodoc]] SwinConfig\n \n-<frameworkcontent>\n-<pt>\n-\n ## SwinModel\n \n [[autodoc]] SwinModel\n@@ -109,24 +105,3 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n [[autodoc]] transformers.SwinForImageClassification\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFSwinModel\n-\n-[[autodoc]] TFSwinModel\n-    - call\n-\n-## TFSwinForMaskedImageModeling\n-\n-[[autodoc]] TFSwinForMaskedImageModeling\n-    - call\n-\n-## TFSwinForImageClassification\n-\n-[[autodoc]] transformers.TFSwinForImageClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "4dfac5edce378144c167f9c8dae27945b94d9b0f",
            "filename": "docs/source/en/model_doc/tapas.md",
            "status": "modified",
            "additions": 17,
            "deletions": 287,
            "changes": 304,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapas.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,21 +19,20 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n \n The TAPAS model was proposed in [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349)\n-by Jonathan Herzig, PaweÅ‚ Krzysztof Nowak, Thomas MÃ¼ller, Francesco Piccinno and Julian Martin Eisenschlos. It's a BERT-based model specifically \n-designed (and pre-trained) for answering questions about tabular data. Compared to BERT, TAPAS uses relative position embeddings and has 7 \n-token types that encode tabular structure. TAPAS is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising \n-millions of tables from English Wikipedia and corresponding texts. \n+by Jonathan Herzig, PaweÅ‚ Krzysztof Nowak, Thomas MÃ¼ller, Francesco Piccinno and Julian Martin Eisenschlos. It's a BERT-based model specifically\n+designed (and pre-trained) for answering questions about tabular data. Compared to BERT, TAPAS uses relative position embeddings and has 7\n+token types that encode tabular structure. TAPAS is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising\n+millions of tables from English Wikipedia and corresponding texts.\n \n-For question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or summing) among selected cells. TAPAS has been fine-tuned on several datasets: \n+For question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or summing) among selected cells. TAPAS has been fine-tuned on several datasets:\n - [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n - [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\n-- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce). \n+- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce).\n \n It achieves state-of-the-art on both SQA and WTQ, while having comparable performance to SOTA on WikiSQL, with a much simpler architecture.\n \n@@ -44,11 +43,11 @@ The abstract from the paper is the following:\n In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creating a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. The authors of TAPAS call this further pre-training intermediate pre-training (since TAPAS is first pre-trained on MLM, and then on another dataset). They found that intermediate pre-training further improves performance on SQA, achieving a new state-of-the-art as well as state-of-the-art on [TabFact](https://github.com/wenhuchen/Table-Fact-Checking), a large-scale dataset with 16k Wikipedia tables for table entailment (a binary classification task). For more details, see their follow-up paper: [Understanding tables with intermediate pre-training](https://www.aclweb.org/anthology/2020.findings-emnlp.27/) by Julian Martin Eisenschlos, Syrine Krichene and Thomas MÃ¼ller.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tapas_architecture.png\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n <small> TAPAS architecture. Taken from the <a href=\"https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html\">original blog post</a>.</small>\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The Tensorflow version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/tapas).\n+This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/google-research/tapas).\n \n ## Usage tips\n \n@@ -77,8 +76,7 @@ To summarize:\n | Weak supervision for aggregation    | WTQ                 | Questions might involve aggregation, and the model must learn this given only the answer as supervision |\n | Strong supervision for aggregation  | WikiSQL-supervised  | Questions might involve aggregation, and the model must learn this given the gold aggregation operator  |\n \n-<frameworkcontent>\n-<pt>\n+\n Initializing a model with a pre-trained base and randomly initialized classification heads from the hub can be done as shown below.\n \n ```py\n@@ -106,37 +104,7 @@ Of course, you don't necessarily have to follow one of these three ways in which\n >>> # initializing the pre-trained base sized model with our custom classification heads\n >>> model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n ```\n-</pt>\n-<tf>\n-Initializing a model with a pre-trained base and randomly initialized classification heads from the hub can be done as shown below. Be sure to have installed the [tensorflow_probability](https://github.com/tensorflow/probability) dependency:\n-\n-```py\n->>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n \n->>> # for example, the base sized model with default SQA configuration\n->>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n-\n->>> # or, the base sized model with WTQ configuration\n->>> config = TapasConfig.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n->>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n-\n->>> # or, the base sized model with WikiSQL configuration\n->>> config = TapasConfig(\"google-base-finetuned-wikisql-supervised\")\n->>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n-```\n-\n-Of course, you don't necessarily have to follow one of these three ways in which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters you want when initializing [`TapasConfig`], and then create a [`TFTapasForQuestionAnswering`] based on that configuration. For example, if you have a dataset that has both conversational questions and questions that might involve aggregation, then you can do it this way. Here's an example:\n-\n-```py\n->>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n-\n->>> # you can initialize the classification heads any way you want (see docs of TapasConfig)\n->>> config = TapasConfig(num_aggregation_labels=3, average_logits_per_cell=True)\n->>> # initializing the pre-trained base sized model with our custom classification heads\n->>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n-```\n-</tf>\n-</frameworkcontent>\n \n What you can also do is start from an already fine-tuned checkpoint. A note here is that the already fine-tuned checkpoint on WTQ has some issues due to the L2-loss which is somewhat brittle. See [here](https://github.com/google-research/tapas/issues/91#issuecomment-735719340) for more info.\n \n@@ -160,8 +128,7 @@ The tables themselves should be present in a folder, each table being a separate\n \n **STEP 3: Convert your data into tensors using TapasTokenizer**\n \n-<frameworkcontent>\n-<pt>\n+\n Third, given that you've prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular data), you can then use [`TapasTokenizer`] to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids` and so on. Again, based on which of the three cases you picked above, [`TapasForQuestionAnswering`] requires different\n inputs to be fine-tuned:\n \n@@ -246,114 +213,14 @@ Of course, this only shows how to encode a single training example. It is advise\n >>> train_dataset = TableDataset(data, tokenizer)\n >>> train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n ```\n-</pt>\n-<tf>\n-Third, given that you've prepared your data in this TSV/CSV format (and corresponding CSV files containing the tabular data), you can then use [`TapasTokenizer`] to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids` and so on. Again, based on which of the three cases you picked above, [`TFTapasForQuestionAnswering`] requires different\n-inputs to be fine-tuned:\n-\n-| **Task**                           | **Required inputs**                                                                                                 |\n-|------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n-| Conversational                     | `input_ids`, `attention_mask`, `token_type_ids`, `labels`                                                           |\n-|  Weak supervision for aggregation  | `input_ids`, `attention_mask`, `token_type_ids`, `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |\n-| Strong supervision for aggregation | `input ids`, `attention mask`, `token type ids`, `labels`, `aggregation_labels`                                     |\n-\n-[`TapasTokenizer`] creates the `labels`, `numeric_values` and `numeric_values_scale` based on the `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer` and `aggregation_labels` are already in the TSV file of step 2. Here's an example:\n-\n-```py\n->>> from transformers import TapasTokenizer\n->>> import pandas as pd\n-\n->>> model_name = \"google/tapas-base\"\n->>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n-\n->>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n->>> queries = [\n-...     \"What is the name of the first actor?\",\n-...     \"How many movies has George Clooney played in?\",\n-...     \"What is the total number of movies?\",\n-... ]\n->>> answer_coordinates = [[(0, 0)], [(2, 1)], [(0, 1), (1, 1), (2, 1)]]\n->>> answer_text = [[\"Brad Pitt\"], [\"69\"], [\"209\"]]\n->>> table = pd.DataFrame.from_dict(data)\n->>> inputs = tokenizer(\n-...     table=table,\n-...     queries=queries,\n-...     answer_coordinates=answer_coordinates,\n-...     answer_text=answer_text,\n-...     padding=\"max_length\",\n-...     return_tensors=\"tf\",\n-... )\n->>> inputs\n-{'input_ids': tensor([[ ... ]]), 'attention_mask': tensor([[...]]), 'token_type_ids': tensor([[[...]]]),\n-'numeric_values': tensor([[ ... ]]), 'numeric_values_scale: tensor([[ ... ]]), labels: tensor([[ ... ]])}\n-```\n \n-Note that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.astype(str)` on a dataframe to turn it into text-only data.\n-Of course, this only shows how to encode a single training example. It is advised to create a dataloader to iterate over batches:\n-\n-```py\n->>> import tensorflow as tf\n->>> import pandas as pd\n-\n->>> tsv_path = \"your_path_to_the_tsv_file\"\n->>> table_csv_path = \"your_path_to_a_directory_containing_all_csv_files\"\n-\n-\n->>> class TableDataset:\n-...     def __init__(self, data, tokenizer):\n-...         self.data = data\n-...         self.tokenizer = tokenizer\n-\n-...     def __iter__(self):\n-...         for idx in range(self.__len__()):\n-...             item = self.data.iloc[idx]\n-...             table = pd.read_csv(table_csv_path + item.table_file).astype(\n-...                 str\n-...             )  # be sure to make your table data text only\n-...             encoding = self.tokenizer(\n-...                 table=table,\n-...                 queries=item.question,\n-...                 answer_coordinates=item.answer_coordinates,\n-...                 answer_text=item.answer_text,\n-...                 truncation=True,\n-...                 padding=\"max_length\",\n-...                 return_tensors=\"tf\",\n-...             )\n-...             # remove the batch dimension which the tokenizer adds by default\n-...             encoding = {key: tf.squeeze(val, 0) for key, val in encoding.items()}\n-...             # add the float_answer which is also required (weak supervision for aggregation case)\n-...             encoding[\"float_answer\"] = tf.convert_to_tensor(item.float_answer, dtype=tf.float32)\n-...             yield encoding[\"input_ids\"], encoding[\"attention_mask\"], encoding[\"numeric_values\"], encoding[\n-...                 \"numeric_values_scale\"\n-...             ], encoding[\"token_type_ids\"], encoding[\"labels\"], encoding[\"float_answer\"]\n-\n-...     def __len__(self):\n-...         return len(self.data)\n-\n-\n->>> data = pd.read_csv(tsv_path, sep=\"\\t\")\n->>> train_dataset = TableDataset(data, tokenizer)\n->>> output_signature = (\n-...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n-...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n-...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n-...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n-...     tf.TensorSpec(shape=(512, 7), dtype=tf.int32),\n-...     tf.TensorSpec(shape=(512,), dtype=tf.int32),\n-...     tf.TensorSpec(shape=(512,), dtype=tf.float32),\n-... )\n->>> train_dataloader = tf.data.Dataset.from_generator(train_dataset, output_signature=output_signature).batch(32)\n-```\n-</tf>\n-</frameworkcontent>\n \n Note that here, we encode each table-question pair independently. This is fine as long as your dataset is **not conversational**. In case your dataset involves conversational questions (such as in SQA), then you should first group together the `queries`, `answer_coordinates` and `answer_text` per table (in the order of their `position`\n-index) and batch encode each table with its questions. This will make sure that the `prev_labels` token types (see docs of [`TapasTokenizer`]) are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info. See [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info regarding using the TensorFlow model.\n+index) and batch encode each table with its questions. This will make sure that the `prev_labels` token types (see docs of [`TapasTokenizer`]) are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) for more info.\n \n **STEP 4: Train (fine-tune) the model\n \n-<frameworkcontent>\n-<pt>\n+\n You can then fine-tune [`TapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):\n \n ```py\n@@ -404,63 +271,12 @@ You can then fine-tune [`TapasForQuestionAnswering`] as follows (shown here for\n ...         loss.backward()\n ...         optimizer.step()\n ```\n-</pt>\n-<tf>\n-You can then fine-tune [`TFTapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):\n \n-```py\n->>> import tensorflow as tf\n->>> from transformers import TapasConfig, TFTapasForQuestionAnswering\n-\n->>> # this is the default WTQ configuration\n->>> config = TapasConfig(\n-...     num_aggregation_labels=4,\n-...     use_answer_as_supervision=True,\n-...     answer_loss_cutoff=0.664694,\n-...     cell_selection_preference=0.207951,\n-...     huber_loss_delta=0.121194,\n-...     init_cell_selection_weights_to_zero=True,\n-...     select_one_column=True,\n-...     allow_empty_column_selection=False,\n-...     temperature=0.0352513,\n-... )\n->>> model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n-\n->>> optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n-\n->>> for epoch in range(2):  # loop over the dataset multiple times\n-...     for batch in train_dataloader:\n-...         # get the inputs;\n-...         input_ids = batch[0]\n-...         attention_mask = batch[1]\n-...         token_type_ids = batch[4]\n-...         labels = batch[-1]\n-...         numeric_values = batch[2]\n-...         numeric_values_scale = batch[3]\n-...         float_answer = batch[6]\n-\n-...         # forward + backward + optimize\n-...         with tf.GradientTape() as tape:\n-...             outputs = model(\n-...                 input_ids=input_ids,\n-...                 attention_mask=attention_mask,\n-...                 token_type_ids=token_type_ids,\n-...                 labels=labels,\n-...                 numeric_values=numeric_values,\n-...                 numeric_values_scale=numeric_values_scale,\n-...                 float_answer=float_answer,\n-...             )\n-...         grads = tape.gradient(outputs.loss, model.trainable_weights)\n-...         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Usage: inference\n \n-<frameworkcontent>\n-<pt>\n-Here we explain how you can use [`TapasForQuestionAnswering`] or [`TFTapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.\n+\n+Here we explain how you can use [`TapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.\n \n However, note that inference is **different** depending on whether or not the setup is conversational. In a non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here's an example of that:\n \n@@ -516,68 +332,9 @@ Predicted answer: COUNT > 69\n What is the total number of movies?\n Predicted answer: SUM > 87, 53, 69\n ```\n-</pt>\n-<tf>\n-Here we explain how you can use [`TFTapasForQuestionAnswering`] for inference (i.e. making predictions on new data). For inference, only `input_ids`, `attention_mask` and `token_type_ids` (which you can obtain using [`TapasTokenizer`]) have to be provided to the model to obtain the logits. Next, you can use the handy [`~models.tapas.tokenization_tapas.convert_logits_to_predictions`] method to convert these into predicted coordinates and optional aggregation indices.\n-\n-However, note that inference is **different** depending on whether or not the setup is conversational. In a non-conversational set-up, inference can be done in parallel on all table-question pairs of a batch. Here's an example of that:\n-\n-```py\n->>> from transformers import TapasTokenizer, TFTapasForQuestionAnswering\n->>> import pandas as pd\n \n->>> model_name = \"google/tapas-base-finetuned-wtq\"\n->>> model = TFTapasForQuestionAnswering.from_pretrained(model_name)\n->>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n \n->>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n->>> queries = [\n-...     \"What is the name of the first actor?\",\n-...     \"How many movies has George Clooney played in?\",\n-...     \"What is the total number of movies?\",\n-... ]\n->>> table = pd.DataFrame.from_dict(data)\n->>> inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"tf\")\n->>> outputs = model(**inputs)\n->>> predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n-...     inputs, outputs.logits, outputs.logits_aggregation\n-... )\n-\n->>> # let's print out the results:\n->>> id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3: \"COUNT\"}\n->>> aggregation_predictions_string = [id2aggregation[x] for x in predicted_aggregation_indices]\n-\n->>> answers = []\n->>> for coordinates in predicted_answer_coordinates:\n-...     if len(coordinates) == 1:\n-...         # only a single cell:\n-...         answers.append(table.iat[coordinates[0]])\n-...     else:\n-...         # multiple cells\n-...         cell_values = []\n-...         for coordinate in coordinates:\n-...             cell_values.append(table.iat[coordinate])\n-...         answers.append(\", \".join(cell_values))\n-\n->>> display(table)\n->>> print(\"\")\n->>> for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n-...     print(query)\n-...     if predicted_agg == \"NONE\":\n-...         print(\"Predicted answer: \" + answer)\n-...     else:\n-...         print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)\n-What is the name of the first actor?\n-Predicted answer: Brad Pitt\n-How many movies has George Clooney played in?\n-Predicted answer: COUNT > 69\n-What is the total number of movies?\n-Predicted answer: SUM > 87, 53, 69\n-```\n-</tf>\n-</frameworkcontent>\n-\n-In case of a conversational set-up, then each table-question pair must be provided **sequentially** to the model, such that the `prev_labels` token types can be overwritten by the predicted `labels` of the previous table-question pair. Again, more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) (for PyTorch) and [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) (for TensorFlow).\n+In case of a conversational set-up, then each table-question pair must be provided **sequentially** to the model, such that the `prev_labels` token types can be overwritten by the predicted `labels` of the previous table-question pair. Again, more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb).\n \n ## Resources\n \n@@ -596,45 +353,18 @@ In case of a conversational set-up, then each table-question pair must be provid\n     - convert_logits_to_predictions\n     - save_vocabulary\n \n-<frameworkcontent>\n-<pt>\n-\n ## TapasModel\n [[autodoc]] TapasModel\n     - forward\n-    \n+\n ## TapasForMaskedLM\n [[autodoc]] TapasForMaskedLM\n     - forward\n \n ## TapasForSequenceClassification\n [[autodoc]] TapasForSequenceClassification\n     - forward\n-    \n+\n ## TapasForQuestionAnswering\n [[autodoc]] TapasForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFTapasModel\n-[[autodoc]] TFTapasModel\n-    - call\n-    \n-## TFTapasForMaskedLM\n-[[autodoc]] TFTapasForMaskedLM\n-    - call\n-\n-## TFTapasForSequenceClassification\n-[[autodoc]] TFTapasForSequenceClassification\n-    - call\n-    \n-## TFTapasForQuestionAnswering\n-[[autodoc]] TFTapasForQuestionAnswering\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-\n-"
        },
        {
            "sha": "5d9b92f7946f89be018b5228dd54ff3766de6b77",
            "filename": "docs/source/en/model_doc/transfo-xl.md",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n <Tip warning={true}>\n@@ -118,13 +117,6 @@ TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyT\n \n [[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\n \n-[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput\n-\n-[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## TransfoXLModel\n \n [[autodoc]] TransfoXLModel\n@@ -140,29 +132,6 @@ TransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyT\n [[autodoc]] TransfoXLForSequenceClassification\n     - forward\n \n-</pt>\n-<tf>\n-\n-## TFTransfoXLModel\n-\n-[[autodoc]] TFTransfoXLModel\n-    - call\n-\n-## TFTransfoXLLMHeadModel\n-\n-[[autodoc]] TFTransfoXLLMHeadModel\n-    - call\n-\n-## TFTransfoXLForSequenceClassification\n-\n-[[autodoc]] TFTransfoXLForSequenceClassification\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-\n ## Internal Layers\n \n [[autodoc]] AdaptiveEmbedding\n-\n-[[autodoc]] TFAdaptiveEmbedding"
        },
        {
            "sha": "b8b9867e8812ddc125c6b3ea59621db1fd9edb0a",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n     </div>\n@@ -30,7 +29,7 @@ rendered properly in your Markdown viewer.\n [ViTMAE](https://huggingface.co/papers/2111.06377) is a self-supervised vision model that is pretrained by masking large portions of an image (~75%). An encoder processes the visible image patches and a decoder reconstructs the missing pixels from the encoded patches and mask tokens. After pretraining, the encoder can be reused for downstream tasks like image classification or object detection â€” often outperforming models trained with supervised learning.\n \n <img src=\"https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png\"\n-alt=\"drawing\" width=\"600\"/> \n+alt=\"drawing\" width=\"600\"/>\n \n You can find all the original ViTMAE checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=vit-mae) organization.\n \n@@ -79,9 +78,6 @@ reconstruction = outputs.logits\n \n [[autodoc]] ViTMAEConfig\n \n-<frameworkcontent>\n-<pt>\n-\n ## ViTMAEModel\n \n [[autodoc]] ViTMAEModel\n@@ -91,19 +87,3 @@ reconstruction = outputs.logits\n \n [[autodoc]] transformers.ViTMAEForPreTraining\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFViTMAEModel\n-\n-[[autodoc]] TFViTMAEModel\n-    - call\n-\n-## TFViTMAEForPreTraining\n-\n-[[autodoc]] transformers.TFViTMAEForPreTraining\n-    - call\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "ce4631201c41d78032d1a1c08cb23dd3efc86e3c",
            "filename": "docs/source/en/model_doc/xlm.md",
            "status": "modified",
            "additions": 20,
            "deletions": 62,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n     </div>\n </div>\n \n@@ -37,33 +36,33 @@ The example below demonstrates how to predict the `<mask>` token with [`Pipeline\n <hfoption id=\"Pipeline\">\n \n ```python\n-import torch  \n-from transformers import pipeline  \n-\n-pipeline = pipeline(  \n-    task=\"fill-mask\",  \n-    model=\"facebook/xlm-roberta-xl\",  \n-    torch_dtype=torch.float16,  \n-    device=0  \n-)  \n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"fill-mask\",\n+    model=\"facebook/xlm-roberta-xl\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n pipeline(\"Bonjour, je suis un modÃ¨le <mask>.\")\n ```\n \n </hfoption>\n <hfoption id=\"AutoModel\">\n \n ```python\n-import torch  \n-from transformers import AutoModelForMaskedLM, AutoTokenizer  \n-\n-tokenizer = AutoTokenizer.from_pretrained(  \n-    \"FacebookAI/xlm-mlm-en-2048\",  \n-)  \n-model = AutoModelForMaskedLM.from_pretrained(  \n-    \"FacebookAI/xlm-mlm-en-2048\",  \n-    torch_dtype=torch.float16,  \n-    device_map=\"auto\",  \n-)  \n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"FacebookAI/xlm-mlm-en-2048\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+    \"FacebookAI/xlm-mlm-en-2048\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+)\n inputs = tokenizer(\"Hello, I'm a <mask> model.\", return_tensors=\"pt\").to(\"cuda\")\n \n with torch.no_grad():\n@@ -99,9 +98,6 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n \n [[autodoc]] models.xlm.modeling_xlm.XLMForQuestionAnsweringOutput\n \n-<frameworkcontent>\n-<pt>\n-\n ## XLMModel\n \n [[autodoc]] XLMModel\n@@ -136,41 +132,3 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n \n [[autodoc]] XLMForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFXLMModel\n-\n-[[autodoc]] TFXLMModel\n-    - call\n-\n-## TFXLMWithLMHeadModel\n-\n-[[autodoc]] TFXLMWithLMHeadModel\n-    - call\n-\n-## TFXLMForSequenceClassification\n-\n-[[autodoc]] TFXLMForSequenceClassification\n-    - call\n-\n-## TFXLMForMultipleChoice\n-\n-[[autodoc]] TFXLMForMultipleChoice\n-    - call\n-\n-## TFXLMForTokenClassification\n-\n-[[autodoc]] TFXLMForTokenClassification\n-    - call\n-\n-## TFXLMForQuestionAnsweringSimple\n-\n-[[autodoc]] TFXLMForQuestionAnsweringSimple\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n-\n-"
        },
        {
            "sha": "b0e5ef1ed08f00a409f0d6e0c16a3b15549e0621",
            "filename": "docs/source/en/model_doc/xlnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c031f6f99427ebeacaea33d5842fb623845ef766/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md?ref=c031f6f99427ebeacaea33d5842fb623845ef766",
            "patch": "@@ -19,7 +19,6 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n </div>\n \n ## Overview\n@@ -95,21 +94,6 @@ This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The o\n \n [[autodoc]] models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput\n \n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput\n-\n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput\n-\n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput\n-\n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput\n-\n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput\n-\n-[[autodoc]] models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput\n-\n-<frameworkcontent>\n-<pt>\n-\n ## XLNetModel\n \n [[autodoc]] XLNetModel\n@@ -144,39 +128,3 @@ This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The o\n \n [[autodoc]] XLNetForQuestionAnswering\n     - forward\n-\n-</pt>\n-<tf>\n-\n-## TFXLNetModel\n-\n-[[autodoc]] TFXLNetModel\n-    - call\n-\n-## TFXLNetLMHeadModel\n-\n-[[autodoc]] TFXLNetLMHeadModel\n-    - call\n-\n-## TFXLNetForSequenceClassification\n-\n-[[autodoc]] TFXLNetForSequenceClassification\n-    - call\n-\n-## TFXLNetForMultipleChoice\n-\n-[[autodoc]] TFXLNetForMultipleChoice\n-    - call\n-\n-## TFXLNetForTokenClassification\n-\n-[[autodoc]] TFXLNetForTokenClassification\n-    - call\n-\n-## TFXLNetForQuestionAnsweringSimple\n-\n-[[autodoc]] TFXLNetForQuestionAnsweringSimple\n-    - call\n-\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 1710,
        "additions": 123,
        "deletions": 1587
    }
}