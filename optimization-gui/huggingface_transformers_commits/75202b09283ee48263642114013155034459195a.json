{
    "author": "Cyrilvallez",
    "message": "Read config pattern for Qwen3Next (#40792)\n\nread it",
    "sha": "75202b09283ee48263642114013155034459195a",
    "files": [
        {
            "sha": "9db251a0689332dbe3dcbf3b1c4310db3dbb9172",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75202b09283ee48263642114013155034459195a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75202b09283ee48263642114013155034459195a/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=75202b09283ee48263642114013155034459195a",
            "patch": "@@ -243,8 +243,10 @@ def __init__(\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n+            interval_pattern = kwargs.get(\"full_attention_interval\", 4)\n             self.layer_types = [\n-                \"linear_attention\" if bool((i + 1) % 4) else \"full_attention\" for i in range(self.num_hidden_layers)\n+                \"linear_attention\" if bool((i + 1) % interval_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types)\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}