{
    "author": "yao-matrix",
    "message": "clean autoawq cases on xpu (#38163)\n\n* clean autoawq cases on xpu\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>",
    "sha": "7f28da285076c8267ff4c907e3948157f490fb69",
    "files": [
        {
            "sha": "195480be497a45fe93a948e862fbfe3325602b69",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f28da285076c8267ff4c907e3948157f490fb69/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f28da285076c8267ff4c907e3948157f490fb69/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=7f28da285076c8267ff4c907e3948157f490fb69",
            "patch": "@@ -21,6 +21,7 @@\n     backend_empty_cache,\n     require_accelerate,\n     require_auto_awq,\n+    require_flash_attn,\n     require_intel_extension_for_pytorch,\n     require_torch_accelerator,\n     require_torch_gpu,\n@@ -243,7 +244,7 @@ def test_save_pretrained(self):\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     @require_torch_multi_accelerator\n-    def test_quantized_model_multi_gpu(self):\n+    def test_quantized_model_multi_accelerator(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n         \"\"\"\n@@ -305,7 +306,7 @@ class AwqFusedTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def _check_fused_modules(self, model):\n@@ -359,6 +360,8 @@ def test_fused_modules_to_not_convert(self):\n         torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n         \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n     )\n+    @require_flash_attn\n+    @require_torch_gpu\n     def test_generation_fused(self):\n         \"\"\"\n         Test generation quality for fused models - single batch case\n@@ -382,6 +385,8 @@ def test_generation_fused(self):\n \n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n     @unittest.skipIf(\n         torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n         \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n@@ -433,6 +438,7 @@ def test_generation_llava_fused(self):\n \n         self.assertEqual(outputs[0][\"generated_text\"], EXPECTED_OUTPUT)\n \n+    @require_flash_attn\n     @require_torch_multi_gpu\n     @unittest.skipIf(\n         torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n@@ -473,8 +479,9 @@ def test_generation_custom_model(self):\n         outputs = model.generate(**inputs, max_new_tokens=12)\n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_CUSTOM_MODEL)\n \n-    @unittest.skip(reason=\"Not enough GPU memory on CI runners\")\n+    @require_flash_attn\n     @require_torch_multi_gpu\n+    @unittest.skip(reason=\"Not enough GPU memory on CI runners\")\n     def test_generation_mixtral_fused(self):\n         \"\"\"\n         Text generation test for Mixtral + AWQ + fused"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 10,
        "deletions": 3
    }
}