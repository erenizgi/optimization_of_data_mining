{
    "author": "JJJYmmm",
    "message": "Fix SDPA attention precision issue in Qwen2.5-VL (#37363)\n\n* solve conflicts and remove  redundant attention_mask in qwenvit\n\n* update decoded text check\n\n* remove trailing whitespace",
    "sha": "25343aafee10da8a13b217a39f3825f88c6d8dbe",
    "files": [
        {
            "sha": "57d50c181c80c32e1ff2c704b71dffffcc50df20",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -296,7 +296,6 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -320,27 +319,51 @@ def forward(\n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask=attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scaling,\n-            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seq_lens_k=cu_seqlens,\n-            max_length_q=max_seqlen,\n-            max_length_k=max_seqlen,\n-            is_causal=False,\n-            **kwargs,\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n \n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n@@ -361,15 +384,13 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -467,25 +488,6 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -515,14 +517,12 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n         seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         hidden_states = self.embeddings(hidden_states, seqlens, grid_thw, image_type_ids[:, 0], image_type_ids[:, 1])\n-        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens=cu_seqlens)\n \n         for blk in self.blocks:\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens,\n                 position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n             )\n \n         hidden_states = self.post_layernorm(hidden_states)"
        },
        {
            "sha": "3f8f593c208d981fddd8b07a5d02a083a1f49f5b",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -603,25 +603,6 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -651,14 +632,12 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n         seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         hidden_states = self.embeddings(hidden_states, seqlens, grid_thw, image_type_ids[:, 0], image_type_ids[:, 1])\n-        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens=cu_seqlens)\n \n         for blk in self.blocks:\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens,\n                 position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n             )\n \n         hidden_states = self.post_layernorm(hidden_states)"
        },
        {
            "sha": "c76c6a447cd042b51837cc016fa9a5f82eb9f790",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -957,7 +957,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -970,27 +969,51 @@ def forward(\n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask=attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scaling,\n-            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seq_lens_k=cu_seqlens,\n-            max_length_q=max_seqlen,\n-            max_length_k=max_seqlen,\n-            is_causal=False,\n-            **kwargs,\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n \n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n@@ -1024,14 +1047,12 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -1191,25 +1212,6 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -1257,12 +1259,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n \n-            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "c64e89a9ef433ebf58262eb2c8f2571313c31096",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -1935,7 +1935,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -1948,27 +1947,51 @@ def forward(\n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask=attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scaling,\n-            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seq_lens_k=cu_seqlens,\n-            max_length_q=max_seqlen,\n-            max_length_k=max_seqlen,\n-            is_causal=False,\n-            **kwargs,\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n \n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n@@ -1985,14 +2008,12 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -2007,25 +2028,6 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) ->\n         super().__init__(config, *inputs, **kwargs)\n         self.blocks = nn.ModuleList([Qwen2_5OmniVisionBlock(config) for _ in range(config.depth)])\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -2073,12 +2075,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n \n-            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "0285d18cd2fe9df6fe15a9e9b33d576d45186c51",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -215,7 +215,6 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -239,27 +238,51 @@ def forward(\n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask=attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scaling,\n-            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seq_lens_k=cu_seqlens,\n-            max_length_q=max_seqlen,\n-            max_length_k=max_seqlen,\n-            is_causal=False,\n-            **kwargs,\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n \n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n@@ -280,15 +303,13 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -422,25 +443,6 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -488,12 +490,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n \n-            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "07f41356e800acc1df313df0c32e050212767b5b",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -159,15 +159,13 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -289,25 +287,6 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -355,12 +334,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n \n-            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "a8b2ebf1a9fe44379f4babe64026ac4c83bc3143",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -333,7 +333,6 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -357,27 +356,51 @@ def forward(\n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask=attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scaling,\n-            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seq_lens_k=cu_seqlens,\n-            max_length_q=max_seqlen,\n-            max_length_k=max_seqlen,\n-            is_causal=False,\n-            **kwargs,\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n \n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n@@ -400,15 +423,13 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n-            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -721,25 +742,6 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n \n-    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n-        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n-        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n-        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n-        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return None\n-\n-        seq_length = inputs_tensor.shape[0]\n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(inputs_tensor.dtype).min,\n-            device=inputs_tensor.device,\n-            dtype=inputs_tensor.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-        return attention_mask\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -765,14 +767,12 @@ def forward(\n             dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n         )\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n-        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens)\n \n         for blk in self.blocks:\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens,\n                 position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "e5211e59f2edd503ff6abdedbb09a6283e1ee058",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/25343aafee10da8a13b217a39f3825f88c6d8dbe/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25343aafee10da8a13b217a39f3825f88c6d8dbe/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=25343aafee10da8a13b217a39f3825f88c6d8dbe",
            "patch": "@@ -419,7 +419,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture has a stocky build, thick fur, and a face that's\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n             \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n         ]  # fmt: skip\n         self.assertEqual("
        }
    ],
    "stats": {
        "total": 446,
        "additions": 201,
        "deletions": 245
    }
}