{
    "author": "mht-sharma",
    "message": "Add PyTorch version check for FA backend on AMD GPUs (#35813)\n\nDisable FA backend for SDPA on AMD GPUs (PyTorch < 2.4.1)",
    "sha": "fdcc62c855b3a0565e8bf173ac57842f4939b19d",
    "files": [
        {
            "sha": "a1e2db6c0883ae20c6b3128b825fb2b7938880c4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fdcc62c855b3a0565e8bf173ac57842f4939b19d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fdcc62c855b3a0565e8bf173ac57842f4939b19d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=fdcc62c855b3a0565e8bf173ac57842f4939b19d",
            "patch": "@@ -1546,6 +1546,7 @@ def _autoset_attn_implementation(\n                 torch.version.hip is not None\n                 and config._attn_implementation == \"sdpa\"\n                 and torch.cuda.device_count() > 1\n+                and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n             ):\n                 logger.warning_once(\n                     \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\""
        }
    ],
    "stats": {
        "total": 1,
        "additions": 1,
        "deletions": 0
    }
}