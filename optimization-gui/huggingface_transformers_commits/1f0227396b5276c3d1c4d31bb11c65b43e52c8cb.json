{
    "author": "JuanFKurucz",
    "message": "Fix reference to imagenet 1k dataset (#42348)",
    "sha": "1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
    "files": [
        {
            "sha": "9e8b0737261e43565df17c04832d64128cf4a58a",
            "filename": "docs/source/en/model_doc/levit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -56,7 +56,7 @@ This model was contributed by [anugunj](https://huggingface.co/anugunj). The ori\n   one takes the average prediction between both heads as final prediction. (2) is also called \"fine-tuning with distillation\",\n   because one relies on a teacher that has already been fine-tuned on the downstream dataset. In terms of models, (1) corresponds\n   to [`LevitForImageClassification`] and (2) corresponds to [`LevitForImageClassificationWithTeacher`].\n-- All released checkpoints were pre-trained and fine-tuned on  [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)\n+- All released checkpoints were pre-trained and fine-tuned on  [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k)\n   (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). only. No external data was used. This is in\n   contrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21k for\n   pre-training."
        },
        {
            "sha": "d2b276a15397bc36f01b3d1deed14892f594af68",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -87,7 +87,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n - Checkpoint names follow the pattern `mobilenet_v1_{depth_multiplier}_{resolution}`, like `mobilenet_v1_1.0_224`. `1.0` is the depth multiplier and `224` is the image resolution.\n - While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV1ImageProcessor`] handles the necessary preprocessing.\n-- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n+- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n - The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV1Config`].\n \n     ```python"
        },
        {
            "sha": "7adb9cd05eb6f8ac0108f5b28ed9f57d966dbb91",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -84,7 +84,7 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n \n - Classification checkpoint names follow the pattern `mobilenet_v2_{depth_multiplier}_{resolution}`, like `mobilenet_v2_1.4_224`. `1.4` is the depth multiplier and `224` is the image resolution. Segmentation checkpoint names follow the pattern `deeplabv3_mobilenet_v2_{depth_multiplier}_{resolution}`.\n - While trained on images of a specific sizes, the model architecture works with images of different sizes (minimum 32x32). The [`MobileNetV2ImageProcessor`] handles the necessary preprocessing.\n-- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n+- MobileNet is pretrained on [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k), a dataset with 1000 classes. However, the model actually predicts 1001 classes. The additional class is an extra \"background\" class (index 0).\n - The segmentation models use a [DeepLabV3+](https://huggingface.co/papers/1802.02611) head which is often pretrained on datasets like [PASCAL VOC](https://huggingface.co/datasets/merve/pascal-voc).\n - The original TensorFlow checkpoints determines the padding amount at inference because it depends on the input image size. To use the native PyTorch padding behavior, set `tf_padding=False` in [`MobileNetV2Config`].\n "
        },
        {
            "sha": "be3fe94b5e59abd586840110d8af51d67b2fa655",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -92,7 +92,7 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n - Feature maps are used directly instead of token embeddings.\n - Use [`MobileViTImageProcessor`] to preprocess images.\n - If using custom preprocessing, ensure that images are in **BGR** format (not RGB), as expected by the pretrained weights.\n-- The classification models are pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k).\n+- The classification models are pretrained on [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k).\n - The segmentation models use a [DeepLabV3](https://huggingface.co/papers/1706.05587) head and are pretrained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n \n ## MobileViTConfig"
        },
        {
            "sha": "237e474e10863f5c94212f5d88992734ec7d7e57",
            "filename": "docs/source/en/model_doc/mobilevitv2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -38,7 +38,7 @@ The original code can be found [here](https://github.com/apple/ml-cvnets).\n \n - MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map.\n - One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).\n-- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).\n+- The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).\n - The segmentation model uses a [DeepLabV3](https://huggingface.co/papers/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n \n ## MobileViTV2Config"
        },
        {
            "sha": "c9f288ac36b6718d8d7b11ceead5c33b2276f62c",
            "filename": "examples/pytorch/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/examples%2Fpytorch%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0227396b5276c3d1c4d31bb11c65b43e52c8cb/examples%2Fpytorch%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2FREADME.md?ref=1f0227396b5276c3d1c4d31bb11c65b43e52c8cb",
            "patch": "@@ -43,7 +43,7 @@ Coming soon!\n | [**`speech-recognition`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition) | [TIMIT](https://huggingface.co/datasets/timit_asr) | ✅ | - |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)\n | [**`multi-lingual speech-recognition`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition) | [Common Voice](https://huggingface.co/datasets/common_voice) | ✅ | - |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)\n | [**`audio-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) | [SUPERB KS](https://huggingface.co/datasets/superb) | ✅ | - |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)\n-| [**`image-pretraining`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining) | [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) | ✅ | - |✅ | /\n+| [**`image-pretraining`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining) | [ImageNet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k) | ✅ | - |✅ | /\n | [**`image-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) | [CIFAR-10](https://huggingface.co/datasets/cifar10) | ✅ | ✅ |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n | [**`semantic-segmentation`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation) | [SCENE_PARSE_150](https://huggingface.co/datasets/scene_parse_150) | ✅ | ✅ |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)\n | [**`object-detection`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection) | [CPPE-5](https://huggingface.co/datasets/cppe-5) | ✅ | ✅ |✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/object_detection.ipynb)"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}