{
    "author": "yonigozlan",
    "message": "Fix rescale normalize inconsistencies in fast image processors (#36388)\n\n* fix fused rescale normalize inconsistencies\n\n* fix siglip2 fast image processor\n\n* refactor kwargs validation and fused nirmalize rescale\n\n* cleanup kwargs handling in preprocess\n\n* update new procs after refactor",
    "sha": "79254c9b61ac2e1a6e0f698c38b1654159095e2e",
    "files": [
        {
            "sha": "0a201220b6773d3d786a18cf981d35660893bbae",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 136,
            "deletions": 72,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -40,8 +40,8 @@\n     get_image_type,\n     infer_channel_dimension_format,\n     make_flat_list_of_images,\n-    validate_fast_preprocess_arguments,\n     validate_kwargs,\n+    validate_preprocess_arguments,\n )\n from .processing_utils import Unpack\n from .utils import (\n@@ -72,6 +72,49 @@\n logger = logging.get_logger(__name__)\n \n \n+@lru_cache(maxsize=10)\n+def validate_fast_preprocess_arguments(\n+    do_rescale: Optional[bool] = None,\n+    rescale_factor: Optional[float] = None,\n+    do_normalize: Optional[bool] = None,\n+    image_mean: Optional[Union[float, List[float]]] = None,\n+    image_std: Optional[Union[float, List[float]]] = None,\n+    do_pad: Optional[bool] = None,\n+    size_divisibility: Optional[int] = None,\n+    do_center_crop: Optional[bool] = None,\n+    crop_size: Optional[SizeDict] = None,\n+    do_resize: Optional[bool] = None,\n+    size: Optional[SizeDict] = None,\n+    resample: Optional[\"PILImageResampling\"] = None,\n+    return_tensors: Optional[Union[str, TensorType]] = None,\n+    data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+):\n+    \"\"\"\n+    Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method.\n+    Raises `ValueError` if arguments incompatibility is caught.\n+    \"\"\"\n+    validate_preprocess_arguments(\n+        do_rescale=do_rescale,\n+        rescale_factor=rescale_factor,\n+        do_normalize=do_normalize,\n+        image_mean=image_mean,\n+        image_std=image_std,\n+        do_pad=do_pad,\n+        size_divisibility=size_divisibility,\n+        do_center_crop=do_center_crop,\n+        crop_size=crop_size,\n+        do_resize=do_resize,\n+        size=size,\n+        resample=resample,\n+    )\n+    # Extra checks for ImageProcessorFast\n+    if return_tensors is not None and return_tensors != \"pt\":\n+        raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n+\n+    if data_format != ChannelDimension.FIRST:\n+        raise ValueError(\"Only channel first data format is currently supported.\")\n+\n+\n def safe_squeeze(tensor: \"torch.Tensor\", axis: Optional[int] = None) -> \"torch.Tensor\":\n     \"\"\"\n     Squeezes a tensor, but only if the axis specified has dim 1.\n@@ -380,6 +423,23 @@ def normalize(\n         \"\"\"\n         return F.normalize(image, mean, std)\n \n+    @lru_cache(maxsize=10)\n+    def _fuse_mean_std_and_rescale_factor(\n+        self,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        device: Optional[\"torch.device\"] = None,\n+    ) -> tuple:\n+        if do_rescale and do_normalize:\n+            # Fused rescale and normalize\n+            image_mean = torch.tensor(image_mean, device=device) * (1.0 / rescale_factor)\n+            image_std = torch.tensor(image_std, device=device) * (1.0 / rescale_factor)\n+            do_rescale = False\n+        return image_mean, image_std, do_rescale\n+\n     def rescale_and_normalize(\n         self,\n         images: \"torch.Tensor\",\n@@ -392,12 +452,19 @@ def rescale_and_normalize(\n         \"\"\"\n         Rescale and normalize images.\n         \"\"\"\n-        if do_rescale and do_normalize:\n+        image_mean, image_std, do_rescale = self._fuse_mean_std_and_rescale_factor(\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            device=images.device,\n+        )\n+        # if/elif as we use fused rescale and normalize if both are set to True\n+        if do_normalize:\n             images = self.normalize(images.to(dtype=torch.float32), image_mean, image_std)\n         elif do_rescale:\n-            images = images * rescale_factor\n-        elif do_normalize:\n-            images = self.normalize(images, image_mean, image_std)\n+            images = self.rescale(images, rescale_factor)\n \n         return images\n \n@@ -527,25 +594,60 @@ def _prepare_input_images(\n \n         return processed_images\n \n-    @lru_cache(maxsize=10)\n-    def _prepare_process_arguments(\n+    def _further_process_kwargs(\n         self,\n-        do_resize: bool = None,\n-        size: Dict[str, int] = None,\n-        resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n-        do_center_crop: bool = None,\n-        crop_size: int = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        size: Optional[SizeDict] = None,\n+        crop_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if crop_size is not None:\n+            crop_size = SizeDict(**get_size_dict(crop_size, param_name=\"crop_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"crop_size\"] = crop_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+    def _validate_preprocess_kwargs(\n+        self,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, tuple[float]]] = None,\n+        image_std: Optional[Union[float, tuple[float]]] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[SizeDict] = None,\n+        do_center_crop: Optional[bool] = None,\n+        crop_size: Optional[SizeDict] = None,\n+        resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        device: Optional[\"torch.device\"] = None,\n-    ) -> tuple:\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ):\n         \"\"\"\n-        Prepare the arguments for the process method.\n+        validate the kwargs for the preprocess method.\n         \"\"\"\n         validate_fast_preprocess_arguments(\n             do_rescale=do_rescale,\n@@ -562,23 +664,8 @@ def _prepare_process_arguments(\n             data_format=data_format,\n         )\n \n-        if do_rescale and do_normalize:\n-            # Fused rescale and normalize\n-            image_mean = torch.tensor(image_mean, device=device) * (1.0 / rescale_factor)\n-            image_std = torch.tensor(image_std, device=device) * (1.0 / rescale_factor)\n-\n-        interpolation = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        return image_mean, image_std, interpolation\n-\n     @add_start_docstrings(BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS)\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n@@ -589,51 +676,28 @@ def preprocess(\n         do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n         input_data_format = kwargs.pop(\"input_data_format\")\n         device = kwargs.pop(\"device\")\n-\n+        # Prepare input images\n         images = self._prepare_input_images(\n             images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n         )\n \n-        # Pop kwargs that need further processing or won't be used in _preprocess\n-        default_to_square = kwargs.pop(\"default_to_square\")\n-        size = kwargs.pop(\"size\")\n-        crop_size = kwargs.pop(\"crop_size\")\n-        image_mean = kwargs.pop(\"image_mean\")\n-        image_std = kwargs.pop(\"image_std\")\n-        data_format = kwargs.pop(\"data_format\")\n-        resample = kwargs.pop(\"resample\")\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n \n-        # Make hashable for cache\n-        size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square)) if size is not None else None\n-        crop_size = SizeDict(**get_size_dict(crop_size, param_name=\"crop_size\")) if crop_size is not None else None\n-        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n-        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n \n-        image_mean, image_std, interpolation = self._prepare_process_arguments(\n-            size=size,\n-            crop_size=crop_size,\n-            resample=resample,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            data_format=data_format if data_format is not None else ChannelDimension.FIRST,\n-            device=images[0].device,\n-            do_resize=kwargs.get(\"do_resize\"),\n-            do_center_crop=kwargs.get(\"do_center_crop\"),\n-            do_rescale=kwargs.get(\"do_rescale\"),\n-            rescale_factor=kwargs.get(\"rescale_factor\"),\n-            do_normalize=kwargs.get(\"do_normalize\"),\n-            return_tensors=kwargs.get(\"return_tensors\"),\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n         )\n \n-        return self._preprocess(\n-            images=images,\n-            size=size,\n-            crop_size=crop_size,\n-            interpolation=interpolation,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            **kwargs,\n-        )\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        return self._preprocess(images=images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "bde61e3803b57065e8d732644041d5775ea49807",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -26,7 +26,6 @@\n \n from .utils import (\n     ExplicitEnum,\n-    TensorType,\n     is_av_available,\n     is_cv2_available,\n     is_decord_available,\n@@ -942,48 +941,6 @@ def validate_preprocess_arguments(\n         raise ValueError(\"`size` and `resample` must be specified if `do_resize` is `True`.\")\n \n \n-def validate_fast_preprocess_arguments(\n-    do_rescale: Optional[bool] = None,\n-    rescale_factor: Optional[float] = None,\n-    do_normalize: Optional[bool] = None,\n-    image_mean: Optional[Union[float, List[float]]] = None,\n-    image_std: Optional[Union[float, List[float]]] = None,\n-    do_pad: Optional[bool] = None,\n-    size_divisibility: Optional[int] = None,\n-    do_center_crop: Optional[bool] = None,\n-    crop_size: Optional[Dict[str, int]] = None,\n-    do_resize: Optional[bool] = None,\n-    size: Optional[Dict[str, int]] = None,\n-    resample: Optional[\"PILImageResampling\"] = None,\n-    return_tensors: Optional[Union[str, TensorType]] = None,\n-    data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-):\n-    \"\"\"\n-    Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method.\n-    Raises `ValueError` if arguments incompatibility is caught.\n-    \"\"\"\n-    validate_preprocess_arguments(\n-        do_rescale=do_rescale,\n-        rescale_factor=rescale_factor,\n-        do_normalize=do_normalize,\n-        image_mean=image_mean,\n-        image_std=image_std,\n-        do_pad=do_pad,\n-        size_divisibility=size_divisibility,\n-        do_center_crop=do_center_crop,\n-        crop_size=crop_size,\n-        do_resize=do_resize,\n-        size=size,\n-        resample=resample,\n-    )\n-    # Extra checks for ImageProcessorFast\n-    if return_tensors is not None and return_tensors != \"pt\":\n-        raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n-\n-    if data_format != ChannelDimension.FIRST:\n-        raise ValueError(\"Only channel first data format is currently supported.\")\n-\n-\n # In the future we can add a TF implementation here when we have TF models.\n class ImageFeatureExtractionMixin:\n     \"\"\""
        },
        {
            "sha": "5d400f3d139c1b54fd21a9ce92937ed16b58d98e",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -691,15 +691,8 @@ def _preprocess(\n                         target_size=resized_image.size()[-2:],\n                     )\n                 image = resized_image\n-\n-            if do_rescale and do_normalize:\n-                # fused rescale and normalize\n-                image = F.normalize(image.to(dtype=torch.float32), image_mean, image_std)\n-            elif do_rescale:\n-                image = image * rescale_factor\n-            elif do_normalize:\n-                image = F.normalize(image, image_mean, image_std)\n-\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n             if do_convert_annotations and annotations is not None:\n                 annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n "
        },
        {
            "sha": "c6374db9342e8052806064d0a68b990b7e636a54",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -716,15 +716,8 @@ def _preprocess(\n                         target_size=resized_image.size()[-2:],\n                     )\n                 image = resized_image\n-\n-            if do_rescale and do_normalize:\n-                # fused rescale and normalize\n-                image = F.normalize(image.to(dtype=torch.float32), image_mean, image_std)\n-            elif do_rescale:\n-                image = image * rescale_factor\n-            elif do_normalize:\n-                image = F.normalize(image, image_mean, image_std)\n-\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n             if do_convert_annotations and annotations is not None:\n                 annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n "
        },
        {
            "sha": "6af4985dfd7d674e6a4bd9d44d86f62b02f27b43",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 57,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -25,7 +25,6 @@\n     BaseImageProcessorFast,\n     BatchFeature,\n     DefaultFastImageProcessorKwargs,\n-    get_size_dict,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -37,7 +36,6 @@\n     SizeDict,\n     get_image_size,\n     make_nested_list_of_images,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -255,61 +253,7 @@ def preprocess(\n         images: ImageInput,\n         **kwargs: Unpack[Gemma3FastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Pop kwargs that need further processing or won't be used in _preprocess\n-        default_to_square = kwargs.pop(\"default_to_square\")\n-        size = kwargs.pop(\"size\")\n-        crop_size = kwargs.pop(\"crop_size\")\n-        image_mean = kwargs.pop(\"image_mean\")\n-        image_std = kwargs.pop(\"image_std\")\n-        data_format = kwargs.pop(\"data_format\")\n-        resample = kwargs.pop(\"resample\")\n-\n-        # Make hashable for cache\n-        size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square)) if size is not None else None\n-        crop_size = SizeDict(**get_size_dict(crop_size, param_name=\"crop_size\")) if crop_size is not None else None\n-        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n-        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n-\n-        image_mean, image_std, interpolation = self._prepare_process_arguments(\n-            size=size,\n-            crop_size=crop_size,\n-            resample=resample,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            data_format=data_format if data_format is not None else ChannelDimension.FIRST,\n-            device=images[0][0].device,\n-            do_resize=kwargs.get(\"do_resize\"),\n-            do_center_crop=kwargs.get(\"do_center_crop\"),\n-            do_rescale=kwargs.get(\"do_rescale\"),\n-            rescale_factor=kwargs.get(\"rescale_factor\"),\n-            do_normalize=kwargs.get(\"do_normalize\"),\n-            return_tensors=kwargs.get(\"return_tensors\"),\n-        )\n-\n-        return self._preprocess(\n-            images=images,\n-            size=size,\n-            crop_size=crop_size,\n-            interpolation=interpolation,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            **kwargs,\n-        )\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "20b7543671101c9e45245ec50f26b88151194615",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -49,7 +49,6 @@\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n-    is_vision_available,\n     logging,\n )\n from .image_processing_qwen2_vl import smart_resize\n@@ -58,13 +57,14 @@\n if is_torch_available():\n     import torch\n \n-if is_vision_available():\n-    pass\n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-elif is_torchvision_available():\n-    from torchvision.transforms import functional as F\n+if is_torchvision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n \n logger = logging.get_logger(__name__)\n \n@@ -311,19 +311,22 @@ def preprocess(\n         image_mean = tuple(image_mean) if image_mean is not None else None\n         image_std = tuple(image_std) if image_std is not None else None\n \n-        image_mean, image_std, interpolation = self._prepare_process_arguments(\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n+        self._validate_preprocess_kwargs(\n             do_rescale=do_rescale,\n             rescale_factor=rescale_factor,\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n             return_tensors=return_tensors,\n             data_format=data_format,\n-            device=device,\n         )\n+        interpolation = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n         if images is not None:\n             images = make_flat_list_of_images(images)\n         if videos is not None:"
        },
        {
            "sha": "8e96a13b2062050884ee5343393f5d04fd4f928b",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -486,15 +486,8 @@ def _preprocess(\n                         target_size=resized_image.size()[-2:],\n                     )\n                 image = resized_image\n-\n-            if do_rescale and do_normalize:\n-                # fused rescale and normalize\n-                image = F.normalize(image.to(dtype=torch.float32), image_mean, image_std)\n-            elif do_rescale:\n-                image = image * rescale_factor\n-            elif do_normalize:\n-                image = F.normalize(image, image_mean, image_std)\n-\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n             if do_convert_annotations and annotations is not None:\n                 annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n "
        },
        {
            "sha": "74a849dd4a0bfad2f8a2ade5d5da0787cffc26ae",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -266,15 +266,8 @@ def _preprocess(\n                         target_size=resized_image.size()[-2:],\n                     )\n                 image = resized_image\n-\n-            if do_rescale and do_normalize:\n-                # fused rescale and normalize\n-                image = F.normalize(image.to(dtype=torch.float32), image_mean, image_std)\n-            elif do_rescale:\n-                image = image * rescale_factor\n-            elif do_normalize:\n-                image = F.normalize(image, image_mean, image_std)\n-\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n             if do_convert_annotations and annotations is not None:\n                 annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n "
        },
        {
            "sha": "dcd7cef6293e7e2e3fb42b39c5486e348b4c0efb",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/79254c9b61ac2e1a6e0f698c38b1654159095e2e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=79254c9b61ac2e1a6e0f698c38b1654159095e2e",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for SigLIP2.\"\"\"\n \n-from functools import lru_cache\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -117,11 +116,10 @@ class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[Siglip2FastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    @lru_cache(maxsize=10)\n-    def _prepare_process_arguments(self, **kwargs) -> tuple:\n+    def _validate_preprocess_kwargs(self, **kwargs) -> tuple:\n         # Remove do_resize from kwargs to not raise an error as size is None\n         kwargs.pop(\"do_resize\", None)\n-        return super()._prepare_process_arguments(**kwargs)\n+        return super()._validate_preprocess_kwargs(**kwargs)\n \n     @add_start_docstrings(\n         BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,"
        }
    ],
    "stats": {
        "total": 386,
        "additions": 162,
        "deletions": 224
    }
}