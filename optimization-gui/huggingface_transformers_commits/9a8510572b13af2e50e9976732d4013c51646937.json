{
    "author": "mreraser",
    "message": "Updated the model card for ViTMAE (#38302)\n\n* Update vit_mae.md\n\n* badge float:right\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vit_mae.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update model_doc/vit_mae.md\n\n* fix\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "9a8510572b13af2e50e9976732d4013c51646937",
    "files": [
        {
            "sha": "787253f32fe82bab4efbeaedf90bd77259d84f28",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 36,
            "deletions": 60,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a8510572b13af2e50e9976732d4013c51646937/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a8510572b13af2e50e9976732d4013c51646937/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=9a8510572b13af2e50e9976732d4013c51646937",
            "patch": "@@ -14,87 +14,63 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# ViTMAE\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The ViTMAE model was proposed in [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377v2) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\n-Piotr DollÃ¡r, Ross Girshick. The paper shows that, by pre-training a Vision Transformer (ViT) to reconstruct pixel values for masked patches, one can get results after\n-fine-tuning that outperform supervised pre-training.\n-\n-The abstract from the paper is the following:\n+# ViTMAE\n \n-*This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the\n-input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates\n-only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask\n-tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs\n-enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity\n-models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream\n-tasks outperforms supervised pre-training and shows promising scaling behavior.*\n+[ViTMAE](https://huggingface.co/papers/2111.06377) is a self-supervised vision model that is pretrained by masking large portions of an image (~75%). An encoder processes the visible image patches and a decoder reconstructs the missing pixels from the encoded patches and mask tokens. After pretraining, the encoder can be reused for downstream tasks like image classification or object detection â€” often outperforming models trained with supervised learning.\n \n <img src=\"https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> MAE architecture. Taken from the <a href=\"https://arxiv.org/abs/2111.06377\">original paper.</a> </small>\n+You can find all the original ViTMAE checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=vit-mae) organization.\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [sayakpaul](https://github.com/sayakpaul) and \n-[ariG23498](https://github.com/ariG23498) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/mae). \n+> [!TIP]\n+> Click on the ViTMAE models in the right sidebar for more examples of how to apply ViTMAE to vision tasks.\n \n-## Usage tips\n+The example below demonstrates how to reconstruct the missing pixels with the [`ViTMAEForPreTraining`] class.\n \n-- MAE (masked auto encoding) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training objective is relatively simple:\n-by masking a large portion (75%) of the image patches, the model must reconstruct raw pixel values. One can use [`ViTMAEForPreTraining`] for this purpose.\n-- After pre-training, one \"throws away\" the decoder used to reconstruct pixels, and one uses the encoder for fine-tuning/linear probing. This means that after\n-fine-tuning, one can directly plug in the weights into a [`ViTForImageClassification`].\n-- One can use [`ViTImageProcessor`] to prepare images for the model. See the code examples for more info.\n-- Note that the encoder of MAE is only used to encode the visual patches. The encoded patches are then concatenated with mask tokens, which the decoder (which also\n-consists of Transformer blocks) takes as input. Each mask token is a shared, learned vector that indicates the presence of a missing patch to be predicted. Fixed\n-sin/cos position embeddings are added both to the input of the encoder and the decoder.\n-- For a visual understanding of how MAEs work you can check out this [post](https://keras.io/examples/vision/masked_image_modeling/).\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n-### Using Scaled Dot Product Attention (SDPA)\n+```python\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import ViTImageProcessor, ViTMAEForPreTraining\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n-`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+processor = ViTImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n+inputs = processor(image, return_tensors=\"pt\")\n+inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n \n-```\n-from transformers import ViTMAEModel\n-model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n-...\n-```\n+model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\", attn_implementation=\"sdpa\").to(\"cuda\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+reconstruction = outputs.logits\n+```\n \n-On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `facebook/vit-mae-base` model, we saw the following speedups during inference.\n+</hfoption>\n+</hfoptions>\n \n-|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n-|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n-|            1 |                                        11 |                                         6 |                      1.83 |\n-|            2 |                                         8 |                                         6 |                      1.33 |\n-|            4 |                                         8 |                                         6 |                      1.33 |\n-|            8 |                                         8 |                                         6 |                      1.33 |\n+## Notes\n+- ViTMAE is typically used in two stages. Self-supervised pretraining with [`ViTMAEForPreTraining`], and then discarding the decoder and fine-tuning the encoder. After fine-tuning, the weights can be plugged into a model like [`ViTForImageClassification`].\n+- Use [`ViTImageProcessor`] for input preparation.\n \n ## Resources\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViTMAE.\n-\n-- [`ViTMAEForPreTraining`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining), allowing you to pre-train the model from scratch/further pre-train the model on custom data.\n-- A notebook that illustrates how to visualize reconstructed pixel values with [`ViTMAEForPreTraining`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb).\n-\n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb) to learn how to visualize the reconstructed pixels from [`ViTMAEForPreTraining`].\n \n ## ViTMAEConfig\n "
        }
    ],
    "stats": {
        "total": 96,
        "additions": 36,
        "deletions": 60
    }
}