{
    "author": "flavioialongo",
    "message": "Add codebook_dim attribute to DacVectorQuantize for DacResidualVectorQuantize.from_latents() (#40665)\n\n* Add instance attribute to DacVectorQuantize for use in DacResidualVectorQuantize.from_latents\n\n* add from_latent tests\n\n* style fix\n\n* Fix style for test_modeling_dac.py",
    "sha": "91b34be9cf780e3ed4cc46358083444335c22552",
    "files": [
        {
            "sha": "e97c8183651e1fa45d0c0886e258aa564b701239",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/91b34be9cf780e3ed4cc46358083444335c22552/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91b34be9cf780e3ed4cc46358083444335c22552/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=91b34be9cf780e3ed4cc46358083444335c22552",
            "patch": "@@ -115,6 +115,7 @@ class DacVectorQuantize(nn.Module):\n     def __init__(self, config: DacConfig):\n         super().__init__()\n \n+        self.codebook_dim = config.codebook_dim\n         self.in_proj = nn.Conv1d(config.hidden_size, config.codebook_dim, kernel_size=1)\n         self.out_proj = nn.Conv1d(config.codebook_dim, config.hidden_size, kernel_size=1)\n         self.codebook = nn.Embedding(config.codebook_size, config.codebook_dim)"
        },
        {
            "sha": "cb7d6b388c1950539d6bd6d2b2302276b30835d7",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/91b34be9cf780e3ed4cc46358083444335c22552/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91b34be9cf780e3ed4cc46358083444335c22552/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=91b34be9cf780e3ed4cc46358083444335c22552",
            "patch": "@@ -375,6 +375,23 @@ def test_identity_shortcut(self):\n         config.use_conv_shortcut = False\n         self.model_tester.create_and_check_model_forward(config, inputs_dict)\n \n+    def test_quantizer_from_latents(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n+        model = DacModel(config=config).to(torch_device).eval()\n+        self.assertTrue(\n+            all(hasattr(quantizer, \"codebook_dim\") for quantizer in model.quantizer.quantizers),\n+            msg=\"All quantizers should have the attribute codebook_dim\",\n+        )\n+        with torch.no_grad():\n+            encoder_outputs = model.encode(inputs_dict[\"input_values\"])\n+            latents = encoder_outputs.projected_latents\n+            quantizer_representation, quantized_latents = model.quantizer.from_latents(latents=latents)\n+\n+        self.assertIsInstance(quantizer_representation, torch.Tensor)\n+        self.assertIsInstance(quantized_latents, torch.Tensor)\n+        self.assertEqual(quantized_latents.shape[0], latents.shape[0])\n+        self.assertEqual(quantized_latents.shape[1], latents.shape[1])\n+\n \n # Copied from transformers.tests.encodec.test_modeling_encodec.normalize\n def normalize(arr):\n@@ -872,3 +889,39 @@ def test_integration_batch(self, model_name):\n             # make sure forward and decode gives same result\n             enc_dec = model(inputs[\"input_values\"])[1]\n             torch.testing.assert_close(decoded_outputs[\"audio_values\"], enc_dec, rtol=1e-6, atol=1e-6)\n+\n+    @parameterized.expand([(model_name,) for model_name in EXPECTED_PREPROC_SHAPE_BATCH.keys()])\n+    def test_quantizer_from_latents_integration(self, model_name):\n+        model_id = f\"descript/{model_name}\"\n+        model = DacModel.from_pretrained(model_id).to(torch_device)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # load audio sample\n+        librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n+        audio_sample = librispeech_dummy[0][\"audio\"][\"array\"]\n+\n+        # check on processor audio shape\n+        inputs = processor(\n+            raw_audio=audio_sample,\n+            sampling_rate=processor.sampling_rate,\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        input_values = inputs[\"input_values\"]\n+        with torch.no_grad():\n+            encoder_outputs = model.encode(input_values)\n+            latents = encoder_outputs.projected_latents\n+\n+            # reconstruction using from_latents\n+            quantizer_representation, quantized_latents = model.quantizer.from_latents(latents=latents)\n+            reconstructed = model.decode(quantized_representation=quantizer_representation).audio_values\n+\n+            # forward pass\n+            original_reconstructed = model(input_values).audio_values\n+\n+        # ensure forward and decode are the same\n+        self.assertTrue(\n+            torch.allclose(reconstructed, original_reconstructed, atol=1e-6),\n+            msg=\"Reconstructed codes from latents should match original quantized codes\",\n+        )"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 54,
        "deletions": 0
    }
}