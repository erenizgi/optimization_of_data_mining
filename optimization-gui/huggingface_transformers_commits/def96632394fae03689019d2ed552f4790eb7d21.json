{
    "author": "kmehant",
    "message": "feat: support indivisible shards for TP model loading and TPlizing. (#37220)\n\n* feat: support uneven loading and sharding\nresolve merge conflicts\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* fix: allow for empty tensor computations\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* test: add llama1b test case\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* due to q_proj colwise it has to be multi of 2\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* refactor: use slice API\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* refactor: use slice API\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* refactor: use slice API\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* refactor: use slice API\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n---------\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>",
    "sha": "def96632394fae03689019d2ed552f4790eb7d21",
    "files": [
        {
            "sha": "61c20ecf6db1e17224525f6e072c5bb23b27afdf",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 58,
            "deletions": 8,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/def96632394fae03689019d2ed552f4790eb7d21/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def96632394fae03689019d2ed552f4790eb7d21/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=def96632394fae03689019d2ed552f4790eb7d21",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n from __future__ import annotations\n \n+import math\n import operator\n import os\n import re\n@@ -280,7 +281,48 @@ def repack_weights(\n def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n     \"\"\"\n     Generalized tensor sharding across a multi-dimensional device mesh.\n-\n+    Extract only the fraction of the parameter owned by the given `rank` when the parameter would have gone sharding at provided `dim`.\n+    Extraction follows the pytorch `Shard` placement so that sharding and materializing back to full tensor follows `Shard` semantics.\n+    `Shard` follows torch.chunk style sharding of the tensor. We demonstrate some cases below on how sharding happens including some edge cases\n+    such as some ranks having an empty tensor as shard. Below implementation is robut to all these cases.\n+\n+    Case (1)\n+    empty_param                 (16, 5120, 8190)\n+    dim                         0\n+    device_mesh.size()          4\n+    rank 0 gets\t\t\t\t\t(4, 5120, 8190)\t\t\t (0 ... 4, 5120, 8190)\n+    rank 1 gets\t\t\t\t\t(4, 5120, 8190)\t\t\t (4 ... 8, 5120, 8190)\n+    rank 2 gets\t\t\t\t\t(4, 5120, 8190)\t\t\t (8 ... 12, 5120, 8190)\n+    rank 3 gets\t\t\t\t\t(4, 5120, 8190)\t\t\t (12 ... 16, 5120, 8190)\n+\n+    Case (2)\n+    empty_param                 (16, 5120, 8190)\n+    dim                         0\n+    device_mesh.size()          14\n+    rank 0 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (0 ... 2, 5120, 8190)\n+    rank 1 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (2 ... 4, 5120, 8190)\n+    rank 2 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (4 ... 6, 5120, 8190)\n+    rank 3 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (6 ... 8, 5120, 8190)\n+    rank 4 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (8 ... 10, 5120, 8190)\n+    rank 5 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (10 ... 12, 5120, 8190)\n+    rank 6 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (12 ... 14, 5120, 8190)\n+    rank 7 gets\t\t\t\t\t(2, 5120, 8190)\t\t\t (14 ... 16, 5120, 8190)\n+    rank 8 gets\t\t\t\t\t(0, 5120, 8190)\n+    rank 9 gets\t\t\t\t\t(0, 5120, 8190)\n+    rank 10 gets\t\t\t    (0, 5120, 8190)\n+    rank 11 gets\t\t\t\t(0, 5120, 8190)\n+    rank 12 gets\t\t\t\t(0, 5120, 8190)\n+    rank 13 gets\t\t\t\t(0, 5120, 8190)\n+\n+    Case (3)\n+    empty_param                 (16, 5120, 8190)\n+    dim                         0\n+    device_mesh.size()          3\n+    rank 0 gets\t\t\t\t\t(6, 5120, 8190)\t\t\t (0 ... 6, 5120, 8190)\n+    rank 1 gets\t\t\t\t\t(6, 5120, 8190)\t\t\t (6 ... 12, 5120, 8190)\n+    rank 2 gets\t\t\t\t\t(4, 5120, 8190)\t\t\t (12 ... 16, 5120, 8190)\n+\n+    In case (2), empty shards are returned with appropriate dimension to allow for operations to work smoothly.\n     Args:\n         param (torch.Tensor): The tensor to shard.\n         empty_param (torch.Tensor): A tensor used for shape reference.\n@@ -289,6 +331,7 @@ def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n         dim (int): Dimension along which to shard the tensor.\n     \"\"\"\n     param_dim = empty_param.dim()\n+\n     if dim < 0:\n         dim = param_dim + dim\n     if dim >= param_dim:\n@@ -301,15 +344,18 @@ def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n     if rank >= world_size:\n         raise ValueError(f\"Rank {rank} is out of bounds for mesh size {world_size}\")\n \n-    shard_size = empty_param.shape[dim] // world_size\n+    shard_size = math.ceil(empty_param.shape[dim] / world_size)\n     start = rank * shard_size\n-    end = start + shard_size\n \n     # Construct slicing index dynamically\n+    end = min(start + shard_size, empty_param.shape[dim])\n     slice_indices = [slice(None)] * param_dim\n-    slice_indices[dim] = slice(start, end)\n-\n-    return param[tuple(slice_indices)]\n+    if start < empty_param.shape[dim]:\n+        slice_indices[dim] = slice(start, end)\n+        return param[tuple(slice_indices)]\n+    dimensions = list(param.shape)\n+    dimensions[dim] = 0\n+    return torch.empty(tuple(dimensions), dtype=torch.int64)\n \n \n def distribute_module(\n@@ -500,7 +546,9 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+            parameter = DTensor.from_local(\n+                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+            )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod\n@@ -574,7 +622,9 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+            parameter = DTensor.from_local(\n+                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+            )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 58,
        "deletions": 8
    }
}