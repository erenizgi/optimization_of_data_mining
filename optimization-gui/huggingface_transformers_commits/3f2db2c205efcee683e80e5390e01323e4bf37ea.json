{
    "author": "Rocketknight1",
    "message": "Simplify pipeline padding logic (#41667)\n\n* Remove a lot of unnecessary pad logic\n\n* Remove unnecessary clone() calls since we're just doing a slice assignment\n\n* Just make the full tensor instead of adding to a zeros tensor",
    "sha": "3f2db2c205efcee683e80e5390e01323e4bf37ea",
    "files": [
        {
            "sha": "92867eee15294bea75130eccc95355b7e9a62ba6",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 11,
            "deletions": 30,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f2db2c205efcee683e80e5390e01323e4bf37ea/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f2db2c205efcee683e80e5390e01323e4bf37ea/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=3f2db2c205efcee683e80e5390e01323e4bf37ea",
            "patch": "@@ -78,7 +78,7 @@ def _pad(items, key, padding_value, padding_side):\n     if isinstance(items[0][key], torch.Tensor):\n         # Others include `attention_mask` etc...\n         shape = items[0][key].shape\n-        dim = len(shape)\n+        dim = items[0][key].ndim\n         if dim == 1:\n             # We have a list of 1-dim torch tensors, which can be stacked without padding\n             return torch.cat([item[key] for item in items], dim=0)\n@@ -93,37 +93,18 @@ def _pad(items, key, padding_value, padding_side):\n         min_length = min(item[key].shape[1] for item in items)\n         dtype = items[0][key].dtype\n \n-        tensor = None\n-        if dim == 2:\n-            if max_length == min_length:\n-                # Bypass for `ImageGPT` which doesn't provide a padding value, yet\n-                # we can consistently pad since the size should be matching\n-                return torch.cat([item[key] for item in items], dim=0)\n-            tensor = torch.zeros((batch_size, max_length), dtype=dtype) + padding_value\n-        elif dim == 3:\n-            tensor = torch.zeros((batch_size, max_length, shape[-1]), dtype=dtype) + padding_value\n-        elif dim == 4:\n-            tensor = torch.zeros((batch_size, max_length, shape[-2], shape[-1]), dtype=dtype) + padding_value\n-\n-        if tensor is None:\n-            raise ValueError(f\"Unable to create tensor for padding from {key} with dimension {dim}\")\n+        if dim == 2 and max_length == min_length:\n+            # Bypass for `ImageGPT` which doesn't provide a padding value, yet\n+            # we can consistently pad since the size should be matching\n+            return torch.cat([item[key] for item in items], dim=0)\n+        else:\n+            tensor = torch.full([batch_size, max_length] + list(shape[2:]), fill_value=padding_value, dtype=dtype)\n \n         for i, item in enumerate(items):\n-            if dim == 2:\n-                if padding_side == \"left\":\n-                    tensor[i, -len(item[key][0]) :] = item[key][0].clone()\n-                else:\n-                    tensor[i, : len(item[key][0])] = item[key][0].clone()\n-            elif dim == 3:\n-                if padding_side == \"left\":\n-                    tensor[i, -len(item[key][0]) :, :] = item[key][0].clone()\n-                else:\n-                    tensor[i, : len(item[key][0]), :] = item[key][0].clone()\n-            elif dim == 4:\n-                if padding_side == \"left\":\n-                    tensor[i, -len(item[key][0]) :, :, :] = item[key][0].clone()\n-                else:\n-                    tensor[i, : len(item[key][0]), :, :] = item[key][0].clone()\n+            if padding_side == \"left\":\n+                tensor[i, -len(item[key][0]) :] = item[key][0]\n+            else:\n+                tensor[i, : len(item[key][0])] = item[key][0]\n \n         return tensor\n     else:"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 11,
        "deletions": 30
    }
}