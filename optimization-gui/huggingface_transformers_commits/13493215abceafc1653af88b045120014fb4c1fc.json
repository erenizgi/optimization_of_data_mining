{
    "author": "gante",
    "message": "ðŸ§¼ remove v4.44 deprecations (#34245)\n\n* remove v4.44 deprecations\r\n\r\n* PR comments\r\n\r\n* deprecations scheduled for v4.50\r\n\r\n* hub version update\r\n\r\n* make fiuxp\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "13493215abceafc1653af88b045120014fb4c1fc",
    "files": [
        {
            "sha": "922258d65efab71002b37563b562079ab5a54744",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -117,7 +117,7 @@\n     \"fugashi>=1.0\",\n     \"GitPython<3.1.19\",\n     \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub>=0.23.2,<1.0\",\n+    \"huggingface-hub>=0.24.0,<1.0\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n     \"isort>=5.5.4\","
        },
        {
            "sha": "9543b58ad40d9137c0784583eaa626743c5983cd",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -24,7 +24,7 @@\n     \"fugashi\": \"fugashi>=1.0\",\n     \"GitPython\": \"GitPython<3.1.19\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub\": \"huggingface-hub>=0.23.2,<1.0\",\n+    \"huggingface-hub\": \"huggingface-hub>=0.24.0,<1.0\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"isort\": \"isort>=5.5.4\","
        },
        {
            "sha": "71555e2f7c3162023b807df5df6400181c620b85",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 87,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -94,7 +94,7 @@\n     replace_return_docstrings,\n     strtobool,\n )\n-from .utils.hub import convert_file_size_to_int, create_and_tag_model_card, get_checkpoint_shard_files\n+from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n from .utils.import_utils import (\n     ENV_VARS_TRUE_VALUES,\n     is_sagemaker_mp_enabled,\n@@ -381,92 +381,6 @@ def check_support_param_buffer_assignment(model_to_load, state_dict, start_prefi\n     return False\n \n \n-def shard_checkpoint(\n-    state_dict: Dict[str, torch.Tensor], max_shard_size: Union[int, str] = \"10GB\", weights_name: str = WEIGHTS_NAME\n-):\n-    \"\"\"\n-    Splits a model state dictionary in sub-checkpoints so that the final size of each sub-checkpoint does not exceed a\n-    given size.\n-\n-    The sub-checkpoints are determined by iterating through the `state_dict` in the order of its keys, so there is no\n-    optimization made to make each sub-checkpoint as close as possible to the maximum size passed. For example, if the\n-    limit is 10GB and we have weights of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get sharded as [6GB], [6+2GB],\n-    [6+2+2GB] and not [6+2+2GB], [6+2GB], [6GB].\n-\n-    <Tip warning={true}>\n-\n-    If one of the model's weight is bigger than `max_shard_size`, it will end up in its own sub-checkpoint which will\n-    have a size greater than `max_shard_size`.\n-\n-    </Tip>\n-\n-    Args:\n-        state_dict (`Dict[str, torch.Tensor]`): The state dictionary of a model to save.\n-        max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n-            The maximum size of each sub-checkpoint. If expressed as a string, needs to be digits followed by a unit\n-            (like `\"5MB\"`).\n-        weights_name (`str`, *optional*, defaults to `\"pytorch_model.bin\"`):\n-            The name of the model save file.\n-    \"\"\"\n-    logger.warning(\n-        \"Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using \"\n-        \"split_torch_state_dict_into_shards from huggingface_hub library\"\n-    )\n-    max_shard_size = convert_file_size_to_int(max_shard_size)\n-\n-    sharded_state_dicts = [{}]\n-    last_block_size = 0\n-    total_size = 0\n-    storage_id_to_block = {}\n-\n-    for key, weight in state_dict.items():\n-        # when bnb serialization is used the weights in the state dict can be strings\n-        # check: https://github.com/huggingface/transformers/pull/24416 for more details\n-        if isinstance(weight, str):\n-            continue\n-        else:\n-            storage_id = id_tensor_storage(weight)\n-\n-        # If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`\n-        if storage_id in storage_id_to_block and weight.device != torch.device(\"meta\"):\n-            block_id = storage_id_to_block[storage_id]\n-            sharded_state_dicts[block_id][key] = weight\n-            continue\n-\n-        weight_size = weight.numel() * dtype_byte_size(weight.dtype)\n-        # If this weight is going to tip up over the maximal size, we split, but only if we have put at least one\n-        # weight in the current shard.\n-        if last_block_size + weight_size > max_shard_size and len(sharded_state_dicts[-1]) > 0:\n-            sharded_state_dicts.append({})\n-            last_block_size = 0\n-\n-        sharded_state_dicts[-1][key] = weight\n-        last_block_size += weight_size\n-        total_size += weight_size\n-        storage_id_to_block[storage_id] = len(sharded_state_dicts) - 1\n-\n-    # If we only have one shard, we return it\n-    if len(sharded_state_dicts) == 1:\n-        return {weights_name: sharded_state_dicts[0]}, None\n-\n-    # Otherwise, let's build the index\n-    weight_map = {}\n-    shards = {}\n-    for idx, shard in enumerate(sharded_state_dicts):\n-        shard_file = weights_name.replace(\".bin\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.bin\")\n-        shard_file = shard_file.replace(\n-            \".safetensors\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.safetensors\"\n-        )\n-        shards[shard_file] = shard\n-        for key in shard.keys():\n-            weight_map[key] = shard_file\n-\n-    # Add the metadata\n-    metadata = {\"total_size\": total_size}\n-    index = {\"metadata\": metadata, \"weight_map\": weight_map}\n-    return shards, index\n-\n-\n def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"\"\"\n     This is the same as"
        },
        {
            "sha": "d34528b7431453ecc6aa1a5de4fe439e4838b0e8",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -2203,7 +2203,7 @@ def forward(\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat(\n@@ -2326,7 +2326,7 @@ def generate(\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat("
        },
        {
            "sha": "4129920f9b36639deea6a8241d24ca5f066f09a8",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -153,7 +153,7 @@ def __call__(\n                 logger.warning_once(\n                     \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n                     \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n                 )\n \n             # cast to desired return tensors type"
        },
        {
            "sha": "e5622185bc39a8e26c8ecce59286aa668dd63f07",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -1471,7 +1471,7 @@ def forward(\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat(\n@@ -1610,7 +1610,7 @@ def generate(\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat("
        },
        {
            "sha": "a96d97fb07e1d9be1f204ce51c8c5f28788b6f39",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -148,7 +148,7 @@ def __call__(\n                     logger.warning_once(\n                         \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n                         \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n                     )\n \n             # cast to desired return tensors type after concatenating"
        },
        {
            "sha": "e8536ee50f94bb8966eb522f86c0232198f15bfc",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -485,7 +485,7 @@ def forward(\n                 \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                 \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             # prefill stage vs decoding stage (legacy behavior copied)\n             if input_ids.shape[1] != 1:"
        },
        {
            "sha": "820fa581711a63b6def780c477e2d9578b6a3498",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -160,7 +160,7 @@ def __call__(\n                     \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n                     \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                     \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n                 )\n \n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])"
        },
        {
            "sha": "269663c7d6141ac88295f61d9a2129bb2c40611f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -868,7 +868,7 @@ def forward(\n                 \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                 \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             if input_ids.shape[1] != 1:\n                 inputs_embeds = inputs_embeds.to(image_features.dtype)"
        },
        {
            "sha": "89b885f0f1abb23bada1b9a44b04091cf4992ea7",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -143,7 +143,7 @@ def __call__(\n                     \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n                     \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                     \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n                 )\n             else:\n                 image_sizes = iter(image_inputs[\"image_sizes\"])"
        },
        {
            "sha": "284d8a3d454848efaa7a522899ac36c363620505",
            "filename": "src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -21,10 +21,11 @@\n import types\n \n import torch\n+from huggingface_hub import split_torch_state_dict_into_shards\n from packaging import version\n \n from transformers import AutoTokenizer, GPT2Config\n-from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME, shard_checkpoint\n+from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n \n \n def add_checkpointing_args(parser):\n@@ -571,7 +572,15 @@ def convert_checkpoint_from_megatron_to_transformers(args):\n \n     # Store the state_dict to file.\n     max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n-    shards, index = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n+    state_dict_split = split_torch_state_dict_into_shards(output_state_dict, max_shard_size=max_shard_size)\n+    shards = index = None\n+    for tensors in state_dict_split.filename_to_tensors.values():\n+        shards = {tensor: state_dict[tensor] for tensor in tensors}\n+    if state_dict_split.is_sharded:\n+        index = {\n+            \"metadata\": state_dict_split.metadata,\n+            \"weight_map\": state_dict_split.tensor_to_filename,\n+        }\n \n     # Save the model\n     for shard_file, shard in shards.items():"
        },
        {
            "sha": "a0c97fc4e234abbcbf9bbed29b9143f0aaf12293",
            "filename": "src/transformers/models/rwkv/convert_rwkv_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fconvert_rwkv_checkpoint_to_hf.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -21,10 +21,10 @@\n import re\n \n import torch\n-from huggingface_hub import hf_hub_download\n+from huggingface_hub import hf_hub_download, split_torch_state_dict_into_shards\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerFast, RwkvConfig\n-from transformers.modeling_utils import WEIGHTS_INDEX_NAME, shard_checkpoint\n+from transformers.modeling_utils import WEIGHTS_INDEX_NAME\n \n \n NUM_HIDDEN_LAYERS_MAPPING = {\n@@ -116,7 +116,16 @@ def convert_rmkv_checkpoint_to_hf_format(\n     state_dict = convert_state_dict(state_dict)\n \n     # 4. Split in shards and save\n-    shards, index = shard_checkpoint(state_dict)\n+    state_dict_split = split_torch_state_dict_into_shards(state_dict)\n+    shards = index = None\n+    for tensors in state_dict_split.filename_to_tensors.values():\n+        shards = {tensor: state_dict[tensor] for tensor in tensors}\n+    if state_dict_split.is_sharded:\n+        index = {\n+            \"metadata\": state_dict_split.metadata,\n+            \"weight_map\": state_dict_split.tensor_to_filename,\n+        }\n+\n     for shard_file, shard in shards.items():\n         torch.save(shard, os.path.join(output_dir, shard_file))\n "
        },
        {
            "sha": "30adcb6ab5c08971e26a1f09a8a479c86210322f",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -578,7 +578,7 @@ def forward(\n                 \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                 \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             if input_ids.shape[1] != 1:\n                 for features, frames in ((image_features, 1), (video_features, num_frames)):"
        },
        {
            "sha": "597d94cc2f00316780d659ddd58083a9ea792431",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -149,9 +149,10 @@ def __call__(\n         if encoded_images is not None and (self.patch_size is None or self.vision_feature_select_strategy is None):\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set \"\n+                \"directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = \"\n+                \"{{vision_feature_select_strategy}}`. Using processors without these attributes in the config is \"\n+                \"deprecated and will throw an error in v4.50.\"\n             )\n         # Replace the image/video tokens with the expanded token sequence\n         elif encoded_images is not None:"
        },
        {
            "sha": "b45325d2194e2482d4ebe7772b8b9abba3b876a6",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -476,7 +476,7 @@ def forward(\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in VipLLaVa should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's image processing config. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n             )\n             # prefill stage vs decoding stage (legacy behavior copied)\n             if input_ids.shape[1] != 1:"
        },
        {
            "sha": "96a30df7e5587f02589a4466f34f6b25058e932e",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/13493215abceafc1653af88b045120014fb4c1fc/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13493215abceafc1653af88b045120014fb4c1fc/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=13493215abceafc1653af88b045120014fb4c1fc",
            "patch": "@@ -105,7 +105,6 @@\n         _find_disjoint,\n         _find_identical,\n         dtype_byte_size,\n-        shard_checkpoint,\n     )\n     from transformers.pytorch_utils import isin_mps_friendly\n \n@@ -668,71 +667,6 @@ def test_no_super_init_config_and_model(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-    def test_shard_checkpoint(self):\n-        # This is the model we will use, total size 340,000 bytes.\n-        model = torch.nn.Sequential(\n-            torch.nn.Linear(100, 200, bias=False),  # size 80,000\n-            torch.nn.Linear(200, 200, bias=False),  # size 160,000\n-            torch.nn.Linear(200, 100, bias=False),  # size 80,000\n-            torch.nn.Linear(100, 50, bias=False),  # size 20,000\n-        )\n-        state_dict = model.state_dict()\n-\n-        with self.subTest(\"No shard when max size is bigger than model size\"):\n-            shards, index = shard_checkpoint(state_dict)\n-            self.assertIsNone(index)\n-            self.assertDictEqual(shards, {WEIGHTS_NAME: state_dict})\n-\n-        with self.subTest(\"Test sharding, no weights bigger than max size\"):\n-            shards, index = shard_checkpoint(state_dict, max_shard_size=\"300kB\")\n-            # Split is first two layers then last two.\n-            self.assertDictEqual(\n-                index,\n-                {\n-                    \"metadata\": {\"total_size\": 340000},\n-                    \"weight_map\": {\n-                        \"0.weight\": \"pytorch_model-00001-of-00002.bin\",\n-                        \"1.weight\": \"pytorch_model-00001-of-00002.bin\",\n-                        \"2.weight\": \"pytorch_model-00002-of-00002.bin\",\n-                        \"3.weight\": \"pytorch_model-00002-of-00002.bin\",\n-                    },\n-                },\n-            )\n-\n-            shard1 = {\"0.weight\": state_dict[\"0.weight\"], \"1.weight\": state_dict[\"1.weight\"]}\n-            shard2 = {\"2.weight\": state_dict[\"2.weight\"], \"3.weight\": state_dict[\"3.weight\"]}\n-            self.assertDictEqual(\n-                shards, {\"pytorch_model-00001-of-00002.bin\": shard1, \"pytorch_model-00002-of-00002.bin\": shard2}\n-            )\n-\n-        with self.subTest(\"Test sharding with weights bigger than max size\"):\n-            shards, index = shard_checkpoint(state_dict, max_shard_size=\"100kB\")\n-            # Split is first layer, second layer then last 2.\n-            self.assertDictEqual(\n-                index,\n-                {\n-                    \"metadata\": {\"total_size\": 340000},\n-                    \"weight_map\": {\n-                        \"0.weight\": \"pytorch_model-00001-of-00003.bin\",\n-                        \"1.weight\": \"pytorch_model-00002-of-00003.bin\",\n-                        \"2.weight\": \"pytorch_model-00003-of-00003.bin\",\n-                        \"3.weight\": \"pytorch_model-00003-of-00003.bin\",\n-                    },\n-                },\n-            )\n-\n-            shard1 = {\"0.weight\": state_dict[\"0.weight\"]}\n-            shard2 = {\"1.weight\": state_dict[\"1.weight\"]}\n-            shard3 = {\"2.weight\": state_dict[\"2.weight\"], \"3.weight\": state_dict[\"3.weight\"]}\n-            self.assertDictEqual(\n-                shards,\n-                {\n-                    \"pytorch_model-00001-of-00003.bin\": shard1,\n-                    \"pytorch_model-00002-of-00003.bin\": shard2,\n-                    \"pytorch_model-00003-of-00003.bin\": shard3,\n-                },\n-            )\n-\n     def test_checkpoint_sharding_local_bin(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n "
        }
    ],
    "stats": {
        "total": 217,
        "additions": 42,
        "deletions": 175
    }
}