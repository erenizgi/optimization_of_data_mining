{
    "author": "leaderofARS",
    "message": "Fix incorrect library name in BitNet integration warning (#42966)\n\n* Fix incorrect library name in BitNet integration warning\n\n* Fix typos in BitNet integration docstrings\n\n* Fix docstring parameter mismatch in BitLinear",
    "sha": "f7d139c17a252bb7ea1190afe673a6b3bda54f57",
    "files": [
        {
            "sha": "c3425b1f371c3df3072638833ac2c890f2909a1a",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7d139c17a252bb7ea1190afe673a6b3bda54f57/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7d139c17a252bb7ea1190afe673a6b3bda54f57/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=f7d139c17a252bb7ea1190afe673a6b3bda54f57",
            "patch": "@@ -89,7 +89,7 @@ def unpack_weights(packed: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n \n     Explanation of the example:\n     ---------------------------\n-    Let's take the first value for example 0b10100001, we we will only focus on the first column,\n+    Let's take the first value for example 0b10100001, we will only focus on the first column,\n     because every element is unpacked across the first dimension\n     - First 2 bits: `01` → 0 at [0][0]\n     - Second 2 bits: `00` → -1 at [0][2]\n@@ -170,7 +170,7 @@ def activation_quant(self, input, num_bits=8):\n         Activation function : Performs symmetric, per-token quantization on the input activations.\n         Parameters:\n         -----------\n-        x : torch.Tensor\n+        input : torch.Tensor\n             Input activations to be quantized.\n         num_bits : int, optional (default=8)\n             Number of bits to use for quantization, determining the quantization range.\n@@ -362,7 +362,7 @@ def replace_with_bitnet_linear(model, modules_to_not_convert: list[str] | None =\n \n     if not has_been_replaced:\n         logger.warning(\n-            \"You are loading your model using eetq but no linear modules were found in your model.\"\n+            \"You are loading your model using bitnet but no linear modules were found in your model.\"\n             \" Please double check your model architecture, or submit an issue on github if you think this is\"\n             \" a bug.\"\n         )"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}