{
    "author": "manueldeprada",
    "message": "ðŸš¨ Remove Group Beam Search decoding strategy (#40495)\n\n* Squashed remove-constrastive-search\n\n* testing that tests pass using hub\n\n* fix\n\n* aaand remove tests after all green!!",
    "sha": "3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
    "files": [
        {
            "sha": "63b70899af4d431b29d28a04e4c9e478442f9946",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -225,28 +225,6 @@ outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=to\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n ```\n-### Diverse beam search\n-\n-[Diverse beam search](https://hf.co/papers/1610.02424) is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups.\n-\n-Enable diverse beam search with the `num_beams`, `num_beam_groups` and `diversity_penalty` parameters (the `num_beams` parameter should be divisible by `num_beam_groups`).\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n-\n-device = infer_device()\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n-# explicitly set to 100 because Llama2 generation length is 4096\n-outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-'Hugging Face is an open-source company ðŸ¤—\\nWe are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a'\n-```\n-\n \n ## Custom generation methods\n "
        },
        {
            "sha": "9deb926b905fa8dd7f56cc279ea29f8b21b95299",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -108,9 +108,6 @@ generation.\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] HammingDiversityLogitsProcessor\n-    - __call__\n-\n [[autodoc]] InfNanRemoveLogitsProcessor\n     - __call__\n \n@@ -219,10 +216,6 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - process\n     - finalize\n \n-[[autodoc]] BeamSearchScorer\n-    - process\n-    - finalize\n-\n [[autodoc]] ConstrainedBeamSearchScorer\n     - process\n     - finalize"
        },
        {
            "sha": "372816d2e050a86e34d9762691c987215ee15cad",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -146,7 +146,7 @@ tokenizer = AutoTokenizer.from_pretrained(ckpt)\n model = AutoModelForCausalLM.from_pretrained(ckpt, dtype=torch.float16, device_map=\"auto\")\n prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n-beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\n+beams = { \"num_beams\": 40, \"num_return_sequences\": 20, \"max_new_tokens\": 23, \"early_stopping\": True, }\n out = resilient_generate(model, **inputs, **beams)\n responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)\n ```"
        },
        {
            "sha": "45eec30c0765050e1769f581facdc8f236d8d63a",
            "filename": "docs/source/ja/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fgeneration_strategies.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -241,43 +241,6 @@ time.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the l\n 'Das Haus ist wunderbar.'\n ```\n \n-### Diverse beam search decoding\n-\n-å¤šæ§˜ãªãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒæˆ¦ç•¥ã®æ‹¡å¼µã§ã‚ã‚Šã€é¸æŠžè‚¢ã‹ã‚‰ã‚ˆã‚Šå¤šæ§˜ãªãƒ“ãƒ¼ãƒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã“ã®ä»•çµ„ã¿ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://huggingface.co/papers/1610.02424) ã‚’ã”å‚ç…§ãã ã•ã„ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¯ã€`num_beams`ã€`num_beam_groups`ã€ãŠã‚ˆã³ `diversity_penalty` ã¨ã„ã†3ã¤ã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚å¤šæ§˜æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ã¯ã€å‡ºåŠ›ãŒã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç•°ãªã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã¯å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n-\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-\n->>> checkpoint = \"google/pegasus-xsum\"\n->>> prompt = (\n-...     \"The Permaculture Design Principles are a set of universal design principles \"\n-...     \"that can be applied to any location, climate and culture, and they allow us to design \"\n-...     \"the most efficient and sustainable human habitation and food production systems. \"\n-...     \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n-...     \"as ecology, landscape design, environmental science and energy conservation, and the \"\n-...     \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n-...     \"design principle itself embodies a complete conceptual framework based on sound \"\n-...     \"scientific principles. When we bring all these separate  principles together, we can \"\n-...     \"create a design system that both looks at whole systems, the parts that these systems \"\n-...     \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n-...     \"living system. Each design principle serves as a tool that allows us to integrate all \"\n-...     \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n-...     \"whole system, where the elements harmoniously interact and work together in the most \"\n-...     \"efficient way possible.\"\n-... )\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-\n->>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'The Design Principles are a set of universal design principles that can be applied to any location, climate and\n-culture, and they allow us to design the'\n-```\n-\n ### Assisted Decoding\n \n ã‚¢ã‚·ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€ä¸Šè¨˜ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å¤‰æ›´ã—ãŸã‚‚ã®ã§ã€åŒã˜ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆç†æƒ³çš„ã«ã¯ã¯ã‚‹ã‹ã«å°ã•ãªãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ã„ãã¤ã‹ã®å€™è£œãƒˆãƒ¼ã‚¯ãƒ³ã‚’è²ªæ¬²ã«ç”Ÿæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãã®å¾Œã€ä¸»è¦ãªãƒ¢ãƒ‡ãƒ«ã¯å€™è£œãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ã®å‰å‘ããƒ‘ã‚¹ã§æ¤œè¨¼ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’é«˜é€ŸåŒ–ã—ã¾ã™ã€‚ç¾åœ¨ã€ã‚¢ã‚·ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã¯è²ªæ¬²æ¤œç´¢ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€ãƒãƒƒãƒå…¥åŠ›ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¢ã‚·ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹](https://huggingface.co/blog/assisted-generation) ã‚’ã”è¦§ãã ã•ã„ã€‚"
        },
        {
            "sha": "c01d86f54bc0bebf7e97bf8e6b1631e3c7811fec",
            "filename": "docs/source/ja/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -139,9 +139,6 @@ generation_output[:2]\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] HammingDiversityLogitsProcessor\n-    - __call__\n-\n [[autodoc]] InfNanRemoveLogitsProcessor\n     - __call__\n \n@@ -321,10 +318,6 @@ generation_output[:2]\n     - process\n     - finalize\n \n-[[autodoc]] BeamSearchScorer\n-    - process\n-    - finalize\n-\n [[autodoc]] ConstrainedBeamSearchScorer\n     - process\n     - finalize"
        },
        {
            "sha": "c59eff4111f3ac919918556e379f6be543256c3c",
            "filename": "docs/source/ko/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fko%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgeneration_strategies.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -232,44 +232,6 @@ time.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the l\n 'Das Haus ist wunderbar.'\n ```\n \n-### ë‹¤ì–‘í•œ ë¹” íƒìƒ‰ ë””ì½”ë”©(Diverse beam search decoding)[[diverse-beam-search-decoding]]\n-\n-ë‹¤ì–‘í•œ ë¹” íƒìƒ‰(Decoding) ì „ëžµì€ ì„ íƒí•  ìˆ˜ ìžˆëŠ” ë” ë‹¤ì–‘í•œ ë¹” ì‹œí€€ìŠ¤ ì§‘í•©ì„ ìƒì„±í•  ìˆ˜ ìžˆê²Œ í•´ì£¼ëŠ” ë¹” íƒìƒ‰ ì „ëžµì˜ í™•ìž¥ìž…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì–´ë–»ê²Œ ìž‘ë™í•˜ëŠ”ì§€ ì•Œì•„ë³´ë ¤ë©´, [ë‹¤ì–‘í•œ ë¹” íƒìƒ‰: ì‹ ê²½ ì‹œí€€ìŠ¤ ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ ë””ì½”ë”©í•˜ê¸°](https://huggingface.co/papers/1610.02424)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì„¸ ê°€ì§€ ì£¼ìš” ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìžˆìŠµë‹ˆë‹¤: `num_beams`, `num_beam_groups`, ê·¸ë¦¬ê³  `diversity_penalty`. ë‹¤ì–‘ì„± íŒ¨ë„í‹°ëŠ” ê·¸ë£¹ ê°„ì— ì¶œë ¥ì´ ì„œë¡œ ë‹¤ë¥´ê²Œ í•˜ê¸° ìœ„í•œ ê²ƒì´ë©°, ê° ê·¸ë£¹ ë‚´ì—ì„œ ë¹” íƒìƒ‰ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-\n->>> checkpoint = \"google/pegasus-xsum\"\n->>> prompt = (\n-...     \"The Permaculture Design Principles are a set of universal design principles \"\n-...     \"that can be applied to any location, climate and culture, and they allow us to design \"\n-...     \"the most efficient and sustainable human habitation and food production systems. \"\n-...     \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n-...     \"as ecology, landscape design, environmental science and energy conservation, and the \"\n-...     \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n-...     \"design principle itself embodies a complete conceptual framework based on sound \"\n-...     \"scientific principles. When we bring all these separate  principles together, we can \"\n-...     \"create a design system that both looks at whole systems, the parts that these systems \"\n-...     \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n-...     \"living system. Each design principle serves as a tool that allows us to integrate all \"\n-...     \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n-...     \"whole system, where the elements harmoniously interact and work together in the most \"\n-...     \"efficient way possible.\"\n-... )\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n->>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-\n->>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'The Design Principles are a set of universal design principles that can be applied to any location, climate and\n-culture, and they allow us to design the'\n-```\n-\n-ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëžµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. [`generate`] ë©”ì„œë“œì— ëŒ€í•œ ê³ ê¸‰ ë§¤ê°œë³€ìˆ˜ê°€ ì¡´ìž¬í•˜ë¯€ë¡œ [`generate`] ë©”ì„œë“œì˜ ë™ìž‘ì„ ë”ìš± ì„¸ë¶€ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ì „ì²´ ëª©ë¡ì€ [API ë¬¸ì„œ](./main_classes/text_generation)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n ### ì¶”ë¡  ë””ì½”ë”©(Speculative Decoding)[[speculative-decoding]]\n \n ì¶”ë¡  ë””ì½”ë”©(ë³´ì¡° ë””ì½”ë”©(assisted decoding)ìœ¼ë¡œë„ ì•Œë ¤ì§)ì€ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” í›¨ì”¬ ìž‘ì€ ë³´ì¡° ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ëª‡ ê°€ì§€ í›„ë³´ í† í°ì„ ìƒì„±í•˜ëŠ” ìƒìœ„ ëª¨ë¸ì˜ ë””ì½”ë”© ì „ëžµì„ ìˆ˜ì •í•œ ê²ƒìž…ë‹ˆë‹¤. ì£¼ ëª¨ë¸ì€ ë‹¨ì¼ ì „ë°© í†µê³¼ë¡œ í›„ë³´ í† í°ì„ ê²€ì¦í•¨ìœ¼ë¡œì¨ ë””ì½”ë”© ê³¼ì •ì„ ê°€ì†í™”í•©ë‹ˆë‹¤. `do_sample=True`ì¼ ê²½ìš°, [ì¶”ë¡  ë””ì½”ë”© ë…¼ë¬¸](https://huggingface.co/papers/2211.17192)ì— ì†Œê°œëœ í† í° ê²€ì¦ê³¼ ìž¬ìƒ˜í”Œë§ ë°©ì‹ì´ ì‚¬ìš©ë©ë‹ˆë‹¤."
        },
        {
            "sha": "9bd669e34d2b69d39c5ba2fb330ab8017c0e07f5",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -131,9 +131,6 @@ generation_output[:2]\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] HammingDiversityLogitsProcessor\n-    - __call__\n-\n [[autodoc]] InfNanRemoveLogitsProcessor\n     - __call__\n \n@@ -326,10 +323,6 @@ generation_output[:2]\n     - process\n     - finalize\n \n-[[autodoc]] BeamSearchScorer\n-    - process\n-    - finalize\n-\n [[autodoc]] ConstrainedBeamSearchScorer\n     - process\n     - finalize"
        },
        {
            "sha": "b33ac4be9c92b2ac74381f544bf7dfcae3f88789",
            "filename": "docs/source/zh/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -133,9 +133,6 @@ generation_output[:2]\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] HammingDiversityLogitsProcessor\n-    - __call__\n-\n [[autodoc]] InfNanRemoveLogitsProcessor\n     - __call__\n \n@@ -316,10 +313,6 @@ generation_output[:2]\n     - process\n     - finalize\n \n-[[autodoc]] BeamSearchScorer\n-    - process\n-    - finalize\n-\n [[autodoc]] ConstrainedBeamSearchScorer\n     - process\n     - finalize"
        },
        {
            "sha": "14b8967278efaed72b4a59545770e12d4cdf64bf",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -411,7 +411,6 @@\n             \"BayesianDetectorConfig\",\n             \"BayesianDetectorModel\",\n             \"BeamScorer\",\n-            \"BeamSearchScorer\",\n             \"ClassifierFreeGuidanceLogitsProcessor\",\n             \"ConstrainedBeamSearchScorer\",\n             \"Constraint\",\n@@ -426,7 +425,6 @@\n             \"ForcedBOSTokenLogitsProcessor\",\n             \"ForcedEOSTokenLogitsProcessor\",\n             \"GenerationMixin\",\n-            \"HammingDiversityLogitsProcessor\",\n             \"InfNanRemoveLogitsProcessor\",\n             \"LogitNormalization\",\n             \"LogitsProcessor\",\n@@ -656,7 +654,6 @@\n     from .generation import BayesianDetectorConfig as BayesianDetectorConfig\n     from .generation import BayesianDetectorModel as BayesianDetectorModel\n     from .generation import BeamScorer as BeamScorer\n-    from .generation import BeamSearchScorer as BeamSearchScorer\n     from .generation import ClassifierFreeGuidanceLogitsProcessor as ClassifierFreeGuidanceLogitsProcessor\n     from .generation import CompileConfig as CompileConfig\n     from .generation import ConstrainedBeamSearchScorer as ConstrainedBeamSearchScorer\n@@ -687,7 +684,6 @@\n     from .generation import ForcedEOSTokenLogitsProcessor as ForcedEOSTokenLogitsProcessor\n     from .generation import GenerationConfig as GenerationConfig\n     from .generation import GenerationMixin as GenerationMixin\n-    from .generation import HammingDiversityLogitsProcessor as HammingDiversityLogitsProcessor\n     from .generation import InfNanRemoveLogitsProcessor as InfNanRemoveLogitsProcessor\n     from .generation import LogitNormalization as LogitNormalization\n     from .generation import LogitsProcessor as LogitsProcessor"
        },
        {
            "sha": "d450193dbc2d09940eaea672ce4f1803762bd476",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1121,8 +1121,6 @@ def _get_global_generation_defaults() -> dict[str, Any]:\n             \"do_sample\": False,\n             \"early_stopping\": False,\n             \"num_beams\": 1,\n-            \"num_beam_groups\": 1,\n-            \"diversity_penalty\": 0.0,\n             \"temperature\": 1.0,\n             \"top_k\": 50,\n             \"top_p\": 1.0,\n@@ -1141,6 +1139,9 @@ def _get_global_generation_defaults() -> dict[str, Any]:\n             \"exponential_decay_length_penalty\": None,\n             \"suppress_tokens\": None,\n             \"begin_suppress_tokens\": None,\n+            # Deprecated arguments (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n+            \"num_beam_groups\": 1,\n+            \"diversity_penalty\": 0.0,\n         }\n \n     def _get_non_default_generation_parameters(self) -> dict[str, Any]:"
        },
        {
            "sha": "4fb3d32213f83cfadd6a6d169e494dc34eccec60",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -44,7 +44,6 @@\n     _import_structure[\"beam_search\"] = [\n         \"BeamHypotheses\",\n         \"BeamScorer\",\n-        \"BeamSearchScorer\",\n         \"ConstrainedBeamSearchScorer\",\n     ]\n     _import_structure[\"candidate_generator\"] = [\n@@ -63,7 +62,6 @@\n         \"ExponentialDecayLengthPenalty\",\n         \"ForcedBOSTokenLogitsProcessor\",\n         \"ForcedEOSTokenLogitsProcessor\",\n-        \"HammingDiversityLogitsProcessor\",\n         \"InfNanRemoveLogitsProcessor\",\n         \"LogitNormalization\",\n         \"LogitsProcessor\",\n@@ -209,7 +207,7 @@\n         pass\n     else:\n         from .beam_constraints import Constraint, ConstraintListState, DisjunctiveConstraint, PhrasalConstraint\n-        from .beam_search import BeamHypotheses, BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n+        from .beam_search import BeamHypotheses, BeamScorer, ConstrainedBeamSearchScorer\n         from .candidate_generator import (\n             AssistedCandidateGenerator,\n             CandidateGenerator,\n@@ -227,7 +225,6 @@\n             ExponentialDecayLengthPenalty,\n             ForcedBOSTokenLogitsProcessor,\n             ForcedEOSTokenLogitsProcessor,\n-            HammingDiversityLogitsProcessor,\n             InfNanRemoveLogitsProcessor,\n             LogitNormalization,\n             LogitsProcessor,"
        },
        {
            "sha": "08af5755e3d769e7362d49ec757b2a5b359b7148",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 8,
            "deletions": 329,
            "changes": 337,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -45,8 +45,6 @@\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n         beam_indices (`torch.LongTensor`, *optional*):\n             Beam indices indicating to which beam hypothesis each token correspond.\n-        group_index (`int`, *optional*):\n-            The index of the group of beams. Used with [`~PreTrainedModel.group_beam_search`].\n \n     Return:\n         `UserDict`: A dictionary composed of the fields as defined above:\n@@ -120,302 +118,6 @@ def finalize(\n         raise NotImplementedError(\"This is an abstract method.\")\n \n \n-class BeamSearchScorer(BeamScorer):\n-    r\"\"\"\n-    [`BeamScorer`] implementing standard beam search decoding.\n-\n-    Adapted in part from [Facebook's XLM beam search\n-    code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n-\n-    Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan's DBS\n-    implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)\n-\n-    Args:\n-        batch_size (`int`):\n-            Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n-        num_beams (`int`):\n-            Number of beams for beam search.\n-        device (`torch.device`):\n-            Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n-            allocated.\n-        length_penalty (`float`, *optional*, defaults to 1.0):\n-            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n-            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n-            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n-            `length_penalty` < 0.0 encourages shorter sequences.\n-        do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n-            Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n-            `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n-            heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n-            `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n-            beam search algorithm).\n-        num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n-            The number of beam hypotheses that shall be returned upon calling\n-            [`~transformers.BeamSearchScorer.finalize`].\n-        num_beam_groups (`int`, *optional*, defaults to 1):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n-            See [this paper](https://huggingface.co/papers/1610.02424) for more details.\n-        max_length (`int`, *optional*):\n-            The maximum length of the sequence to be generated.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        batch_size: int,\n-        num_beams: int,\n-        device: torch.device,\n-        length_penalty: Optional[float] = 1.0,\n-        do_early_stopping: Optional[Union[bool, str]] = False,\n-        num_beam_hyps_to_keep: Optional[int] = 1,\n-        num_beam_groups: Optional[int] = 1,\n-        max_length: Optional[int] = None,\n-    ):\n-        self.num_beams = num_beams\n-        self.device = device\n-        self.length_penalty = length_penalty\n-        self.do_early_stopping = do_early_stopping\n-        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n-        self.num_beam_groups = num_beam_groups\n-        self.group_size = self.num_beams // self.num_beam_groups\n-\n-        self._is_init = False\n-        # self._beam_hyps[i*self.num_beam_groups+j] is the beam_hyps of the j-th group in the i-th mini-batch.\n-        # If group_beam_search is not used, the list consists of `batch_size` beam_hyps.\n-        self._beam_hyps = [\n-            BeamHypotheses(\n-                num_beams=self.group_size,\n-                length_penalty=self.length_penalty,\n-                early_stopping=self.do_early_stopping,\n-                max_length=max_length,\n-            )\n-            for _ in range(batch_size * self.num_beam_groups)\n-        ]\n-        # self._done[i*self.num_beam_groups+j] indicates whether the generation of the beam_hyps of the j-th group\n-        # in the i-th mini-batch is complete.\n-        self._done = torch.tensor(\n-            [False for _ in range(batch_size * self.num_beam_groups)], dtype=torch.bool, device=self.device\n-        )\n-\n-        if not isinstance(num_beams, int) or num_beams <= 1:\n-            raise ValueError(\n-                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n-                \" one should make use of `greedy_search` instead.\"\n-            )\n-\n-        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n-            raise ValueError(\n-                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n-                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n-            )\n-\n-    @property\n-    def is_done(self) -> bool:\n-        return self._done.all()\n-\n-    def process(\n-        self,\n-        input_ids: torch.LongTensor,\n-        next_scores: torch.FloatTensor,\n-        next_tokens: torch.LongTensor,\n-        next_indices: torch.LongTensor,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        group_index: Optional[int] = 0,\n-        decoder_prompt_len: Optional[int] = 0,\n-    ) -> dict[str, torch.Tensor]:\n-        # add up to the length which the next_scores is calculated on (including decoder prompt)\n-        cur_len = input_ids.shape[-1] + 1\n-        batch_size = len(self._beam_hyps) // self.num_beam_groups\n-\n-        if batch_size != (input_ids.shape[0] // self.group_size):\n-            if self.num_beam_groups > 1:\n-                raise ValueError(\n-                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n-                    f\"size of {self.group_size} is expected by the beam scorer.\"\n-                )\n-            else:\n-                raise ValueError(\n-                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n-                    f\"{self.group_size} is expected by the beam scorer.\"\n-                )\n-\n-        device = input_ids.device\n-        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n-        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n-        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        for batch_idx in range(batch_size):\n-            batch_group_idx = batch_idx * self.num_beam_groups + group_index\n-            if self._done[batch_group_idx]:\n-                if self.num_beams < len(self._beam_hyps[batch_group_idx]):\n-                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n-                if eos_token_id is None or pad_token_id is None:\n-                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n-                # pad the batch\n-                next_beam_scores[batch_idx, :] = 0\n-                next_beam_tokens[batch_idx, :] = pad_token_id\n-                next_beam_indices[batch_idx, :] = 0\n-                continue\n-\n-            # next tokens for this sentence\n-            beam_idx = 0\n-            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n-                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n-            ):\n-                batch_beam_idx = batch_idx * self.group_size + next_index\n-                # add to generated hypotheses if end of sentence\n-                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n-                    # if beam_token does not belong to top num_beams tokens, it should not be added\n-                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n-                    if is_beam_token_worse_than_top_num_beams:\n-                        continue\n-                    if beam_indices is not None:\n-                        beam_index = beam_indices[batch_beam_idx]\n-                        beam_index = beam_index + (batch_beam_idx,)\n-                    else:\n-                        beam_index = None\n-\n-                    self._beam_hyps[batch_group_idx].add(\n-                        input_ids[batch_beam_idx].clone(),\n-                        next_score.item(),\n-                        beam_indices=beam_index,\n-                        generated_len=cur_len - decoder_prompt_len,\n-                    )\n-                else:\n-                    # add next predicted token since it is not eos_token\n-                    next_beam_scores[batch_idx, beam_idx] = next_score\n-                    next_beam_tokens[batch_idx, beam_idx] = next_token\n-                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n-                    beam_idx += 1\n-\n-                # once the beam for next step is full, don't add more tokens to it.\n-                if beam_idx == self.group_size:\n-                    break\n-\n-            if beam_idx < self.group_size:\n-                raise ValueError(\n-                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n-                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n-                )\n-\n-            # Check if we are done so that we can save a pad step if all(done)\n-            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(\n-                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n-            )\n-\n-        return UserDict(\n-            {\n-                \"next_beam_scores\": next_beam_scores.view(-1),\n-                \"next_beam_tokens\": next_beam_tokens.view(-1),\n-                \"next_beam_indices\": next_beam_indices.view(-1),\n-            }\n-        )\n-\n-    def finalize(\n-        self,\n-        input_ids: torch.LongTensor,\n-        final_beam_scores: torch.FloatTensor,\n-        final_beam_tokens: torch.LongTensor,\n-        final_beam_indices: torch.LongTensor,\n-        max_length: int,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: Optional[int] = 0,\n-    ) -> tuple[torch.LongTensor]:\n-        batch_size = len(self._beam_hyps) // self.num_beam_groups\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        # finalize all open beam hypotheses and add to generated hypotheses\n-        for batch_group_idx, beam_hyp in enumerate(self._beam_hyps):\n-            if self._done[batch_group_idx]:\n-                continue\n-\n-            # all open beam hypotheses are added to the beam hypothesis\n-            # beam hypothesis class automatically keeps the best beams\n-            for index_per_group in range(self.group_size):\n-                batch_beam_idx = batch_group_idx * self.group_size + index_per_group\n-                final_score = final_beam_scores[batch_beam_idx].item()\n-                final_tokens = input_ids[batch_beam_idx]\n-                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n-                generated_len = final_tokens.shape[-1] - decoder_prompt_len\n-                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n-\n-        # select the best hypotheses\n-        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n-        best = []\n-        best_indices = []\n-        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n-\n-        # retrieve best hypotheses\n-        for i in range(batch_size):\n-            beam_hyps_in_batch = self._beam_hyps[i * self.num_beam_groups : (i + 1) * self.num_beam_groups]\n-            candidate_beams = [beam for beam_hyp in beam_hyps_in_batch for beam in beam_hyp.beams]\n-            sorted_hyps = sorted(candidate_beams, key=lambda x: x[0])\n-            for j in range(self.num_beam_hyps_to_keep):\n-                best_hyp_tuple = sorted_hyps.pop()\n-                best_score = best_hyp_tuple[0]\n-                best_hyp = best_hyp_tuple[1]\n-                best_index = best_hyp_tuple[2]\n-                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n-\n-                # append hyp to lists\n-                best.append(best_hyp)\n-\n-                # append indices to list\n-                best_indices.append(best_index)\n-\n-                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n-\n-        # prepare for adding eos\n-        sent_lengths_max = sent_lengths.max().item() + 1\n-        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n-        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-\n-        if len(best_indices) > 0 and best_indices[0] is not None:\n-            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-        else:\n-            indices = None\n-\n-        # shorter batches are padded if needed\n-        if sent_lengths.min().item() != sent_lengths.max().item():\n-            if pad_token_id is None:\n-                raise ValueError(\"`pad_token_id` has to be defined\")\n-            decoded.fill_(pad_token_id)\n-\n-        if indices is not None:\n-            indices.fill_(-1)\n-\n-        # fill with hypotheses and eos_token_id if the latter fits in\n-        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n-            decoded[i, : sent_lengths[i]] = hypo\n-\n-            if indices is not None:\n-                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n-\n-            if sent_lengths[i] < sent_max_len:\n-                # inserting only the first eos_token_id\n-                decoded[i, sent_lengths[i]] = eos_token_id[0]\n-\n-        return UserDict(\n-            {\n-                \"sequences\": decoded,\n-                \"sequence_scores\": best_scores,\n-                \"beam_indices\": indices,\n-            }\n-        )\n-\n-\n class ConstrainedBeamSearchScorer(BeamScorer):\n     r\"\"\"\n     [`BeamScorer`] implementing constrained beam search decoding.\n@@ -446,9 +148,6 @@ class ConstrainedBeamSearchScorer(BeamScorer):\n         num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n             The number of beam hypotheses that shall be returned upon calling\n             [`~transformers.BeamSearchScorer.finalize`].\n-        num_beam_groups (`int`, *optional*, defaults to 1):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n-            See [this paper](https://huggingface.co/papers/1610.02424) for more details.\n         max_length (`int`, *optional*):\n             The maximum length of the sequence to be generated.\n     \"\"\"\n@@ -462,16 +161,13 @@ def __init__(\n         length_penalty: Optional[float] = 1.0,\n         do_early_stopping: Optional[Union[bool, str]] = False,\n         num_beam_hyps_to_keep: Optional[int] = 1,\n-        num_beam_groups: Optional[int] = 1,\n         max_length: Optional[int] = None,\n     ):\n         self.num_beams = num_beams\n         self.device = device\n         self.length_penalty = length_penalty\n         self.do_early_stopping = do_early_stopping\n         self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n-        self.num_beam_groups = num_beam_groups\n-        self.group_size = self.num_beams // self.num_beam_groups\n         self.constraints = constraints\n \n         self._is_init = False\n@@ -492,12 +188,6 @@ def __init__(\n                 \" one should make use of `greedy_search` instead.\"\n             )\n \n-        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n-            raise ValueError(\n-                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n-                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n-            )\n-\n     @property\n     def is_done(self) -> bool:\n         return self._done.all()\n@@ -564,23 +254,12 @@ def process(\n         # add up to the length which the next_scores is calculated on (including decoder prompt)\n         cur_len = input_ids.shape[-1] + 1\n         batch_size = len(self._beam_hyps)\n-        if batch_size != (input_ids.shape[0] // self.group_size):\n-            if self.num_beam_groups > 1:\n-                raise ValueError(\n-                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n-                    f\"size of {self.group_size} is expected by the beam scorer.\"\n-                )\n-            else:\n-                raise ValueError(\n-                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n-                    f\"{self.group_size} is expected by the beam scorer.\"\n-                )\n \n         device = input_ids.device\n \n-        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n-        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n-        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n+        next_beam_scores = torch.zeros((batch_size, self.num_beams), dtype=next_scores.dtype, device=device)\n+        next_beam_tokens = torch.zeros((batch_size, self.num_beams), dtype=next_tokens.dtype, device=device)\n+        next_beam_indices = torch.zeros((batch_size, self.num_beams), dtype=next_indices.dtype, device=device)\n \n         if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n             if isinstance(eos_token_id, int):\n@@ -604,11 +283,11 @@ def process(\n             for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n                 zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n             ):\n-                batch_beam_idx = batch_idx * self.group_size + next_index\n+                batch_beam_idx = batch_idx * self.num_beams + next_index\n                 # add to generated hypotheses if end of sentence\n                 if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n                     # if beam_token does not belong to top num_beams tokens, it should not be added\n-                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n+                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.num_beams\n                     if is_beam_token_worse_than_top_num_beams:\n                         continue\n \n@@ -634,7 +313,7 @@ def process(\n                     beam_idx += 1\n \n                 # once the beam for next step is full, don't add more tokens to it.\n-                if beam_idx == self.group_size:\n+                if beam_idx == self.num_beams:\n                     break\n \n             new_scores, new_tokens, new_indices = self.step_sentence_constraint(\n@@ -650,9 +329,9 @@ def process(\n             next_beam_tokens[batch_idx] = new_tokens\n             next_beam_indices[batch_idx] = new_indices\n \n-            if beam_idx < self.group_size:\n+            if beam_idx < self.num_beams:\n                 raise ValueError(\n-                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n+                    f\"At most {self.num_beams} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n                     f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n                 )\n "
        },
        {
            "sha": "177fa806485734e28d3cc47565dda255ae1b293e",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 38,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -89,7 +89,6 @@ class GenerationConfig(PushToHubMixin):\n         - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n         - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n         - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n-        - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n         - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n         - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n \n@@ -134,9 +133,6 @@ class GenerationConfig(PushToHubMixin):\n             Whether or not to use sampling ; use greedy decoding otherwise.\n         num_beams (`int`, *optional*, defaults to 1):\n             Number of beams for beam search. 1 means no beam search.\n-        num_beam_groups (`int`, *optional*, defaults to 1):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n-            [this paper](https://huggingface.co/papers/1610.02424) for more details.\n \n         > Parameters that control the cache\n \n@@ -190,9 +186,6 @@ class GenerationConfig(PushToHubMixin):\n             probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3,\n             depending on the size of the model. See [Truncation Sampling as Language Model\n             Desmoothing](https://huggingface.co/papers/2210.15191) for more details.\n-        diversity_penalty (`float`, *optional*, defaults to 0.0):\n-            This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n-            particular time. Note that `diversity_penalty` is only effective if `group beam search` is enabled.\n         repetition_penalty (`float`, *optional*, defaults to 1.0):\n             The parameter for repetition penalty. 1.0 means no penalty. See [this\n             paper](https://huggingface.co/papers/1909.05858) for more details.\n@@ -359,7 +352,6 @@ def __init__(self, **kwargs):\n         # Parameters that control the generation strategy used\n         self.do_sample = kwargs.pop(\"do_sample\", False)\n         self.num_beams = kwargs.pop(\"num_beams\", 1)\n-        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n \n         # Parameters that control the cache\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n@@ -377,7 +369,6 @@ def __init__(self, **kwargs):\n         self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n         self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", 0.0)\n         self.eta_cutoff = kwargs.pop(\"eta_cutoff\", 0.0)\n-        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n         self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n         self.encoder_repetition_penalty = kwargs.pop(\"encoder_repetition_penalty\", 1.0)\n         self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n@@ -441,6 +432,8 @@ def __init__(self, **kwargs):\n         self.low_memory = kwargs.pop(\"low_memory\", None)\n         self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n         self.dola_layers = kwargs.pop(\"dola_layers\", None)\n+        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n+        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n \n         # The remaining attributes do not parametrize `.generate()`, but are informative and/or used by the hub\n         # interface.\n@@ -628,14 +621,6 @@ def validate(self, strict=False):\n                 minor_issues[\"early_stopping\"] = single_beam_wrong_parameter_msg.format(\n                     flag_name=\"early_stopping\", flag_value=self.early_stopping\n                 )\n-            if self.num_beam_groups is not None and self.num_beam_groups != 1:\n-                minor_issues[\"num_beam_groups\"] = single_beam_wrong_parameter_msg.format(\n-                    flag_name=\"num_beam_groups\", flag_value=self.num_beam_groups\n-                )\n-            if self.diversity_penalty is not None and self.diversity_penalty != 0.0:\n-                minor_issues[\"diversity_penalty\"] = single_beam_wrong_parameter_msg.format(\n-                    flag_name=\"diversity_penalty\", flag_value=self.diversity_penalty\n-                )\n             if self.length_penalty is not None and self.length_penalty != 1.0:\n                 minor_issues[\"length_penalty\"] = single_beam_wrong_parameter_msg.format(\n                     flag_name=\"length_penalty\", flag_value=self.length_penalty\n@@ -658,27 +643,6 @@ def validate(self, strict=False):\n                     raise ValueError(\n                         constrained_wrong_parameter_msg.format(flag_name=\"do_sample\", flag_value=self.do_sample)\n                     )\n-                if self.num_beam_groups is not None and self.num_beam_groups != 1:\n-                    raise ValueError(\n-                        constrained_wrong_parameter_msg.format(\n-                            flag_name=\"num_beam_groups\", flag_value=self.num_beam_groups\n-                        )\n-                    )\n-            # group beam search\n-            elif self.diversity_penalty != 0.0 or self.num_beam_groups != 1:\n-                group_error_prefix = (\n-                    \"`diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. In \"\n-                    \"this generation mode, \"\n-                )\n-                if self.do_sample is True:\n-                    raise ValueError(group_error_prefix + \"`do_sample` must be set to `False`\")\n-                if self.num_beams % self.num_beam_groups != 0:\n-                    raise ValueError(group_error_prefix + \"`num_beams` should be divisible by `num_beam_groups`\")\n-                if self.diversity_penalty == 0.0:\n-                    raise ValueError(\n-                        group_error_prefix\n-                        + \"`diversity_penalty` should be greater than `0.0`, otherwise your groups will be identical.\"\n-                    )\n \n         # 2.4. check `num_return_sequences`\n         if self.num_return_sequences != 1:"
        },
        {
            "sha": "abc08ef2eb5cdc3bd13a18626964040383f1f411",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1441,142 +1441,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         return scores_processed\n \n \n-class HammingDiversityLogitsProcessor(LogitsProcessor):\n-    r\"\"\"\n-    [`LogitsProcessor`] that enforces diverse beam search.\n-\n-    Note that this logits processor is only effective for [`PreTrainedModel.group_beam_search`]. See [Diverse Beam\n-    Search: Decoding Diverse Solutions from Neural Sequence Models](https://huggingface.co/papers/1610.02424) for more\n-    details.\n-\n-    Traditional beam search often generates very similar sequences across different beams.\n-    `HammingDiversityLogitsProcessor` addresses this by penalizing beams that generate tokens already chosen by other\n-    beams in the same time step.\n-\n-    Args:\n-        diversity_penalty (`float`):\n-            This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n-            particular time. A higher `diversity_penalty` will enforce greater diversity among the beams. Adjusting\n-            this value can help strike a balance between diversity and natural likelihood.\n-        num_beams (`int`):\n-            Number of beams for beam search. 1 means no beam search.\n-        num_beam_groups (`int`):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n-            [this paper](https://huggingface.co/papers/1610.02424) for more details.\n-\n-    Examples:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-    >>> import torch\n-\n-    >>> # Initialize the model and tokenizer\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n-    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n-\n-    >>> # A long text about the solar system\n-    >>> text = (\n-    ...     \"The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, \"\n-    ...     \"either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight \"\n-    ...     \"planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System \"\n-    ...     \"bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant \"\n-    ...     \"interstellar molecular cloud.\"\n-    ... )\n-    >>> inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\")\n-\n-    >>> # Generate diverse summary\n-    >>> outputs_diverse = model.generate(\n-    ...     **inputs,\n-    ...     num_beam_groups=2,\n-    ...     diversity_penalty=10.0,\n-    ...     max_length=100,\n-    ...     num_beams=4,\n-    ...     num_return_sequences=2,\n-    ... )\n-    >>> summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)\n-\n-    >>> # Generate non-diverse summary\n-    >>> outputs_non_diverse = model.generate(\n-    ...     **inputs,\n-    ...     max_length=100,\n-    ...     num_beams=4,\n-    ...     num_return_sequences=2,\n-    ... )\n-    >>> summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)\n-\n-    >>> # With `diversity_penalty`, the resulting beams are much more diverse\n-    >>> print(summary_non_diverse)\n-    ['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n-    'the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.']\n-\n-    >>> print(summaries_diverse)\n-    ['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n-    'the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets. the rest of the objects are smaller objects, such as the five dwarf planets and small solar system bodies.']\n-    ```\n-    \"\"\"\n-\n-    def __init__(self, diversity_penalty: float, num_beams: int, num_beam_groups: int):\n-        if not isinstance(diversity_penalty, float) or (not diversity_penalty > 0.0):\n-            raise ValueError(\"`diversity_penalty` should be a float strictly larger than 0.\")\n-        self._diversity_penalty = diversity_penalty\n-        if not isinstance(num_beams, int) or num_beams < 2:\n-            raise ValueError(\"`num_beams` should be an integer strictly larger than 1.\")\n-        self._num_beams = num_beams\n-        if not isinstance(num_beam_groups, int) or num_beam_groups < 2:\n-            raise ValueError(\"`num_beam_groups` should be an integer strictly larger than 1.\")\n-        if num_beam_groups > num_beams:\n-            raise ValueError(\"`beam_groups` has to be smaller or equal to `num_beams`.\")\n-        self._num_sub_beams = num_beams // num_beam_groups\n-\n-    def __call__(\n-        self,\n-        input_ids: torch.LongTensor,\n-        scores: torch.FloatTensor,\n-        current_tokens: torch.LongTensor,\n-        beam_group_idx: int,\n-    ) -> torch.FloatTensor:\n-        r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n-            scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n-                Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n-                beam search or log softmax for each vocabulary token when using beam search\n-            current_tokens (`torch.LongTensor` of shape `(batch_size)`):\n-                Indices of input sequence tokens in the vocabulary, corresponding to the tokens selected by the other\n-                beam groups in the current generation step.\n-            beam_group_idx (`int`):\n-                The index of the beam group currently being processed.\n-\n-        Return:\n-            `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`:\n-                The processed prediction scores.\n-        \"\"\"\n-        # hamming diversity: penalise using same token in current group which was used in previous groups at\n-        # the same time step\n-        batch_size = current_tokens.shape[0] // self._num_beams\n-        group_start_idx = beam_group_idx * self._num_sub_beams\n-        group_end_idx = min(group_start_idx + self._num_sub_beams, self._num_beams)\n-        group_size = group_end_idx - group_start_idx\n-        vocab_size = scores.shape[-1]\n-\n-        if group_start_idx == 0:\n-            return scores\n-\n-        scores_processed = scores.clone()\n-        for batch_idx in range(batch_size):\n-            # predicted tokens of last time step of previous groups\n-            previous_group_tokens = current_tokens[\n-                batch_idx * self._num_beams : batch_idx * self._num_beams + group_start_idx\n-            ]\n-            token_frequency = torch.bincount(previous_group_tokens, minlength=vocab_size).to(scores.device)\n-            scores_processed[batch_idx * group_size : (batch_idx + 1) * group_size] -= (\n-                self._diversity_penalty * token_frequency\n-            )\n-\n-        return scores_processed\n-\n-\n class ForcedBOSTokenLogitsProcessor(LogitsProcessor):\n     r\"\"\"\n     [`LogitsProcessor`] that enforces the specified token as the first generated token. Used with encoder-decoder"
        },
        {
            "sha": "f5cbbbe91eae785a50c02d9c28e805e7fd0b0350",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 327,
            "changes": 343,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -54,7 +54,7 @@\n     logging,\n )\n from .beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n-from .beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n+from .beam_search import ConstrainedBeamSearchScorer\n from .candidate_generator import (\n     AssistantVocabTranslatorCache,\n     AssistedCandidateGenerator,\n@@ -82,7 +82,6 @@\n     ExponentialDecayLengthPenalty,\n     ForcedBOSTokenLogitsProcessor,\n     ForcedEOSTokenLogitsProcessor,\n-    HammingDiversityLogitsProcessor,\n     InfNanRemoveLogitsProcessor,\n     LogitNormalization,\n     LogitsProcessorList,\n@@ -371,7 +370,6 @@ class GenerationMixin(ContinuousMixin):\n         - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n         - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n         - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n-        - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n         - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n         - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n \n@@ -1114,14 +1112,6 @@ def _get_logits_processor(\n         if generation_config.sequence_bias is not None:\n             processors.append(SequenceBiasLogitsProcessor(sequence_bias=generation_config.sequence_bias))\n \n-        if generation_config.diversity_penalty is not None and generation_config.diversity_penalty > 0.0:\n-            processors.append(\n-                HammingDiversityLogitsProcessor(\n-                    diversity_penalty=generation_config.diversity_penalty,\n-                    num_beams=generation_config.num_beams,\n-                    num_beam_groups=generation_config.num_beam_groups,\n-                )\n-            )\n         if (\n             generation_config.encoder_repetition_penalty is not None\n             and generation_config.encoder_repetition_penalty != 1.0\n@@ -1196,7 +1186,7 @@ def _get_logits_processor(\n             processors.append(\n                 PrefixConstrainedLogitsProcessor(\n                     prefix_allowed_tokens_fn,\n-                    generation_config.num_beams // generation_config.num_beam_groups,\n+                    generation_config.num_beams,\n                 )\n             )\n         if generation_config.forced_bos_token_id is not None:\n@@ -2559,28 +2549,22 @@ def generate(\n \n         elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n             logger.warning_once(\n-                \"Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \"\n-                \"To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\"\n-            )\n-            # 11. prepare beam search scorer\n-            beam_scorer = BeamSearchScorer(\n-                batch_size=batch_size,\n-                num_beams=generation_config.num_beams,\n-                device=inputs_tensor.device,\n-                length_penalty=generation_config.length_penalty,\n-                do_early_stopping=generation_config.early_stopping,\n-                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n-                num_beam_groups=generation_config.num_beam_groups,\n-                max_length=generation_config.max_length,\n+                \"Group Beam Search was moved to a `custom_generate` repo: https://hf.co/transformers-community/group-beam-search. \"\n+                \"To prevent loss of backward compatibility, add `custom_generate='transformers-community/group-beam-search'` \"\n+                \"to your `generate` call before v4.62.0.\"\n             )\n-            result = self._group_beam_search(\n-                input_ids,\n-                beam_scorer,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n+            if not trust_remote_code:\n+                raise ValueError(\n+                    \"Group Beam Search requires `trust_remote_code=True` in your `generate` call, since \"\n+                    \"it loads https://hf.co/transformers-community/group-beam-search.\"\n+                )\n+            return GenerationMixin.generate(\n+                self,\n+                inputs,\n+                custom_generate=\"transformers-community/group-beam-search\",\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                **model_kwargs,\n+                trust_remote_code=trust_remote_code,\n+                **kwargs,\n             )\n \n         elif generation_mode == GenerationMode.CONSTRAINED_BEAM_SEARCH:\n@@ -3527,301 +3511,6 @@ def _beam_search(\n         else:\n             return sequences\n \n-    def _group_beam_search(\n-        self,\n-        input_ids: torch.LongTensor,\n-        beam_scorer: BeamScorer,\n-        logits_processor: LogitsProcessorList,\n-        stopping_criteria: StoppingCriteriaList,\n-        generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        **model_kwargs,\n-    ):\n-        r\"\"\"\n-        Generates sequences of token ids for models with a language modeling head using **diverse beam search\n-        decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n-\n-        Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size*num_beams, sequence_length)`):\n-                The sequence used as a prompt for the generation.\n-            beam_scorer (`BeamScorer`):\n-                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n-                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n-            logits_processor (`LogitsProcessorList`):\n-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n-                used to modify the prediction scores of the language modeling head applied at each generation step.\n-            stopping_criteria (`StoppingCriteriaList`):\n-                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n-                used to tell if the generation loop should stop.\n-            generation_config ([`~generation.GenerationConfig`]):\n-                The generation configuration to be used as parametrization of the decoding method.\n-            synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n-                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n-            model_kwargs:\n-                Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n-                model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n-\n-        Return:\n-            [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n-            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n-            [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n-            `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n-            `model.config.is_encoder_decoder=True`.\n-        \"\"\"\n-        # init values\n-        pad_token_id = generation_config._pad_token_tensor\n-        eos_token_id = generation_config._eos_token_tensor\n-        output_attentions = generation_config.output_attentions\n-        output_hidden_states = generation_config.output_hidden_states\n-        output_scores = generation_config.output_scores\n-        output_logits = generation_config.output_logits\n-        return_dict_in_generate = generation_config.return_dict_in_generate\n-\n-        num_beams = beam_scorer.num_beams\n-        num_beam_groups = beam_scorer.num_beam_groups\n-        num_sub_beams = num_beams // num_beam_groups\n-        batch_size = len(beam_scorer._beam_hyps) // num_beam_groups\n-        device = input_ids.device\n-\n-        batch_beam_size, cur_len = input_ids.shape\n-        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n-\n-        if return_dict_in_generate and output_scores:\n-            beam_indices = [tuple(() for _ in range(num_sub_beams * batch_size)) for _ in range(num_beam_groups)]\n-        else:\n-            beam_indices = None\n-\n-        if num_beams * batch_size != batch_beam_size:\n-            raise ValueError(\n-                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n-            )\n-\n-        # init attention / hidden states / scores tuples\n-        scores = () if (return_dict_in_generate and output_scores) else None\n-        raw_logits = () if (return_dict_in_generate and output_logits) else None\n-        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n-        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n-\n-        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n-        if return_dict_in_generate and self.config.is_encoder_decoder:\n-            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n-            encoder_hidden_states = (\n-                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n-            )\n-\n-        # initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in\n-        # the same group don't produce same tokens every time.\n-        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n-        beam_scores[:, ::num_sub_beams] = 0\n-        beam_scores = beam_scores.view((batch_size * num_beams,))\n-\n-        this_peer_finished = False\n-\n-        decoder_prompt_len = input_ids.shape[1]  # record the prompt length of decoder\n-        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            # predicted tokens in cur_len step\n-            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n-\n-            # indices which will form the beams in the next time step\n-            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n-\n-            # do one decoder step on all beams of all sentences in batch\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n-\n-            # prepare variable output controls (note: some models won't accept all output controls)\n-            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n-            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n-\n-            outputs = self(**model_inputs, return_dict=True)\n-\n-            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n-            if synced_gpus and this_peer_finished:\n-                cur_len = cur_len + 1\n-                continue\n-\n-            if output_scores:\n-                processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n-            if output_logits:\n-                # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n-                # (the clone itself is always small)\n-                raw_logit_score = outputs.logits[:, -1, :].to(copy=True, device=input_ids.device)\n-\n-            for beam_group_idx in range(num_beam_groups):\n-                group_start_idx = beam_group_idx * num_sub_beams\n-                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n-                group_size = group_end_idx - group_start_idx\n-\n-                # indices of beams of current group among all sentences in batch\n-                batch_group_indices = []\n-\n-                for batch_idx in range(batch_size):\n-                    batch_group_indices.extend(\n-                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n-                    )\n-                group_input_ids = input_ids[batch_group_indices]\n-\n-                # select outputs of beams of current group only\n-                # No need to clone() the logits here as they will not retain outputs.logits at the end of the loop\n-                # .float() is needed to retain precision for later logits manipulations\n-                next_token_logits = outputs.logits[batch_group_indices, -1, :].to(\n-                    dtype=torch.float32, device=input_ids.device\n-                )\n-\n-                next_token_scores = nn.functional.log_softmax(\n-                    next_token_logits, dim=-1\n-                )  # (batch_size * group_size, vocab_size)\n-                vocab_size = next_token_scores.shape[-1]\n-\n-                next_token_scores_processed = logits_processor(\n-                    group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n-                )\n-                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n-                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n-\n-                if output_scores:\n-                    processed_score[batch_group_indices] = next_token_scores_processed\n-\n-                # reshape for beam search\n-                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n-\n-                # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n-                n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n-                next_token_scores, next_tokens = torch.topk(\n-                    next_token_scores, max(2, 1 + n_eos_tokens) * group_size, dim=1, largest=True, sorted=True\n-                )\n-\n-                next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n-                next_tokens = next_tokens % vocab_size\n-\n-                # stateless\n-                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n-                beam_outputs = beam_scorer.process(\n-                    group_input_ids,\n-                    next_token_scores,\n-                    next_tokens,\n-                    next_indices,\n-                    pad_token_id=pad_token_id,\n-                    eos_token_id=eos_token_id,\n-                    beam_indices=process_beam_indices,\n-                    group_index=beam_group_idx,\n-                    decoder_prompt_len=decoder_prompt_len,\n-                )\n-                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n-                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n-                beam_idx = beam_outputs[\"next_beam_indices\"]\n-\n-                if return_dict_in_generate and output_scores:\n-                    beam_indices[beam_group_idx] = tuple(\n-                        beam_indices[beam_group_idx][beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices[0]))\n-                    )\n-\n-                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n-                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n-                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n-\n-                # (beam_idx // group_size) -> batch_idx\n-                # (beam_idx % group_size) -> offset of idx inside the group\n-                reordering_indices[batch_group_indices] = (\n-                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\")\n-                    + group_start_idx\n-                    + (beam_idx % group_size)\n-                )\n-\n-            # Store scores, attentions and hidden_states when required\n-            if return_dict_in_generate:\n-                if output_scores:\n-                    scores += (processed_score,)\n-                if output_logits:\n-                    raw_logits += (raw_logit_score,)\n-                if output_attentions:\n-                    decoder_attentions += (\n-                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n-                    )\n-                    if self.config.is_encoder_decoder:\n-                        cross_attentions += (outputs.cross_attentions,)\n-\n-                if output_hidden_states:\n-                    decoder_hidden_states += (\n-                        (outputs.decoder_hidden_states,)\n-                        if self.config.is_encoder_decoder\n-                        else (outputs.hidden_states,)\n-                    )\n-\n-            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n-\n-            # This is needed to properly delete outputs.logits which may be very large for first iteration\n-            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n-            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n-            # (that way the memory peak does not include outputs.logits)\n-            del outputs\n-\n-            # NOTE: we need to check if `self._reorder_cache` exists for special models like RAG, RecurrentGemma etc.\n-            if model_kwargs.get(\"past_key_values\", None) is not None:\n-                if hasattr(self, \"_reorder_cache\"):\n-                    model_kwargs[\"past_key_values\"] = self._reorder_cache(\n-                        model_kwargs[\"past_key_values\"], reordering_indices\n-                    )\n-                else:\n-                    model_kwargs[\"past_key_values\"].reorder_cache(reordering_indices)\n-\n-            # increase cur_len\n-            cur_len = cur_len + 1\n-\n-            if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n-                this_peer_finished = True\n-\n-        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n-        sequence_outputs = beam_scorer.finalize(\n-            input_ids,\n-            beam_scores,\n-            next_tokens,\n-            next_indices,\n-            pad_token_id=pad_token_id,\n-            eos_token_id=eos_token_id,\n-            max_length=stopping_criteria.max_length,\n-            beam_indices=final_beam_indices,\n-            decoder_prompt_len=decoder_prompt_len,\n-        )\n-\n-        if return_dict_in_generate:\n-            if not output_scores:\n-                sequence_outputs[\"sequence_scores\"] = None\n-\n-            if self.config.is_encoder_decoder:\n-                return GenerateBeamEncoderDecoderOutput(\n-                    sequences=sequence_outputs[\"sequences\"],\n-                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n-                    scores=scores,\n-                    logits=raw_logits,\n-                    beam_indices=sequence_outputs[\"beam_indices\"],\n-                    encoder_attentions=encoder_attentions,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    decoder_attentions=decoder_attentions,\n-                    cross_attentions=cross_attentions,\n-                    decoder_hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n-                )\n-            else:\n-                return GenerateBeamDecoderOnlyOutput(\n-                    sequences=sequence_outputs[\"sequences\"],\n-                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n-                    scores=scores,\n-                    logits=raw_logits,\n-                    beam_indices=sequence_outputs[\"beam_indices\"],\n-                    attentions=decoder_attentions,\n-                    hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n-                )\n-        else:\n-            return sequence_outputs[\"sequences\"]\n-\n     def _constrained_beam_search(\n         self,\n         input_ids: torch.LongTensor,"
        },
        {
            "sha": "22b607ec286525f76e329281c65a4d0b694735ee",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -400,7 +400,7 @@ def _main_generate_loop(\n         else:\n             raise ValueError(\n                 \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n     @torch.no_grad()"
        },
        {
            "sha": "1b75d250505165e7f785074f1b1482e7946bd790",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1270,7 +1270,7 @@ def generate(\n         if generation_config.get_generation_mode() not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n             raise ValueError(\n                 \"Got incompatible mode for Image Generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         # Validate the configuration and model kwargs"
        },
        {
            "sha": "2866929510c68a453a0bccf75d7af475c4bdebeb",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1130,7 +1130,7 @@ def generate(\n         if generation_config.get_generation_mode() not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n             raise ValueError(\n                 \"Got incompatible mode for Image Generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         # Validate the configuration and model kwargs"
        },
        {
            "sha": "92d2d049ae3938bae6f48e2ac8f54fe181166661",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1321,7 +1321,7 @@ def generate(\n         else:\n             raise ValueError(\n                 \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         if generation_config.return_dict_in_generate:\n@@ -2371,7 +2371,7 @@ def generate(\n         else:\n             raise ValueError(\n                 \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         if generation_config.return_dict_in_generate:"
        },
        {
            "sha": "8133d33bac4746ad734fc3c9d9a0150479c3a825",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -1235,7 +1235,7 @@ def generate(\n         else:\n             raise ValueError(\n                 \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         if generation_config.return_dict_in_generate:\n@@ -2236,7 +2236,7 @@ def generate(\n         else:\n             raise ValueError(\n                 \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n             )\n \n         if generation_config.return_dict_in_generate:"
        },
        {
            "sha": "ce3fd036aef255c66d9a45a9e6c695428fe9877e",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -177,13 +177,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class BeamSearchScorer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ClassifierFreeGuidanceLogitsProcessor(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -282,13 +275,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class HammingDiversityLogitsProcessor(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class InfNanRemoveLogitsProcessor(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "69bb0a40e2925fbfd8794f64a9cd839007da25a1",
            "filename": "tests/generation/test_beam_search.py",
            "status": "modified",
            "additions": 0,
            "deletions": 236,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_beam_search.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -26,230 +26,12 @@\n \n     from transformers.generation import (\n         BeamHypotheses,\n-        BeamSearchScorer,\n         ConstrainedBeamSearchScorer,\n         DisjunctiveConstraint,\n         PhrasalConstraint,\n     )\n \n \n-class BeamSearchTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=3,\n-        sequence_length=10,\n-        vocab_size=99,\n-        pad_token_id=0,\n-        max_length=20,\n-        num_beams=4,\n-        length_penalty=2.0,\n-        do_early_stopping=True,\n-        num_beam_hyps_to_keep=2,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.sequence_length = sequence_length\n-        self.vocab_size = vocab_size\n-        self.pad_token_id = pad_token_id\n-        self.max_length = max_length\n-        self.num_beams = num_beams\n-        self.length_penalty = length_penalty\n-        self.do_early_stopping = do_early_stopping\n-        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n-\n-        # cannot be randomly generated\n-        self.eos_token_id = vocab_size + 1\n-\n-    def prepare_beam_scorer(self, **kwargs):\n-        return BeamSearchScorer(\n-            batch_size=kwargs.get(\"batch_size\", self.batch_size),\n-            num_beams=kwargs.get(\"num_beams\", self.num_beams),\n-            device=torch_device,\n-            length_penalty=kwargs.get(\"length_penalty\", self.length_penalty),\n-            do_early_stopping=kwargs.get(\"do_early_stopping\", self.do_early_stopping),\n-            num_beam_hyps_to_keep=kwargs.get(\"num_beam_hyps_to_keep\", self.num_beam_hyps_to_keep),\n-        )\n-\n-    def prepare_inputs(self):\n-        input_ids = ids_tensor((self.batch_size * self.num_beams, self.sequence_length), self.vocab_size)\n-        next_tokens = ids_tensor((self.batch_size, 2 * self.num_beams), self.vocab_size).to(torch_device)\n-        next_indices = ids_tensor((self.batch_size, 2 * self.num_beams), self.num_beams).to(torch_device)\n-        next_scores, _ = (-floats_tensor((self.batch_size, 2 * self.num_beams)).to(torch_device)).sort(descending=True)\n-        return (input_ids, next_tokens, next_indices, next_scores)\n-\n-    def check_beam_hypotheses(self, input_ids, *args):\n-        # check that correct number of beam hypotheses is set in beam scorer\n-        beam_scorer = self.prepare_beam_scorer(do_early_stopping=True)\n-        beam_hyp = beam_scorer._beam_hyps[0]\n-\n-        self.parent.assertEqual(len(beam_scorer._beam_hyps), self.batch_size)\n-\n-        # check correct type\n-        self.parent.assertTrue(isinstance(beam_hyp, BeamHypotheses))\n-\n-        # check that num_beams is correctly set\n-        self.parent.assertEqual(beam_hyp.num_beams, self.num_beams)\n-\n-        # check for early stopping deactivated\n-        for beam_idx in range(self.num_beams):\n-            beam_hyp.add(input_ids[beam_idx], -10.0)\n-\n-        # if early stopping True -> score does not matter\n-        self.parent.assertTrue(beam_hyp.is_done(-10.0, 5))\n-\n-        # re-init\n-        beam_scorer = self.prepare_beam_scorer(do_early_stopping=False)\n-        beam_hyp = beam_scorer._beam_hyps[0]\n-\n-        # add `num_beams + 1` beams to change `worst_score`\n-        for beam_idx in range(self.num_beams + 1):\n-            beam_hyp.add(input_ids[beam_idx], -10.0 + float(beam_idx))\n-\n-        # -10.0 is removed => -9.0 is worst score\n-        self.parent.assertAlmostEqual(beam_hyp.worst_score, -9.0 / (self.sequence_length**beam_hyp.length_penalty))\n-\n-        # -5.0 is better than worst score => should not be finished\n-        self.parent.assertFalse(beam_hyp.is_done(-5.0, self.sequence_length))\n-\n-        # -20.0 is worse than worst score => should be finished\n-        self.parent.assertTrue(beam_hyp.is_done(-20.0, self.sequence_length))\n-\n-    def check_beam_scorer_update(self, input_ids, next_tokens, next_indices, next_scores):\n-        # check too many eos tokens\n-        beam_scorer = self.prepare_beam_scorer()\n-\n-        tokens = next_tokens.clone()\n-        tokens[0, :] = self.eos_token_id\n-\n-        with self.parent.assertRaises(ValueError):\n-            beam_scorer.process(input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id)\n-\n-        # check all batches are done\n-        beam_scorer = self.prepare_beam_scorer()\n-\n-        tokens = next_tokens.clone()\n-        tokens[:, : self.num_beams] = self.eos_token_id\n-        beam_indices = torch.zeros_like(input_ids) + torch.arange(input_ids.shape[-1], device=input_ids.device)\n-        beam_indices = tuple(tuple(b) for b in beam_indices)\n-        beam_scorer.process(\n-            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id, beam_indices=beam_indices\n-        )\n-        # beam scorer should be done\n-        self.parent.assertTrue(beam_scorer.is_done)\n-\n-        # check\n-        beam_scorer = self.prepare_beam_scorer()\n-\n-        tokens = next_tokens.clone()\n-        tokens[:, 1] = self.eos_token_id\n-        beam_outputs = beam_scorer.process(\n-            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id, beam_indices=beam_indices\n-        )\n-        output_scores = beam_outputs[\"next_beam_scores\"]\n-        output_tokens = beam_outputs[\"next_beam_tokens\"]\n-        output_indices = beam_outputs[\"next_beam_indices\"]\n-\n-        def cut_expected_tensor(tensor):\n-            return torch.cat([tensor[:, :1], tensor[:, 2 : self.num_beams + 1]], dim=1).flatten()\n-\n-        # check all outptus\n-        # cut out id of eos token and take best `num_beams` outputs\n-        expected_output_tokens = cut_expected_tensor(tokens)\n-        expected_output_scores = cut_expected_tensor(next_scores)\n-\n-        # add num_beams * batch_idx\n-        offset = torch.div(\n-            torch.arange(self.num_beams * self.batch_size, device=torch_device), self.num_beams, rounding_mode=\"floor\"\n-        )\n-        expected_output_indices = cut_expected_tensor(next_indices) + offset * self.num_beams\n-\n-        self.parent.assertListEqual(expected_output_tokens.tolist(), output_tokens.tolist())\n-        self.parent.assertListEqual(expected_output_indices.tolist(), output_indices.tolist())\n-        self.parent.assertTrue(torch.allclose(expected_output_scores, output_scores, atol=1e-3))\n-\n-        # make sure ids of eos token are correctly saved in beam_hyps of beam scorer\n-        expected_beam_indices = list(range(10))\n-        for batch_idx in range(self.batch_size):\n-            correct_idx = batch_idx * self.num_beams + next_indices[batch_idx, 1]\n-            self.parent.assertListEqual(\n-                input_ids[correct_idx].tolist(), beam_scorer._beam_hyps[batch_idx].beams[0][1].tolist()\n-            )\n-            self.parent.assertListEqual(\n-                expected_beam_indices + [correct_idx],\n-                torch.tensor(beam_scorer._beam_hyps[batch_idx].beams[0][2]).tolist(),\n-            )\n-\n-    def check_beam_scores_finalize(self, input_ids, next_tokens, next_indices, next_scores):\n-        # max_length should be only one more than current input_ids to check that eos is correctly appended\n-        max_length = self.sequence_length + 1\n-        beam_scorer = self.prepare_beam_scorer(num_beam_hyps_to_keep=1, length_penalty=1.0, do_early_stopping=False)\n-\n-        # update beams and append to input_ids\n-        tokens = next_tokens.clone()\n-        # first batch, first output has to finish with eos token id since scores are correctly sorted\n-        tokens[0, 0] = self.eos_token_id\n-        # make sure corresponding score is as good as possible to surely be picked first\n-        next_scores[0, 0] = 0.0\n-        beam_outputs = beam_scorer.process(\n-            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id\n-        )\n-        output_scores = beam_outputs[\"next_beam_scores\"]\n-        output_tokens = beam_outputs[\"next_beam_tokens\"]\n-        output_indices = beam_outputs[\"next_beam_indices\"]\n-\n-        input_ids = torch.cat([input_ids[output_indices, :], output_tokens.unsqueeze(-1)], dim=-1)\n-\n-        # finalize\n-        beam_indices = torch.zeros_like(input_ids) + torch.arange(input_ids.shape[-1], device=input_ids.device)\n-        beam_indices = tuple(tuple(b) for b in beam_indices)\n-        sequence_output = beam_scorer.finalize(\n-            input_ids,\n-            output_scores,\n-            output_tokens,\n-            output_indices,\n-            pad_token_id=self.pad_token_id,\n-            eos_token_id=self.eos_token_id,\n-            max_length=max_length,\n-            beam_indices=beam_indices,\n-        )\n-\n-        sequences = sequence_output[\"sequences\"]\n-        sequence_scores = sequence_output[\"sequence_scores\"]\n-\n-        # since `num_beam_hyps_to_keep` = 1 => only return `batch_size` x `max_length`\n-        self.parent.assertListEqual(list(sequences.shape), [self.batch_size, max_length])\n-        self.parent.assertListEqual(list(sequence_scores.shape), [self.batch_size])\n-\n-        # check sequence_scores\n-        self.parent.assertFalse((sequence_scores > 0).any().item())\n-\n-        # first batch has to finish with eos_token\n-        self.parent.assertEqual(sequences[0, -1].item(), self.eos_token_id)\n-\n-        # other batches cannot finish with eos token\n-        self.parent.assertNotEqual(sequences[1, -1].item(), self.eos_token_id)\n-        self.parent.assertNotEqual(sequences[2, -1].item(), self.eos_token_id)\n-\n-        # now test that if `num_beam_hyps_to_keep` is 3 => all beams are returned\n-        beam_scorer.num_beam_hyps_to_keep = self.num_beams\n-        sequence_output = beam_scorer.finalize(\n-            input_ids,\n-            output_scores,\n-            output_tokens,\n-            output_indices,\n-            pad_token_id=self.pad_token_id,\n-            eos_token_id=self.eos_token_id,\n-            max_length=max_length,\n-            beam_indices=beam_indices,\n-        )\n-        sequences = sequence_output[\"sequences\"]\n-        sequence_scores = sequence_output[\"sequence_scores\"]\n-\n-        self.parent.assertListEqual(list(sequences.shape), [self.num_beams * self.batch_size, max_length])\n-        self.parent.assertListEqual(list(sequence_scores.shape), [self.num_beams * self.batch_size])\n-\n-\n class ConstrainedBeamSearchTester:\n     def __init__(\n         self,\n@@ -540,24 +322,6 @@ def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n         return flag\n \n \n-@require_torch\n-class BeamSearchTest(unittest.TestCase):\n-    def setUp(self):\n-        self.beam_search_tester = BeamSearchTester(self)\n-\n-    def test_beam_hypotheses(self):\n-        inputs = self.beam_search_tester.prepare_inputs()\n-        self.beam_search_tester.check_beam_hypotheses(*inputs)\n-\n-    def test_beam_scorer_update(self):\n-        inputs = self.beam_search_tester.prepare_inputs()\n-        self.beam_search_tester.check_beam_scorer_update(*inputs)\n-\n-    def test_beam_scorer_finalize(self):\n-        inputs = self.beam_search_tester.prepare_inputs()\n-        self.beam_search_tester.check_beam_scores_finalize(*inputs)\n-\n-\n @require_torch\n class ConstrainedBeamSearchTest(unittest.TestCase):\n     def setUp(self):"
        },
        {
            "sha": "e67e5ba325e0d8d3550bb0003291f6ca079d8f01",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -39,7 +39,6 @@\n     ForcedBOSTokenLogitsProcessor,\n     ForcedEOSTokenLogitsProcessor,\n     GenerationMode,\n-    HammingDiversityLogitsProcessor,\n     MinLengthLogitsProcessor,\n     MinNewTokensLengthLogitsProcessor,\n     MinPLogitsWarper,\n@@ -536,31 +535,6 @@ def prefix_allowed_tokens_fn(batch_id, inputs_ids):\n         )\n         self.assertEqual(prefix_constrained_logits_proc._num_beams, num_beams)\n \n-    def test_serialize_generation_diversity_penalty_and_num_bean_groups(self):\n-        \"\"\"Tests that GenerationConfig is serialized and HammingDiversityLogitsProcessor is initialized with diversity_penalty_and_num_bean_groups\"\"\"\n-        num_beams = 2\n-        num_beam_groups = 2\n-        diversity_penalty = 1.0\n-\n-        generation_config = GenerationConfig(\n-            num_beams=num_beams, diversity_penalty=diversity_penalty, num_beam_groups=num_beam_groups\n-        )\n-        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n-            generation_config.save_pretrained(tmp_dir)\n-            new_config = GenerationConfig.from_pretrained(tmp_dir)\n-        self.assertEqual(new_config.num_beams, num_beams)\n-        self.assertEqual(new_config.diversity_penalty, diversity_penalty)\n-        self.assertEqual(new_config.num_beam_groups, num_beam_groups)\n-\n-        diversity_logits_processor = HammingDiversityLogitsProcessor(\n-            diversity_penalty=new_config.diversity_penalty,\n-            num_beams=new_config.num_beams,\n-            num_beam_groups=new_config.num_beam_groups,\n-        )\n-        self.assertEqual(diversity_logits_processor._num_beams, num_beams)\n-        self.assertEqual(diversity_logits_processor._diversity_penalty, diversity_penalty)\n-        self.assertEqual(diversity_logits_processor._num_sub_beams, num_beams // num_beam_groups)\n-\n     def test_serialize_generation_bos_token_id(self):\n         \"\"\"Tests that GenerationConfig is serialized and ForcedBOSTokenLogitsProcessor is initialized with bos_token_id\"\"\"\n         bos_token_id = 0"
        },
        {
            "sha": "768e216ef534eaa8514ef323ce3fb31fe9f6f42c",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -36,7 +36,6 @@\n         ExponentialDecayLengthPenalty,\n         ForcedBOSTokenLogitsProcessor,\n         ForcedEOSTokenLogitsProcessor,\n-        HammingDiversityLogitsProcessor,\n         InfNanRemoveLogitsProcessor,\n         LogitNormalization,\n         LogitsProcessorList,\n@@ -796,36 +795,6 @@ def empty_prefix_allowed_tokens_fn(batch_id, inputs_ids):\n         # processor should not change logits in-place\n         self.assertFalse(torch.all(scores == filtered_scores))\n \n-    def test_hamming_diversity(self):\n-        vocab_size = 4\n-        num_beams = 2\n-        num_beam_groups = 2\n-\n-        scores = self._get_uniform_logits(num_beams, vocab_size)\n-        # batch_idx = 0 -> index batch_idx * num_beam_groups -> idx = 0 * 2 = 0 -> penalises tokens 1\n-        # batch_idx = 1 -> index batch_idx * num_beam_groups -> idx = 1 * 2 = 2 -> penalises tokens 1\n-        current_tokens = torch.tensor([0, 3, 1, 2], device=torch_device, dtype=torch.long)\n-\n-        diversity_logits_processor = HammingDiversityLogitsProcessor(\n-            diversity_penalty=1.0, num_beams=num_beams, num_beam_groups=num_beam_groups\n-        )\n-\n-        processed_scores = diversity_logits_processor(None, scores, current_tokens, 1)\n-\n-        self.assertTrue(\n-            torch.allclose(\n-                processed_scores[0], torch.tensor([-0.7500, 0.2500, 0.2500, 0.2500], device=torch_device), atol=1e-3\n-            )\n-        )\n-        self.assertTrue(\n-            torch.allclose(\n-                processed_scores[1], torch.tensor([0.2500, -0.7500, 0.2500, 0.2500], device=torch_device), atol=1e-3\n-            )\n-        )\n-\n-        # processor should not change logits in-place\n-        self.assertFalse(torch.all(scores == processed_scores))\n-\n     def test_forced_bos_token_logits_processor(self):\n         vocab_size = 20\n         batch_size = 4"
        },
        {
            "sha": "71023f733075ebd89f2facf78c53383235308f2d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 114,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -209,17 +209,6 @@ def _get_beam_kwargs(self, num_return_sequences=1):\n         }\n         return beam_kwargs\n \n-    def _get_diverse_beam_kwargs(self, num_return_sequences=1):\n-        beam_kwargs = {\n-            \"early_stopping\": False,\n-            \"length_penalty\": 2.0,\n-            \"num_beams\": 2,\n-            \"num_return_sequences\": num_return_sequences,\n-            \"num_beam_groups\": 2,  # one beam per group\n-            \"diversity_penalty\": 2.0,\n-        }\n-        return beam_kwargs\n-\n     def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n         beam_kwargs = {\n             \"early_stopping\": False,\n@@ -351,36 +340,6 @@ def _beam_sample_generate(\n \n         return output_generate\n \n-    def _group_beam_search_generate(\n-        self,\n-        model,\n-        inputs_dict,\n-        beam_kwargs,\n-        output_scores=False,\n-        output_logits=False,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict_in_generate=False,\n-        use_cache=True,\n-    ):\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n-        output_generate = model.generate(\n-            do_sample=False,\n-            max_new_tokens=self.max_new_tokens,\n-            min_new_tokens=self.max_new_tokens,\n-            output_scores=output_scores,\n-            output_logits=output_logits,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict_in_generate=return_dict_in_generate,\n-            use_cache=use_cache,\n-            **beam_kwargs,\n-            **logits_processor_kwargs,\n-            **inputs_dict,\n-        )\n-\n-        return output_generate\n-\n     def _constrained_beam_search_generate(\n         self,\n         model,\n@@ -747,77 +706,6 @@ def test_generate_without_input_ids(self):\n             )\n             self.assertIsNotNone(output_ids_generate)\n \n-    @pytest.mark.generate\n-    def test_group_beam_search_generate(self):\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            model = model_class(config).to(torch_device).eval()\n-            # check `generate()` and `group_beam_search()` are equal\n-            beam_kwargs = self._get_diverse_beam_kwargs()\n-            output_generate = self._group_beam_search_generate(\n-                model=model,\n-                inputs_dict=inputs_dict,\n-                beam_kwargs=beam_kwargs,\n-            )\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n-            else:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n-\n-            # check `group_beam_search` for higher than 1 `num_return_sequences`\n-            num_return_sequences = 2\n-            beam_kwargs = self._get_diverse_beam_kwargs(num_return_sequences=num_return_sequences)\n-            output_generate = self._group_beam_search_generate(\n-                model=model,\n-                inputs_dict=inputs_dict,\n-                beam_kwargs=beam_kwargs,\n-            )\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n-            else:\n-                self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n-\n-    @pytest.mark.generate\n-    def test_group_beam_search_generate_dict_output(self):\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            if self.has_attentions:\n-                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n-\n-            model = model_class(config).to(torch_device).eval()\n-            beam_kwargs = self._get_diverse_beam_kwargs()\n-            output_generate = self._group_beam_search_generate(\n-                model=model,\n-                inputs_dict=inputs_dict,\n-                beam_kwargs=beam_kwargs,\n-                output_scores=True,\n-                output_logits=True,\n-                output_hidden_states=True,\n-                output_attentions=self.has_attentions,\n-                return_dict_in_generate=True,\n-                use_cache=False,\n-            )\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n-                self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n-                self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n-                # Retrocompatibility check\n-                self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n-            else:\n-                self.assertTrue(\n-                    output_generate.sequences.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1]\n-                )\n-                self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n-                # Retrocompatibility check\n-                self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n-\n-            self._check_generate_outputs(\n-                output_generate,\n-                model.config,\n-                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n-                num_beams=beam_kwargs[\"num_beams\"],\n-            )\n-\n     @is_flaky()  # Some models have position-specific tokens, this test may try to force them in an invalid position\n     @pytest.mark.generate\n     def test_constrained_beam_search_generate(self):\n@@ -2664,6 +2552,7 @@ def floats_tensor(shape, scale=1.0, rng=None, name=None):\n @pytest.mark.generate\n @require_torch\n class GenerationIntegrationTests(unittest.TestCase):\n+    # TODO joao, manuel: remove in v4.62.0\n     @slow\n     def test_diverse_beam_search(self):\n         article = \"\"\"Justin Timberlake and Jessica Biel, welcome to parenthood.\n@@ -2682,6 +2571,8 @@ def test_diverse_beam_search(self):\n             num_beam_groups=4,\n             diversity_penalty=2.0,\n             remove_invalid_values=True,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/group-beam-search\",\n         )\n \n         generated_text = bart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -2841,6 +2732,7 @@ def test_generate_input_values_as_encoder_kwarg(self):\n         self.assertListEqual(output_sequences.tolist(), output_sequences_kwargs.tolist())\n         self.assertEqual(output_sequences.shape, (2, 5))\n \n+    # TODO joao, manuel: remove in v4.62.0\n     def test_transition_scores_group_beam_search_encoder_decoder(self):\n         articles = [\n             \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n@@ -2849,20 +2741,27 @@ def test_transition_scores_group_beam_search_encoder_decoder(self):\n         tokenizer = BartTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n         model = BartForConditionalGeneration.from_pretrained(\n             \"hf-internal-testing/tiny-random-bart\",\n+            eos_token_id=None,\n+        )\n+        generation_config = GenerationConfig(\n             max_length=10,\n             num_beams=2,\n             num_beam_groups=2,\n             num_return_sequences=2,\n             diversity_penalty=1.0,\n-            eos_token_id=None,\n             return_dict_in_generate=True,\n             output_scores=True,\n             length_penalty=0.0,\n         )\n         model = model.to(torch_device)\n \n         input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids.to(torch_device)\n-        outputs = model.generate(input_ids=input_ids)\n+        outputs = model.generate(\n+            input_ids=input_ids,\n+            generation_config=generation_config,\n+            trust_remote_code=True,\n+            custom_generate=\"transformers-community/group-beam-search\",\n+        )\n \n         transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n         transition_scores_sum = transition_scores.sum(-1)\n@@ -4833,6 +4732,16 @@ def test_generate_custom_cache_position(self):\n         [\n             (\"transformers-community/dola\", {\"dola_layers\": \"low\"}),\n             (\"transformers-community/contrastive-search\", {\"penalty_alpha\": 0.6, \"top_k\": 4}),\n+            (\n+                \"transformers-community/group-beam-search\",\n+                {\n+                    \"do_sample\": False,\n+                    \"num_beams\": 2,\n+                    \"num_beam_groups\": 2,\n+                    \"diversity_penalty\": 2.0,\n+                    \"length_penalty\": 2.0,\n+                },\n+            ),\n         ]\n     )\n     def test_hub_gen_strategies(self, custom_generate, extra_kwargs):"
        },
        {
            "sha": "d77a86a201cb6c59f99ee8b7b0476b771a1c8592",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -272,16 +272,6 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n     def test_beam_sample_generate_dict_output(self):\n         pass\n \n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support group beam search.\")\n-    def test_group_beam_search_generate(self):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"CSM does not support group beam search.\")\n-    def test_group_beam_search_generate_dict_output(self):\n-        pass\n-\n     @pytest.mark.generate\n     @unittest.skip(reason=\"CSM does not support constrained beam search.\")\n     def test_constrained_beam_search_generate(self):"
        },
        {
            "sha": "2f09b65cf8f3162116d57bcb2ce5aa41d9971386",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -237,7 +237,6 @@ def skip_non_greedy_generate(self):\n         skippable_tests = [\n             \"test_sample_generate_dict_output\",  # return sequences > 1\n             \"test_beam\",\n-            \"test_group_beam\",\n             \"test_constrained_beam\",\n             \"test_contrastive\",\n             \"test_assisted\","
        },
        {
            "sha": "bcb7259004ba4d9183e586b622de4a3aec97112f",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -138,16 +138,6 @@ def test_constrained_beam_search_generate_dict_output(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n-    @pytest.mark.generate\n-    def test_group_beam_search_generate(self):\n-        pass\n-\n-    @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n-    @pytest.mark.generate\n-    def test_group_beam_search_generate_dict_output(self):\n-        pass\n-\n     @unittest.skip(reason=\"RecurrentGemma is unusual and fails a lot of generation tests\")\n     @pytest.mark.generate\n     def test_constrained_beam_search_generate(self):"
        },
        {
            "sha": "8682c1e75c58fe3e37143b78b67b271be8de4860",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -401,13 +401,6 @@ def test_greedy_generate_dict_outputs(self):\n         super().test_greedy_generate_dict_outputs()\n         self.has_attentions = old_has_attentions\n \n-    def test_group_beam_search_generate_dict_output(self):\n-        # This model has a custom attention output shape AND config flags, let's skip those checks\n-        old_has_attentions = self.has_attentions\n-        self.has_attentions = False\n-        super().test_group_beam_search_generate_dict_output()\n-        self.has_attentions = old_has_attentions\n-\n     def test_sample_generate_dict_output(self):\n         # This model has a custom attention output shape AND config flags, let's skip those checks\n         old_has_attentions = self.has_attentions"
        },
        {
            "sha": "8715f1e708d6dfb9b096b265c8135f487ba5b587",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -404,12 +404,6 @@ def _get_beam_kwargs(self, num_return_sequences=1):\n         beam_kwargs[\"num_return_sequences\"] = beam_kwargs[\"num_beams\"]\n         return beam_kwargs\n \n-    def _get_diverse_beam_kwargs(self, num_return_sequences=1):\n-        # Overwritten from `GenerationTesterMixin`, Whisper's `num_return_sequences` differs from the core `generate`\n-        beam_kwargs = super()._get_diverse_beam_kwargs(num_return_sequences=num_return_sequences)\n-        beam_kwargs[\"num_return_sequences\"] = beam_kwargs[\"num_beams\"]\n-        return beam_kwargs\n-\n     def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n         # Overwritten from `GenerationTesterMixin`, Whisper's `num_return_sequences` differs from the core `generate`\n         beam_kwargs = super()._get_constrained_beam_kwargs(num_return_sequences=num_return_sequences)"
        },
        {
            "sha": "b3b03c49f5e3a799858390cab84558d700bb0b06",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bccb02616a232cf97e4ebfd879c82a7ac0ec0df/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=3bccb02616a232cf97e4ebfd879c82a7ac0ec0df",
            "patch": "@@ -434,9 +434,7 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n         common = {\n             \"num_beams\": 4,\n-            \"num_beam_groups\": 2,\n             \"num_return_sequences\": 4,\n-            \"diversity_penalty\": 1.0,\n             \"max_new_tokens\": 20,\n             \"early_stopping\": True,\n         }"
        }
    ],
    "stats": {
        "total": 1491,
        "additions": 61,
        "deletions": 1430
    }
}