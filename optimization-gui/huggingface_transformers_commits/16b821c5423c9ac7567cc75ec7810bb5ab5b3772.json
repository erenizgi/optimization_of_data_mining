{
    "author": "ydshieh",
    "message": "Avoid `T5GemmaModelTest::test_eager_matches_sdpa_inference` being flaky (#40702)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "16b821c5423c9ac7567cc75ec7810bb5ab5b3772",
    "files": [
        {
            "sha": "e742258d7a27432926d3ad653b6a27b6ad2b18ff",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/16b821c5423c9ac7567cc75ec7810bb5ab5b3772/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16b821c5423c9ac7567cc75ec7810bb5ab5b3772/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=16b821c5423c9ac7567cc75ec7810bb5ab5b3772",
            "patch": "@@ -202,6 +202,15 @@ def prepare_config_and_inputs(self):\n         input_ids = torch.where(input_ids == self.bos_token_id, 42, input_ids)\n         decoder_input_ids = torch.where(decoder_input_ids == self.bos_token_id, 42, decoder_input_ids)\n \n+        # Avoid leading PAD tokens from inputs.\n+        # `T5GemmaForTokenClassification` and `T5GemmaForSequenceClassification` specify `use_cache=False` when\n+        # calling `self.model`. For `self.use_attention_mask=False` case below, the model goes through\n+        # `make_default_2d_attention_mask`. When there are some pad tokens at the beginning of a sequence, it can't\n+        # attend to any place, and the computed mask `[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38]`\n+        # causes larger differences in some equivalence tests.\n+        # Let's avoid such leading PAD tokens.\n+        decoder_input_ids[:, 0] = self.pad_token_id + 1\n+\n         attention_mask = None\n         decoder_attention_mask = None\n         if self.use_attention_mask:"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}