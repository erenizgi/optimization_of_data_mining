{
    "author": "gante",
    "message": "Generation: deprecate `PreTrainedModel` inheriting from `GenerationMixin`  (#33203)",
    "sha": "e15687fffe5c9d20598a19aeab721ae0a7580f8a",
    "files": [
        {
            "sha": "c1aa338a7d8f2f0c3d372aee119e211b24b66c52",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -34,13 +34,6 @@\n )\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n-from ..models.auto import (\n-    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n-    MODEL_FOR_CAUSAL_LM_MAPPING,\n-    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n-    MODEL_FOR_VISION_2_SEQ_MAPPING,\n-)\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n from ..utils import (\n@@ -1117,26 +1110,21 @@ def _validate_model_class(self):\n         Confirms that the model class is compatible with generation. If not, raises an exception that points to the\n         right class to use.\n         \"\"\"\n+        # TODO(joao): remove this function in v4.50, i.e. when we remove the inheritance of `GenerationMixin` from\n+        # `PreTrainedModel`. With that inheritance removed, all model classes inheriting from `GenerationMixin` can\n+        # safely call `GenerationMixin.generate`\n         if not is_torchdynamo_compiling() and not self.can_generate():\n-            generate_compatible_mappings = [\n-                MODEL_FOR_CAUSAL_LM_MAPPING,\n-                MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n-                MODEL_FOR_VISION_2_SEQ_MAPPING,\n-                MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-                MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n+            terminations_with_generation_support = [\n+                \"ForCausalLM\",\n+                \"ForConditionalGeneration\",\n+                \"ForSpeechSeq2Seq\",\n+                \"ForVision2Seq\",\n             ]\n-            generate_compatible_classes = set()\n-            for model_mapping in generate_compatible_mappings:\n-                supported_models = model_mapping.get(type(self.config), default=None)\n-                if supported_models is not None:\n-                    generate_compatible_classes.add(supported_models.__name__)\n-            exception_message = (\n+            raise TypeError(\n                 f\"The current model class ({self.__class__.__name__}) is not compatible with `.generate()`, as \"\n-                \"it doesn't have a language model head.\"\n+                \"it doesn't have a language model head. Classes that support generation often end in one of these \"\n+                f\"names: {terminations_with_generation_support}.\"\n             )\n-            if generate_compatible_classes:\n-                exception_message += f\" Please use one of the following classes instead: {generate_compatible_classes}\"\n-            raise TypeError(exception_message)\n \n     def _validate_assistant(self, assistant_model):\n         if assistant_model is None:"
        },
        {
            "sha": "6fff23f6b6df13fd01de1a76c621efd0a94746ce",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 8,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -212,7 +212,7 @@ def _skip_init(*args, **kwargs):\n                 setattr(torch.nn.init, name, init_func)\n \n \n-def get_parameter_device(parameter: Union[nn.Module, GenerationMixin, \"ModuleUtilsMixin\"]):\n+def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     try:\n         return next(parameter.parameters()).device\n     except StopIteration:\n@@ -227,7 +227,7 @@ def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n         return first_tuple[1].device\n \n \n-def get_first_parameter_dtype(parameter: Union[nn.Module, GenerationMixin, \"ModuleUtilsMixin\"]):\n+def get_first_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     \"\"\"\n     Returns the first parameter dtype (can be non-floating) or asserts if none were found.\n     \"\"\"\n@@ -245,7 +245,7 @@ def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n         return first_tuple[1].dtype\n \n \n-def get_parameter_dtype(parameter: Union[nn.Module, GenerationMixin, \"ModuleUtilsMixin\"]):\n+def get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     \"\"\"\n     Returns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\n     \"\"\"\n@@ -1309,6 +1309,7 @@ def floating_point_ops(\n         return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n \n \n+# TODO (joao): remove `GenerationMixin` inheritance in v4.50\n class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):\n     r\"\"\"\n     Base class for all models.\n@@ -1638,11 +1639,30 @@ def can_generate(cls) -> bool:\n         Returns:\n             `bool`: Whether this model can generate sequences with `.generate()`.\n         \"\"\"\n-        # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.\n-        # Alternativelly, the model can also have a custom `generate` function.\n-        if \"GenerationMixin\" in str(cls.prepare_inputs_for_generation) and \"GenerationMixin\" in str(cls.generate):\n-            return False\n-        return True\n+        # Directly inherits `GenerationMixin` -> can generate\n+        if \"GenerationMixin\" in str(cls.__bases__):\n+            return True\n+        # Model class overwrites `generate` (e.g. time series models) -> can generate\n+        if str(cls.__name__) in str(cls.generate):\n+            return True\n+        # BC: Detects whether `prepare_inputs_for_generation` has been overwritten in the model. Prior to v4.45, this\n+        # was how we detected whether a model could generate.\n+        if \"GenerationMixin\" not in str(cls.prepare_inputs_for_generation):\n+            logger.warning_once(\n+                f\"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly \"\n+                \"overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, \"\n+                \"`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability \"\n+                \"to call `generate` and other related functions.\"\n+                \"\\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the \"\n+                \"model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\"\n+                \"\\n  - If you are the owner of the model architecture code, please modify your model class such that \"\n+                \"it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\"\n+                \"\\n  - If you are not the owner of the model architecture class, please contact the model code owner \"\n+                \"to update it.\"\n+            )\n+            return True\n+        # Otherwise, can't generate\n+        return False\n \n     @classmethod\n     def _check_and_enable_flash_attn_2("
        },
        {
            "sha": "bfd8e38687accc7960207d831280e0f58f4894f5",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -983,7 +984,7 @@ def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n     \"Albert Model with a `language modeling` head on top.\",\n     ALBERT_START_DOCSTRING,\n )\n-class AlbertForMaskedLM(AlbertPreTrainedModel):\n+class AlbertForMaskedLM(AlbertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "7809b2a6cc2cfc08de558ea7e2f364c4c02d9202",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,12 +29,17 @@\n     extract_commit_hash,\n     find_adapter_config_file,\n     is_peft_available,\n+    is_torch_available,\n     logging,\n     requires_backends,\n )\n from .configuration_auto import AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n \n \n+if is_torch_available():\n+    from ...generation import GenerationMixin\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -428,6 +433,7 @@ def from_config(cls, config, **kwargs):\n             model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)\n             cls.register(config.__class__, model_class, exist_ok=True)\n             _ = kwargs.pop(\"code_revision\", None)\n+            model_class = add_generation_mixin_to_remote_model(model_class)\n             return model_class._from_config(config, **kwargs)\n         elif type(config) in cls._model_mapping.keys():\n             model_class = _get_model_class(config, cls._model_mapping)\n@@ -549,6 +555,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             )\n             _ = hub_kwargs.pop(\"code_revision\", None)\n             cls.register(config.__class__, model_class, exist_ok=True)\n+            model_class = add_generation_mixin_to_remote_model(model_class)\n             return model_class.from_pretrained(\n                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n             )\n@@ -698,6 +705,34 @@ def getattribute_from_module(module, attr):\n         raise ValueError(f\"Could not find {attr} in {transformers_module}!\")\n \n \n+def add_generation_mixin_to_remote_model(model_class):\n+    \"\"\"\n+    Adds `GenerationMixin` to the inheritance of `model_class`, if `model_class` is a PyTorch model.\n+\n+    This function is used for backwards compatibility purposes: in v4.45, we've started a deprecation cycle to make\n+    `PreTrainedModel` stop inheriting from `GenerationMixin`. Without this function, older models dynamically loaded\n+    from the Hub may not have the `generate` method after we remove the inheritance.\n+    \"\"\"\n+    # 1. If it is not a PT model (i.e. doesn't inherit Module), do nothing\n+    if \"torch.nn.modules.module.Module\" not in str(model_class.__mro__):\n+        return model_class\n+\n+    # 2. If it already **directly** inherits from GenerationMixin, do nothing\n+    if \"GenerationMixin\" in str(model_class.__bases__):\n+        return model_class\n+\n+    # 3. Prior to v4.45, we could detect whether a model was `generate`-compatible if it had its own `generate` and/or\n+    # `prepare_inputs_for_generation` method.\n+    has_custom_generate = \"GenerationMixin\" not in str(getattr(model_class, \"generate\"))\n+    has_custom_prepare_inputs = \"GenerationMixin\" not in str(getattr(model_class, \"prepare_inputs_for_generation\"))\n+    if has_custom_generate or has_custom_prepare_inputs:\n+        model_class_with_generation_mixin = type(\n+            model_class.__name__, (model_class, GenerationMixin), {**model_class.__dict__}\n+        )\n+        return model_class_with_generation_mixin\n+    return model_class\n+\n+\n class _LazyAutoMapping(OrderedDict):\n     \"\"\"\n     \" A mapping config to object (model or tokenizer for instance) that will load keys and values when it is accessed."
        },
        {
            "sha": "3102ada542d57d91989704fbd2275ae6482bfef6",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ...generation import GenerationMixin\n from ...generation.logits_process import (\n     AlternatingCodebooksLogitsProcessor,\n     BarkEosPrioritizerLogitsProcessor,\n@@ -546,7 +547,7 @@ def device(self) -> torch.device:\n \n \n # GPT2-like autoregressive model\n-class BarkCausalModel(BarkPreTrainedModel):\n+class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):\n     config_class = BarkSubModelConfig\n \n     def __init__(self, config):"
        },
        {
            "sha": "2e4e6dcaeb2d11f4293d36c1f5b18c5e34d4687f",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1557,7 +1558,7 @@ def forward(\n @add_start_docstrings(\n     \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n )\n-class BartForConditionalGeneration(BartPreTrainedModel):\n+class BartForConditionalGeneration(BartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n@@ -2010,7 +2011,7 @@ def forward(self, *args, **kwargs):\n     \"\"\",\n     BART_START_DOCSTRING,\n )\n-class BartForCausalLM(BartPreTrainedModel):\n+class BartForCausalLM(BartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "b62746da5c6f15a96b67cdb2a7fbd2803a89ee90",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -28,6 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n@@ -1280,7 +1281,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BERT_START_DOCSTRING\n )\n-class BertLMHeadModel(BertPreTrainedModel):\n+class BertLMHeadModel(BertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "8496d1f6072f021b39312d9448e59ca8e5d42174",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n@@ -863,7 +864,7 @@ def _tie_weights(self):\n     \"\"\"BertGeneration Model with a `language modeling` head on top for CLM fine-tuning.\"\"\",\n     BERT_GENERATION_START_DOCSTRING,\n )\n-class BertGenerationDecoder(BertGenerationPreTrainedModel):\n+class BertGenerationDecoder(BertGenerationPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "41045cb5f0001f131fee91c2e553e8b94080fbce",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -2495,7 +2496,7 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n @add_start_docstrings(\n     \"\"\"BigBird Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BIG_BIRD_START_DOCSTRING\n )\n-class BigBirdForCausalLM(BigBirdPreTrainedModel):\n+class BigBirdForCausalLM(BigBirdPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "e26dce1edfc20f90bb68831c38eb815c9a18c1cd",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -2436,7 +2437,7 @@ def forward(\n     BIGBIRD_PEGASUS_START_DOCSTRING,\n )\n # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n-class BigBirdPegasusForConditionalGeneration(BigBirdPegasusPreTrainedModel):\n+class BigBirdPegasusForConditionalGeneration(BigBirdPegasusPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n@@ -2882,7 +2883,7 @@ def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n \n \n-class BigBirdPegasusForCausalLM(BigBirdPegasusPreTrainedModel):\n+class BigBirdPegasusForCausalLM(BigBirdPegasusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "7ad1dcbd661c325e708cecba33815c1d81a7ffd7",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -719,7 +720,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"BioGPT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BIOGPT_START_DOCSTRING\n )\n-class BioGptForCausalLM(BioGptPreTrainedModel):\n+class BioGptForCausalLM(BioGptPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"output_projection.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "4ea5926d854c98ed2730fe012e9d074cc8fa78e0",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1196,7 +1197,7 @@ def forward(\n @add_start_docstrings(\n     \"The Blenderbot Model with a language modeling head. Can be used for summarization.\", BLENDERBOT_START_DOCSTRING\n )\n-class BlenderbotForConditionalGeneration(BlenderbotPreTrainedModel):\n+class BlenderbotForConditionalGeneration(BlenderbotPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1397,7 +1398,7 @@ def forward(self, *args, **kwargs):\n \n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Blenderbot, facebook/bart-base->facebook/blenderbot-400M-distill\n-class BlenderbotForCausalLM(BlenderbotPreTrainedModel):\n+class BlenderbotForCausalLM(BlenderbotPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "3e378f483a317a59bab2b777b8adc6089b701de7",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1163,7 +1164,7 @@ def forward(\n     \"The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\",\n     BLENDERBOT_SMALL_START_DOCSTRING,\n )\n-class BlenderbotSmallForConditionalGeneration(BlenderbotSmallPreTrainedModel):\n+class BlenderbotSmallForConditionalGeneration(BlenderbotSmallPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1349,7 +1350,7 @@ def forward(self, *args, **kwargs):\n \n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->BlenderbotSmall, facebook/bart-base->facebook/blenderbot_small-90M\n-class BlenderbotSmallForCausalLM(BlenderbotSmallPreTrainedModel):\n+class BlenderbotSmallForCausalLM(BlenderbotSmallPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "aef9b8cebec91f70287e3d263766e64a7eb3d247",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn.functional import normalize\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -1035,7 +1036,7 @@ def forward(\n     \"\"\",\n     BLIP_START_DOCSTRING,\n )\n-class BlipForConditionalGeneration(BlipPreTrainedModel):\n+class BlipForConditionalGeneration(BlipPreTrainedModel, GenerationMixin):\n     config_class = BlipConfig\n     _tied_weights_keys = [\"text_decoder.cls.predictions.decoder.bias\"]\n     main_input_name = \"pixel_values\""
        },
        {
            "sha": "78384e6ce2f74b61d23e7cb70d0e32cb9286ec01",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -808,7 +809,7 @@ def forward(\n \n \n # Adapted from https://github.com/salesforce/BLIP/blob/main/models/med.py#L811\n-class BlipTextLMHeadModel(BlipTextPreTrainedModel):\n+class BlipTextLMHeadModel(BlipTextPreTrainedModel, GenerationMixin):\n     def __init__(self, config):\n         super().__init__(config)\n "
        },
        {
            "sha": "0b33572a689c2a247db4c5d2fcba34e758676589",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -2006,7 +2007,7 @@ def forward(\n     \"\"\",\n     BLIP_2_START_DOCSTRING,\n )\n-class Blip2ForConditionalGeneration(Blip2PreTrainedModel):\n+class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n "
        },
        {
            "sha": "0992a5519f953d4ad8caf6824f350b0795ece711",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -860,7 +861,7 @@ def _update_causal_mask(\n     \"\"\",\n     BLOOM_START_DOCSTRING,\n )\n-class BloomForCausalLM(BloomPreTrainedModel):\n+class BloomForCausalLM(BloomPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: BloomConfig):"
        },
        {
            "sha": "95540f96d3b6f6ca506f6a4c99f8a496c1735945",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n@@ -1544,7 +1545,7 @@ def forward(\n     \"\"\"CamemBERT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", CAMEMBERT_START_DOCSTRING\n )\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with Roberta->Camembert, ROBERTA->CAMEMBERT, FacebookAI/roberta-base->almanach/camembert-base\n-class CamembertForCausalLM(CamembertPreTrainedModel):\n+class CamembertForCausalLM(CamembertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "c631181f00c59e053e041806cd998c4927cb79dd",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -1496,7 +1497,7 @@ def _update_causal_mask(\n     \"Chameleon Model with a head on top used for outputting logits for next token prediction.\",\n     CHAMELEON_START_DOCSTRING,\n )\n-class ChameleonForConditionalGeneration(ChameleonPreTrainedModel):\n+class ChameleonForConditionalGeneration(ChameleonPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "f438226064ec2de3bda0f40ea170183c5bb99ef0",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...generation import GenerationConfig\n+from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1278,7 +1278,7 @@ def forward(\n     \"The CLVP decoder model with a language modelling head on top.\",\n     CLVP_START_DOCSTRING,\n )\n-class ClvpForCausalLM(ClvpPreTrainedModel):\n+class ClvpForCausalLM(ClvpPreTrainedModel, GenerationMixin):\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1509,7 +1509,7 @@ def _reorder_cache(\n     \"together to filter out the best speech_ids.\",\n     CLVP_START_DOCSTRING,\n )\n-class ClvpModelForConditionalGeneration(ClvpPreTrainedModel):\n+class ClvpModelForConditionalGeneration(ClvpPreTrainedModel, GenerationMixin):\n     config_class = ClvpConfig\n \n     def __init__(self, config: ClvpConfig):"
        },
        {
            "sha": "7d6f64d6461a2e80fbf0f7f8964ca7a9c8db6600",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -702,7 +703,7 @@ def _update_causal_mask(\n     \"\"\",\n     CODEGEN_START_DOCSTRING,\n )\n-class CodeGenForCausalLM(CodeGenPreTrainedModel):\n+class CodeGenForCausalLM(CodeGenPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "12586af23f0d7b358c4c60c75e9e776cb7cd7808",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -32,6 +32,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1068,7 +1069,7 @@ def _update_causal_mask(\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere\n-class CohereForCausalLM(CoherePreTrainedModel):\n+class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Ignore copy"
        },
        {
            "sha": "964d0bbfd1456b0c5bfd69aa503b807ef341b5e3",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n@@ -736,7 +737,7 @@ def forward(\n     \"\"\",\n     CPMANT_START_DOCSTRING,\n )\n-class CpmAntForCausalLM(CpmAntPreTrainedModel):\n+class CpmAntForCausalLM(CpmAntPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: CpmAntConfig):"
        },
        {
            "sha": "6d921621d47dcbbbb49eb3876b8dee7b2c2e2d4d",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_linear_layer\n@@ -503,7 +504,7 @@ def forward(\n     \"\"\",\n     CTRL_START_DOCSTRING,\n )\n-class CTRLLMHeadModel(CTRLPreTrainedModel):\n+class CTRLLMHeadModel(CTRLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "fcddeab7a595ea167a06bee2a65aef9abc181d86",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -866,7 +867,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"Data2VecText Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", DATA2VECTEXT_START_DOCSTRING\n )\n-class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel):\n+class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "46de60e24f1a048e618cfffff68f44795ba5e981",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -1227,7 +1228,7 @@ def _update_causal_mask(\n \n \n @add_start_docstrings(\"The DBRX Model transformer for causal language modeling.\", DBRX_START_DOCSTRING)\n-class DbrxForCausalLM(DbrxPreTrainedModel):\n+class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n     def __init__(self, config: DbrxConfig):\n         super().__init__(config)\n         self.transformer = DbrxModel(config)"
        },
        {
            "sha": "a200d716d451e2eb649f084a9b40cc252c4433b5",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1524,7 +1525,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"ELECTRA Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ELECTRA_START_DOCSTRING\n )\n-class ElectraForCausalLM(ElectraPreTrainedModel):\n+class ElectraForCausalLM(ElectraPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"generator_lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "6d81c97da02302ca5a4c7db56f28a784f6ca3c8d",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -1081,7 +1082,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"Ernie Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ERNIE_START_DOCSTRING\n )\n-class ErnieForCausalLM(ErniePreTrainedModel):\n+class ErnieForCausalLM(ErniePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n     # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.__init__ with BertLMHeadModel->ErnieForCausalLM,Bert->Ernie,bert->ernie"
        },
        {
            "sha": "270845c20aae2e14588b2099ad944dedfd37a630",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import get_activation\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n@@ -1239,7 +1240,7 @@ def _update_causal_mask(\n     \"The Falcon Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\",\n     FALCON_START_DOCSTRING,\n )\n-class FalconForCausalLM(FalconPreTrainedModel):\n+class FalconForCausalLM(FalconPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: FalconConfig):"
        },
        {
            "sha": "011197d9854273c22f7c356d03bc832b502681ea",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import MambaCache\n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -717,7 +718,7 @@ def forward(\n     FALCONMAMBA_START_DOCSTRING,\n )\n # Copied from transformers.models.mamba.modeling_mamba.MambaForCausalLM with MAMBA->FALCONMAMBA,Mamba->FalconMamba,mamba->falcon_mamba,FalconMambaCache->MambaCache\n-class FalconMambaForCausalLM(FalconMambaPreTrainedModel):\n+class FalconMambaForCausalLM(FalconMambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "ef1501e780350dbf538ac1a36c621db6e2311911",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -644,7 +645,7 @@ def forward(\n     FLAUBERT_START_DOCSTRING,\n )\n # Copied transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel with XLM_INPUTS->FLAUBERT_INPUTS,XLM->Flaubert\n-class FlaubertWithLMHeadModel(FlaubertPreTrainedModel):\n+class FlaubertWithLMHeadModel(FlaubertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"pred_layer.proj.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "4d50f9bb5925b448e449a0ff2d371a22ceafd3dc",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -35,6 +35,7 @@\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1173,7 +1174,7 @@ def set_output_embeddings(self, value):\n @add_start_docstrings(\n     \"The FSMT Model with a language modeling head. Can be used for summarization.\", FSMT_START_DOCSTRING\n )\n-class FSMTForConditionalGeneration(PretrainedFSMTModel):\n+class FSMTForConditionalGeneration(PretrainedFSMTModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"decoder.output_projection.weight\"]\n "
        },
        {
            "sha": "0aabbf6b3654b740de4c14e6b55ae8d873cbebce",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -20,6 +20,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n+from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...models.auto.modeling_auto import AutoModelForCausalLM\n@@ -145,7 +146,7 @@ def _init_weights(self, module):\n     \"Fuyu Model with a language modeling head on top for causal language model conditioned on image patches and text.\",\n     FUYU_START_DOCSTRING,\n )\n-class FuyuForCausalLM(FuyuPreTrainedModel):\n+class FuyuForCausalLM(FuyuPreTrainedModel, GenerationMixin):\n     def __init__(self, config: FuyuConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id"
        },
        {
            "sha": "dcc43bc74aece9805aeb2c2b35ab90337af08157",
            "filename": "src/transformers/models/gemma/diff_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -34,6 +34,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n@@ -527,7 +528,7 @@ def forward(\n \n \n # Example where we ony modify the docstring and call super\n-class GemmaForCausalLM(LlamaForCausalLM):\n+class GemmaForCausalLM(LlamaForCausalLM, GenerationMixin):\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "8d9bb88686de244c19452edface27f0045194fa5",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -988,7 +989,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class GemmaForCausalLM(GemmaPreTrainedModel):\n+class GemmaForCausalLM(GemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "a66ce3160b5fd1a4d4a09865c161aa1cfa4a458a",
            "filename": "src/transformers/models/gemma2/diff_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fdiff_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fdiff_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fdiff_gemma2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -33,6 +33,7 @@\n )\n \n from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n \n@@ -473,7 +474,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class Gemma2ForCausalLM(GemmaForCausalLM):\n+class Gemma2ForCausalLM(GemmaForCausalLM, GenerationMixin):\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "6b55500739b40bd18eb12ff885a412ec50b693cf",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -28,6 +28,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -931,7 +932,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class Gemma2ForCausalLM(Gemma2PreTrainedModel):\n+class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "59d3a406ec35356765bb8990e42ebf1a22d6ab92",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...file_utils import ModelOutput\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1324,7 +1325,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"GIT Model with a `language modeling` head on top for autoregressive language modeling.\"\"\", GIT_START_DOCSTRING\n )\n-class GitForCausalLM(GitPreTrainedModel):\n+class GitForCausalLM(GitPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"output.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "e99f4b126246d814c3bd62749f3cc4b9234fd732",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -28,6 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1182,7 +1183,7 @@ def forward(\n     \"\"\",\n     GPT2_START_DOCSTRING,\n )\n-class GPT2LMHeadModel(GPT2PreTrainedModel):\n+class GPT2LMHeadModel(GPT2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):\n@@ -1384,7 +1385,7 @@ def _reorder_cache(\n \"\"\",\n     GPT2_START_DOCSTRING,\n )\n-class GPT2DoubleHeadsModel(GPT2PreTrainedModel):\n+class GPT2DoubleHeadsModel(GPT2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "ca1c03fcd9f911cd69dcc68059bb453acd31a77d",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1040,7 +1041,7 @@ def forward(\n     \"\"\",\n     GPT_BIGCODE_START_DOCSTRING,\n )\n-class GPTBigCodeForCausalLM(GPTBigCodePreTrainedModel):\n+class GPTBigCodeForCausalLM(GPTBigCodePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "2fae1753154ccfbd47de89a8e52664a63a2e939e",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -917,7 +918,7 @@ def _update_causal_mask(\n     \"\"\",\n     GPT_NEO_START_DOCSTRING,\n )\n-class GPTNeoForCausalLM(GPTNeoPreTrainedModel):\n+class GPTNeoForCausalLM(GPTNeoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "c1b2aa899985c86a879465acfa32342f2b752181",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n     add_start_docstrings_to_model_forward,\n     replace_return_docstrings,\n )\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1110,7 +1111,7 @@ def _update_causal_mask(\n @add_start_docstrings(\n     \"\"\"GPTNeoX Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", GPT_NEOX_START_DOCSTRING\n )\n-class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel):\n+class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"embed_out.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "3db2099511bc6b7d55659099f285920466fe5130",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n@@ -815,7 +816,7 @@ def _update_causal_mask(\n     \"\"\"GPTNeoXJapanese Model with a `language modeling` head on top for Classifier Model fine-tuning.\"\"\",\n     GPT_NEOX_JAPANESE_START_DOCSTRING,\n )\n-class GPTNeoXJapaneseForCausalLM(GPTNeoXJapanesePreTrainedModel):\n+class GPTNeoXJapaneseForCausalLM(GPTNeoXJapanesePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"embed_out.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "9eeb26c5e403e04012c99cabe3a72b6e036a9acf",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1011,7 +1012,7 @@ def _update_causal_mask(\n     \"\"\",\n     GPTJ_START_DOCSTRING,\n )\n-class GPTJForCausalLM(GPTJPreTrainedModel):\n+class GPTJForCausalLM(GPTJPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "9a8d4570e7befe56491fe8306153be6d37c7e01b",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -1004,7 +1005,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class GraniteForCausalLM(GranitePreTrainedModel):\n+class GraniteForCausalLM(GranitePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Granite"
        },
        {
            "sha": "d724485990b938990aad87f3708c897c1d8564cc",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -1234,7 +1235,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class GraniteMoeForCausalLM(GraniteMoePreTrainedModel):\n+class GraniteMoeForCausalLM(GraniteMoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: GraniteMoeConfig):"
        },
        {
            "sha": "9273d91ac401ff3eedc6545707dd07500d4dc1e6",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,11 +23,12 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -1450,7 +1451,7 @@ def forward(\n     \"\"\"The Idefics2 Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top. \"\"\",\n     IDEFICS2_START_DOCSTRING,\n )\n-class Idefics2ForConditionalGeneration(Idefics2PreTrainedModel):\n+class Idefics2ForConditionalGeneration(Idefics2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "a027876b43d3696c68ba5c0872f8a9c6456dccc1",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -880,7 +881,7 @@ def forward(\n     \"\"\",\n     IMAGEGPT_START_DOCSTRING,\n )\n-class ImageGPTForCausalImageModeling(ImageGPTPreTrainedModel):\n+class ImageGPTForCausalImageModeling(ImageGPTPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: ImageGPTConfig):"
        },
        {
            "sha": "dff897f59d2d267ef28ebfa8a64b3104f127b817",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1283,7 +1284,7 @@ def forward(\n     \"\"\",\n     INSTRUCTBLIP_START_DOCSTRING,\n )\n-class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel):\n+class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipConfig\n     main_input_name = \"pixel_values\"\n "
        },
        {
            "sha": "be569abc9137c2e29c9ef8d4fe9ae4df81488132",
            "filename": "src/transformers/models/instructblipvideo/diff_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fdiff_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fdiff_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fdiff_instructblipvideo.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -45,6 +45,7 @@\n     InstructBlipVisionModel,\n )\n \n+from ...generation import GenerationMixin\n from ...utils import logging\n \n \n@@ -128,7 +129,7 @@ class InstructBlipVideoQFormerModel(InstructBlipQFormerModel):\n     pass\n \n \n-class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):\n+class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration, GenerationMixin):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "bcc299b1ba7831ef89b950c0ff7d0f5df8ceeaea",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1292,7 +1293,7 @@ def forward(\n     \"\"\",\n     INSTRUCTBLIPVIDEO_START_DOCSTRING,\n )\n-class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel):\n+class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipVideoConfig\n     main_input_name = \"pixel_values\"\n "
        },
        {
            "sha": "4b8630efbfa9463cf9e1150630ee7e3b4c0f984f",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache  # we need __iter__ and __len__ of pkv\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n@@ -1424,7 +1425,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n # Adapted from transformers.models.mixtral.modeling_mixtral.MixtralForCausalLM with MIXTRAL->JAMBA, Mixtral->Jamba\n-class JambaForCausalLM(JambaPreTrainedModel):\n+class JambaForCausalLM(JambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: JambaConfig):"
        },
        {
            "sha": "e9c06960499136ee2fc0ab1c08c45582ccc94f42",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n@@ -1202,7 +1203,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class JetMoeForCausalLM(JetMoePreTrainedModel):\n+class JetMoeForCausalLM(JetMoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "90e21ed2f5582b10bba0784ccb85e7cc55e0d5bc",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1521,7 +1522,7 @@ def forward(\n     \"\"\",\n     KOSMOS2_START_DOCSTRING,\n )\n-class Kosmos2TextForCausalLM(Kosmos2PreTrainedModel):\n+class Kosmos2TextForCausalLM(Kosmos2PreTrainedModel, GenerationMixin):\n     config_class = Kosmos2TextConfig\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -1864,7 +1865,7 @@ def forward(\n     \"\"\",\n     KOSMOS2_START_DOCSTRING,\n )\n-class Kosmos2ForConditionalGeneration(Kosmos2PreTrainedModel):\n+class Kosmos2ForConditionalGeneration(Kosmos2PreTrainedModel, GenerationMixin):\n     config_class = Kosmos2Config\n     main_input_name = \"pixel_values\"\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]"
        },
        {
            "sha": "f96bfd82b52638a1aa443ccb1b840962849bbd31",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -2298,7 +2299,7 @@ def forward(\n @add_start_docstrings(\n     \"The LED Model with a language modeling head. Can be used for summarization.\", LED_START_DOCSTRING\n )\n-class LEDForConditionalGeneration(LEDPreTrainedModel):\n+class LEDForConditionalGeneration(LEDPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"led\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\", \"lm_head.weight\"]"
        },
        {
            "sha": "73b6bcd8b4a4d7c51b743f23c7f3e49fff6cf09b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -28,6 +28,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -1101,7 +1102,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class LlamaForCausalLM(LlamaPreTrainedModel):\n+class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "092008873d1e27f3d96fd8457e11522fcf73de23",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -21,9 +21,10 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -237,7 +238,7 @@ def _supports_sdpa(self):\n     \"\"\"The LLAVA model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_START_DOCSTRING,\n )\n-class LlavaForConditionalGeneration(LlavaPreTrainedModel):\n+class LlavaForConditionalGeneration(LlavaPreTrainedModel, GenerationMixin):\n     def __init__(self, config: LlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "a96b0d89420437d975d697f243a40a38b9070813",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,10 +23,11 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -349,7 +350,7 @@ def _supports_sdpa(self):\n     \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_NEXT_START_DOCSTRING,\n )\n-class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel):\n+class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel, GenerationMixin):\n     def __init__(self, config: LlavaNextConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "c5ca2bf00324d4965b9ff9d544f6f2b090c5e082",
            "filename": "src/transformers/models/llava_next_video/diff_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n     image_size_to_num_patches,\n )\n \n+from ...generation import GenerationMixin\n from ...utils import (\n     logging,\n     replace_return_docstrings,\n@@ -218,7 +219,7 @@ class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n     pass\n \n \n-class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n+class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration, GenerationMixin):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)\n         self.vision_resampler = LlavaNextVideoPooler(config)"
        },
        {
            "sha": "7ad9e0769eb35e7fbc43434e72f2b8f04b452f25",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,10 +29,11 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -387,7 +388,7 @@ def _supports_sdpa(self):\n     \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_NEXT_VIDEO_START_DOCSTRING,\n )\n-class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel):\n+class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel, GenerationMixin):\n     def __init__(\n         self,\n         config: LlavaNextVideoConfig,"
        },
        {
            "sha": "948efbc922b70d85026b7282dc5a228b3131c958",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,10 +23,11 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     logging,\n@@ -358,7 +359,7 @@ def _init_weights(self, module):\n     \"\"\"The LLaVA-Onevision model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_ONEVISION_START_DOCSTRING,\n )\n-class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel):\n+class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):\n     def __init__(self, config: LlavaOnevisionConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config("
        },
        {
            "sha": "8f9385c0fe76eda40593333135db8b9edeacff40",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1900,7 +1901,7 @@ def forward(\n \n \n @add_start_docstrings(\"\"\"LONGT5 Model with a `language modeling` head on top.\"\"\", LONGT5_START_DOCSTRING)\n-class LongT5ForConditionalGeneration(LongT5PreTrainedModel):\n+class LongT5ForConditionalGeneration(LongT5PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_unexpected = [\n         r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n     ]"
        },
        {
            "sha": "86a4378da29cdbcd7b9ed2c27647a56106932631",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -1342,7 +1343,7 @@ def forward(\n @add_start_docstrings(\n     \"The M2M100 Model with a language modeling head. Can be used for summarization.\", M2M_100_START_DOCSTRING\n )\n-class M2M100ForConditionalGeneration(M2M100PreTrainedModel):\n+class M2M100ForConditionalGeneration(M2M100PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n "
        },
        {
            "sha": "6bed1caab23ab75e48aa3beba6c3467fdfb93bc4",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import MambaCache\n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -657,7 +658,7 @@ def forward(\n     \"\"\",\n     MAMBA_START_DOCSTRING,\n )\n-class MambaForCausalLM(MambaPreTrainedModel):\n+class MambaForCausalLM(MambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "01074af38a510b23351e7e8809d443763c296de1",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -932,7 +933,7 @@ def forward(\n     \"\"\",\n     MAMBA2_START_DOCSTRING,\n )\n-class Mamba2ForCausalLM(Mamba2PreTrainedModel):\n+class Mamba2ForCausalLM(Mamba2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = []\n \n     def __init__(self, config):"
        },
        {
            "sha": "cb26bb11e094cd958b4eb94add15ff1185546349",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1224,7 +1225,7 @@ def forward(\n @add_start_docstrings(\n     \"The Marian Model with a language modeling head. Can be used for summarization.\", MARIAN_START_DOCSTRING\n )\n-class MarianMTModel(MarianPreTrainedModel):\n+class MarianMTModel(MarianPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\n         \"final_logits_bias\",\n@@ -1504,7 +1505,7 @@ def forward(self, *args, **kwargs):\n \n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->Marian, facebook/bart-base->Helsinki-NLP/opus-mt-fr-en\n-class MarianForCausalLM(MarianPreTrainedModel):\n+class MarianForCausalLM(MarianPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "3f2d6cb8e2ba8d8c822632bdae5a71a062a41e0f",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1526,7 +1527,7 @@ def forward(\n     \"The MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\",\n     MBART_START_DOCSTRING,\n )\n-class MBartForConditionalGeneration(MBartPreTrainedModel):\n+class MBartForConditionalGeneration(MBartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1967,7 +1968,7 @@ def forward(self, *args, **kwargs):\n \n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->MBart, facebook/bart-base->facebook/mbart-large-cc25\n-class MBartForCausalLM(MBartPreTrainedModel):\n+class MBartForCausalLM(MBartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "20506f91bcbcb20127a02699aa3fcc6ee5a4442b",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -27,6 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -1110,7 +1111,7 @@ def forward(\n     \"\"\"MegatronBert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\",\n     MEGATRON_BERT_START_DOCSTRING,\n )\n-class MegatronBertForCausalLM(MegatronBertPreTrainedModel):\n+class MegatronBertForCausalLM(MegatronBertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "ffa1a18307e9825bd8bb50b44ca7f5d20de0baf3",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -950,7 +951,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class MistralForCausalLM(MistralPreTrainedModel):\n+class MistralForCausalLM(MistralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "a1786fbb17e3c5c74f8e016781f3593f1df2ef4e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n@@ -1186,7 +1187,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class MixtralForCausalLM(MixtralPreTrainedModel):\n+class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "9c826c370b752aeaf68b6db3c76e17e1f35803d0",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import functional as F\n \n from ...file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -500,7 +501,7 @@ def forward(\n     \"\"\",\n     MPT_START_DOCSTRING,\n )\n-class MptForCausalLM(MptPreTrainedModel):\n+class MptForCausalLM(MptPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: MptConfig):"
        },
        {
            "sha": "6a7406f11b5b56c708be4d00dc4ec36e38a67db0",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1550,7 +1551,7 @@ def forward(\n \n \n @add_start_docstrings(\"\"\"MT5 Model with a `language modeling` head on top.\"\"\", MT5_START_DOCSTRING)\n-class MT5ForConditionalGeneration(MT5PreTrainedModel):\n+class MT5ForConditionalGeneration(MT5PreTrainedModel, GenerationMixin):\n     r\"\"\"\n     Examples:\n "
        },
        {
            "sha": "3109c4fc2431182861e7ac01e1a208ae8a1a7828",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,9 +26,14 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...generation.configuration_utils import GenerationConfig, GenerationMode\n-from ...generation.logits_process import ClassifierFreeGuidanceLogitsProcessor, LogitsProcessorList\n-from ...generation.stopping_criteria import StoppingCriteriaList\n+from ...generation import (\n+    ClassifierFreeGuidanceLogitsProcessor,\n+    GenerationConfig,\n+    GenerationMixin,\n+    GenerationMode,\n+    LogitsProcessorList,\n+    StoppingCriteriaList,\n+)\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1206,7 +1211,7 @@ def forward(\n     \"The MusicGen decoder model with a language modelling head on top.\",\n     MUSICGEN_START_DOCSTRING,\n )\n-class MusicgenForCausalLM(MusicgenPreTrainedModel):\n+class MusicgenForCausalLM(MusicgenPreTrainedModel, GenerationMixin):\n     def __init__(self, config: MusicgenDecoderConfig):\n         super().__init__(config)\n \n@@ -1658,7 +1663,7 @@ def generate(\n     \"for music generation tasks with one or both of text and audio prompts.\",\n     MUSICGEN_START_DOCSTRING,\n )\n-class MusicgenForConditionalGeneration(PreTrainedModel):\n+class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     config_class = MusicgenConfig\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\""
        },
        {
            "sha": "c8345870b2537e0a0827c8b2edb3f513b6591f9b",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,9 +26,14 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...generation.configuration_utils import GenerationConfig, GenerationMode\n-from ...generation.logits_process import ClassifierFreeGuidanceLogitsProcessor, LogitsProcessorList\n-from ...generation.stopping_criteria import StoppingCriteriaList\n+from ...generation import (\n+    ClassifierFreeGuidanceLogitsProcessor,\n+    GenerationConfig,\n+    GenerationMixin,\n+    GenerationMode,\n+    LogitsProcessorList,\n+    StoppingCriteriaList,\n+)\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1117,7 +1122,7 @@ def forward(\n     MUSICGEN_MELODY_START_DOCSTRING,\n )\n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody,MusicGen->Musicgen Melody\n-class MusicgenMelodyForCausalLM(MusicgenMelodyPreTrainedModel):\n+class MusicgenMelodyForCausalLM(MusicgenMelodyPreTrainedModel, GenerationMixin):\n     def __init__(self, config: MusicgenMelodyDecoderConfig):\n         super().__init__(config)\n \n@@ -1585,7 +1590,7 @@ def generate(\n         decoder (`Optional[MusicgenMelodyForCausalLM]`, *optional*): MusicGen Melody decoder used to generate audio codes.\n     \"\"\",\n )\n-class MusicgenMelodyForConditionalGeneration(PreTrainedModel):\n+class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     config_class = MusicgenMelodyConfig\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "c47c4b26b539f7eb93bd967e0dd338e9079c58e2",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1351,7 +1352,7 @@ def forward(\n @add_start_docstrings(\n     \"The MVP Model with a language modeling head. Can be used for various text generation tasks.\", MVP_START_DOCSTRING\n )\n-class MvpForConditionalGeneration(MvpPreTrainedModel):\n+class MvpForConditionalGeneration(MvpPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n \n     def __init__(self, config: MvpConfig):\n@@ -1791,7 +1792,7 @@ def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n \n \n-class MvpForCausalLM(MvpPreTrainedModel):\n+class MvpForCausalLM(MvpPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "aa699853d557623351973af12fa0f7aac68e1eee",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n@@ -980,7 +981,7 @@ def _update_causal_mask(\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n-class NemotronForCausalLM(NemotronPreTrainedModel):\n+class NemotronForCausalLM(NemotronPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "c33844da0f55b8f68b2263db90194bb4a9892ed1",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -1604,7 +1605,7 @@ def forward(\n @add_start_docstrings(\n     \"The NllbMoe Model with a language modeling head. Can be used for summarization.\", NLLB_MOE_START_DOCSTRING\n )\n-class NllbMoeForConditionalGeneration(NllbMoePreTrainedModel):\n+class NllbMoeForConditionalGeneration(NllbMoePreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n "
        },
        {
            "sha": "a44b7d2a0a4c4d884d39e3a9aa8b65dadb7eb990",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1022,7 +1023,7 @@ def _update_causal_mask(\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO,Llama->Olmo\n-class OlmoForCausalLM(OlmoPreTrainedModel):\n+class OlmoForCausalLM(OlmoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "d30cace3a7055da62802a8a1c3b0617b5b862d5e",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n@@ -1173,7 +1174,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class OlmoeForCausalLM(OlmoePreTrainedModel):\n+class OlmoeForCausalLM(OlmoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "0aa02a6f5d842417b35b879cafc46604effcc73c",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu_new, silu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel, SequenceSummary\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n@@ -524,7 +525,7 @@ def forward(\n     \"\"\",\n     OPENAI_GPT_START_DOCSTRING,\n )\n-class OpenAIGPTLMHeadModel(OpenAIGPTPreTrainedModel):\n+class OpenAIGPTLMHeadModel(OpenAIGPTPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "f7782b8f6172b9e6286cd3b2d4e76e01e8089d93",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -882,7 +883,7 @@ def forward(\n         )\n \n \n-class OPTForCausalLM(OPTPreTrainedModel):\n+class OPTForCausalLM(OPTPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "b5fddce1d6a914dd1b3c14dc2e4091ecf2849225",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...cache_utils import Cache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -302,7 +303,7 @@ def _supports_sdpa(self):\n     \"\"\"The PALIGEMMA model which consists of a vision backbone and a language model.\"\"\",\n     PALIGEMMA_START_DOCSTRING,\n )\n-class PaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel):\n+class PaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel, GenerationMixin):\n     def __init__(self, config: PaliGemmaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)"
        },
        {
            "sha": "03d1574e9be28e7746c7313fd54fbe293a50ad1f",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1244,7 +1245,7 @@ def forward(\n @add_start_docstrings(\n     \"The PEGASUS Model with a language modeling head. Can be used for summarization.\", PEGASUS_START_DOCSTRING\n )\n-class PegasusForConditionalGeneration(PegasusPreTrainedModel):\n+class PegasusForConditionalGeneration(PegasusPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1456,7 +1457,7 @@ def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n \n \n-class PegasusForCausalLM(PegasusPreTrainedModel):\n+class PegasusForCausalLM(PegasusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "77c0b32e6433c49d42bb817192d8f6638b8d077a",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1464,7 +1465,7 @@ def forward(\n \n \n @add_start_docstrings(\"The PEGASUS-X for conditional generation (e.g. summarization).\", PEGASUS_X_START_DOCSTRING)\n-class PegasusXForConditionalGeneration(PegasusXPreTrainedModel):\n+class PegasusXForConditionalGeneration(PegasusXPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n "
        },
        {
            "sha": "4f122e14284d83d5a055754030f1cb80ade03bd5",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -847,7 +848,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class PersimmonForCausalLM(PersimmonPreTrainedModel):\n+class PersimmonForCausalLM(PersimmonPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with LLAMA->PERSIMMON,Llama->Persimmon"
        },
        {
            "sha": "03ed19bc34ac84bc33e25c8ed7b3758f568f0487",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1139,7 +1140,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class PhiForCausalLM(PhiPreTrainedModel):\n+class PhiForCausalLM(PhiPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi,bias=False->bias=True"
        },
        {
            "sha": "12ee9f017f814981c536354ebc89702f43302efe",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1160,7 +1161,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class Phi3ForCausalLM(Phi3PreTrainedModel):\n+class Phi3ForCausalLM(Phi3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi3"
        },
        {
            "sha": "f209d7d8828785911f6749fa5cba377d05e79493",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -1553,7 +1554,7 @@ def forward(\n     \"A conditional generation model with a language modeling head. Can be used for sequence generation tasks.\",\n     PIX2STRUCT_START_DOCSTRING,\n )\n-class Pix2StructForConditionalGeneration(Pix2StructPreTrainedModel):\n+class Pix2StructForConditionalGeneration(Pix2StructPreTrainedModel, GenerationMixin):\n     config_class = Pix2StructConfig\n     main_input_name = \"flattened_patches\"\n     _tied_weights_keys = [\"decoder.lm_head.weight\"]"
        },
        {
            "sha": "d15e079770a3ed5a6c76961bab95618912eb3b17",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1254,7 +1255,7 @@ def forward(\n     \"The PLBART Model with a language modeling head. Can be used for code-to-text, text-to-code and code-to-code.\",\n     PLBART_START_DOCSTRING,\n )\n-class PLBartForConditionalGeneration(PLBartPreTrainedModel):\n+class PLBartForConditionalGeneration(PLBartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1568,7 +1569,7 @@ def forward(self, *args, **kwargs):\n \n \n # Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->PLBart, facebook/bart-base->uclanlp/plbart-base\n-class PLBartForCausalLM(PLBartPreTrainedModel):\n+class PLBartForCausalLM(PLBartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "e6488898e8a93844924195f16b5a6bb405136acd",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from transformers.generation import GenerationConfig\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1001,7 +1002,7 @@ def forward(self, feature, index_value, embedding_offset):\n \n \n @add_start_docstrings(\"\"\"Pop2Piano Model with a `language modeling` head on top.\"\"\", Pop2Piano_START_DOCSTRING)\n-class Pop2PianoForConditionalGeneration(Pop2PianoPreTrainedModel):\n+class Pop2PianoForConditionalGeneration(Pop2PianoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n \n     def __init__(self, config: Pop2PianoConfig):"
        },
        {
            "sha": "7d23088f6e575e1b4bbbfbd420e02adec6c3ce6d",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import LayerNorm\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -1856,7 +1857,7 @@ def forward(\n     \"The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.\",\n     PROPHETNET_START_DOCSTRING,\n )\n-class ProphetNetForConditionalGeneration(ProphetNetPreTrainedModel):\n+class ProphetNetForConditionalGeneration(ProphetNetPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"encoder.word_embeddings.weight\", \"decoder.word_embeddings.weight\", \"lm_head.weight\"]\n \n     def __init__(self, config: ProphetNetConfig):\n@@ -2073,7 +2074,7 @@ def get_decoder(self):\n     \" language modeling.\",\n     PROPHETNET_START_DOCSTRING,\n )\n-class ProphetNetForCausalLM(ProphetNetPreTrainedModel):\n+class ProphetNetForCausalLM(ProphetNetPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\n         \"prophetnet.word_embeddings.weight\",\n         \"prophetnet.decoder.word_embeddings.weight\","
        },
        {
            "sha": "10c0b6f38669da83bc4e9da6ff5dad5122cdd051",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1078,7 +1079,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class Qwen2ForCausalLM(Qwen2PreTrainedModel):\n+class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "bf48e1c6a97ef97d30aebf73d30b8ac354c65c11",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,10 +22,11 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...cache_utils import Cache, EncoderDecoderCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -855,7 +856,7 @@ def forward(self, audio_features):\n     \"\"\"The QWEN2AUDIO model which consists of a audio backbone and a language model.\"\"\",\n     QWEN2AUDIO_START_DOCSTRING,\n )\n-class Qwen2AudioForConditionalGeneration(Qwen2AudioPreTrainedModel):\n+class Qwen2AudioForConditionalGeneration(Qwen2AudioPreTrainedModel, GenerationMixin):\n     def __init__(self, config: Qwen2AudioConfig):\n         super().__init__(config)\n         self.audio_tower = AutoModel.from_config(config.audio_config, attn_implementation=config._attn_implementation)"
        },
        {
            "sha": "1b28e9baf25f6646d9460a514aa975b286c9f9b5",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n@@ -1253,7 +1254,7 @@ def _update_causal_mask(\n         return causal_mask\n \n \n-class Qwen2MoeForCausalLM(Qwen2MoePreTrainedModel):\n+class Qwen2MoeForCausalLM(Qwen2MoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "938ec4d5e423626f2aeae4c3e4985c2c0355b12c",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -31,6 +31,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n@@ -1416,7 +1417,7 @@ def _update_causal_mask(\n \"\"\"\n \n \n-class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel):\n+class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "e0492948998434480c75ca6b4a8ffe95516f34f3",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithNoAttention, CausalLMOutput\n from ...modeling_utils import PreTrainedModel\n@@ -777,7 +778,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->RECURRENTGEMMA,Llama->RecurrentGemma,llama->gemma\n-class RecurrentGemmaForCausalLM(RecurrentGemmaPreTrainedModel):\n+class RecurrentGemmaForCausalLM(RecurrentGemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "37b675539e66cf2a36bc8f50af347b03476c7334",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutput, MaskedLMOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n@@ -2183,7 +2184,7 @@ def _pad_to_mult_of_chunk_length(\n \n \n @add_start_docstrings(\"\"\"Reformer Model with a `language modeling` head on top.\"\"\", REFORMER_START_DOCSTRING)\n-class ReformerModelWithLMHead(ReformerPreTrainedModel):\n+class ReformerModelWithLMHead(ReformerPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "99016c1be429cdf329327161463d8fb1cd2345ac",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -1002,7 +1003,7 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n @add_start_docstrings(\n     \"\"\"RemBERT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", REMBERT_START_DOCSTRING\n )\n-class RemBertForCausalLM(RemBertPreTrainedModel):\n+class RemBertForCausalLM(RemBertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "91500e1926d753534843a4e2dde7b2e04cf0a142",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n@@ -1003,7 +1004,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ROBERTA_START_DOCSTRING\n )\n-class RobertaForCausalLM(RobertaPreTrainedModel):\n+class RobertaForCausalLM(RobertaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "9ed9b11d9431dfd7ccf7f9bce8c3ebf59f70c0fe",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -855,7 +856,7 @@ def forward(\n     ROBERTA_PRELAYERNORM_START_DOCSTRING,\n )\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with FacebookAI/roberta-base->andreasmadsen/efficient_mlm_m0.40,ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm, RobertaPreLayerNormTokenizer->RobertaTokenizer\n-class RobertaPreLayerNormForCausalLM(RobertaPreLayerNormPreTrainedModel):\n+class RobertaPreLayerNormForCausalLM(RobertaPreLayerNormPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "2969f7f1a3d0a863636d835bad137bb7f6713ad3",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -1403,7 +1404,7 @@ def prepare_inputs_for_generation(\n @add_start_docstrings(\n     \"\"\"RoCBert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ROC_BERT_START_DOCSTRING\n )\n-class RoCBertForCausalLM(RoCBertPreTrainedModel):\n+class RoCBertForCausalLM(RoCBertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n \n     # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.__init__ with BertLMHeadModel->RoCBertForCausalLM,Bert->RoCBert,bert->roc_bert"
        },
        {
            "sha": "c98b525abe081b27e27b76c3176afd58bf58c1fc",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -1033,7 +1034,7 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n @add_start_docstrings(\n     \"\"\"RoFormer Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ROFORMER_START_DOCSTRING\n )\n-class RoFormerForCausalLM(RoFormerPreTrainedModel):\n+class RoFormerForCausalLM(RoFormerPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "8361afbf727b649509201033ff83395b468f22c1",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -751,7 +752,7 @@ def _bnb_4bit_dequantize_and_rescale(self, target_layer, block_id):\n     \"\"\",\n     RWKV_START_DOCSTRING,\n )\n-class RwkvForCausalLM(RwkvPreTrainedModel):\n+class RwkvForCausalLM(RwkvPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "8e226d92a10580f62cf1dc38261ac4110cb7726c",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...deepspeed import is_deepspeed_zero3_enabled\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -2150,7 +2151,7 @@ def forward(\n         embed_tokens_decoder (`nn.Embedding`, *optional*): input embedding of the decoder.\n     \"\"\",\n )\n-class SeamlessM4TTextToUnitForConditionalGeneration(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TTextToUnitForConditionalGeneration(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\n         \"vocoder\",\n         \"speech_encoder\",\n@@ -2664,7 +2665,7 @@ def remove_weight_norm(self):\n     \"The text-to-text SeamlessM4T Model transformer which can be used for T2TT.\",\n     SEAMLESS_M4T_START_DOCSTRING,\n )\n-class SeamlessM4TForTextToText(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TForTextToText(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_ids\"\n "
        },
        {
            "sha": "aa710ad95266ff046e3d67aded2bcb0ea7388847",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...deepspeed import is_deepspeed_zero3_enabled\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -2439,7 +2440,7 @@ def forward(\n         embed_tokens_decoder (`nn.Embedding`, *optional*): input embedding of the decoder.\n     \"\"\",\n )\n-class SeamlessM4Tv2TextToUnitForConditionalGeneration(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2TextToUnitForConditionalGeneration(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\n         \"vocoder\",\n         \"speech_encoder\",\n@@ -2922,7 +2923,7 @@ def remove_weight_norm(self):\n     SEAMLESS_M4T_V2_START_DOCSTRING,\n )\n # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToText with SeamlessM4T->SeamlessM4Tv2,SeamlessM4Tv2Tokenizer->SeamlessM4TTokenizer, SeamlessM4Tv2Processor->SeamlessM4TProcessor\n-class SeamlessM4Tv2ForTextToText(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2ForTextToText(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_ids\"\n "
        },
        {
            "sha": "bdd532fa25e82a38b3edb339a1eda5623351f5df",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1207,7 +1208,7 @@ def forward(\n     \"The Speech2Text Model with a language modeling head. Can be used for summarization.\",\n     SPEECH_TO_TEXT_START_DOCSTRING,\n )\n-class Speech2TextForConditionalGeneration(Speech2TextPreTrainedModel):\n+class Speech2TextForConditionalGeneration(Speech2TextPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "463a30fabe77c0423fdc256d7da58c1cd145cd5c",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1123,7 +1124,7 @@ def _update_causal_mask(\n \n \n # Copied from transformers.models.persimmon.modeling_persimmon.PersimmonForCausalLM with PERSIMMON->STABLELM,Persimmon->StableLm\n-class StableLmForCausalLM(StableLmPreTrainedModel):\n+class StableLmForCausalLM(StableLmPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with LLAMA->STABLELM,Llama->StableLm"
        },
        {
            "sha": "079ad1298fb99950e7263a98358e37ca2317b3c5",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -1053,7 +1054,7 @@ def _update_causal_mask(\n \n \n # Copied from transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM with QWEN2->STARCODER2,Qwen2->Starcoder2\n-class Starcoder2ForCausalLM(Starcoder2PreTrainedModel):\n+class Starcoder2ForCausalLM(Starcoder2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "96b6c7334b1535f7436089d5fa327b4cb5e11a4c",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n@@ -1456,7 +1457,7 @@ def forward(\n @add_start_docstrings(\n     \"\"\"SWITCH_TRANSFORMERS Model with a `language modeling` head on top.\"\"\", SWITCH_TRANSFORMERS_START_DOCSTRING\n )\n-class SwitchTransformersForConditionalGeneration(SwitchTransformersPreTrainedModel):\n+class SwitchTransformersForConditionalGeneration(SwitchTransformersPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n \n     def __init__(self, config: SwitchTransformersConfig):"
        },
        {
            "sha": "43e3f3afa4a837dd11946878cf9d345909b4519a",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1542,7 +1543,7 @@ def forward(\n \n \n @add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top.\"\"\", T5_START_DOCSTRING)\n-class T5ForConditionalGeneration(T5PreTrainedModel):\n+class T5ForConditionalGeneration(T5PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_unexpected = [\n         \"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n     ]"
        },
        {
            "sha": "67b97cf9c85223289c51cbd71a6ff67492b88c05",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n@@ -736,7 +737,7 @@ def forward(self, *args, **kwargs):\n     \" [`VisionEncoderDecoder`].\",\n     TROCR_START_DOCSTRING,\n )\n-class TrOCRForCausalLM(TrOCRPreTrainedModel):\n+class TrOCRForCausalLM(TrOCRPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"output_projection.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "c621b742323db2bb60027a46de356097b88f5923",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -34,6 +34,7 @@\n )\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n@@ -1679,7 +1680,7 @@ def forward(\n     This class is based on [`T5ForConditionalGeneration`], extended to deal with images and layout (2D) data.\"\"\",\n     UDOP_START_DOCSTRING,\n )\n-class UdopForConditionalGeneration(UdopPreTrainedModel):\n+class UdopForConditionalGeneration(UdopPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\n         \"encoder.embed_tokens.weight\",\n         \"decoder.embed_tokens.weight\","
        },
        {
            "sha": "a7d1e5bacc65e503f72fb6ff4368b4bddec1aed3",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1101,7 +1102,7 @@ def forward(\n \n \n @add_start_docstrings(\"\"\"UMT5 Model with a `language modeling` head on top.\"\"\", UMT5_START_DOCSTRING)\n-class UMT5ForConditionalGeneration(UMT5PreTrainedModel):\n+class UMT5ForConditionalGeneration(UMT5PreTrainedModel, GenerationMixin):\n     r\"\"\"\n     Examples:\n "
        },
        {
            "sha": "7c7cfec20959ea1bd05780b4009e7ea42babe4f8",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -21,9 +21,10 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPooling, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -239,7 +240,7 @@ def _supports_sdpa(self):\n     \"\"\"The VideoLlava model which consists of a vision backbone and a language model.\"\"\",\n     VIDEO_LLAVA_START_DOCSTRING,\n )\n-class VideoLlavaForConditionalGeneration(VideoLlavaPreTrainedModel):\n+class VideoLlavaForConditionalGeneration(VideoLlavaPreTrainedModel, GenerationMixin):\n     def __init__(self, config: VideoLlavaConfig):\n         super().__init__(config)\n         self.video_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "95129d46bbd8e5e0f893ec57ec183674fc3ff1fc",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -21,9 +21,10 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -240,7 +241,7 @@ def _supports_sdpa(self):\n     VIPLLAVA_START_DOCSTRING,\n )\n # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration with LLAVA->VIPLLAVA,Llava->VipLlava\n-class VipLlavaForConditionalGeneration(VipLlavaPreTrainedModel):\n+class VipLlavaForConditionalGeneration(VipLlavaPreTrainedModel, GenerationMixin):\n     def __init__(self, config: VipLlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "7a4e9487288e93c44fdfb96068a2c57d8435d12c",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,7 +25,7 @@\n \n from transformers.cache_utils import EncoderDecoderCache\n \n-from ...generation.configuration_utils import GenerationConfig\n+from ...generation import GenerationConfig, GenerationMixin\n from ...generation.logits_process import (\n     LogitsProcessorList,\n     SuppressTokensAtBeginLogitsProcessor,\n@@ -172,7 +172,7 @@ def _pad_to_max_length(\n     return sequences\n \n \n-class WhisperGenerationMixin:\n+class WhisperGenerationMixin(GenerationMixin):\n     def _extract_token_timestamps(self, generate_outputs, alignment_heads, time_precision=0.02, num_frames=None):\n         \"\"\"\n         Calculates token-level timestamps using the encoder-decoder cross-attentions and dynamic time-warping (DTW) to"
        },
        {
            "sha": "93ec57fcf4b4002c7fc9b7659a5db57924949af8",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1915,7 +1916,7 @@ def forward(self, *args, **kwargs):\n     \"\"\",\n     WHISPER_START_DOCSTRING,\n )\n-class WhisperForCausalLM(WhisperPreTrainedModel):\n+class WhisperForCausalLM(WhisperPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"proj_out.weight\"]\n     main_input_name = \"input_ids\"\n "
        },
        {
            "sha": "3090bc2973cd564bfcf904b1d996211fc3cad745",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n@@ -696,7 +697,7 @@ def forward(\n     \"\"\",\n     XGLM_START_DOCSTRING,\n )\n-class XGLMForCausalLM(XGLMPreTrainedModel):\n+class XGLMForCausalLM(XGLMPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "3acec2353b69fcb5d070a2b604bb08862728772e",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -27,6 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -657,7 +658,7 @@ def forward(self, x, y=None):\n     \"\"\",\n     XLM_START_DOCSTRING,\n )\n-class XLMWithLMHeadModel(XLMPreTrainedModel):\n+class XLMWithLMHeadModel(XLMPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"pred_layer.proj.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "a153f09468937b2a0a70aad4796984dc2ac9314a",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n@@ -1006,7 +1007,7 @@ def forward(\n     XLM_ROBERTA_START_DOCSTRING,\n )\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with Roberta->XLMRoberta, ROBERTA->XLM_ROBERTA\n-class XLMRobertaForCausalLM(XLMRobertaPreTrainedModel):\n+class XLMRobertaForCausalLM(XLMRobertaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "0c384ad45c523b8b2a5070044bfc5cd9e3031541",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask_for_sdpa,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n@@ -986,7 +987,7 @@ def forward(\n     \"\"\"XLM-RoBERTa-XL Model with a `language modeling` head on top for CLM fine-tuning.\"\"\",\n     XLM_ROBERTA_XL_START_DOCSTRING,\n )\n-class XLMRobertaXLForCausalLM(XLMRobertaXLPreTrainedModel):\n+class XLMRobertaXLForCausalLM(XLMRobertaXLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "7681fbafad6db5cab8359aede16b181b3734da92",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...modeling_utils import PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits, PreTrainedModel, SequenceSummary\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n@@ -1286,7 +1287,7 @@ def forward(\n     \"\"\",\n     XLNET_START_DOCSTRING,\n )\n-class XLNetLMHeadModel(XLNetPreTrainedModel):\n+class XLNetLMHeadModel(XLNetPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_loss.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "71474cc9c45ba95d72acf5778f672178c4dd43b8",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -956,7 +957,7 @@ def forward(\n     \"X-MOD Model with a `language modeling` head on top for CLM fine-tuning.\",\n     XMOD_START_DOCSTRING,\n )\n-class XmodForCausalLM(XmodPreTrainedModel):\n+class XmodForCausalLM(XmodPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n     # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM.__init__ with Roberta->Xmod"
        },
        {
            "sha": "600942a7ac082216256a56b33430425e729d545f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -2099,6 +2099,15 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n             )\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n+    @pytest.mark.generate\n+    def test_inherits_generation_mixin(self):\n+        \"\"\"\n+        Tests that the model class directly inherits `GenerationMixin`, as opposed to relying on `PreTrainedModel`\n+        to inherit it.\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n+\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, seq_length = input_ids.shape\n         config = config.text_config if hasattr(config, \"text_config\") else config"
        },
        {
            "sha": "95d716898343d02ed99a819cc5268a7a42d5ebd1",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -67,6 +67,7 @@\n         BertModel,\n         FunnelBaseModel,\n         FunnelModel,\n+        GenerationMixin,\n         GPT2Config,\n         GPT2LMHeadModel,\n         ResNetBackbone,\n@@ -571,3 +572,20 @@ def test_dynamic_saving_from_local_repo(self):\n             _ = AutoModelForCausalLM.from_pretrained(tmp_dir_out, trust_remote_code=True)\n             self.assertTrue((Path(tmp_dir_out) / \"modeling_fake_custom.py\").is_file())\n             self.assertTrue((Path(tmp_dir_out) / \"configuration_fake_custom.py\").is_file())\n+\n+    def test_custom_model_patched_generation_inheritance(self):\n+        \"\"\"\n+        Tests that our inheritance patching for generate-compatible models works as expected. Without this feature,\n+        old Hub models lose the ability to call `generate`.\n+        \"\"\"\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/test_dynamic_model_generation\", trust_remote_code=True\n+        )\n+        self.assertTrue(model.__class__.__name__ == \"NewModelForCausalLM\")\n+\n+        # It inherits from GenerationMixin. This means it can `generate`. Because `PreTrainedModel` is scheduled to\n+        # stop inheriting from `GenerationMixin` in v4.50, this check will fail if patching is not present.\n+        self.assertTrue(isinstance(model, GenerationMixin))\n+        # More precisely, it directly inherits from GenerationMixin. This check would fail prior to v4.45 (inheritance\n+        # patching was added in v4.45)\n+        self.assertTrue(\"GenerationMixin\" in str(model.__class__.__bases__))"
        },
        {
            "sha": "5155647059f1540224fd75ff46121fb189d4cac0",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e15687fffe5c9d20598a19aeab721ae0a7580f8a/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=e15687fffe5c9d20598a19aeab721ae0a7580f8a",
            "patch": "@@ -90,6 +90,7 @@\n         BertConfig,\n         BertModel,\n         CLIPTextModel,\n+        GenerationMixin,\n         PreTrainedModel,\n         T5Config,\n         T5ForConditionalGeneration,\n@@ -1715,6 +1716,32 @@ def test_isin_mps_friendly(self):\n             torch.equal(torch.isin(random_ids, random_test_tensor), isin_mps_friendly(random_ids, random_test_tensor))\n         )\n \n+    def test_can_generate(self):\n+        \"\"\"Tests the behavior of `PreTrainedModel.can_generate` method.\"\"\"\n+        # 1 - By default, a model CAN'T generate\n+        self.assertFalse(BertModel.can_generate())\n+\n+        # 2 - The most common case for a model to be able to generate is to inherit from `GenerationMixin` directly\n+        class DummyBertWithMixin(BertModel, GenerationMixin):\n+            pass\n+\n+        self.assertTrue(DummyBertWithMixin.can_generate())\n+\n+        # 3 - Alternatively, a model can implement a `generate` method\n+        class DummyBertWithGenerate(BertModel):\n+            def generate(self):\n+                pass\n+\n+        self.assertTrue(DummyBertWithGenerate.can_generate())\n+\n+        # 4 - BC: models with a custom `prepare_inputs_for_generation` can generate (it was assumed they inherited\n+        # `GenerationMixin`)\n+        class DummyBertWithPrepareInputs(BertModel):\n+            def prepare_inputs_for_generation(self):\n+                pass\n+\n+        self.assertTrue(DummyBertWithPrepareInputs.can_generate())\n+\n     def test_save_and_load_config_with_custom_generation(self):\n         \"\"\"\n         Regression test for the ability to save and load a config with a custom generation kwarg (i.e. a parameter"
        }
    ],
    "stats": {
        "total": 591,
        "additions": 407,
        "deletions": 184
    }
}