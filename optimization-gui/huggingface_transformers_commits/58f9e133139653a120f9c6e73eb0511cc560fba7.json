{
    "author": "Sai-Suraj-27",
    "message": "Fixed Type-hints in function defintions (#41525)\n\n* Explicitly annotate default None parameters as Optional\n\n* make style.\n\n* make style.\n\n* Fixed check_copies.\n\n* fix consistency.",
    "sha": "58f9e133139653a120f9c6e73eb0511cc560fba7",
    "files": [
        {
            "sha": "1b6557cf2f53fe72f75d42a9cac5d9088a259f93",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -1724,7 +1724,7 @@ def get_ids_area(masks, scores, dedup=False):\n \n     # inspired by https://github.com/facebookresearch/detr/blob/master/models/detr.py#L258\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n     ):\n         \"\"\"\n         Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "f2be84ece9b320542105b36862e7e9530474cc79",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -948,7 +948,7 @@ def get_ids_area(masks, scores, dedup=False):\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n     ):\n         \"\"\"\n         Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "1bdc46fbba82b9dad86e6cd478213e92f0d18fcc",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -2673,9 +2673,9 @@ def replace_multimodal_special_tokens(\n     def __call__(\n         self,\n         text: TextInput = None,\n-        images: ImageInput = None,\n-        videos: VideoInput = None,\n-        audio: AudioInput = None,\n+        images: Optional[ImageInput] = None,\n+        videos: Optional[VideoInput] = None,\n+        audio: Optional[AudioInput] = None,\n         **kwargs,\n     ):\n         \"\"\""
        },
        {
            "sha": "9f3a894c114d93cbd8b8faa6f8f3e153941a01e2",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -20,7 +20,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -123,9 +123,9 @@ def __init__(\n     def __call__(\n         self,\n         text: TextInput = None,\n-        images: ImageInput = None,\n-        videos: VideoInput = None,\n-        audio: AudioInput = None,\n+        images: Optional[ImageInput] = None,\n+        videos: Optional[VideoInput] = None,\n+        audio: Optional[AudioInput] = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "e207c55dc63628ee4fad73bc0df3b003b047734e",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -241,7 +241,7 @@ def from_pretrained_question_encoder_generator(\n         cls,\n         question_encoder_pretrained_model_name_or_path: Optional[str] = None,\n         generator_pretrained_model_name_or_path: Optional[str] = None,\n-        retriever: RagRetriever = None,\n+        retriever: Optional[RagRetriever] = None,\n         **kwargs,\n     ) -> PreTrainedModel:\n         r\"\"\""
        },
        {
            "sha": "14993a5a5c9adb9a849c95defd1bb3265d60f9d6",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -978,7 +978,7 @@ def post_process_object_detection(\n         self,\n         outputs,\n         threshold: float = 0.5,\n-        target_sizes: Union[TensorType, list[tuple]] = None,\n+        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n         use_focal_loss: bool = True,\n     ):\n         \"\"\""
        },
        {
            "sha": "f6ed7a49fd438f231cb2824744276e93117c2c91",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -635,9 +635,9 @@ def init_video_session(\n         self,\n         video: Optional[VideoInput] = None,\n         inference_device: Union[str, \"torch.device\"] = \"cpu\",\n-        inference_state_device: Union[str, \"torch.device\"] = None,\n-        processing_device: Union[str, \"torch.device\"] = None,\n-        video_storage_device: Union[str, \"torch.device\"] = None,\n+        inference_state_device: Optional[Union[str, \"torch.device\"]] = None,\n+        processing_device: Optional[Union[str, \"torch.device\"]] = None,\n+        video_storage_device: Optional[Union[str, \"torch.device\"]] = None,\n         max_vision_features_cache_size: int = 1,\n         dtype: torch.dtype = torch.float32,\n     ):"
        },
        {
            "sha": "8e09ee23b9a44db3520aa348bdb2de89e254c7d1",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -530,9 +530,9 @@ def init_video_session(\n         self,\n         video: Optional[VideoInput] = None,\n         inference_device: Union[str, \"torch.device\"] = \"cpu\",\n-        inference_state_device: Union[str, \"torch.device\"] = None,\n-        processing_device: Union[str, \"torch.device\"] = None,\n-        video_storage_device: Union[str, \"torch.device\"] = None,\n+        inference_state_device: Optional[Union[str, \"torch.device\"]] = None,\n+        processing_device: Optional[Union[str, \"torch.device\"]] = None,\n+        video_storage_device: Optional[Union[str, \"torch.device\"]] = None,\n         max_vision_features_cache_size: int = 1,\n         dtype: torch.dtype = torch.float32,\n     ):"
        },
        {
            "sha": "403922eee93cdd381edab0b4fe4b27c227d2a151",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -211,9 +211,9 @@ def vocab_size(self):\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair_target: Optional[\n             Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n         ] = None,"
        },
        {
            "sha": "081dcec7dd99153139f52ea42e694e43b5835ea8",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -371,9 +371,9 @@ def _from_pretrained(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair_target: Optional[\n             Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n         ] = None,"
        },
        {
            "sha": "6f5a8b6cb0bc1a42d19ca5c41414675e3b030300",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -53,7 +53,7 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[TrOCRProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "26eb7fa82e7aa7a3fef35f2dcfe27ba3be8b0b94",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -508,11 +508,11 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n     @add_end_docstrings(UDOP_ENCODE_KWARGS_DOCSTRING)\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, list[PreTokenizedInput]]] = None,\n         boxes: Optional[Union[list[list[int]], list[list[list[int]]]]] = None,\n         word_labels: Optional[Union[list[int], list[list[int]]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair_target: Optional[\n             Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n         ] = None,\n@@ -703,7 +703,7 @@ def batch_encode_plus_boxes(\n         word_labels: Optional[list[list[int]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n@@ -771,7 +771,7 @@ def encode_boxes(\n         word_labels: Optional[list[list[int]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -814,7 +814,7 @@ def encode_plus_boxes(\n         word_labels: Optional[list[list[int]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,"
        },
        {
            "sha": "927d662fb587b051fc7ffe720fe8d4ed3fe0ec65",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -93,8 +93,8 @@ def __call__(\n         images: Optional[ImageInput] = None,\n         videos: Optional[ImageInput] = None,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length=None,\n+        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n+        max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "beb197f987ef6d5d3491c1dbed649b9c60c024c0",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -595,7 +595,7 @@ def post_process_pose_estimation(\n         boxes: Union[list[list[list[float]]], np.ndarray],\n         kernel_size: int = 11,\n         threshold: Optional[float] = None,\n-        target_sizes: Union[TensorType, list[tuple]] = None,\n+        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n     ):\n         \"\"\"\n         Transform the heatmaps into keypoint predictions and transform them back to the image."
        },
        {
            "sha": "8f039f1574ade787169cc10811dfa214d7f5ce67",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -1474,7 +1474,7 @@ def compute_contrastive_logits(\n         target_features: torch.FloatTensor,\n         negative_features: torch.FloatTensor,\n         predicted_features: torch.FloatTensor,\n-        temperature: int = 0.1,\n+        temperature: float = 0.1,\n     ):\n         \"\"\"\n         Compute logits for contrastive loss based using cosine similarity as the distance measure between"
        },
        {
            "sha": "4230a28f9a267a8cea4af031cb571fea631c830a",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -1228,7 +1228,7 @@ def compute_contrastive_logits(\n         target_features: torch.FloatTensor,\n         negative_features: torch.FloatTensor,\n         predicted_features: torch.FloatTensor,\n-        temperature: int = 0.1,\n+        temperature: float = 0.1,\n     ):\n         \"\"\"\n         Compute logits for contrastive loss based using cosine similarity as the distance measure between"
        },
        {
            "sha": "7281e1719033beac2b65e2157d57d25438e55a9d",
            "filename": "src/transformers/models/xcodec/configuration_xcodec.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -98,8 +98,8 @@ def __init__(\n         codebook_size: int = 1024,\n         codebook_dim: Optional[int] = None,\n         initializer_range: float = 0.02,\n-        acoustic_model_config: Union[dict, DacConfig] = None,\n-        semantic_model_config: Union[dict, HubertConfig] = None,\n+        acoustic_model_config: Optional[Union[dict, DacConfig]] = None,\n+        semantic_model_config: Optional[Union[dict, HubertConfig]] = None,\n         **kwargs,\n     ):\n         if acoustic_model_config is None:"
        },
        {
            "sha": "0d33b6c761bf2837734d26d4bd25384a92cc333f",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -1471,7 +1471,7 @@ def post_process(self, outputs, target_sizes):\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection with Detr->Yolos\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None\n+        self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None\n     ):\n         \"\"\"\n         Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,"
        },
        {
            "sha": "f1bb9da8c20271b92dd5668264865cd5b1587792",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -1128,11 +1128,11 @@ def __init__(\n         in_features: int = -1,\n         indices_as_float: bool = False,\n         is_indice_packed: bool = True,\n-        num_centroids: tuple = [-1, -1],\n-        num_res_centroids: tuple = [-1, -1],\n+        num_centroids: list = [-1, -1],\n+        num_res_centroids: list = [-1, -1],\n         out_features: int = -1,\n         outlier_size: int = 0,\n-        vector_lens: tuple = [-1, -1],\n+        vector_lens: list = [-1, -1],\n         **kwargs,\n     ):\n         self.enable_norm = enable_norm"
        },
        {
            "sha": "40b72e7e1ec18e859c211f51ffdb26b4e57ab4c8",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58f9e133139653a120f9c6e73eb0511cc560fba7/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58f9e133139653a120f9c6e73eb0511cc560fba7/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=58f9e133139653a120f9c6e73eb0511cc560fba7",
            "patch": "@@ -47,7 +47,7 @@ def __init__(\n         do_pad: bool = True,\n         pad_size: dict[str, int] = {\"height\": 80, \"width\": 80},\n         fill: Optional[int] = None,\n-        pad_mode: PaddingMode = None,\n+        pad_mode: Optional[PaddingMode] = None,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, list[float]]] = [0.48145466, 0.4578275, 0.40821073],\n         image_std: Optional[Union[float, list[float]]] = [0.26862954, 0.26130258, 0.27577711],"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 39,
        "deletions": 39
    }
}