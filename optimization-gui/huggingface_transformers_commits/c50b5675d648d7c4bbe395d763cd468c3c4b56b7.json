{
    "author": "meliksahturker",
    "message": "prepare_fa2_from_position_ids function bugfix (#33269)\n\ncontiguous() is called before view() for key and value within prepare_fa2_from_position_ids function",
    "sha": "c50b5675d648d7c4bbe395d763cd468c3c4b56b7",
    "files": [
        {
            "sha": "1b9274e21f5205e8c70e1e7731a00d049516e749",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50b5675d648d7c4bbe395d763cd468c3c4b56b7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50b5675d648d7c4bbe395d763cd468c3c4b56b7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=c50b5675d648d7c4bbe395d763cd468c3c4b56b7",
            "patch": "@@ -163,8 +163,8 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n     query = query.view(-1, query.size(-2), query.size(-1))\n-    key = key.view(-1, key.size(-2), key.size(-1))\n-    value = value.view(-1, value.size(-2), value.size(-1))\n+    key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n+    value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n     position_ids = position_ids.flatten()\n     indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}