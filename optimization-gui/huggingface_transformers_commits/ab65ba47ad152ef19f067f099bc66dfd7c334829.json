{
    "author": "BlackNoodle",
    "message": "fix: Propagate `lr_scheduler_kwargs` options to create LR Scheduler when LayerWiseDummyOptimizer is used (#34559)\n\nfix: fix get_scheduler",
    "sha": "ab65ba47ad152ef19f067f099bc66dfd7c334829",
    "files": [
        {
            "sha": "ffeebaed5d0a523ef45bcab87c8a0eb96bc1ff90",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab65ba47ad152ef19f067f099bc66dfd7c334829/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab65ba47ad152ef19f067f099bc66dfd7c334829/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=ab65ba47ad152ef19f067f099bc66dfd7c334829",
            "patch": "@@ -549,6 +549,7 @@ def get_scheduler(\n                 optimizer=optimizer_dict[param],\n                 num_warmup_steps=num_warmup_steps,\n                 num_training_steps=num_training_steps,\n+                scheduler_specific_kwargs=scheduler_specific_kwargs,\n             )\n \n         def scheduler_hook(param):"
        }
    ],
    "stats": {
        "total": 1,
        "additions": 1,
        "deletions": 0
    }
}