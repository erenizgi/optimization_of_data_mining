{
    "author": "kenza-bouzid",
    "message": "Refactor image features selection in LlaVa (#33696)\n\n* refactor image features selection\r\n\r\n* break line\r\n\r\n* remove whitespace\r\n\r\n* add pr comments: include projection and rename function\r\n\r\n* make fix-copies\r\n\r\n* fix get_image_feature in vip llava",
    "sha": "88d960937c81a32bfb63356a2e8ecf7999619681",
    "files": [
        {
            "sha": "90c7464ea799b7f1bd860de5d9a987005235bb53",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 20,
            "deletions": 11,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/88d960937c81a32bfb63356a2e8ecf7999619681/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88d960937c81a32bfb63356a2e8ecf7999619681/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=88d960937c81a32bfb63356a2e8ecf7999619681",
            "patch": "@@ -279,6 +279,21 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n         self.vocab_size = model_embeds.num_embeddings\n         return model_embeds\n \n+    def get_image_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+    ):\n+        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n+        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.\n+        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_image_feature = selected_image_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_image_feature = selected_image_feature\n+        else:\n+            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        return image_features\n+\n     def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n         num_images, num_image_patches, embed_dim = image_features.shape\n         batch_size, sequence_length = input_ids.shape\n@@ -450,17 +465,11 @@ def forward(\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n         if pixel_values is not None:\n-            image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n-            # this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.\n-            selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n-            if vision_feature_select_strategy == \"default\":\n-                selected_image_feature = selected_image_feature[:, 1:]\n-            elif vision_feature_select_strategy == \"full\":\n-                selected_image_feature = selected_image_feature\n-            else:\n-                raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n-\n-            image_features = self.multi_modal_projector(selected_image_feature)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n \n             if legacy_processing:\n                 logger.warning_once("
        },
        {
            "sha": "2228f99d4d2aa89253d8763a786918feaa54810c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/88d960937c81a32bfb63356a2e8ecf7999619681/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88d960937c81a32bfb63356a2e8ecf7999619681/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=88d960937c81a32bfb63356a2e8ecf7999619681",
            "patch": "@@ -282,6 +282,17 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n         self.vocab_size = model_embeds.num_embeddings\n         return model_embeds\n \n+    # Ignore copy\n+    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: list[int]):\n+        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n+\n+        # For VIP-llava, the image features are computed this way\n+        # We select the features from index 1: for the layers -2, -5, -8, -11 and 6\n+        image_features = [image_outputs.hidden_states[index][:, 1:] for index in vision_feature_layers]\n+        image_features = torch.cat(image_features, dim=-1)\n+        image_features = self.multi_modal_projector(image_features)\n+        return image_features\n+\n     def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n         num_images, num_image_patches, embed_dim = image_features.shape\n         batch_size, sequence_length = input_ids.shape\n@@ -451,13 +462,9 @@ def forward(\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n         if pixel_values is not None:\n-            image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n-\n-            # For VIP-llava, the image features are computed this way\n-            # We select the features from index 1: for the layers -2, -5, -8, -11 and 6\n-            image_features = [image_outputs.hidden_states[index][:, 1:] for index in vision_feature_layers]\n-            image_features = torch.cat(image_features, dim=-1)\n-            image_features = self.multi_modal_projector(image_features)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n+            )\n \n             if legacy_processing:\n                 logger.warning_once("
        }
    ],
    "stats": {
        "total": 52,
        "additions": 34,
        "deletions": 18
    }
}