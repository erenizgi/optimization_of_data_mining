{
    "author": "bd793fcb",
    "message": "fix wandb hp search unable to resume from sweep_id (#35883)\n\n* fix wandb hp search unable to resume from sweep_id\n\n* format styles\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "87b30c35892568f9b83d4e8d1233956b8e0cd96c",
    "files": [
        {
            "sha": "95668fefd7b201513852b2e6077dfae44da5cb40",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/87b30c35892568f9b83d4e8d1233956b8e0cd96c/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87b30c35892568f9b83d4e8d1233956b8e0cd96c/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=87b30c35892568f9b83d4e8d1233956b8e0cd96c",
            "patch": "@@ -585,11 +585,19 @@ def _objective():\n \n         return trainer.objective\n \n-    sweep_id = wandb.sweep(sweep_config, project=project, entity=entity) if not sweep_id else sweep_id\n+    if not sweep_id:\n+        sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)\n+    else:\n+        import wandb.env\n+\n+        if entity:\n+            wandb.env.set_entity(entity)\n+        wandb.env.set_project(project)\n+\n     logger.info(f\"wandb sweep id - {sweep_id}\")\n     wandb.agent(sweep_id, function=_objective, count=n_trials)\n \n-    return BestRun(best_trial[\"run_id\"], best_trial[\"objective\"], best_trial[\"hyperparameters\"])\n+    return BestRun(best_trial[\"run_id\"], best_trial[\"objective\"], best_trial[\"hyperparameters\"], sweep_id)\n \n \n def get_available_reporting_integrations():"
        },
        {
            "sha": "c8d9f34ff5657b6ebd8e6be201340fbf3fdc3280",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 25,
            "deletions": 9,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/87b30c35892568f9b83d4e8d1233956b8e0cd96c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/87b30c35892568f9b83d4e8d1233956b8e0cd96c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=87b30c35892568f9b83d4e8d1233956b8e0cd96c",
            "patch": "@@ -5707,9 +5707,6 @@ def setUp(self):\n         self.batch_size = args.train_batch_size\n \n     def test_hyperparameter_search(self):\n-        class MyTrialShortNamer(TrialShortNamer):\n-            DEFAULTS = {\"a\": 0, \"b\": 0}\n-\n         def hp_space(trial):\n             return {\n                 \"method\": \"random\",\n@@ -5731,9 +5728,6 @@ def model_init(config):\n \n             return RegressionPreTrainedModel(model_config)\n \n-        def hp_name(params):\n-            return MyTrialShortNamer.shortname(params)\n-\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(\n                 output_dir=tmp_dir,\n@@ -5748,9 +5742,31 @@ def hp_name(params):\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n-            trainer.hyperparameter_search(\n-                direction=\"minimize\", hp_space=hp_space, hp_name=hp_name, backend=\"wandb\", n_trials=4, anonymous=\"must\"\n-            )\n+            sweep_kwargs = {\n+                \"direction\": \"minimize\",\n+                \"hp_space\": hp_space,\n+                \"backend\": \"wandb\",\n+                \"n_trials\": 4,\n+            }\n+            best_run = trainer.hyperparameter_search(**sweep_kwargs)\n+\n+            self.assertIsNotNone(best_run.run_id)\n+            self.assertIsNotNone(best_run.run_summary)\n+            hp_keys = set(best_run.hyperparameters.keys())\n+            self.assertSetEqual(hp_keys, {\"a\", \"b\", \"assignments\", \"metric\"})\n+\n+            # pretend restarting the process purged the environ\n+            import os\n+\n+            del os.environ[\"WANDB_ENTITY\"]\n+            del os.environ[\"WANDB_PROJECT\"]\n+            sweep_kwargs[\"sweep_id\"] = best_run.run_summary\n+            updated_best_run = trainer.hyperparameter_search(**sweep_kwargs)\n+\n+            self.assertIsNotNone(updated_best_run.run_id)\n+            self.assertEqual(updated_best_run.run_summary, best_run.run_summary)\n+            updated_hp_keys = set(updated_best_run.hyperparameters.keys())\n+            self.assertSetEqual(updated_hp_keys, {\"a\", \"b\", \"assignments\", \"metric\"})\n \n \n class HyperParameterSearchBackendsTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 35,
        "deletions": 11
    }
}