{
    "author": "cyyever",
    "message": "Fix np array typing (#40741)\n\nFix typing\n\nSigned-off-by: cyy <cyyever@outlook.com>\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "def7558f746046e2220991dfe03fc9b19533c939",
    "files": [
        {
            "sha": "9591503d975bca53bb064e3d1f83fab530eef07d",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -29,7 +29,8 @@ class CacheLayerMixin(ABC):\n     is_compileable = False\n \n     def __init__(self):\n-        self.keys, self.values = None, None\n+        self.keys: Optional[torch.Tensor] = None\n+        self.values: Optional[torch.Tensor] = None\n \n     def __repr__(self):\n         return f\"{self.__class__.__name__}\""
        },
        {
            "sha": "782ac6db38faefa1bb846aa9380fc8bd9e98c0b6",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -107,7 +107,7 @@ def __init__(\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n-        logits_processor: \"LogitsProcessorList\" = None,\n+        logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         # Make sure all data at the same device as assistant model\n         device = assistant_model.device\n@@ -371,7 +371,7 @@ def __init__(\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n-        logits_processor: \"LogitsProcessorList\" = None,\n+        logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         super().__init__(input_ids, assistant_model, generation_config, model_kwargs, inputs_tensor, logits_processor)\n \n@@ -897,7 +897,7 @@ def __init__(\n         model_kwargs: dict,\n         atm_translator: AssistantToTargetTranslator,\n         inputs_tensor: Optional[torch.Tensor] = None,\n-        logits_processor: \"LogitsProcessorList\" = None,\n+        logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         # Initialize translator before parent class\n         self._atm_translator = atm_translator\n@@ -1140,7 +1140,7 @@ def __init__(\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n-        logits_processor: \"LogitsProcessorList\" = None,\n+        logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         super().__init__(\n             input_ids=input_ids,"
        },
        {
            "sha": "e62742ef7514463554eb96e3e1aba8cd7c8e9727",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -179,7 +179,7 @@ def _score_ngrams_in_passage(self, input_ids: torch.LongTensor):\n                 )\n         return num_tokens_scored_batch, green_token_count_batch\n \n-    def _compute_z_score(self, green_token_count: np.array, total_num_tokens: np.array) -> np.array:\n+    def _compute_z_score(self, green_token_count: np.ndarray, total_num_tokens: np.ndarray) -> np.array:\n         expected_count = self.greenlist_ratio\n         numer = green_token_count - expected_count * total_num_tokens\n         denom = np.sqrt(total_num_tokens * expected_count * (1 - expected_count))"
        },
        {
            "sha": "cb7c4bbf422a33ae6043f27641fa0cb59463d54f",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -359,7 +359,7 @@ def get_channel_dimension_axis(\n     raise ValueError(f\"Unsupported data format: {input_data_format}\")\n \n \n-def get_image_size(image: np.ndarray, channel_dim: ChannelDimension = None) -> tuple[int, int]:\n+def get_image_size(image: np.ndarray, channel_dim: Optional[ChannelDimension] = None) -> tuple[int, int]:\n     \"\"\"\n     Returns the (height, width) dimensions of the image.\n "
        },
        {
            "sha": "a06cec1c8c6012951ca43c71d256aaa6989b652f",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -733,7 +733,7 @@ def converted(self) -> Tokenizer:\n }\n \n \n-def convert_gguf_tokenizer(architecture, tokenizer_dict) -> Tokenizer:\n+def convert_gguf_tokenizer(architecture: str, tokenizer_dict) -> tuple[Tokenizer, dict]:\n     \"\"\"\n     Utilities to convert a slow tokenizer instance in a fast tokenizer instance.\n "
        },
        {
            "sha": "07840f0e2c8fe5c9a2b881660421963d1c60e1ff",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -47,7 +47,7 @@ def pad_to_block(tensor, dims, had_block_size, value=0):\n     return nn.functional.pad(tensor, pad_dims, \"constant\", value)\n \n \n-def get_higgs_grid(p: int, n: int):\n+def get_higgs_grid(p: int, n: int) -> \"torch.Tensor\":\n     if (p, n) == (2, 256):\n         return torch.tensor(\n             ["
        },
        {
            "sha": "4fc2fcf7ec6bd52570a6b6d75e927991d04a8711",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -43,7 +43,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n@@ -341,7 +341,7 @@ def preprocess(\n         )\n \n     def _resize_for_patching(\n-        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n@@ -374,7 +374,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n         return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n \n     def _pad_for_patching(\n-        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n@@ -454,7 +454,7 @@ def pad(\n \n     def get_image_patches(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         grid_pinpoints: list[tuple[int, int]],\n         patch_size: int,\n         resample: PILImageResampling,"
        },
        {
            "sha": "790003d853c47e0f9ffc9eb5dce1ed0384c39c28",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -727,7 +727,7 @@ def preprocess(\n         )\n \n     def _resize_for_patching(\n-        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n@@ -760,7 +760,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n         return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n \n     def _pad_for_patching(\n-        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n@@ -840,7 +840,7 @@ def pad(\n \n     def get_image_patches(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         grid_pinpoints: list[tuple[int, int]],\n         patch_size: int,\n         resample: PILImageResampling,"
        },
        {
            "sha": "e333248c18eddbe252d672bf3c56571bde83e0b1",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -152,7 +152,7 @@ def to_dict(self) -> dict[str, Any]:\n             del output[\"mel_filters_slaney\"]\n         return output\n \n-    def _np_extract_fbank_features(self, waveform: np.array, mel_filters: Optional[np.array] = None) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform: np.ndarray, mel_filters: Optional[np.array] = None) -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the provided `waveform` using the Hann window. In CLAP, two different filter\n         banks are used depending on the truncation pattern:\n@@ -199,7 +199,7 @@ def _random_mel_fusion(self, mel, total_frames, chunk_frames):\n         mel_fusion = np.stack([mel_shrink, mel_chunk_front, mel_chunk_middle, mel_chunk_back], axis=0)\n         return mel_fusion\n \n-    def _get_input_mel(self, waveform: np.array, max_length, truncation, padding) -> np.array:\n+    def _get_input_mel(self, waveform: np.ndarray, max_length, truncation, padding) -> np.array:\n         \"\"\"\n         Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.\n         Four different path are possible:"
        },
        {
            "sha": "160666ef78c073ac249c2192e4d589fb9e1786f1",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -106,7 +106,7 @@ def __init__(\n             mel_scale=\"htk\",\n         )\n \n-    def _np_extract_fbank_features(self, waveform: np.array) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform: np.ndarray) -> np.ndarray:\n         \"\"\"\n         This method first computes the log-mel spectrogram of the provided audio then applies normalization along the\n         each mel-filterbank, if `mel_norms` is provided."
        },
        {
            "sha": "966a160f91b07177b16043a00ccd76685926a422",
            "filename": "src/transformers/models/deprecated/mctct/feature_extraction_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -104,7 +104,7 @@ def __init__(\n         self.n_fft = optimal_fft_length(self.sample_size)\n         self.n_freqs = (self.n_fft // 2) + 1\n \n-    def _extract_mfsc_features(self, one_waveform: np.array) -> np.ndarray:\n+    def _extract_mfsc_features(self, one_waveform: np.ndarray) -> np.ndarray:\n         \"\"\"\n         Extracts MFSC Features for one waveform vector (unbatched). Adapted from Flashlight's C++ MFSC code.\n         \"\"\""
        },
        {
            "sha": "3c65f4314616302bef513e6d7a9e3db3adbd3abd",
            "filename": "src/transformers/models/deprecated/tvlt/feature_extraction_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -93,7 +93,7 @@ def __init__(\n             mel_scale=\"slaney\",\n         ).T\n \n-    def _np_extract_fbank_features(self, waveform: np.array) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform: np.ndarray) -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the provided audio, gives similar results to Whisper's original torch\n         implementation with 1e-5 tolerance."
        },
        {
            "sha": "bad8ef3b3c4054a5428efc1e6af76bbcfa552e7c",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -241,7 +241,7 @@ def resize(\n \n     def pad_image(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         size_divisor: int,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "aaf3afa41733a539ebf4aa5b451519ceb6c11dd6",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -482,7 +482,7 @@ def postprocess(\n \n     def unnormalize(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "499631b83220892d82c2cf2f2cb63a05472b1ef0",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -463,7 +463,7 @@ def postprocess(\n \n     def unnormalize(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "a9d15a3f52e1af19f193dc671a369896e9eed047",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -1554,7 +1554,7 @@ def postprocess(\n \n     def unnormalize(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "3887c9c7ad4b9fff73a0e39a84800a8811a420e1",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -58,7 +58,7 @@\n     from PIL import Image\n \n \n-def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n@@ -86,7 +86,7 @@ def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> li\n     return patches\n \n \n-def expand_to_square(image: np.array, background_color, input_data_format) -> np.array:\n+def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.array:\n     \"\"\"\n     Expands an image to a square by adding a background color.\n     \"\"\"\n@@ -399,7 +399,7 @@ def _preprocess(\n         return images\n \n     def _resize_for_patching(\n-        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n@@ -432,7 +432,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n         return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n \n     def _pad_for_patching(\n-        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n@@ -446,7 +446,7 @@ def _pad_for_patching(\n \n     def get_image_patches(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         grid_pinpoints,\n         size: tuple,\n         patch_size: int,"
        },
        {
            "sha": "837eda460802d3c6fff10441b64afc8524b201b6",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -58,7 +58,7 @@\n \n \n # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches\n-def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n@@ -87,7 +87,7 @@ def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> li\n \n \n # Copied from transformers.models.llava_next.image_processing_llava_next.expand_to_square\n-def expand_to_square(image: np.array, background_color, input_data_format) -> np.array:\n+def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.array:\n     \"\"\"\n     Expands an image to a square by adding a background color.\n     \"\"\"\n@@ -291,7 +291,7 @@ def pad(\n \n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._resize_for_patching\n     def _resize_for_patching(\n-        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n@@ -326,7 +326,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n \n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_patching\n     def _pad_for_patching(\n-        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+        self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n@@ -341,7 +341,7 @@ def _pad_for_patching(\n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor.get_image_patches\n     def get_image_patches(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         grid_pinpoints,\n         size: tuple,\n         patch_size: int,"
        },
        {
            "sha": "38b4b8fa4a509b20a6083712023a22f96062127d",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -122,7 +122,7 @@ def __init__(\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n \n-    def python_find_non_zero(self, image: np.array):\n+    def python_find_non_zero(self, image: np.ndarray):\n         \"\"\"This is a reimplementation of a findNonZero function equivalent to cv2.\"\"\"\n         non_zero_indices = np.column_stack(np.nonzero(image))\n         idxvec = non_zero_indices[:, [1, 0]]\n@@ -140,7 +140,7 @@ def python_bounding_rect(self, coordinates):\n \n     def crop_margin(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         gray_threshold: int = 200,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "64399d433f5e46157d344aa2a18ad17879afbdda",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -269,7 +269,7 @@ def __init__(\n \n     def pad(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):"
        },
        {
            "sha": "e397d111a0a46eb2242b311c6a95caf753dc7e1b",
            "filename": "src/transformers/models/rag/retrieval_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -173,7 +173,7 @@ def init_index(self):\n         self._deserialize_index()\n         self._index_initialized = True\n \n-    def get_doc_dicts(self, doc_ids: np.array):\n+    def get_doc_dicts(self, doc_ids: np.ndarray):\n         doc_list = []\n         for doc_ids_i in doc_ids:\n             ids = [str(int(doc_id)) for doc_id in doc_ids_i]"
        },
        {
            "sha": "5bdefe3064bb7fcd36c7bc155d35bec851704ce3",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -373,7 +373,7 @@ def __init__(\n \n     def affine_transform(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         center: tuple[float],\n         scale: tuple[float],\n         rotation: float,"
        },
        {
            "sha": "2d92c1b547be96b1d04dc2a472d2acd894d2c070",
            "filename": "src/transformers/models/whisper/feature_extraction_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -105,7 +105,7 @@ def __init__(\n             mel_scale=\"slaney\",\n         )\n \n-    def _np_extract_fbank_features(self, waveform_batch: np.array, device: str) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform_batch: np.ndarray, device: str) -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the provided audio, gives similar results to Whisper's original torch\n         implementation with 1e-5 tolerance.\n@@ -135,7 +135,7 @@ def _np_extract_fbank_features(self, waveform_batch: np.array, device: str) -> n\n         log_spec_batch = np.array(log_spec_batch)\n         return log_spec_batch\n \n-    def _torch_extract_fbank_features(self, waveform: np.array, device: str = \"cpu\") -> np.ndarray:\n+    def _torch_extract_fbank_features(self, waveform: np.ndarray, device: str = \"cpu\") -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the audio using PyTorch's GPU-accelerated STFT implementation with batching,\n         yielding results similar to cpu computing with 1e-5 tolerance."
        },
        {
            "sha": "973b5279822c4250c6e5e6c1fca14d7dbb1d4895",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -247,7 +247,7 @@ def resize(\n \n     def pad_image(\n         self,\n-        image: np.array,\n+        image: np.ndarray,\n         mode: PaddingMode = PaddingMode.REFLECT,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "6f39e5336717804ebf8bc55a3ca5f28ab8d8667a",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -192,10 +192,10 @@ class AutomaticSpeechRecognitionPipeline(ChunkPipeline):\n     def __init__(\n         self,\n         model: \"PreTrainedModel\",\n-        feature_extractor: Union[\"SequenceFeatureExtractor\", str] = None,\n+        feature_extractor: Optional[Union[\"SequenceFeatureExtractor\", str]] = None,\n         tokenizer: Optional[PreTrainedTokenizer] = None,\n         decoder: Optional[Union[\"BeamSearchDecoderCTC\", str]] = None,\n-        device: Union[int, \"torch.device\"] = None,\n+        device: Optional[Union[int, \"torch.device\"]] = None,\n         **kwargs,\n     ):\n         # set the model type so we can check we have the right pre- and post-processing parameters"
        },
        {
            "sha": "944c7a90a184582be9dca3ed0e20e7b85c61002b",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -951,7 +951,7 @@ def __init__(\n         modelcard: Optional[ModelCard] = None,\n         framework: Optional[str] = None,\n         task: str = \"\",\n-        device: Union[int, \"torch.device\"] = None,\n+        device: Optional[Union[int, \"torch.device\"]] = None,\n         binary_output: bool = False,\n         **kwargs,\n     ):"
        },
        {
            "sha": "22c63f10da0ca2092fcfe136e60e342bc139cde9",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -93,7 +93,7 @@ class PreTrainedTokenizerFast(PreTrainedTokenizerBase):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class: PreTrainedTokenizer = None\n+    slow_tokenizer_class: Optional[type[PreTrainedTokenizer]] = None\n \n     def __init__(self, *args, **kwargs):\n         tokenizer_object = kwargs.pop(\"tokenizer_object\", None)"
        },
        {
            "sha": "f3edd080e464c173ddb9920b6de12ef6bdb11e57",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -5577,7 +5577,7 @@ def _fsdp_qlora_plugin_updates(self):\n \n     def get_batch_samples(\n         self, epoch_iterator: Iterator, num_batches: int, device: torch.device\n-    ) -> tuple[list, Optional[torch.Tensor]]:\n+    ) -> tuple[list, Optional[Union[torch.Tensor, int]]]:\n         \"\"\"\n         Collects a specified number of batches from the epoch iterator and optionally counts the number of items in the batches to properly scale the loss.\n         \"\"\""
        },
        {
            "sha": "1749b0b3b1c57f2a4cade74a1f41fcd7de6ee495",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/def7558f746046e2220991dfe03fc9b19533c939/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=def7558f746046e2220991dfe03fc9b19533c939",
            "patch": "@@ -61,9 +61,9 @@\n \n VideoInput = Union[\n     list[\"PIL.Image.Image\"],\n-    \"np.ndarray\",\n+    np.ndarray,\n     \"torch.Tensor\",\n-    list[\"np.ndarray\"],\n+    list[np.ndarray],\n     list[\"torch.Tensor\"],\n     list[list[\"PIL.Image.Image\"]],\n     list[list[np.ndarray]],\n@@ -152,7 +152,7 @@ def is_scaled_video(video: np.ndarray) -> bool:\n     return np.min(video) >= 0 and np.max(video) <= 1\n \n \n-def convert_pil_frames_to_video(videos: list[VideoInput]) -> list[Union[\"np.ndarray\", \"torch.Tensor\"]]:\n+def convert_pil_frames_to_video(videos: list[VideoInput]) -> list[Union[np.ndarray, \"torch.Tensor\"]]:\n     \"\"\"\n     Given a batch of videos, converts each video to a 4D array. If video is already in array type,\n     it is simply returned. We assume that all inputs in the list are in the same format, based on the type of the first element.\n@@ -173,7 +173,7 @@ def convert_pil_frames_to_video(videos: list[VideoInput]) -> list[Union[\"np.ndar\n     return video_converted\n \n \n-def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\", \"URL\", \"Path\"]]:\n+def make_batched_videos(videos) -> list[Union[np.ndarray, \"torch.Tensor\", \"URL\", \"Path\"]]:\n     \"\"\"\n     Ensure that the input is a list of videos. If the input is a single video, it is converted to a list of length 1.\n     If the input is a batch of videos, it is converted to a list of 4D video arrays. Videos passed as list `PIL.Image`\n@@ -217,7 +217,7 @@ def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\", \"URL\n \n def make_batched_metadata(videos: VideoInput, video_metadata: Union[VideoMetadata, dict]):\n     if video_metadata is None:\n-        # Create default metadata and fill attrbiutes we can infer from given video\n+        # Create default metadata and fill attributes we can infer from given video\n         video_metadata = [\n             {\n                 \"total_num_frames\": len(video),\n@@ -713,10 +713,10 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n \n \n def convert_to_rgb(\n-    video: np.array,\n+    video: np.ndarray,\n     data_format: Optional[ChannelDimension] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> np.array:\n+) -> np.ndarray:\n     \"\"\"\n     Convert video to RGB by blending the transparency layer if it's in RGBA format, otherwise simply returns it.\n "
        }
    ],
    "stats": {
        "total": 111,
        "additions": 56,
        "deletions": 55
    }
}