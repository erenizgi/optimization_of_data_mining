{
    "author": "jackzhxng",
    "message": "Update executorch.md (#41582)\n\n* Update executorch.md\n\n* Update executorch.md\n\n* Update executorch.md\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "19df66dcba7690fab0b82105c154685c63016a21",
    "files": [
        {
            "sha": "dd92b15ffd4a98ad458c5be8d43cd96a95b1818a",
            "filename": "docs/source/en/executorch.md",
            "status": "modified",
            "additions": 12,
            "deletions": 39,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/19df66dcba7690fab0b82105c154685c63016a21/docs%2Fsource%2Fen%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/19df66dcba7690fab0b82105c154685c63016a21/docs%2Fsource%2Fen%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexecutorch.md?ref=19df66dcba7690fab0b82105c154685c63016a21",
            "patch": "@@ -16,44 +16,17 @@ rendered properly in your Markdown viewer.\n \n # ExecuTorch\n \n-[ExecuTorch](https://pytorch.org/executorch/stable/index.html) is a platform that enables PyTorch training and inference programs to be run on mobile and edge devices. It is powered by [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html) and [torch.export](https://pytorch.org/docs/main/export.html) for performance and deployment.\n-\n-You can use ExecuTorch with Transformers with [torch.export](https://pytorch.org/docs/main/export.html). The [`~transformers.convert_and_export_with_cache`] method converts a [`PreTrainedModel`] into an exportable module. Under the hood, it uses [torch.export](https://pytorch.org/docs/main/export.html) to export the model, ensuring compatibility with ExecuTorch.\n-\n-```py\n-import torch\n-from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig\n-from transformers.integrations.executorch import(\n-    TorchExportableModuleWithStaticCache,\n-    convert_and_export_with_cache\n-)\n-\n-generation_config = GenerationConfig(\n-    use_cache=True,\n-    cache_implementation=\"static\",\n-    cache_config={\n-        \"batch_size\": 1,\n-        \"max_cache_len\": 20,\n-    }\n-)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", pad_token=\"</s>\", padding_side=\"right\")\n-model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", generation_config=generation_config)\n-\n-exported_program = convert_and_export_with_cache(model)\n-```\n-\n-The exported PyTorch model is now ready to be used with ExecuTorch. Wrap the model with [`~transformers.TorchExportableModuleWithStaticCache`] to generate text.\n+[ExecuTorch](https://pytorch.org/executorch/stable/index.html) runs PyTorch models on mobile and edge devices. Export your Transformers models to the ExecuTorch format with [Optimum ExecuTorch](https://github.com/huggingface/optimum-executorch) with the command below.\n \n-```py\n-prompts = [\"Simply put, the theory of relativity states that \"]\n-prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n-prompt_token_ids = prompt_tokens[\"input_ids\"]\n-\n-generated_ids = TorchExportableModuleWithStaticCache.generate(\n-    exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=20,\n-)\n-generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-print(generated_text)\n-['Simply put, the theory of relativity states that 1) the speed of light is the']\n ```\n+optimum-cli export executorch \\\n+    --model \"HuggingFaceTB/SmolLM2-135M-Instruct\" \\\n+    --task \"text-generation\" \\\n+    --recipe \"xnnpack\" \\\n+    --use_custom_sdpa \\\n+    --use_custom_kv_cache \\\n+    --qlinear 8da4w \\\n+    --qembedding 8w \\\n+    --output_dir=\"hf_smollm2\"\n+```\n+Run `optimum-cli export executorch --help` to see all export options. For detailed export instructions, check the [README](optimum/exporters/executorch/README.md)."
        }
    ],
    "stats": {
        "total": 51,
        "additions": 12,
        "deletions": 39
    }
}