{
    "author": "Rocketknight1",
    "message": "Don't accidentally mutate the base_model_tp_plan (#36677)\n\n* Don't accidentally mutate the base_model_tp_plan\n\n* Co-authored by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Trigger tests\n\n* Marking grad accum test as slow\n\n* Add a flaky decorator\n\n* Add a flaky decorator\n\n* Use cyril's codeblock\n\n* Don't copy() when it's None\n\n* Use cyril's new codeblock\n\n* make fixup",
    "sha": "c7eb95581a8da3e41ae98decfeb98f7be4d10d82",
    "files": [
        {
            "sha": "a33f556f843a76ee2124cec7fe869b9c92c9ad80",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c7eb95581a8da3e41ae98decfeb98f7be4d10d82",
            "patch": "@@ -1895,9 +1895,15 @@ def post_init(self):\n \n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n-            self._pp_plan = self.config.base_model_pp_plan\n-\n-        self._tp_plan = self._tp_plan or self.config.base_model_tp_plan or {}\n+            self._pp_plan = (\n+                self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n+            )\n+            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n+        else:\n+            self._tp_plan = self._tp_plan or {}\n+            for name, module in self.named_children():\n+                if plan := getattr(module, \"_tp_plan\", None):\n+                    self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.items()})\n         for name, module in self.named_children():\n             if plan := getattr(module, \"_tp_plan\", None):\n                 self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.items()})"
        },
        {
            "sha": "6d93c77d865c09cb5a3b84c9d540073c28b640c2",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c7eb95581a8da3e41ae98decfeb98f7be4d10d82",
            "patch": "@@ -2305,6 +2305,7 @@ def test_generate_methods_with_logits_to_keep(self):\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     @pytest.mark.generate\n+    @is_flaky\n     def test_assisted_decoding_with_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n             if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):"
        },
        {
            "sha": "1f6441758c940f4e81b677b46d19e14e8822cafc",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7eb95581a8da3e41ae98decfeb98f7be4d10d82/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=c7eb95581a8da3e41ae98decfeb98f7be4d10d82",
            "patch": "@@ -803,6 +803,7 @@ def test_model_init(self):\n             trainer.train()\n             self.check_trained_model(trainer.model, alternate_seed=True)\n \n+    @slow\n     def test_gradient_accumulation_loss_alignment_with_model_loss(self):\n         set_seed(42)\n         import datasets"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 11,
        "deletions": 3
    }
}