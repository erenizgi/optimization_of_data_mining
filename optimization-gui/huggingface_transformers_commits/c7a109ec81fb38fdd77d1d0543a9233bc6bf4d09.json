{
    "author": "tshu-w",
    "message": "Fix `pad_token_tensor` is None in warning (#34005)\n\nFix pad_token_tensor is None in warning",
    "sha": "c7a109ec81fb38fdd77d1d0543a9233bc6bf4d09",
    "files": [
        {
            "sha": "5ef0c0eb81c87acd4c703d31b8bb5ce15a3fd58b",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7a109ec81fb38fdd77d1d0543a9233bc6bf4d09/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7a109ec81fb38fdd77d1d0543a9233bc6bf4d09/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c7a109ec81fb38fdd77d1d0543a9233bc6bf4d09",
            "patch": "@@ -1866,8 +1866,8 @@ def _tensor_or_none(token, device=None):\n                         \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n                         \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n                     )\n-                logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n             pad_token_tensor = eos_token_tensor[0]\n+            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n \n         # Sanity checks/warnings\n         if self.config.is_encoder_decoder and decoder_start_token_tensor is None:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}