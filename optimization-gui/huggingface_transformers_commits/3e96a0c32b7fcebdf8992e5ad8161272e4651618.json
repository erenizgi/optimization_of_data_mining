{
    "author": "ricalanis",
    "message": "Update falcon model card (#37184)\n\n* feat: updated model card for falcon\n\n* fix:rewrite model description\n\n* fix: add link to conversion script\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix: Add suggested changes\n\n* fix: typo in link for quantization\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/falcon.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix: fix indent and close ticks\n\n* fix: add indent\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "3e96a0c32b7fcebdf8992e5ad8161272e4651618",
    "files": [
        {
            "sha": "72c7c3274c244e0e88ffb78aa9c15c9ee2bd5349",
            "filename": "docs/source/en/model_doc/falcon.md",
            "status": "modified",
            "additions": 94,
            "deletions": 31,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e96a0c32b7fcebdf8992e5ad8161272e4651618/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e96a0c32b7fcebdf8992e5ad8161272e4651618/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md?ref=3e96a0c32b7fcebdf8992e5ad8161272e4651618",
            "patch": "@@ -14,48 +14,113 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # Falcon\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[Falcon](https://huggingface.co/papers/2311.16867) is a family of large language models, available in 7B, 40B, and 180B parameters, as pretrained and instruction tuned variants. This model focuses on scaling pretraining over three categories, performance, data, and hardware. Falcon uses multigroup attention to significantly reduce inference memory requirements and rotary positional embeddings (RoPE). These models are pretrained on [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality and deduplicated 5T token dataset.\n+\n+You can find all the original Falcon checkpoints under the [Falcon](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a) collection.\n+\n+> [!TIP]\n+> Click on the Falcon models in the right sidebar for more examples of how to apply Falcon to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text-generation\", \n+    model=\"tiiuae/falcon-7b-instruct\",\n+    torch_dtype=torch.bfloat16,\n+    device=0\n+)\n+pipeline(\n+    \"Write a short poem about coding\",\n+    max_length=100,\n+    do_sample=True,\n+    temperature=0.7\n+)\n+```\n \n-## Overview\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-Falcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints\n-have been trained on >=1T tokens of text, with a particular emphasis on the [RefinedWeb](https://arxiv.org/abs/2306.01116)\n-corpus. They are made available under the Apache 2.0 license.\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n+tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"tiiuae/falcon-7b-instruct\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\",\n+)\n \n-Falcon's architecture is modern and optimized for inference, with multi-query attention and support for efficient\n-attention variants like `FlashAttention`. Both 'base' models trained only as causal language models as well as\n-'instruct' models that have received further fine-tuning are available.\n+input_ids = tokenizer(\"Write a short poem about coding\", return_tensors=\"pt\").to(\"cuda\")\n \n+output = model.generate(**input_ids)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n \n-Falcon models are (as of 2023) some of the largest and most powerful open-source language models,\n-and consistently rank highly in the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n \n-## Converting custom checkpoints \n+```bash\n+# pip install -U flash-attn --no-build-isolation\n+transformers-cli chat --model_name_or_path tiiuae/falcon-7b-instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+```\n \n-<Tip>\n+</hfoption>\n+</hfoptions>\n \n-Falcon models were initially added to the Hugging Face Hub as custom code checkpoints. However, Falcon is now fully\n-supported in the Transformers library. If you fine-tuned a model from a custom code checkpoint, we recommend converting\n-your checkpoint to the new in-library format, as this should give significant improvements to stability and\n-performance, especially for generation, as well as removing the need to use `trust_remote_code=True`!\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-</Tip>\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n \n-You can convert custom code checkpoints to full Transformers checkpoints using the `convert_custom_code_checkpoint.py` \n-script located in the\n-[Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)\n-of the Transformers library. To use this script, simply call it with \n-`python convert_custom_code_checkpoint.py --checkpoint_dir my_model`. This will convert your checkpoint in-place, and\n-you can immediately load it from the directory afterwards with e.g. `from_pretrained()`. If your model hasn't been\n-uploaded to the Hub, we recommend making a backup before attempting the conversion, just in case!\n+```python\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n \n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_use_double_quant=True,\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"tiiuae/falcon-7b\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config,\n+)\n+\n+inputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\n+outputs = model.generate(**inputs, max_new_tokens=100)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n+```\n+\n+## Notes\n+\n+- If you're upgrading from an older custom code checkpoint, remember to convert it to the official Transformers format for better stability and performance using the conversion script located in the [Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon).\n+\n+   ```bash\n+   python convert_custom_code_checkpoint.py --checkpoint_dir my_model\n+   ```\n \n ## FalconConfig\n \n@@ -85,6 +150,4 @@ uploaded to the Hub, we recommend making a backup before attempting the conversi\n ## FalconForQuestionAnswering\n \n [[autodoc]] FalconForQuestionAnswering\n-    - forward\n-\n-\n+    - forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 125,
        "additions": 94,
        "deletions": 31
    }
}