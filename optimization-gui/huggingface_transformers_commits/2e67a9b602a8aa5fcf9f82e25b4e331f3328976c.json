{
    "author": "yonigozlan",
    "message": "Add LightGlue fast image processor (#41670)\n\n* add fast image processor skel\n\n* add working lightglue fast image processor + tests\n\n* remove plot_keypoint_matching",
    "sha": "2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
    "files": [
        {
            "sha": "7deff2222ebe4de5a50667e7e5418918a74e852f",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -88,16 +88,16 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     import torch\n     from PIL import Image\n     import requests\n-    \n+\n     processor = AutoImageProcessor.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n     model = AutoModel.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n-    \n+\n     # LightGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n     with torch.inference_mode():\n         outputs = model(**inputs)\n-    \n+\n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image\n     keypoints1 = outputs.keypoints1  # Keypoints in second image\n@@ -112,7 +112,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # Process outputs for visualization\n     image_sizes = [[(image.height, image.width) for image in images]]\n     processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-    \n+\n     for i, output in enumerate(processed_outputs):\n         print(f\"For the image pair {i}\")\n         for keypoint0, keypoint1, matching_score in zip(\n@@ -147,6 +147,13 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     - post_process_keypoint_matching\n     - visualize_keypoint_matching\n \n+## LightGlueImageProcessorFast\n+\n+[[autodoc]] LightGlueImageProcessorFast\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n+\n ## LightGlueForKeypointMatching\n \n [[autodoc]] LightGlueForKeypointMatching"
        },
        {
            "sha": "60af0f869bad00055f085f181a54fac7c046566f",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -121,7 +121,7 @@\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n             (\"lfm2_vl\", (None, \"Lfm2VlImageProcessorFast\")),\n-            (\"lightglue\", (\"LightGlueImageProcessor\", None)),\n+            (\"lightglue\", (\"LightGlueImageProcessor\", \"LightGlueImageProcessorFast\")),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),"
        },
        {
            "sha": "7ee9c5ffbd64bc05501ecf78456dd182696c8d81",
            "filename": "src/transformers/models/lightglue/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2F__init__.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_lightglue import *\n     from .image_processing_lightglue import *\n+    from .image_processing_lightglue_fast import *\n     from .modeling_lightglue import *\n else:\n     import sys"
        },
        {
            "sha": "54cb70785397a1a4de5e3f1aee4775e0a7cba899",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 12,
            "deletions": 60,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -17,7 +17,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n from typing import Optional, Union\n \n import numpy as np\n@@ -40,20 +39,28 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_matplotlib_available, logging, requires_backends\n+from ...processing_utils import ImagesKwargs\n+from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n from .modeling_lightglue import LightGlueKeypointMatchingOutput\n \n \n-if is_vision_available():\n-    from PIL import Image, ImageDraw\n-\n if is_vision_available():\n     import PIL\n+    from PIL import Image, ImageDraw\n \n logger = logging.get_logger(__name__)\n \n \n+class LightGlueImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: bool\n+\n+\n def is_grayscale(\n     image: np.ndarray,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -461,60 +468,5 @@ def _get_color(self, score):\n         b = 0\n         return (r, g, b)\n \n-    def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n-        \"\"\"\n-        Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n-        matplotlib to be installed.\n-\n-        .. deprecated::\n-            `plot_keypoint_matching` is deprecated and will be removed in a future version. Use `visualize_keypoint_matching` instead.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n-                a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            keypoint_matching_output ([`LightGlueKeypointMatchingOutput`]):\n-                Raw outputs of the model.\n-        \"\"\"\n-        warnings.warn(\n-            \"`plot_keypoint_matching` is deprecated and will be removed in transformers v. \"\n-            \"Use `visualize_keypoint_matching` instead.\",\n-            FutureWarning,\n-        )\n-\n-        if is_matplotlib_available():\n-            import matplotlib.pyplot as plt\n-        else:\n-            raise ImportError(\"Please install matplotlib to use `plot_keypoint_matching` method\")\n-\n-        images = validate_and_format_image_pairs(images)\n-        images = [to_numpy_array(image) for image in images]\n-        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n-\n-        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n-            height0, width0 = image_pair[0].shape[:2]\n-            height1, width1 = image_pair[1].shape[:2]\n-            plot_image = np.zeros((max(height0, height1), width0 + width1, 3))\n-            plot_image[:height0, :width0] = image_pair[0] / 255.0\n-            plot_image[:height1, width0:] = image_pair[1] / 255.0\n-            plt.imshow(plot_image)\n-            plt.axis(\"off\")\n-\n-            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n-            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n-            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n-                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n-            ):\n-                plt.plot(\n-                    [keypoint0_x, keypoint1_x + width0],\n-                    [keypoint0_y, keypoint1_y],\n-                    color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n-                    alpha=0.9,\n-                    linewidth=0.5,\n-                )\n-                plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n-                plt.scatter(keypoint1_x + width0, keypoint1_y, c=\"black\", s=2)\n-            plt.show()\n-\n \n __all__ = [\"LightGlueImageProcessor\"]"
        },
        {
            "sha": "0cfcd7c698182ee22a6b0202e7bf6b3896cb4f18",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue_fast.py",
            "status": "added",
            "additions": 302,
            "deletions": 0,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -0,0 +1,302 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lightglue/modular_lightglue.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lightglue.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+    is_vision_available,\n+    to_numpy_array,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_lightglue import LightGlueImageProcessorKwargs\n+from .modeling_lightglue import LightGlueKeypointMatchingOutput\n+\n+\n+if is_vision_available():\n+    from PIL import Image, ImageDraw\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class LightGlueImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = LightGlueImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[LightGlueImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LightGlueImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: LightGlueKeypointMatchingOutput,\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from .image_processing_lightglue import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return r, g, b\n+\n+\n+__all__ = [\"LightGlueImageProcessorFast\"]"
        },
        {
            "sha": "a5214f0be3c76f1f3eb0b01562e1d52fdadda9f8",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 25,
            "deletions": 125,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Optional, Union\n@@ -22,25 +21,24 @@\n from torch.nn.utils.rnn import pad_sequence\n \n from ...configuration_utils import PreTrainedConfig\n-from ...image_utils import ImageInput, is_vision_available, to_numpy_array\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, TensorType, auto_docstring, is_matplotlib_available, logging\n+from ...utils import ModelOutput, TensorType, auto_docstring, logging\n from ...utils.generic import can_return_tuple\n from ..auto import CONFIG_MAPPING, AutoConfig\n from ..auto.modeling_auto import AutoModelForKeypointDetection\n from ..clip.modeling_clip import CLIPMLP\n from ..cohere.modeling_cohere import apply_rotary_pos_emb\n from ..llama.modeling_llama import LlamaAttention, eager_attention_forward\n-from ..superglue.image_processing_superglue import SuperGlueImageProcessor, validate_and_format_image_pairs\n+from ..superglue.image_processing_superglue import (\n+    SuperGlueImageProcessor,\n+    SuperGlueImageProcessorKwargs,\n+)\n+from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n from ..superpoint import SuperPointConfig\n \n \n-if is_vision_available():\n-    from PIL import Image, ImageDraw\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -217,6 +215,10 @@ class LightGlueKeypointMatchingOutput(ModelOutput):\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n+class LightGlueImageProcessorKwargs(SuperGlueImageProcessorKwargs):\n+    pass\n+\n+\n class LightGlueImageProcessor(SuperGlueImageProcessor):\n     def post_process_keypoint_matching(\n         self,\n@@ -226,123 +228,15 @@ def post_process_keypoint_matching(\n     ) -> list[dict[str, torch.Tensor]]:\n         return super().post_process_keypoint_matching(outputs, target_sizes, threshold)\n \n-    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor.visualize_keypoint_matching with EfficientLoFTR->LightGlue\n-    def visualize_keypoint_matching(\n-        self,\n-        images: ImageInput,\n-        keypoint_matching_output: list[dict[str, torch.Tensor]],\n-    ) -> list[\"Image.Image\"]:\n-        \"\"\"\n-        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2\n-                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n-                A post processed keypoint matching output\n-\n-        Returns:\n-            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n-            keypoints as well as the matching between them.\n-        \"\"\"\n-        images = validate_and_format_image_pairs(images)\n-        images = [to_numpy_array(image) for image in images]\n-        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n-\n-        results = []\n-        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n-            height0, width0 = image_pair[0].shape[:2]\n-            height1, width1 = image_pair[1].shape[:2]\n-            plot_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n-            plot_image[:height0, :width0] = image_pair[0]\n-            plot_image[:height1, width0:] = image_pair[1]\n-\n-            plot_image_pil = Image.fromarray(plot_image)\n-            draw = ImageDraw.Draw(plot_image_pil)\n-\n-            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n-            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n-            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n-                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n-            ):\n-                color = self._get_color(matching_score)\n-                draw.line(\n-                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n-                    fill=color,\n-                    width=3,\n-                )\n-                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n-                draw.ellipse(\n-                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n-                    fill=\"black\",\n-                )\n \n-            results.append(plot_image_pil)\n-        return results\n-\n-    # Copied from transformers.models.efficientloftr.image_processing_efficientloftr.EfficientLoFTRImageProcessor._get_color\n-    def _get_color(self, score):\n-        \"\"\"Maps a score to a color.\"\"\"\n-        r = int(255 * (1 - score))\n-        g = int(255 * score)\n-        b = 0\n-        return (r, g, b)\n-\n-    def plot_keypoint_matching(self, images: ImageInput, keypoint_matching_output: LightGlueKeypointMatchingOutput):\n-        \"\"\"\n-        Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires\n-        matplotlib to be installed.\n-\n-        .. deprecated::\n-            `plot_keypoint_matching` is deprecated and will be removed in a future version. Use `visualize_keypoint_matching` instead.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image pairs to plot. Same as `LightGlueImageProcessor.preprocess`. Expects either a list of 2 images or\n-                a list of list of 2 images list with pixel values ranging from 0 to 255.\n-            keypoint_matching_output ([`LightGlueKeypointMatchingOutput`]):\n-                Raw outputs of the model.\n-        \"\"\"\n-        warnings.warn(\n-            \"`plot_keypoint_matching` is deprecated and will be removed in transformers v. \"\n-            \"Use `visualize_keypoint_matching` instead.\",\n-            FutureWarning,\n-        )\n-\n-        if is_matplotlib_available():\n-            import matplotlib.pyplot as plt\n-        else:\n-            raise ImportError(\"Please install matplotlib to use `plot_keypoint_matching` method\")\n-\n-        images = validate_and_format_image_pairs(images)\n-        images = [to_numpy_array(image) for image in images]\n-        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n-\n-        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n-            height0, width0 = image_pair[0].shape[:2]\n-            height1, width1 = image_pair[1].shape[:2]\n-            plot_image = np.zeros((max(height0, height1), width0 + width1, 3))\n-            plot_image[:height0, :width0] = image_pair[0] / 255.0\n-            plot_image[:height1, width0:] = image_pair[1] / 255.0\n-            plt.imshow(plot_image)\n-            plt.axis(\"off\")\n-\n-            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n-            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n-            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n-                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n-            ):\n-                plt.plot(\n-                    [keypoint0_x, keypoint1_x + width0],\n-                    [keypoint0_y, keypoint1_y],\n-                    color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n-                    alpha=0.9,\n-                    linewidth=0.5,\n-                )\n-                plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n-                plt.scatter(keypoint1_x + width0, keypoint1_y, c=\"black\", s=2)\n-            plt.show()\n+class LightGlueImageProcessorFast(SuperGlueImageProcessorFast):\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: LightGlueKeypointMatchingOutput,\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        return super().post_process_keypoint_matching(outputs, target_sizes, threshold)\n \n \n class LightGluePositionalEncoder(nn.Module):\n@@ -1081,4 +975,10 @@ def forward(\n         )\n \n \n-__all__ = [\"LightGluePreTrainedModel\", \"LightGlueForKeypointMatching\", \"LightGlueConfig\", \"LightGlueImageProcessor\"]\n+__all__ = [\n+    \"LightGluePreTrainedModel\",\n+    \"LightGlueForKeypointMatching\",\n+    \"LightGlueConfig\",\n+    \"LightGlueImageProcessor\",\n+    \"LightGlueImageProcessorFast\",\n+]"
        },
        {
            "sha": "1ffef05462930329ec0a99bbd7e0ed764a0f98fe",
            "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e67a9b602a8aa5fcf9f82e25b4e331f3328976c/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2Ftest_image_processing_lightglue.py?ref=2e67a9b602a8aa5fcf9f82e25b4e331f3328976c",
            "patch": "@@ -18,7 +18,7 @@\n     SuperGlueImageProcessingTester,\n )\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n \n if is_torch_available():\n@@ -30,6 +30,9 @@\n if is_vision_available():\n     from transformers import LightGlueImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import LightGlueImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -90,7 +93,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n-    fast_image_processing_class = None\n+    fast_image_processing_class = LightGlueImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()"
        }
    ],
    "stats": {
        "total": 549,
        "additions": 357,
        "deletions": 192
    }
}