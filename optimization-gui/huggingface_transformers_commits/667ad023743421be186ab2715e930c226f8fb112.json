{
    "author": "rudolfwilliam",
    "message": "Remove double soft-max in load-balancing loss. Fixes #39055 . (#39056)\n\nRemove double soft-max in load-balancing loss. Fixes #39055",
    "sha": "667ad023743421be186ab2715e930c226f8fb112",
    "files": [
        {
            "sha": "92695f5e57781214e0e0f67519cc839414e97aec",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ad023743421be186ab2715e930c226f8fb112/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ad023743421be186ab2715e930c226f8fb112/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=667ad023743421be186ab2715e930c226f8fb112",
            "patch": "@@ -124,7 +124,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n \n \n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor,\n+    gate_probabilities: torch.Tensor,\n     num_experts: int,\n     top_k: int,\n     attention_mask: Optional[torch.Tensor],\n@@ -150,14 +150,12 @@ def load_balancing_loss_func(\n     Returns:\n         The auxiliary loss.\n     \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n+    if gate_probabilities is None or not isinstance(gate_probabilities, tuple):\n         return torch.tensor(0.0)\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+    if isinstance(gate_probabilities, tuple):\n+        compute_device = gate_probabilities[0].device\n+        routing_weights = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_probabilities], dim=0)\n \n     _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n \n@@ -171,7 +169,7 @@ def load_balancing_loss_func(\n         router_prob_per_expert = torch.mean(routing_weights, dim=0)\n     else:\n         batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+        num_hidden_layers = routing_weights.shape[0] // (batch_size * sequence_length)\n \n         # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n         expert_attention_mask = ("
        }
    ],
    "stats": {
        "total": 14,
        "additions": 6,
        "deletions": 8
    }
}