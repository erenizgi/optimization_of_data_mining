{
    "author": "cyr0930",
    "message": "[bug] fix llava processor to calculate unpadding size correctly (#37988)\n\n* fix llava processor to calculate unpad size correctly\n\n* repo consistency\n\n* Revert \"repo consistency\" & \"setUp in llava family\"\n\nThis reverts commit 26a50af8db5b15bb6b700db3d53342fe69579d8e.\n\n* add edge case test for padding & unpadding\n\n* compute unpadding size from original size\n\n* make test config explicit\n\n* Revert \"compute unpadding size from original size\"\n\nThis reverts commit 752cd27ad9710ab056c17a9986760c4651975540.\n\n* Revert \"add edge case test for padding & unpadding\"\n\nThis reverts commit ccbd094d69c3f8f6a259159164284f60ba835bce.\n\n* revert unpad logic\n\n* remove irrelevant tests\n\n* model test\n\n* remove processor from model test\n\n---------\n\nCo-authored-by: jaycha <jaycha@ncsoft.com>",
    "sha": "a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
    "files": [
        {
            "sha": "718c1e96c988f98f9a52bdd2bdc2258cd005ef31",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -20,7 +20,6 @@\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -133,14 +132,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = min(math.ceil(original_height * scale_factor), current_height)\n-        padding, r = divmod(current_height - new_height, 2)\n-        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n+        new_height = int(round(original_height * scale_factor, 7))\n+        padding = (current_height - new_height) // 2\n+        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = min(math.ceil(original_width * scale_factor), current_width)\n-        padding, r = divmod(current_width - new_width, 2)\n-        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n+        new_width = int(round(original_width * scale_factor, 7))\n+        padding = (current_width - new_width) // 2\n+        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "b956b6f1bd0d6f4b875a361caaaabc4c45c03304",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -304,14 +304,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = min(math.ceil(original_height * scale_factor), current_height)\n-        padding, r = divmod(current_height - new_height, 2)\n-        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n+        new_height = int(round(original_height * scale_factor, 7))\n+        padding = (current_height - new_height) // 2\n+        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = min(math.ceil(original_width * scale_factor), current_width)\n-        padding, r = divmod(current_width - new_width, 2)\n-        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n+        new_width = int(round(original_width * scale_factor, 7))\n+        padding = (current_width - new_width) // 2\n+        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "88ef859ad29c6358bcc1158643d985ca4513ea53",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -18,7 +18,6 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from transformers.models.llava_next.modeling_llava_next import ("
        },
        {
            "sha": "a664cfa7b64d1fe93a1f9aa4e5bd6ee6dad514e8",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -644,7 +644,7 @@ def preprocess(\n                 image,\n                 image_grid_pinpoints,\n                 size=size_tuple,\n-                patch_size=size[\"height\"],\n+                patch_size=size_tuple[0],\n                 resample=resample,\n                 data_format=input_data_format,\n                 input_data_format=input_data_format,"
        },
        {
            "sha": "18475d381a01244392c5c3bcfd4c4e62b7dcf18d",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -284,14 +284,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = min(math.ceil(original_height * scale_factor), current_height)\n-        padding, r = divmod(current_height - new_height, 2)\n-        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n+        new_height = int(round(original_height * scale_factor, 7))\n+        padding = (current_height - new_height) // 2\n+        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = min(math.ceil(original_width * scale_factor), current_width)\n-        padding, r = divmod(current_width - new_width, 2)\n-        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n+        new_width = int(round(original_width * scale_factor, 7))\n+        padding = (current_width - new_width) // 2\n+        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "93142f1da68d0bcdd194285f550e9b36c32d522e",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 20,
            "deletions": 11,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -50,7 +50,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches, unpad_image\n+    from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches\n \n \n if is_vision_available():\n@@ -298,18 +298,27 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n-    def test_unpad_image(self):\n-        original_size = (400, 400)\n+    def test_odd_sized_image(self):\n+        # prepare model configuration\n+        config = self.model_tester.get_config()\n \n-        # Test case width is padded\n-        pixel_values = floats_tensor([3, 400, 601])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # prepare input\n+        num_image_tokens = 24\n+        pixel_values = floats_tensor([1, 5, 3, config.vision_config.image_size, config.vision_config.image_size])\n+        input_ids = ids_tensor([1, 64], config.text_config.vocab_size - 2) + 2\n+        input_ids[:, :num_image_tokens] = config.image_token_index\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_sizes\": torch.tensor([[13, 16]]),  # odd-sized image\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n \n-        # Test case height is padded\n-        pixel_values = floats_tensor([3, 503, 400])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # forward with odd-sized image input\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model(**inputs_dict)\n \n     @parameterized.expand(\n         ["
        },
        {
            "sha": "2adf527d78236a74ef602ef108dfb5135cd4bf29",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -11,13 +11,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import json\n+import shutil\n import tempfile\n import unittest\n \n import torch\n \n-from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextProcessor\n+from transformers import LlamaTokenizerFast, LlavaNextProcessor\n from transformers.testing_utils import (\n     require_vision,\n )\n@@ -52,6 +54,10 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n     @staticmethod\n     def prepare_processor_dict():\n         return {\n@@ -73,13 +79,16 @@ def test_chat_template_is_saved(self):\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n \n     def test_image_token_filling(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         processor.patch_size = 14\n         processor.vision_feature_select_strategy = \"default\"\n+        processor.image_processor.crop_size = {\"height\": 336, \"width\": 336}\n+        processor.image_processor.size = {\"shortest_edge\": 336}\n+        processor.image_processor.image_grid_pinpoints = [[672, 336]]\n         # Important to check with non square image\n-        image = torch.randint(0, 2, (3, 500, 316))\n-        expected_image_tokens = 1526\n-        image_token_index = 32000\n+        image = torch.randint(0, 2, (3, 503, 316))\n+        expected_image_tokens = 1525\n+        image_token_index = processor.image_token_id\n \n         messages = [\n             {"
        },
        {
            "sha": "577dd66986064f62b14159b0e50afb8820c25638",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -49,8 +49,6 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.models.llava_next_video.modeling_llava_next_video import unpad_image\n-\n \n if is_vision_available():\n     from PIL import Image\n@@ -314,18 +312,27 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n-    def test_unpad_image(self):\n-        original_size = (400, 400)\n+    def test_odd_sized_image(self):\n+        # prepare model configuration\n+        config = self.model_tester.get_config()\n \n-        # Test case width is padded\n-        pixel_values = floats_tensor([3, 400, 601])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # prepare input\n+        num_image_tokens = 24\n+        pixel_values = floats_tensor([1, 5, 3, config.vision_config.image_size, config.vision_config.image_size])\n+        input_ids = ids_tensor([1, 64], config.text_config.vocab_size - 2) + 2\n+        input_ids[:, :num_image_tokens] = config.image_token_index\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_sizes\": torch.tensor([[13, 16]]),  # odd-sized image\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n \n-        # Test case height is padded\n-        pixel_values = floats_tensor([3, 503, 400])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # forward with odd-sized image input\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model(**inputs_dict)\n \n     @parameterized.expand(\n         ["
        },
        {
            "sha": "49fa33ffc1430e7e9e97ffb6ae36aa705d78f0a9",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 34,
            "deletions": 3,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -17,6 +17,8 @@\n import tempfile\n import unittest\n \n+import torch\n+\n from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextVideoProcessor\n from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n@@ -63,6 +65,10 @@ def get_image_processor(self, **kwargs):\n     def get_video_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n \n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n     @classmethod\n     def prepare_processor_dict(cls):\n         return {\n@@ -84,6 +90,31 @@ def test_chat_template_is_saved(self):\n         processor_dict = self.prepare_processor_dict()\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n \n-    @classmethod\n-    def tearDownClass(cls):\n-        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+    def test_image_token_filling(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor.patch_size = 14\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.image_processor.crop_size = {\"height\": 336, \"width\": 336}\n+        processor.image_processor.size = {\"shortest_edge\": 336}\n+        processor.image_processor.image_grid_pinpoints = [[672, 336]]\n+        # Important to check with non square image\n+        image = torch.randint(0, 2, (3, 503, 316))\n+        expected_image_tokens = 1525\n+        image_token_index = processor.image_token_id\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)"
        },
        {
            "sha": "fba739b9956f9216cad11146c09cfec52404262e",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -49,8 +49,6 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.models.llava_onevision.modeling_llava_onevision import unpad_image\n-\n \n if is_vision_available():\n     from PIL import Image\n@@ -268,18 +266,27 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             torch.testing.assert_close(out_embeds, out_ids)\n \n-    def test_unpad_image(self):\n-        original_size = (400, 400)\n+    def test_odd_sized_image(self):\n+        # prepare model configuration\n+        config = self.model_tester.get_config()\n \n-        # Test case width is padded\n-        pixel_values = floats_tensor([3, 400, 601])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # prepare input\n+        num_image_tokens = 10\n+        pixel_values = floats_tensor([1, 2, 3, config.vision_config.image_size, config.vision_config.image_size])\n+        input_ids = ids_tensor([1, 64], config.text_config.vocab_size - 2) + 2\n+        input_ids[:, :num_image_tokens] = config.image_token_index\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_sizes\": torch.tensor([[13, 16]]),  # odd-sized image\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n \n-        # Test case height is padded\n-        pixel_values = floats_tensor([3, 503, 400])\n-        unpadded_tensor = unpad_image(pixel_values, original_size)\n-        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+        # forward with odd-sized image input\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model(**inputs_dict)\n \n     @parameterized.expand(\n         ["
        },
        {
            "sha": "d4bd5f000256f431abf1f8bf9f6e477c6c2f0da1",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5cc7a67d72b2a8e709f793c636b64fe423f71f6/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=a5cc7a67d72b2a8e709f793c636b64fe423f71f6",
            "patch": "@@ -11,11 +11,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import json\n import shutil\n import tempfile\n import unittest\n \n+import torch\n+\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -90,3 +93,33 @@ def test_chat_template_is_saved(self):\n         # so we check if the same template is loaded\n         processor_dict = self.prepare_processor_dict()\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n+    def test_image_token_filling(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor.patch_size = 14\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.image_processor.crop_size = {\"height\": 336, \"width\": 336}\n+        processor.image_processor.size = {\"shortest_edge\": 336}\n+        processor.image_processor.image_grid_pinpoints = [[672, 336]]\n+        processor.num_image_tokens = (processor.image_processor.size[\"shortest_edge\"] // processor.patch_size) ** 2\n+        # Important to check with non square image\n+        image = torch.randint(0, 2, (3, 503, 316))\n+        expected_image_tokens = 1525\n+        image_token_index = processor.image_token_id\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)"
        }
    ],
    "stats": {
        "total": 222,
        "additions": 158,
        "deletions": 64
    }
}