{
    "author": "Cyrilvallez",
    "message": "[loading] Correctly load params during offloading & careful memory considerations (#42632)\n\n* do not load everything in advance\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix memory leaks during conversion\n\n* oupsi\n\n* fix device_map\n\n* add doc\n\n* fix\n\n* doc\n\n* make it a method\n\n* doc\n\n* first shot at test\n\n* fix test\n\n* fix\n\n* revert test: cpu mem too hard to track correctly\n\n* fix",
    "sha": "1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1",
    "files": [
        {
            "sha": "01bb9c3770b46ff22922104bae8ce4c8d5b8ed1e",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 62,
            "deletions": 35,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1",
            "patch": "@@ -20,7 +20,7 @@\n import re\n from abc import abstractmethod\n from collections import defaultdict\n-from collections.abc import MutableMapping, MutableSet\n+from collections.abc import Callable, MutableMapping, MutableSet\n from concurrent.futures import Future, ThreadPoolExecutor\n from contextlib import contextmanager\n from copy import deepcopy\n@@ -327,10 +327,6 @@ def add_tensor(self, target_key: str, source_key: str, source_pattern: str, futu\n         self.collected_tensors[source_pattern].append(future)\n         self.layer_targets[target_key].add(source_key)\n \n-    def reset(self) -> None:\n-        \"\"\"Clean-up the collected tensors to make sure we don't keep references to past tensors in memory.\"\"\"\n-        self.collected_tensors = defaultdict(list)\n-\n     def rename_source_key(self, source_key: str) -> tuple[str, str | None]:\n         \"\"\"\n         Return a tuple (renamed_key, source_pattern_producing_the_match).\n@@ -375,6 +371,32 @@ def reverse_transform(self) -> WeightTransform:\n \n         return reverse_transform\n \n+    def materialize_tensors(self) -> dict[str, list[torch.Tensor]]:\n+        \"\"\"\n+        Materialize all the tensors that were saved in `self.collected_tensors`. This function removes them from the\n+        internal attribute to avoid keeping them in memory during the different `self.convert` operations, and return\n+        a new dictionary (otherwise we use more memory than needed during loading).\n+\n+        We basically have 3 cases here:\n+        - async loading (default): the tensors are Future instances that we need to wait for\n+        - sync loading: the tensors are Callable, we need to call the Callable to actually load them from disk\n+        - saving: the tensors are already torch.Tensor instances (the existing model weights)\n+        \"\"\"\n+        collected_tensors = {}\n+        for key in set(self.collected_tensors.keys()):\n+            # Remove from internal attribute\n+            tensors = self.collected_tensors.pop(key)\n+            # Async loading\n+            if isinstance(tensors[0], Future):\n+                tensors = [future.result() for future in tensors]\n+            # Sync loading\n+            elif callable(tensors[0]):\n+                tensors = [func() for func in tensors]\n+            # Add them to the new dictionary\n+            collected_tensors[key] = tensors\n+\n+        return collected_tensors\n+\n \n @dataclass(slots=True)\n class WeightRenaming(WeightTransform):\n@@ -389,19 +411,17 @@ def convert(\n         missing_keys: Optional[MutableSet[str]] = None,\n         misc: Optional[MutableMapping[str, str]] = None,\n     ):\n-        # Collect the tensor if using threading\n-        for pattern, futures in self.collected_tensors.items():\n-            self.collected_tensors[pattern] = (\n-                futures if isinstance(futures[0], torch.Tensor) else [future.result() for future in futures]\n-            )\n+        # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n+        # attribute during the whole process\n+        collected_tensors = self.materialize_tensors()\n \n         # Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\n         # only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\n         target_key = self.target_patterns[0]\n-        collected_tensors = {target_key: self.collected_tensors[self.source_patterns[0]]}\n+        collected_tensors = {target_key: collected_tensors[self.source_patterns[0]]}\n \n         if hf_quantizer is not None and self.quantization_operation is not None:\n-            with log_to_misc(layer_name, misc, (self.collected_tensors, layer_name), self.quantization_operation):\n+            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -437,15 +457,12 @@ def convert(\n         missing_keys: Optional[MutableSet[str]] = None,\n         misc: Optional[MutableMapping[str, str]] = None,\n     ):\n-        # Collect all tensors if using threading\n-        for pattern, futures in self.collected_tensors.items():\n-            self.collected_tensors[pattern] = (\n-                futures if isinstance(futures[0], torch.Tensor) else [future.result() for future in futures]\n-            )\n+        # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n+        # attribute during the whole process\n+        collected_tensors = self.materialize_tensors()\n \n-        collected_tensors = self.collected_tensors\n         for op in self.operations:\n-            with log_to_misc(layer_name, misc, (collected_tensors, layer_name), op):\n+            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), op):\n                 collected_tensors = op.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -472,7 +489,7 @@ def convert(\n             pass\n \n         if hf_quantizer is not None and self.quantization_operation is not None:\n-            with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n+            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -501,27 +518,36 @@ def _materialize_copy(tensor: torch.Tensor, device=None, dtype=None) -> torch.Te\n \n def spawn_materialize(\n     thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, device=None, dtype=None\n-) -> Future | torch.Tensor:\n-    \"\"\"Materialize a tensor from file asynchronously if `thread_pool` is provided, or immediately otherwise.\"\"\"\n+) -> Future | Callable:\n+    \"\"\"Materialize a tensor from file asynchronously if `thread_pool` is provided, or return a Callable that will\n+    load the tensor synchronously when called.\"\"\"\n+\n+    def _job():\n+        return _materialize_copy(tensor, device, dtype)\n+\n     if thread_pool is not None:\n-        return thread_pool.submit(_materialize_copy, tensor, device, dtype)\n+        return thread_pool.submit(_job)\n     else:\n-        return _materialize_copy(tensor, device, dtype)\n+        # Return the Callable here, not the Tensor itself, so we actually delay loading to avoid saturating cpu\n+        # memory during Conversion\n+        return _job\n \n \n def spawn_tp_materialize(\n     thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, sharding_method, tensor_idx, dtype=None\n-) -> Future | torch.Tensor:\n+) -> Future | Callable:\n     \"\"\"Materialize and shard a tensor (according to the TP-plan) from file asynchronously if `thread_pool` is provided, or\n-    immediately otherwise.\"\"\"\n+    return a Callable that will load the tensor synchronously when called.\"\"\"\n \n     def _job():\n         return sharding_method.shard_tensor(tensor, param_casting_dtype=dtype, tensor_idx=tensor_idx)[0]\n \n     if thread_pool is not None:\n         return thread_pool.submit(_job)\n     else:\n-        return _job()\n+        # Return the Callable here, not the Tensor itself, so we actually delay loading to avoid saturating cpu\n+        # memory during Conversion\n+        return _job\n \n \n def dot_natural_key(s: str):\n@@ -557,10 +583,10 @@ def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) ->\n \n         op_name = _format_op_name(op)\n         if isinstance(extras, tuple) and len(extras) == 2:\n-            values, target_keys = extras\n+            length, target_keys = extras\n             descriptor = f\"{op_name} \" if op_name else \"\"\n             misc[first_target_key] = (\n-                f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {len(values)}\"\n+                f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {length}\"\n             )\n         elif isinstance(extras, str):\n             suffix = f\" via {op_name}\" if op_name else \"\"\n@@ -796,11 +822,12 @@ def convert_and_load_state_dict_in_model(\n     mismatch_keys = set()\n     unexpected_keys = set()\n \n-    # We use threading by default, if not explicitly deactivated via env variable\n-    if not is_env_variable_true(\"HF_DEACTIVATE_ASYNC_LOAD\"):\n-        thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n-    else:\n+    # We use threading by default, if not explicitly deactivated via env variable. If we have to offload,\n+    # we cannot use it either to control the memory as we are under memory constraints, so we need to be sequential\n+    if is_env_variable_true(\"HF_DEACTIVATE_ASYNC_LOAD\") or \"disk\" in device_map.values():\n         thread_pool = None\n+    else:\n+        thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n \n     renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n     converters = [entry for entry in weight_mapping if isinstance(entry, WeightConverter)]\n@@ -928,8 +955,8 @@ def convert_and_load_state_dict_in_model(\n                                 hf_quantizer,\n                             )\n \n-                    # Cleanup the tensors that were gathered internally in the mapping\n-                    mapping.reset()\n+                    # Cleanup all the tensors that were gathered before next iteration\n+                    del realized_value\n \n                 except SkipLayer:\n                     continue"
        },
        {
            "sha": "efa2b9e3150e2a6d3e1b16271c7ea8dd016e215d",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=1d86d00ec3c9de948cf7d60e8d7d845560c8b8d1",
            "patch": "@@ -392,6 +392,15 @@ def _get_device_map(\n             )\n         else:\n             inferred_max_memory = get_max_memory(max_memory)\n+\n+        # If the user does not provide `max_memory`, accelerate sets the WHOLE cpu available memory as available.\n+        # This is unwanted, as we don't want to set extremely tight bound and pressure for cpu if we are memory-constrained,\n+        # especially if the model uses WeightConverter (because there will be some uncontrollable cpu memory spikes during\n+        # the conversions before we resave the weights). In those cases, it's better to offload to disk a bit more\n+        # if we were in-between, as otherwise we blow-up cpu memory\n+        if max_memory is None:\n+            inferred_max_memory[\"cpu\"] *= 0.90\n+\n         if hf_quantizer is not None:\n             inferred_max_memory = hf_quantizer.adjust_max_memory(inferred_max_memory)\n "
        }
    ],
    "stats": {
        "total": 106,
        "additions": 71,
        "deletions": 35
    }
}