{
    "author": "ydshieh",
    "message": "[testing] Fix `seed_oss` (#41052)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* Update tests/models/seed_oss/test_modeling_seed_oss.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "4f36011545807a9d03a81bfd8fc4f1014badc02d",
    "files": [
        {
            "sha": "e9cccfe9ff207205d5133a7e7090226438efea33",
            "filename": "tests/models/seed_oss/test_modeling_seed_oss.py",
            "status": "modified",
            "additions": 19,
            "deletions": 46,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f36011545807a9d03a81bfd8fc4f1014badc02d/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f36011545807a9d03a81bfd8fc4f1014badc02d/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py?ref=4f36011545807a9d03a81bfd8fc4f1014badc02d",
            "patch": "@@ -90,54 +90,27 @@ class SeedOssIntegrationTest(unittest.TestCase):\n     input_text = [\"How to make pasta?\", \"Hi ByteDance-Seed\"]\n     model_id = \"ByteDance-Seed/Seed-OSS-36B-Base\"\n \n-    def tearDown(self):\n+    def setUp(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    def test_model_36b_fp16(self):\n-        EXPECTED_TEXTS = [\n-            \"How to make pasta?\\nHow to make pasta?\\nPasta is a popular dish that is enjoyed by people all over\",\n-            \"Hi ByteDance-Seed team,\\nI am trying to run the code on my local machine. I have installed all the\",\n-        ]\n-\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16, device_map=\"auto\")\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(\n-            model.model.embed_tokens.weight.device\n-        )\n-\n-        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n \n-    def test_model_36b_bf16(self):\n+    def test_model_36b_eager(self):\n         EXPECTED_TEXTS = [\n             \"How to make pasta?\\nHow to make pasta?\\nPasta is a popular dish that is enjoyed by people all over\",\n-            \"Hi ByteDance-Seed team,\\nI am trying to run the code on my local machine. I have installed all the\",\n+            \"Hi ByteDance-Seed team,\\nI am trying to run the code on the <beginning of the code>seed\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(\n-            model.model.embed_tokens.weight.device\n-        )\n-\n-        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n-\n-    def test_model_36b_eager(self):\n-        EXPECTED_TEXTS = \"\"\n-\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, torch_dtype=torch.bfloat16, attn_implementation=\"eager\", device_map=\"auto\"\n+            \"ByteDance-Seed/Seed-OSS-36B-Base\",\n+            torch_dtype=torch.bfloat16,\n+            attn_implementation=\"eager\",\n+            device_map=\"auto\",\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(\n             model.model.embed_tokens.weight.device\n         )\n \n@@ -149,15 +122,14 @@ def test_model_36b_eager(self):\n     def test_model_36b_sdpa(self):\n         EXPECTED_TEXTS = [\n             \"How to make pasta?\\nHow to make pasta?\\nPasta is a popular dish that is enjoyed by people all over\",\n-            \"Hi ByteDance-Seed team,\\nI am trying to run the code on my local machine. I have installed all the\",\n+            \"Hi ByteDance-Seed team,\\nI am trying to run the code on the <beginning of the code>seed\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\"\n-        )\n+        # default attention is `sdpa` (and this model repo. doesn't specify explicitly) --> we get `sdpa` here\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(\n             model.model.embed_tokens.weight.device\n         )\n \n@@ -170,15 +142,16 @@ def test_model_36b_sdpa(self):\n     @require_torch_large_gpu\n     @pytest.mark.flash_attn_test\n     def test_model_36b_flash_attn(self):\n-        EXPECTED_TEXTS = \"\"\n+        EXPECTED_TEXTS = [\n+            \"How to make pasta?\\nHow to make pasta?\\nPasta is a popular dish that is enjoyed by people all over\",\n+            \"Hi ByteDance-Seed team,\\nI am trying to run the code on the <beginning of the code>seed\",\n+        ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n         )\n-        model.to(torch_device)\n-\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(\n             model.model.embed_tokens.weight.device\n         )\n "
        }
    ],
    "stats": {
        "total": 65,
        "additions": 19,
        "deletions": 46
    }
}