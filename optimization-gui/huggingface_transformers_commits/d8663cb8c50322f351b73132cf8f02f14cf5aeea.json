{
    "author": "tjohnson31415",
    "message": "Fix bugs in mllama image processing (#36156)\n\n* fix: handle input_channel_dim == channels_last\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* fix: default PIL images to channels_last\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* fixup from review batch\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* test: add 1x1 PIL image to ambiguous channel test\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* fix(mllama): avoid 0 dimension for image with impractical aspect ratio\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n---------\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "d8663cb8c50322f351b73132cf8f02f14cf5aeea",
    "files": [
        {
            "sha": "bcb97bbcd9940b93a736c62997daa56270f0f039",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8663cb8c50322f351b73132cf8f02f14cf5aeea/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8663cb8c50322f351b73132cf8f02f14cf5aeea/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=d8663cb8c50322f351b73132cf8f02f14cf5aeea",
            "patch": "@@ -93,7 +93,7 @@ def get_image_size_fit_to_canvas(\n     canvas_height and canvas_width, while ensuring that the image dimensions are not smaller than\n     tile_size. If the image is larger than the canvas, the returned size will fit within the canvas.\n     If the image already fits within the canvas, the size remains unchanged.\n-    The aspect ratio of the original image is preserved.\n+    The aspect ratio of the original image is preserved as much as possible.\n \n     Args:\n         image_height (`int`):\n@@ -120,10 +120,12 @@ def get_image_size_fit_to_canvas(\n \n     if scale_w < scale_h:\n         new_width = target_width\n-        new_height = min(math.floor(image_height * scale_w), target_height)\n+        # minimum height is 1 to avoid invalid height of 0\n+        new_height = min(math.floor(image_height * scale_w) or 1, target_height)\n     else:\n         new_height = target_height\n-        new_width = min(math.floor(image_width * scale_h), target_width)\n+        # minimum width is 1 to avoid invalid width of 0\n+        new_width = min(math.floor(image_width * scale_h) or 1, target_width)\n \n     return new_height, new_width\n \n@@ -695,8 +697,6 @@ def preprocess(\n         if self.do_convert_rgb:\n             images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n \n-        images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n-\n         batch_images = []\n         batch_aspect_ratios = []\n \n@@ -707,6 +707,13 @@ def preprocess(\n \n             # iterate over images in a batch sample\n             for image in images:\n+                # default PIL images to channels_last\n+                if input_data_format is None and isinstance(image, PIL.Image.Image):\n+                    input_data_format = ChannelDimension.LAST\n+\n+                # convert to numpy array for processing\n+                image = to_numpy_array(image)\n+\n                 # convert images to channels first format for faster processing\n                 # LAST is slower for `pad` and not supported by `split_to_tiles`\n                 data_format = ChannelDimension.FIRST\n@@ -735,7 +742,7 @@ def preprocess(\n                     image = self.rescale(\n                         image=image,\n                         scale=rescale_factor,\n-                        input_data_format=input_data_format,\n+                        input_data_format=data_format,\n                         data_format=data_format,\n                     )\n \n@@ -744,7 +751,7 @@ def preprocess(\n                         image=image,\n                         mean=image_mean,\n                         std=image_std,\n-                        input_data_format=input_data_format,\n+                        input_data_format=data_format,\n                         data_format=data_format,\n                     )\n "
        },
        {
            "sha": "4b7fbcb81d93dfd4589bec7b5a06fad0892a6c19",
            "filename": "tests/models/mllama/test_image_processing_mllama.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8663cb8c50322f351b73132cf8f02f14cf5aeea/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8663cb8c50322f351b73132cf8f02f14cf5aeea/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py?ref=d8663cb8c50322f351b73132cf8f02f14cf5aeea",
            "patch": "@@ -224,6 +224,36 @@ def test_call_pil(self):\n             tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n         )\n \n+    def test_call_channels_last(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+\n+        # a white 1x1 pixel RGB image\n+        image_inputs = [[np.full(shape=(1, 1, 3), fill_value=1.0, dtype=float)]]\n+        encoded_images = image_processing(\n+            image_inputs, return_tensors=\"pt\", input_data_format=\"channels_last\"\n+        ).pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+    def test_ambiguous_channel_pil_image(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+\n+        image_inputs = [[Image.new(\"RGB\", (1, 1))], [Image.new(\"RGB\", (100, 1))]]\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+    def test_resize_impractical_aspect_ratio(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # Ensure that no error is raised even if the aspect ratio is impractical\n+        image_inputs = [[Image.new(\"RGB\", (9999999, 1))]]\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n     def test_call_pytorch(self):\n         # Initialize image_processing\n         image_processing = self.image_processing_class(**self.image_processor_dict)"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 44,
        "deletions": 7
    }
}