{
    "author": "cyyever",
    "message": "Avoid unnecessary device operations in loss computing (#36950)\n\n* Avoid unnecessary tensor copy in loss computing\n\n* Add type",
    "sha": "8c5e29bad510749f7b85c60197b8c3ebbe82192f",
    "files": [
        {
            "sha": "0e052aed6a9b6f73266f4cbce4dc8cde38ea9481",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 16,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c5e29bad510749f7b85c60197b8c3ebbe82192f/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c5e29bad510749f7b85c60197b8c3ebbe82192f/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8c5e29bad510749f7b85c60197b8c3ebbe82192f",
            "patch": "@@ -24,7 +24,13 @@\n from .loss_rt_detr import RTDetrForObjectDetectionLoss\n \n \n-def fixed_cross_entropy(source, target, num_items_in_batch: Optional[int] = None, ignore_index: int = -100, **kwargs):\n+def fixed_cross_entropy(\n+    source: torch.Tensor,\n+    target: torch.Tensor,\n+    num_items_in_batch: Optional[int] = None,\n+    ignore_index: int = -100,\n+    **kwargs,\n+) -> torch.Tensor:\n     reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n     if reduction == \"sum\":\n@@ -38,14 +44,13 @@ def ForCausalLMLoss(\n     vocab_size: int,\n     num_items_in_batch: Optional[int] = None,\n     ignore_index: int = -100,\n-    shift_labels=None,\n+    shift_labels: Optional[torch.Tensor] = None,\n     **kwargs,\n-):\n+) -> torch.Tensor:\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()\n \n     if shift_labels is None:\n-        labels = labels.to(logits.device)\n         # Shift so that tokens < n predict n\n         labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n         shift_labels = labels[..., 1:].contiguous()\n@@ -60,11 +65,15 @@ def ForCausalLMLoss(\n \n \n def ForMaskedLMLoss(\n-    logits, labels, vocab_size: int, num_items_in_batch: Optional[int] = None, ignore_index: int = -100, **kwargs\n+    logits: torch.Tensor,\n+    labels: torch.Tensor,\n+    vocab_size: int,\n+    num_items_in_batch: Optional[int] = None,\n+    ignore_index: int = -100,\n+    **kwargs,\n ):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()\n-    labels = labels.to(logits.device)\n \n     # Flatten the tokens\n     logits = logits.view(-1, vocab_size)\n@@ -76,12 +85,12 @@ def ForMaskedLMLoss(\n     return loss\n \n \n-def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n+def ForSequenceClassificationLoss(labels: torch.Tensor, pooled_logits: torch.Tensor, config, **kwargs) -> torch.Tensor:\n     num_labels = config.num_labels\n     if config.problem_type is None:\n         if num_labels == 1:\n             config.problem_type = \"regression\"\n-        elif num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+        elif num_labels > 1 and (labels.dtype in (torch.long, torch.int)):\n             config.problem_type = \"single_label_classification\"\n         else:\n             config.problem_type = \"multi_label_classification\"\n@@ -90,15 +99,17 @@ def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n     if config.problem_type == \"regression\":\n         loss_fct = MSELoss()\n         if num_labels == 1:\n-            loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n+            return loss_fct(pooled_logits.squeeze(), labels.squeeze())\n         else:\n-            loss = loss_fct(pooled_logits, labels)\n-    elif config.problem_type == \"single_label_classification\":\n-        loss = fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)\n-    elif config.problem_type == \"multi_label_classification\":\n+            return loss_fct(pooled_logits, labels)\n+    if config.problem_type == \"single_label_classification\":\n+        return fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)\n+\n+    if config.problem_type == \"multi_label_classification\":\n         loss_fct = BCEWithLogitsLoss()\n-        loss = loss_fct(pooled_logits, labels)\n-    return loss\n+        return loss_fct(pooled_logits, labels)\n+\n+    raise RuntimeError(f\"Invalid problem type: {config.problem_type}\")\n \n \n def ForQuestionAnsweringLoss(start_logits, end_logits, start_positions, end_positions, **kwargs):\n@@ -120,7 +131,7 @@ def ForQuestionAnsweringLoss(start_logits, end_logits, start_positions, end_posi\n     return total_loss\n \n \n-def ForTokenClassification(logits, labels, config, **kwargs):\n+def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.view(-1, config.num_labels)\n     labels = labels.view(-1).to(logits.device)"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 27,
        "deletions": 16
    }
}