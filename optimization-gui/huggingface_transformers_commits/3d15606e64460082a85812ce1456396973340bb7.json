{
    "author": "kmehant",
    "message": "fix: support grad clipping for TP through replicating non-sharded modules (#36132)\n\n* feat: fix tp grad norm:\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n* feat: use implicit replication\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\n\n---------\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "3d15606e64460082a85812ce1456396973340bb7",
    "files": [
        {
            "sha": "d7c567f539118ec6caa2ca306053d1fd17e57bc3",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 6,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d15606e64460082a85812ce1456396973340bb7/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d15606e64460082a85812ce1456396973340bb7/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3d15606e64460082a85812ce1456396973340bb7",
            "patch": "@@ -232,6 +232,7 @@\n         AutocastKwargs,\n         DistributedDataParallelKwargs,\n         DistributedType,\n+        TorchTensorParallelPlugin,\n         load_fsdp_model,\n         load_fsdp_optimizer,\n         save_fsdp_model,\n@@ -2299,7 +2300,9 @@ def _inner_training_loop(\n             else:\n                 debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n \n-        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n+        delay_optimizer_creation = (\n+            is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled or self.is_tp_enabled\n+        )\n \n         # Can't delay optimizer creation when using FSDP2: https://github.com/huggingface/accelerate/blob/3f636d626063ffcf9a337c7d3624d61b7d187d59/src/accelerate/accelerator.py#L1404\n         is_fsdp2 = self.is_fsdp_enabled and (getattr(self.accelerator.state.fsdp_plugin, \"fsdp_version\", 1) == 2)\n@@ -2359,7 +2362,10 @@ def _inner_training_loop(\n                 if self.use_apex:\n                     model = self.accelerator.prepare(self.model)\n                 else:\n-                    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n+                    if delay_optimizer_creation:\n+                        self.optimizer = self.accelerator.prepare(self.optimizer)\n+                    else:\n+                        model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n             else:\n                 # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n                 model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n@@ -2580,10 +2586,16 @@ def _inner_training_loop(\n                                     args.max_grad_norm,\n                                 )\n                             else:\n-                                _grad_norm = self.accelerator.clip_grad_norm_(\n-                                    model.parameters(),\n-                                    args.max_grad_norm,\n-                                )\n+                                grad_norm_context = contextlib.nullcontext\n+                                if self.is_tp_enabled:\n+                                    from torch.distributed._tensor.experimental import implicit_replication\n+\n+                                    grad_norm_context = implicit_replication\n+                                with grad_norm_context():\n+                                    _grad_norm = self.accelerator.clip_grad_norm_(\n+                                        model.parameters(),\n+                                        args.max_grad_norm,\n+                                    )\n \n                             if (\n                                 is_accelerate_available()"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 18,
        "deletions": 6
    }
}