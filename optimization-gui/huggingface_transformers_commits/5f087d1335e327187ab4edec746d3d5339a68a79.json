{
    "author": "eustlb",
    "message": "Add Moonshine  (#34784)\n\n* config draft\r\n\r\n* full encoder forward\r\n\r\n* full decoder forward\r\n\r\n* fix sdpa and FA2\r\n\r\n* fix sdpa and FA2\r\n\r\n* moonshine model\r\n\r\n* moonshine model forward\r\n\r\n* fix attention with past_key_values\r\n\r\n* add MoonshineForConditionalGeneration\r\n\r\n* fix cache handling and causality for cross attention\r\n\r\n* no causal attention mask for the encoder\r\n\r\n* model addition (imports etc)\r\n\r\n* small nit\r\n\r\n* nits\r\n\r\n* Update src/transformers/models/moonshine/convert_usefulsensors_to_hf.py\r\n\r\nCo-authored-by: Joshua Lochner <admin@xenova.com>\r\n\r\n* add rope_theta\r\n\r\n* nits\r\n\r\n* model doc\r\n\r\n* Update src/transformers/models/auto/configuration_auto.py\r\n\r\nCo-authored-by: Joshua Lochner <admin@xenova.com>\r\n\r\n* imports\r\n\r\n* add MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES\r\n\r\n* updates modular\r\n\r\n* make\r\n\r\n* make fix-copies\r\n\r\n* ruff check examples fix\r\n\r\n* fix check_modular_conversion\r\n\r\n* nit\r\n\r\n* nits\r\n\r\n* nits\r\n\r\n* copied from -> imports\r\n\r\n* imports fix\r\n\r\n* integrate attention refacto\r\n\r\n* modular edge case\r\n\r\n* remove encoder\r\n\r\n* convolutions params in config\r\n\r\n* run modular_model_converter\r\n\r\n* make\r\n\r\n* Update docs/source/en/model_doc/moonshine.md\r\n\r\nCo-authored-by: Joshua Lochner <admin@xenova.com>\r\n\r\n* MoonshineModelTest\r\n\r\n* correct typo\r\n\r\n* make style\r\n\r\n* integration tests\r\n\r\n* make\r\n\r\n* modular convert\r\n\r\n* name conversion update (up_proj -> fc1 etc)\r\n\r\n* update config\r\n\r\n* update MLP\r\n\r\n* update attention\r\n\r\n* update encoder layer\r\n\r\n* update decoder layer\r\n\r\n* update convolutions parameters\r\n\r\n* update encoder\r\n\r\n* remove INPUTS_DOCSTRING\r\n\r\n* update decoder\r\n\r\n* update conditional generation\r\n\r\n* update pretrained model\r\n\r\n* imports\r\n\r\n* modular converted\r\n\r\n* update doc\r\n\r\n* fix\r\n\r\n* typo\r\n\r\n* update doc\r\n\r\n* update license\r\n\r\n* update init\r\n\r\n* split config in file\r\n\r\n* two classes for MLP\r\n\r\n* attention from GLM\r\n\r\n* from GlmRotaryEmbedding\r\n\r\n* split MLP\r\n\r\n* apply arthur's review suggestions\r\n\r\n* apply arthur's review suggestions\r\n\r\n* apply arthur's review suggestions\r\n\r\n* auto feature extractor\r\n\r\n* convert modular\r\n\r\n* fix + make\r\n\r\n* convert modular\r\n\r\n* make\r\n\r\n* unsplit config\r\n\r\n* use correct checkpoint\r\n\r\n* wrap generate\r\n\r\n* update tests\r\n\r\n* typos\r\n\r\n* make\r\n\r\n* typo\r\n\r\n* update doc\r\n\r\n---------\r\n\r\nCo-authored-by: Joshua Lochner <admin@xenova.com>",
    "sha": "5f087d1335e327187ab4edec746d3d5339a68a79",
    "files": [
        {
            "sha": "e9b0d465ab800fa940b18dfaba25cb09635476dc",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -505,7 +505,9 @@\n       - local: model_doc/mobilebert\n         title: MobileBERT\n       - local: model_doc/modernbert\n-        title: ModernBERT\n+        title: ModernBert\n+      - local: model_doc/moonshine\n+        title: moonshine\n       - local: model_doc/mpnet\n         title: MPNet\n       - local: model_doc/mpt"
        },
        {
            "sha": "84db43f825eba407163feecf581b44847e90edd7",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -235,6 +235,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                     [MobileViT](model_doc/mobilevit)                     |       âœ…        |         âœ…         |      âŒ      |\n |                   [MobileViTV2](model_doc/mobilevitv2)                   |       âœ…        |         âŒ         |      âŒ      |\n |                    [ModernBERT](model_doc/modernbert)                    |       âœ…        |         âŒ         |      âŒ      |\n+|                     [Moonshine](model_doc/moonshine)                     |       âœ…        |         âŒ         |      âŒ      |\n |                         [Moshi](model_doc/moshi)                         |       âœ…        |         âŒ         |      âŒ      |\n |                         [MPNet](model_doc/mpnet)                         |       âœ…        |         âœ…         |      âŒ      |\n |                           [MPT](model_doc/mpt)                           |       âœ…        |         âŒ         |      âŒ      |"
        },
        {
            "sha": "571e3febdb4fa89cdb273dcb5949cedaf8828b16",
            "filename": "docs/source/en/model_doc/moonshine.md",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,56 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Moonshine\n+\n+## Overview\n+\n+The Moonshine model was proposed in [Moonshine: Speech Recognition for Live Transcription and Voice Commands\n+](https://arxiv.org/abs/2410.15608) by Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden.\n+\n+The abstract from the paper is the following:\n+\n+*This paper introduces Moonshine, a family of speech recognition models optimized for live transcription and voice command processing. Moonshine is based on an encoder-decoder transformer architecture and employs Rotary Position Embedding (RoPE) instead of traditional absolute position embeddings. The model is trained on speech segments of various lengths, but without using zero-padding, leading to greater efficiency for the encoder during inference time. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny demonstrates a 5x reduction in compute requirements for transcribing a 10-second speech segment while incurring no increase in word error rates across standard evaluation datasets. These results highlight Moonshine's potential for real-time and resource-constrained applications.*\n+\n+Tips:\n+\n+- Moonshine improves upon Whisper's architecture:\n+  1. It uses SwiGLU activation instead of GELU in the decoder layers\n+  2. Most importantly, it replaces absolute position embeddings with Rotary Position Embeddings (RoPE). This allows Moonshine to handle audio inputs of any length, unlike Whisper which is restricted to fixed 30-second windows.\n+\n+This model was contributed by [Eustache Le Bihan (eustlb)](https://huggingface.co/eustlb).\n+The original code can be found [here](https://github.com/usefulsensors/moonshine).\n+\n+## Resources\n+\n+- [Automatic speech recognition task guide](../tasks/asr)\n+\n+## MoonshineConfig\n+\n+[[autodoc]] MoonshineConfig\n+\n+## MoonshineModel\n+\n+[[autodoc]] MoonshineModel\n+    - forward\n+    - _mask_input_features\n+\n+## MoonshineForConditionalGeneration\n+\n+[[autodoc]] MoonshineForConditionalGeneration\n+    - forward\n+    - generate\n+"
        },
        {
            "sha": "8cfddb45dba3295a96602b0df8f4c522398ee551",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -68,6 +68,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)\n * [Llava-NeXT-Video](https://huggingface.co/docs/transformers/model_doc/llava_next_video)\n * [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n+* [Moonshine](https://huggingface.co/docs/transformers/model_doc/moonshine#transformers.MoonshineModel)\n * [Mimi](https://huggingface.co/docs/transformers/model_doc/mimi)\n * [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)\n * [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)\n@@ -265,6 +266,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Llava-NeXT-Video](https://huggingface.co/docs/transformers/model_doc/llava_next_video)\n * [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n * [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100#transformers.M2M100Model)\n+* [Moonshine](https://huggingface.co/docs/transformers/model_doc/moonshine#transformers.MoonshineModel)\n * [Mimi](https://huggingface.co/docs/transformers/model_doc/mimi)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mllama](https://huggingface.co/docs/transformers/model_doc/mllama#transformers.MllamaForConditionalGeneration)\n@@ -283,8 +285,8 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)\n * [PhiMoE](https://huggingface.co/docs/transformers/model_doc/phimoe#transformers.PhimoeModel)\n * [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n-* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n * [mBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)\n+* [Moonshine](https://huggingface.co/docs/transformers/model_doc/moonshine#transformers.MoonshineModel)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n * [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)"
        },
        {
            "sha": "56ca17a00042e4c9fb75a996e9238e735a4ce4ad",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -610,6 +610,7 @@\n     \"models.mobilevit\": [\"MobileViTConfig\"],\n     \"models.mobilevitv2\": [\"MobileViTV2Config\"],\n     \"models.modernbert\": [\"ModernBertConfig\"],\n+    \"models.moonshine\": [\"MoonshineConfig\"],\n     \"models.moshi\": [\n         \"MoshiConfig\",\n         \"MoshiDepthConfig\",\n@@ -2907,6 +2908,13 @@\n             \"ModernBertPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.moonshine\"].extend(\n+        [\n+            \"MoonshineForConditionalGeneration\",\n+            \"MoonshineModel\",\n+            \"MoonshinePreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.moshi\"].extend(\n         [\n             \"MoshiForCausalLM\",\n@@ -5633,6 +5641,7 @@\n         MobileViTV2Config,\n     )\n     from .models.modernbert import ModernBertConfig\n+    from .models.moonshine import MoonshineConfig\n     from .models.moshi import (\n         MoshiConfig,\n         MoshiDepthConfig,\n@@ -7652,6 +7661,11 @@\n             ModernBertModel,\n             ModernBertPreTrainedModel,\n         )\n+        from .models.moonshine import (\n+            MoonshineForConditionalGeneration,\n+            MoonshineModel,\n+            MoonshinePreTrainedModel,\n+        )\n         from .models.moshi import (\n             MoshiForCausalLM,\n             MoshiForConditionalGeneration,"
        },
        {
            "sha": "b91e020c1b739bd31c3f5c25ab1f143417d6a814",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -170,6 +170,7 @@\n     mobilevit,\n     mobilevitv2,\n     modernbert,\n+    moonshine,\n     moshi,\n     mpnet,\n     mpt,"
        },
        {
            "sha": "febdf5ae271ca02a99ac5943ab3b22822a133581",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -190,6 +190,7 @@\n         (\"mobilevit\", \"MobileViTConfig\"),\n         (\"mobilevitv2\", \"MobileViTV2Config\"),\n         (\"modernbert\", \"ModernBertConfig\"),\n+        (\"moonshine\", \"MoonshineConfig\"),\n         (\"moshi\", \"MoshiConfig\"),\n         (\"mpnet\", \"MPNetConfig\"),\n         (\"mpt\", \"MptConfig\"),\n@@ -519,6 +520,7 @@\n         (\"mobilevit\", \"MobileViT\"),\n         (\"mobilevitv2\", \"MobileViTV2\"),\n         (\"modernbert\", \"ModernBERT\"),\n+        (\"moonshine\", \"Moonshine\"),\n         (\"moshi\", \"Moshi\"),\n         (\"mpnet\", \"MPNet\"),\n         (\"mpt\", \"MPT\"),"
        },
        {
            "sha": "1b237caabaffd223648bdc983d385a06ee4d5fea",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -73,6 +73,7 @@\n         (\"mobilenet_v1\", \"MobileNetV1FeatureExtractor\"),\n         (\"mobilenet_v2\", \"MobileNetV2FeatureExtractor\"),\n         (\"mobilevit\", \"MobileViTFeatureExtractor\"),\n+        (\"moonshine\", \"Wav2Vec2FeatureExtractor\"),\n         (\"moshi\", \"EncodecFeatureExtractor\"),\n         (\"nat\", \"ViTFeatureExtractor\"),\n         (\"owlvit\", \"OwlViTFeatureExtractor\"),"
        },
        {
            "sha": "4aef84405b8ac45fe7ce0c7f20267bd47fc0d16f",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -179,6 +179,7 @@\n         (\"mobilevit\", \"MobileViTModel\"),\n         (\"mobilevitv2\", \"MobileViTV2Model\"),\n         (\"modernbert\", \"ModernBertModel\"),\n+        (\"moonshine\", \"MoonshineModel\"),\n         (\"moshi\", \"MoshiModel\"),\n         (\"mpnet\", \"MPNetModel\"),\n         (\"mpt\", \"MptModel\"),\n@@ -436,6 +437,7 @@\n         (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForCausalLM\"),\n         (\"mobilebert\", \"MobileBertForMaskedLM\"),\n+        (\"moonshine\", \"MoonshineForConditionalGeneration\"),\n         (\"mpnet\", \"MPNetForMaskedLM\"),\n         (\"mpt\", \"MptForCausalLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n@@ -937,6 +939,7 @@\n \n MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n     [\n+        (\"moonshine\", \"MoonshineForConditionalGeneration\"),\n         (\"pop2piano\", \"Pop2PianoForConditionalGeneration\"),\n         (\"seamless_m4t\", \"SeamlessM4TForSpeechToText\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2ForSpeechToText\"),"
        },
        {
            "sha": "8df4fefeee461554f83c2964af8922f81b45e8e8",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -81,6 +81,7 @@\n         (\"mctct\", \"MCTCTProcessor\"),\n         (\"mgp-str\", \"MgpstrProcessor\"),\n         (\"mllama\", \"MllamaProcessor\"),\n+        (\"moonshine\", \"Wav2Vec2Processor\"),\n         (\"oneformer\", \"OneFormerProcessor\"),\n         (\"owlv2\", \"Owlv2Processor\"),\n         (\"owlvit\", \"OwlViTProcessor\"),"
        },
        {
            "sha": "51b8b590d931a02c4d0d68527816cda1a53918d7",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -321,6 +321,7 @@\n             (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"moonshine\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"moshi\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"mpnet\", (\"MPNetTokenizer\", \"MPNetTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"mpt\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "fa3c1870c2a1d6f5d089df5a396d4b049e4a9d4e",
            "filename": "src/transformers/models/moonshine/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2F__init__.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_moonshine import *\n+    from .modeling_moonshine import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "cabbe9179ba8ef560e0103c9b46029bb07067827",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "added",
            "additions": 224,
            "deletions": 0,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,224 @@\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/moonshine/modular_moonshine.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_moonshine.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class MoonshineConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MoonshineModel`]. It is used to instantiate a Moonshine\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Moonshine\n+    [UsefulSensors/moonshine-tiny](https://huggingface.co/UsefulSensors/moonshine-tiny).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32768):\n+            Vocabulary size of the Moonshine model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MoonshineModel`].\n+        hidden_size (`int`, *optional*, defaults to 288):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 1152):\n+            Dimension of the MLP representations.\n+        encoder_num_hidden_layers (`int`, *optional*, defaults to 6):\n+            Number of hidden layers in the Transformer encoder.\n+        decoder_num_hidden_layers (`int`, *optional*, defaults to 6):\n+            Number of hidden layers in the Transformer decoder.\n+        encoder_num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        decoder_num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        encoder_num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        decoder_num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `decoder_num_attention_heads`.\n+        encoder_hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder.\n+        decoder_hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 512):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        decoder_start_token_id (`int`, *optional*, defaults to 1):\n+            Corresponds to the \"<|startoftranscript|>\" token, which is automatically used when no `decoder_input_ids`\n+            are provided to the `generate` function. It is used to guide the model`s generation process depending on\n+            the task.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models).\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        partial_rotary_factor (`float`, *optional*, defaults to 0.9):\n+            Percentage of the query and keys which will have rotary embedding.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Denotes beginning of sequences token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            Denotes end of sequences token id.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MoonshineModel, MoonshineConfig\n+\n+    >>> # Initializing a Moonshine style configuration\n+    >>> configuration = MoonshineConfig().from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = MoonshineModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"moonshine\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\n+        \"num_key_value_heads\": \"encoder_num_key_value_heads\",\n+        \"num_attention_heads\": \"encoder_num_attention_heads\",\n+        \"num_hidden_layers\": \"encoder_num_hidden_layers\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=32768,\n+        hidden_size=288,\n+        intermediate_size=1152,\n+        encoder_num_hidden_layers=6,\n+        decoder_num_hidden_layers=6,\n+        encoder_num_attention_heads=8,\n+        decoder_num_attention_heads=8,\n+        encoder_num_key_value_heads=None,\n+        decoder_num_key_value_heads=None,\n+        encoder_hidden_act=\"gelu\",\n+        decoder_hidden_act=\"silu\",\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        decoder_start_token_id=1,\n+        use_cache=True,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        partial_rotary_factor=0.9,\n+        is_encoder_decoder=True,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.encoder_num_hidden_layers = encoder_num_hidden_layers\n+        self.decoder_num_hidden_layers = decoder_num_hidden_layers\n+        self.encoder_num_attention_heads = encoder_num_attention_heads\n+        self.decoder_num_attention_heads = decoder_num_attention_heads\n+\n+        if encoder_num_key_value_heads is None:\n+            encoder_num_key_value_heads = encoder_num_attention_heads\n+        self.encoder_num_key_value_heads = encoder_num_key_value_heads\n+\n+        if decoder_num_key_value_heads is None:\n+            decoder_num_key_value_heads = decoder_num_attention_heads\n+        self.decoder_num_key_value_heads = decoder_num_key_value_heads\n+\n+        self.encoder_hidden_act = encoder_hidden_act\n+        self.decoder_hidden_act = decoder_hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.decoder_start_token_id = decoder_start_token_id\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.partial_rotary_factor = partial_rotary_factor\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            is_encoder_decoder=is_encoder_decoder,\n+            decoder_start_token_id=decoder_start_token_id,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"MoonshineConfig\"]"
        },
        {
            "sha": "fa80f2b70964677e20ebebb60b37dd05bfa74860",
            "filename": "src/transformers/models/moonshine/convert_usefulsensors_to_hf.py",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,169 @@\n+# Copyright 2025 Useful Sensors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import re\n+\n+import h5py\n+import numpy as np\n+import torch\n+from huggingface_hub import hf_hub_download\n+\n+from transformers.models.moonshine.modeling_moonshine import MoonshineConfig, MoonshineForConditionalGeneration\n+\n+\n+# Copied from https://github.com/usefulsensors/moonshine/blob/a1d77cc573b0471ac4602b86f67b3f48d67df1a9/moonshine/model.py\n+def _get_weights(model_name):\n+    repo = \"UsefulSensors/moonshine\"\n+\n+    return (\n+        hf_hub_download(repo, f\"{x}.weights.h5\", subfolder=model_name) for x in (\"preprocessor\", \"encoder\", \"decoder\")\n+    )\n+\n+\n+def _read_h5_weights(group, current_key=\"\", weights={}):\n+    for key in group.keys():\n+        full_key = f\"{current_key}.{key}\" if current_key else key\n+        if isinstance(group[key], h5py.Dataset):\n+            w = np.array(group[key])\n+            w = torch.from_numpy(w)\n+            if len(w.shape) > 1:\n+                if len(w.shape) == 3:\n+                    hidden_size = max(list(w.shape))\n+                    try:\n+                        w = w.reshape(hidden_size, hidden_size)\n+                    except RuntimeError:\n+                        # meaning its a conv layers\n+                        pass\n+                w = w.transpose(0, -1)\n+            weights[full_key] = w\n+        else:\n+            _read_h5_weights(group[key], full_key, weights)\n+    return weights\n+\n+\n+def _convert_layer_names(name, gated_mlp=False):\n+    name = re.sub(\n+        r\"layers\\.functional(?:_(\\d+))?\\.layers\",\n+        lambda m: f'layers.{m.group(1) if m.group(1) else \"0\"}',\n+        name,\n+        count=1,\n+    )\n+    if gated_mlp:\n+        name = re.sub(r\"functional\\.layers\\.dense\\.\", \"mlp.fc1.\", name)\n+        name = re.sub(r\"functional\\.layers\\.dense_1\\.\", \"mlp.fc2.\", name)\n+    else:\n+        name = re.sub(r\"functional\\.layers\\.sequential\\.layers\\.dense\\.\", \"mlp.fc1.\", name)\n+        name = re.sub(r\"functional\\.layers\\.sequential\\.layers\\.dense_1\\.\", \"mlp.fc2.\", name)\n+    name = re.sub(r\"layers\\.sequential\\.layers\\.conv1d\\.\", \"conv1.\", name)\n+    name = re.sub(r\"layers\\.sequential\\.layers\\.conv1d_1\\.\", \"conv2.\", name)\n+    name = re.sub(r\"layers\\.sequential\\.layers\\.conv1d_2\\.\", \"conv3.\", name)\n+    name = re.sub(r\"layers\\.sequential\\.layers\\.group_normalization\\.\", \"groupnorm.\", name)\n+    name = re.sub(r\"mha_with_rope\\.key_dense\", \"self_attn.k_proj\", name)\n+    name = re.sub(r\"mha_with_rope\\.query_dense\", \"self_attn.q_proj\", name)\n+    name = re.sub(r\"mha_with_rope\\.value_dense\", \"self_attn.v_proj\", name)\n+    name = re.sub(r\"mha_with_rope\\.output_dense\", \"self_attn.o_proj\", name)\n+    name = re.sub(r\"mha_precomputed_kv\\.key_dense\", \"encoder_attn.k_proj\", name)\n+    name = re.sub(r\"mha_precomputed_kv\\.query_dense\", \"encoder_attn.q_proj\", name)\n+    name = re.sub(r\"mha_precomputed_kv\\.value_dense\", \"encoder_attn.v_proj\", name)\n+    name = re.sub(r\"mha_precomputed_kv\\.output_dense\", \"encoder_attn.o_proj\", name)\n+    name = re.sub(r\"mha_causal_with_rope\\.key_dense\", \"self_attn.k_proj\", name)\n+    name = re.sub(r\"mha_causal_with_rope\\.query_dense\", \"self_attn.q_proj\", name)\n+    name = re.sub(r\"mha_causal_with_rope\\.value_dense\", \"self_attn.v_proj\", name)\n+    name = re.sub(r\"mha_causal_with_rope\\.output_dense\", \"self_attn.o_proj\", name)\n+    name = re.sub(r\"layer_normalization\\.\", \"input_layernorm.\", name)\n+    name = re.sub(r\"layer_normalization_1\\.\", \"post_attention_layernorm.\", name)\n+    name = re.sub(r\"layer_normalization_2\\.\", \"final_layernorm.\", name)\n+    name = re.sub(r\"vars\\.0\", \"weight\", name)\n+    name = re.sub(r\"vars\\.1\", \"bias\", name)\n+    name = re.sub(r\"layers\\.reversible_embedding\", \"embed_tokens\", name)\n+\n+    return name\n+\n+\n+def _convert_weights(weights, encoder=True):\n+    if \"layers.rotary_embedding.vars.0\" in weights:\n+        weights.pop(\"layers.rotary_embedding.vars.0\")\n+\n+    converted_weights = {}\n+    if encoder:\n+        converted_weights[\"layer_norm.weight\"] = weights.pop(\"layers.layer_normalization.vars.0\")\n+    else:\n+        converted_weights[\"norm.weight\"] = weights.pop(\"layers.layer_normalization.vars.0\")\n+\n+    for name, w in weights.items():\n+        if encoder:\n+            new_name = _convert_layer_names(name)\n+        else:\n+            new_name = _convert_layer_names(name, gated_mlp=True)\n+        converted_weights[new_name] = w\n+\n+    return converted_weights\n+\n+\n+def convert_usefulsensors_moonshine_to_hf(model_name, pytorch_dump_folder_path):\n+    preprocessor_weights_path, encoder_weights_path, decoder_weights_path = _get_weights(model_name)\n+\n+    with h5py.File(preprocessor_weights_path, \"r\") as f:\n+        loaded_preprocessor_weights = _read_h5_weights(f, weights={})\n+\n+    with h5py.File(encoder_weights_path, \"r\") as f:\n+        loaded_encoder_weights = _read_h5_weights(f, weights={})\n+\n+    with h5py.File(decoder_weights_path, \"r\") as f:\n+        loaded_decoder_weights = _read_h5_weights(f, weights={})\n+\n+    encoder_state_dict = {**loaded_encoder_weights, **loaded_preprocessor_weights}\n+    converted_encoder_state_dict = _convert_weights(encoder_state_dict)\n+\n+    converted_decoder_state_dict = _convert_weights(loaded_decoder_weights, encoder=False)\n+    converted_decoder_state_dict[\"embed_tokens.weight\"] = converted_decoder_state_dict[\"embed_tokens.weight\"].T\n+\n+    final_weights = {}\n+    for k, v in converted_encoder_state_dict.items():\n+        final_weights[f\"model.encoder.{k}\"] = v\n+\n+    for k, v in converted_decoder_state_dict.items():\n+        final_weights[f\"model.decoder.{k}\"] = v\n+\n+    if model_name == \"tiny\":\n+        config = MoonshineConfig()\n+    elif model_name == \"base\":\n+        config = MoonshineConfig(\n+            hidden_size=416,\n+            intermediate_size=1664,\n+            encoder_num_hidden_layers=8,\n+            decoder_num_hidden_layers=8,\n+            encoder_num_attention_heads=8,\n+            decoder_num_attention_heads=8,\n+            partial_rotary_factor=0.62,\n+        )\n+    else:\n+        raise ValueError(f\"Unknown model name {model_name}\")\n+\n+    final_weights[\"proj_out.weight\"] = converted_decoder_state_dict[\"embed_tokens.weight\"]\n+\n+    model = MoonshineForConditionalGeneration(config)\n+    model.load_state_dict(final_weights)\n+    model.save_pretrained(pytorch_dump_folder_path)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # # Required parameters\n+    parser.add_argument(\"--model_name\", type=str, help=\"Path to the downloaded checkpoints\")\n+    parser.add_argument(\"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model.\")\n+    args = parser.parse_args()\n+\n+    convert_usefulsensors_moonshine_to_hf(args.model_name, args.pytorch_dump_folder_path)"
        },
        {
            "sha": "f4bf0e7511631f1e6313acc1417941dc2b6de63d",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "added",
            "additions": 1570,
            "deletions": 0,
            "changes": 1570,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,1570 @@\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from src/transformers/models/moonshine/modular_moonshine.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_moonshine.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPast,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_moonshine import MoonshineConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"MoonshineConfig\"\n+\n+\n+class MoonshineEncoderMLP(nn.Module):\n+    def __init__(self, config, hidden_act):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class MoonshineDecoderMLP(nn.Module):\n+    def __init__(self, config, hidden_act):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size * 2)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states, gate = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.activation_fn(gate) * hidden_states\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., 0::2]\n+    x2 = x[..., 1::2]\n+    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Interleave them instead of usual shape\n+    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+\n+    # Keep half or full tensor for later concatenation\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n+\n+    # Concatenate back to full shape\n+    q_embed = torch.cat([q_embed, q_pass], dim=-1)\n+    k_embed = torch.cat([k_embed, k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n+class MoonshineAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(\n+        self,\n+        config: MoonshineConfig,\n+        layer_idx: int,\n+        is_causal: bool,\n+        num_attention_heads: int,\n+        num_key_value_heads: int,\n+    ):\n+        super().__init__()\n+        config.update({\"num_attention_heads\": num_attention_heads, \"num_key_value_heads\": num_key_value_heads})\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = is_causal\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len = hidden_states.shape[:-1]\n+\n+        query_states = (\n+            self.q_proj(hidden_states).view(bsz, q_len, self.config.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        )\n+\n+        is_cross_attention = key_value_states is not None\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                past_key_value = past_key_value.self_attention_cache\n+\n+        # use key_value_states if cross attention\n+        current_states = key_value_states if key_value_states is not None else hidden_states\n+        if is_cross_attention and past_key_value and is_updated:\n+            key_states = past_key_value.key_cache[self.layer_idx]\n+            value_states = past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = (\n+                self.k_proj(current_states)\n+                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n+                .transpose(1, 2)\n+            )\n+            value_states = (\n+                self.v_proj(current_states)\n+                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n+                .transpose(1, 2)\n+            )\n+            if is_cross_attention and past_key_value is not None:\n+                key_states, value_states = past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+\n+        if not is_cross_attention:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+            if past_key_value is not None:\n+                cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+                key_states, value_states = past_key_value.update(\n+                    key_states, value_states, self.layer_idx, cache_kwargs\n+                )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            is_causal=is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class MoonshineRotaryEmbedding(nn.Module):\n+    def __init__(self, config: MoonshineConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class MoonshineEncoderLayer(nn.Module):\n+    def __init__(self, config: MoonshineConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=False,\n+            num_attention_heads=config.encoder_num_attention_heads,\n+            num_key_value_heads=config.encoder_num_key_value_heads,\n+        )\n+\n+        self.mlp = MoonshineEncoderMLP(config, config.encoder_hidden_act)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class MoonshineDecoderLayer(nn.Module):\n+    def __init__(self, config: MoonshineConfig, layer_idx: int = None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=True,\n+            num_attention_heads=config.decoder_num_attention_heads,\n+            num_key_value_heads=config.decoder_num_key_value_heads,\n+        )\n+        self.encoder_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=False,\n+            num_attention_heads=config.decoder_num_attention_heads,\n+            num_key_value_heads=config.decoder_num_key_value_heads,\n+        )\n+\n+        self.mlp = MoonshineDecoderMLP(config, config.decoder_hidden_act)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        encoder_position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        encoder_position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Cross-Attention Block\n+        cross_attn_weights = None\n+        if encoder_hidden_states is not None:\n+            residual = hidden_states\n+            hidden_states = self.post_attention_layernorm(hidden_states)\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n+                hidden_states=hidden_states,\n+                key_value_states=encoder_hidden_states,\n+                attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n+            hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.final_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+MOONSHINE_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`MoonshineConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Moonshine Model outputting raw hidden-states without any specific head on top.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshinePreTrainedModel(PreTrainedModel):\n+    config_class = MoonshineConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+        output_conv1_length = int((input_lengths - 127) / 64 + 1)\n+        output_conv2_length = int((output_conv1_length - 7) / 3 + 1)\n+        output_conv3_length = int((output_conv2_length - 3) / 2 + 1)\n+\n+        return output_conv3_length\n+\n+\n+class MoonshineEncoder(MoonshinePreTrainedModel):\n+    \"\"\"\n+    Transformer encoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MoonshineEncoderLayer`]\n+\n+    Args:\n+        config: MoonshineConfig\n+    \"\"\"\n+\n+    main_input_name = \"input_values\"\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.conv1 = nn.Conv1d(1, embed_dim, kernel_size=127, stride=64, bias=False)\n+        self.conv2 = nn.Conv1d(embed_dim, 2 * embed_dim, kernel_size=7, stride=3)\n+        self.conv3 = nn.Conv1d(2 * embed_dim, embed_dim, kernel_size=3, stride=2)\n+        self.groupnorm = nn.GroupNorm(num_groups=1, num_channels=embed_dim, eps=1e-5)\n+\n+        self.rotary_emb = MoonshineRotaryEmbedding(config=config)\n+\n+        self.layers = nn.ModuleList(\n+            [MoonshineEncoderLayer(config, idx) for idx in range(config.encoder_num_hidden_layers)]\n+        )\n+        self.layer_norm = nn.LayerNorm(embed_dim, bias=False)\n+\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.conv1\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.conv1 = value\n+\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n+                Float values of the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n+                and conversion into a tensor of type `torch.FloatTensor`.\n+            attention_mask (`torch.Tensor`)`, *optional*):\n+                Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n+                but it is not used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+                tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+                more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if input_values is None:\n+            raise ValueError(\"You must specify input_values.\")\n+\n+        # conv downsampling\n+        input_values = input_values.unsqueeze(1)\n+        hidden_states = nn.functional.tanh(self.conv1(input_values))\n+        hidden_states = self.groupnorm(hidden_states)\n+        hidden_states = nn.functional.gelu(self.conv2(hidden_states))\n+        hidden_states = nn.functional.gelu(self.conv3(hidden_states))\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+\n+        position_ids = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # encoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    None,\n+                    position_ids,\n+                    None,\n+                    output_attentions,\n+                    False,\n+                    None,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    position_ids=position_ids,\n+                    output_attentions=output_attentions,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        # add hidden states from the last encoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+MOONSHINE_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Moonshine Model outputting raw hidden-states without any specific head on top.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshineDecoder(MoonshinePreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MoonshineDecoderLayer`]\n+\n+    Args:\n+        config: MoonshineConfig\n+    \"\"\"\n+\n+    main_input_name = \"input_ids\"\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [MoonshineDecoderLayer(config, idx) for idx in range(config.decoder_num_hidden_layers)]\n+        )\n+        self.norm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.rotary_emb = MoonshineRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MOONSHINE_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        \"\"\"\n+        Args:\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            self_attention_cache = DynamicCache()\n+            cross_attention_cache = DynamicCache()\n+            past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    encoder_hidden_states,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.sum(-1).detach().tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+MOONSHINE_MODEL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n+            Float values of the raw speech waveform. Raw speech waveform can be\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n+            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n+            and conversion into a tensor of type `torch.FloatTensor`.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor`)`, *optional*):\n+            Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n+            but it is not used.\n+        decoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `decoder_input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `decoder_position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Moonshine Model outputting raw hidden-states without any specific head on top.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshineModel(MoonshinePreTrainedModel):\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+\n+        self.encoder = MoonshineEncoder(config)\n+        self.decoder = MoonshineDecoder(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.decoder.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.decoder.embed_tokens = value\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def freeze_encoder(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the Moonshine encoder so that its parameters will\n+        not be updated during training.\n+        \"\"\"\n+        self.encoder._freeze_parameters()\n+\n+    def _mask_input_features(\n+        self,\n+        input_features: torch.FloatTensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"\n+        Masks extracted features along time axis and/or along feature axis according to\n+        [SpecAugment](https://arxiv.org/abs/1904.08779).\n+        \"\"\"\n+\n+        # `config.apply_spec_augment` can set masking to False\n+        if not getattr(self.config, \"apply_spec_augment\", True):\n+            return input_features\n+\n+        # generate indices & apply SpecAugment along time axis\n+        batch_size, hidden_size, sequence_length = input_features.size()\n+\n+        if self.config.mask_time_prob > 0 and self.training:\n+            # generate indices & apply SpecAugment along time axis\n+            mask_time_indices = _compute_mask_indices(\n+                (batch_size, sequence_length),\n+                mask_prob=self.config.mask_time_prob,\n+                mask_length=self.config.mask_time_length,\n+                attention_mask=attention_mask,\n+                min_masks=self.config.mask_time_min_masks,\n+            )\n+            mask_time_indices = torch.tensor(mask_time_indices, device=input_features.device, dtype=torch.bool)\n+            mask_time_indices = mask_time_indices[:, None].expand(-1, hidden_size, -1)\n+            input_features[mask_time_indices] = 0\n+\n+        if self.config.mask_feature_prob > 0 and self.training:\n+            # generate indices & apply SpecAugment along feature axis\n+            mask_feature_indices = _compute_mask_indices(\n+                (batch_size, hidden_size),\n+                mask_prob=self.config.mask_feature_prob,\n+                mask_length=self.config.mask_feature_length,\n+                min_masks=self.config.mask_feature_min_masks,\n+            )\n+            mask_feature_indices = torch.tensor(mask_feature_indices, device=input_features.device, dtype=torch.bool)\n+            input_features[mask_feature_indices] = 0\n+\n+        return input_features\n+\n+    @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[EncoderDecoderCache, Tuple[torch.FloatTensor]]] = None,\n+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n+        decoder_position_ids: Optional[Tuple[torch.LongTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoFeatureExtractor, MoonshineModel\n+        >>> from datasets import load_dataset\n+\n+        >>> model = MoonshineModel.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n+        >>> input_values = inputs.input_values\n+        >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n+        >>> last_hidden_state = model(input_values, decoder_input_ids=decoder_input_ids).last_hidden_state\n+        >>> list(last_hidden_state.shape)\n+        [1, 2, 288]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_values,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n+        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            past_key_values=past_key_values,\n+            inputs_embeds=decoder_inputs_embeds,\n+            position_ids=decoder_position_ids,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        if not return_dict:\n+            return decoder_outputs + encoder_outputs\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n+    \"\"\"\n+    Shift input ids one token to the right.\n+    \"\"\"\n+    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n+    shifted_input_ids[:, 0] = decoder_start_token_id\n+\n+    if pad_token_id is None:\n+        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n+    # replace possible -100 values in labels by `pad_token_id`\n+    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+    return shifted_input_ids\n+\n+\n+@add_start_docstrings(\n+    \"The Moonshine Model with a language modeling head. Can be used for automatic speech recognition.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshineForConditionalGeneration(MoonshinePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"proj_out.weight\"]\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.model = MoonshineModel(config)\n+        self.proj_out = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_output_embeddings(self):\n+        return self.proj_out\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.proj_out = new_embeddings\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.model.get_input_embeddings()\n+\n+    @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[EncoderDecoderCache, Tuple[torch.FloatTensor]]] = None,\n+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n+        decoder_position_ids: Optional[Tuple[torch.LongTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n+            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is\n+            only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoProcessor, MoonshineForConditionalGeneration\n+        >>> from datasets import load_dataset\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n+        >>> input_values = inputs.input_values\n+\n+        >>> generated_ids = model.generate(input_values, max_new_tokens=100)\n+\n+        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        >>> transcription\n+        'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if labels is not None:\n+            if decoder_input_ids is None and decoder_inputs_embeds is None:\n+                decoder_input_ids = shift_tokens_right(\n+                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n+                )\n+\n+        outputs = self.model(\n+            input_values,\n+            decoder_input_ids=decoder_input_ids,\n+            encoder_outputs=encoder_outputs,\n+            decoder_attention_mask=decoder_attention_mask,\n+            past_key_values=past_key_values,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            decoder_position_ids=decoder_position_ids,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+        logits = self.proj_out(outputs[0])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+        )\n+\n+    def generate(self, *args, **kwargs):\n+        # TODO: @eustlb do it rather with a custom logits processor\n+        token_limit_factor = 6.5 / 16000.0  # Maximum of 6.5 tokens per second\n+        if kwargs.get(\"max_new_tokens\") is None and kwargs.get(\"max_length\") is None:\n+            if kwargs.get(\"attention_mask\") is not None:\n+                seq_lens = kwargs[\"attention_mask\"].sum(dim=-1)\n+            else:\n+                seq_lens = kwargs[\"input_values\"].shape[-1]\n+            max_length = int(seq_lens.max().item() * token_limit_factor)\n+            logger.warning_once(\n+                f\"Based on the input length, Moonshine will generate up to {max_length} tokens (ratio of 6.5 tokens/second). \"\n+                \"To specify a different length, set either `max_new_tokens` or `max_length`.\"\n+            )\n+            kwargs[\"max_length\"] = max_length\n+\n+        return super().generate(*args, **kwargs)\n+\n+\n+__all__ = [\"MoonshineModel\", \"MoonshinePreTrainedModel\", \"MoonshineForConditionalGeneration\"]"
        },
        {
            "sha": "e3d4a7db703f410019c3d370abc5e2c890f39c05",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "added",
            "additions": 1135,
            "deletions": 0,
            "changes": 1135,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,1135 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPast,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+)\n+from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..glm.modeling_glm import GlmAttention, GlmRotaryEmbedding, apply_rotary_pos_emb\n+from ..llama.modeling_llama import LlamaDecoderLayer, LlamaModel, eager_attention_forward\n+from ..whisper.modeling_whisper import WhisperModel, shift_tokens_right\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"UsefulSensors/moonshine-tiny\"\n+_CONFIG_FOR_DOC = \"MoonshineConfig\"\n+\n+\n+class MoonshineConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MoonshineModel`]. It is used to instantiate a Moonshine\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Moonshine\n+    [UsefulSensors/moonshine-tiny](https://huggingface.co/UsefulSensors/moonshine-tiny).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32768):\n+            Vocabulary size of the Moonshine model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MoonshineModel`].\n+        hidden_size (`int`, *optional*, defaults to 288):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 1152):\n+            Dimension of the MLP representations.\n+        encoder_num_hidden_layers (`int`, *optional*, defaults to 6):\n+            Number of hidden layers in the Transformer encoder.\n+        decoder_num_hidden_layers (`int`, *optional*, defaults to 6):\n+            Number of hidden layers in the Transformer decoder.\n+        encoder_num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        decoder_num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        encoder_num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        decoder_num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `decoder_num_attention_heads`.\n+        encoder_hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder.\n+        decoder_hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 512):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        decoder_start_token_id (`int`, *optional*, defaults to 1):\n+            Corresponds to the \"<|startoftranscript|>\" token, which is automatically used when no `decoder_input_ids`\n+            are provided to the `generate` function. It is used to guide the model`s generation process depending on\n+            the task.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models).\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        partial_rotary_factor (`float`, *optional*, defaults to 0.9):\n+            Percentage of the query and keys which will have rotary embedding.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Denotes beginning of sequences token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            Denotes end of sequences token id.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MoonshineModel, MoonshineConfig\n+\n+    >>> # Initializing a Moonshine style configuration\n+    >>> configuration = MoonshineConfig().from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = MoonshineModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"moonshine\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\n+        \"num_key_value_heads\": \"encoder_num_key_value_heads\",\n+        \"num_attention_heads\": \"encoder_num_attention_heads\",\n+        \"num_hidden_layers\": \"encoder_num_hidden_layers\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=32768,\n+        hidden_size=288,\n+        intermediate_size=1152,\n+        encoder_num_hidden_layers=6,\n+        decoder_num_hidden_layers=6,\n+        encoder_num_attention_heads=8,\n+        decoder_num_attention_heads=8,\n+        encoder_num_key_value_heads=None,\n+        decoder_num_key_value_heads=None,\n+        encoder_hidden_act=\"gelu\",\n+        decoder_hidden_act=\"silu\",\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        decoder_start_token_id=1,\n+        use_cache=True,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        partial_rotary_factor=0.9,\n+        is_encoder_decoder=True,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.encoder_num_hidden_layers = encoder_num_hidden_layers\n+        self.decoder_num_hidden_layers = decoder_num_hidden_layers\n+        self.encoder_num_attention_heads = encoder_num_attention_heads\n+        self.decoder_num_attention_heads = decoder_num_attention_heads\n+\n+        if encoder_num_key_value_heads is None:\n+            encoder_num_key_value_heads = encoder_num_attention_heads\n+        self.encoder_num_key_value_heads = encoder_num_key_value_heads\n+\n+        if decoder_num_key_value_heads is None:\n+            decoder_num_key_value_heads = decoder_num_attention_heads\n+        self.decoder_num_key_value_heads = decoder_num_key_value_heads\n+\n+        self.encoder_hidden_act = encoder_hidden_act\n+        self.decoder_hidden_act = decoder_hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.decoder_start_token_id = decoder_start_token_id\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.partial_rotary_factor = partial_rotary_factor\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            is_encoder_decoder=is_encoder_decoder,\n+            decoder_start_token_id=decoder_start_token_id,\n+            **kwargs,\n+        )\n+\n+\n+class MoonshineEncoderMLP(nn.Module):\n+    def __init__(self, config, hidden_act):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class MoonshineDecoderMLP(nn.Module):\n+    def __init__(self, config, hidden_act):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size * 2)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states, gate = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.activation_fn(gate) * hidden_states\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class MoonshineAttention(GlmAttention):\n+    def __init__(\n+        self,\n+        config: MoonshineConfig,\n+        layer_idx: int,\n+        is_causal: bool,\n+        num_attention_heads: int,\n+        num_key_value_heads: int,\n+    ):\n+        config.update({\"num_attention_heads\": num_attention_heads, \"num_key_value_heads\": num_key_value_heads})\n+        super().__init__(config, layer_idx)\n+        self.is_causal = is_causal\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len = hidden_states.shape[:-1]\n+\n+        query_states = (\n+            self.q_proj(hidden_states).view(bsz, q_len, self.config.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        )\n+\n+        is_cross_attention = key_value_states is not None\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                past_key_value = past_key_value.self_attention_cache\n+\n+        # use key_value_states if cross attention\n+        current_states = key_value_states if key_value_states is not None else hidden_states\n+        if is_cross_attention and past_key_value and is_updated:\n+            key_states = past_key_value.key_cache[self.layer_idx]\n+            value_states = past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = (\n+                self.k_proj(current_states)\n+                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n+                .transpose(1, 2)\n+            )\n+            value_states = (\n+                self.v_proj(current_states)\n+                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n+                .transpose(1, 2)\n+            )\n+            if is_cross_attention and past_key_value is not None:\n+                key_states, value_states = past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+\n+        if not is_cross_attention:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+            if past_key_value is not None:\n+                cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+                key_states, value_states = past_key_value.update(\n+                    key_states, value_states, self.layer_idx, cache_kwargs\n+                )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            is_causal=is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class MoonshineRotaryEmbedding(GlmRotaryEmbedding):\n+    pass\n+\n+\n+class MoonshineEncoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: MoonshineConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.self_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=False,\n+            num_attention_heads=config.encoder_num_attention_heads,\n+            num_key_value_heads=config.encoder_num_key_value_heads,\n+        )\n+\n+        self.mlp = MoonshineEncoderMLP(config, config.encoder_hidden_act)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+\n+\n+class MoonshineDecoderLayer(nn.Module):\n+    def __init__(self, config: MoonshineConfig, layer_idx: int = None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=True,\n+            num_attention_heads=config.decoder_num_attention_heads,\n+            num_key_value_heads=config.decoder_num_key_value_heads,\n+        )\n+        self.encoder_attn = MoonshineAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+            is_causal=False,\n+            num_attention_heads=config.decoder_num_attention_heads,\n+            num_key_value_heads=config.decoder_num_key_value_heads,\n+        )\n+\n+        self.mlp = MoonshineDecoderMLP(config, config.decoder_hidden_act)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        encoder_position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        encoder_position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Cross-Attention Block\n+        cross_attn_weights = None\n+        if encoder_hidden_states is not None:\n+            residual = hidden_states\n+            hidden_states = self.post_attention_layernorm(hidden_states)\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n+                hidden_states=hidden_states,\n+                key_value_states=encoder_hidden_states,\n+                attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n+            hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.final_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+MOONSHINE_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`MoonshineConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Moonshine Model outputting raw hidden-states without any specific head on top.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshinePreTrainedModel(PreTrainedModel):\n+    config_class = MoonshineConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+        output_conv1_length = int((input_lengths - 127) / 64 + 1)\n+        output_conv2_length = int((output_conv1_length - 7) / 3 + 1)\n+        output_conv3_length = int((output_conv2_length - 3) / 2 + 1)\n+\n+        return output_conv3_length\n+\n+\n+class MoonshineEncoder(MoonshinePreTrainedModel):\n+    \"\"\"\n+    Transformer encoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MoonshineEncoderLayer`]\n+\n+    Args:\n+        config: MoonshineConfig\n+    \"\"\"\n+\n+    main_input_name = \"input_values\"\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.conv1 = nn.Conv1d(1, embed_dim, kernel_size=127, stride=64, bias=False)\n+        self.conv2 = nn.Conv1d(embed_dim, 2 * embed_dim, kernel_size=7, stride=3)\n+        self.conv3 = nn.Conv1d(2 * embed_dim, embed_dim, kernel_size=3, stride=2)\n+        self.groupnorm = nn.GroupNorm(num_groups=1, num_channels=embed_dim, eps=1e-5)\n+\n+        self.rotary_emb = MoonshineRotaryEmbedding(config=config)\n+\n+        self.layers = nn.ModuleList(\n+            [MoonshineEncoderLayer(config, idx) for idx in range(config.encoder_num_hidden_layers)]\n+        )\n+        self.layer_norm = nn.LayerNorm(embed_dim, bias=False)\n+\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.conv1\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.conv1 = value\n+\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n+                Float values of the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n+                and conversion into a tensor of type `torch.FloatTensor`.\n+            attention_mask (`torch.Tensor`)`, *optional*):\n+                Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n+                but it is not used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+                tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+                more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if input_values is None:\n+            raise ValueError(\"You must specify input_values.\")\n+\n+        # conv downsampling\n+        input_values = input_values.unsqueeze(1)\n+        hidden_states = nn.functional.tanh(self.conv1(input_values))\n+        hidden_states = self.groupnorm(hidden_states)\n+        hidden_states = nn.functional.gelu(self.conv2(hidden_states))\n+        hidden_states = nn.functional.gelu(self.conv3(hidden_states))\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+\n+        position_ids = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # encoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    None,\n+                    position_ids,\n+                    None,\n+                    output_attentions,\n+                    False,\n+                    None,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    position_ids=position_ids,\n+                    output_attentions=output_attentions,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        # add hidden states from the last encoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class MoonshineDecoder(LlamaModel):\n+    main_input_name = \"input_ids\"\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.norm = nn.LayerNorm(config.hidden_size, bias=False)\n+        self.layers = nn.ModuleList(\n+            [MoonshineDecoderLayer(config, idx) for idx in range(config.decoder_num_hidden_layers)]\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        \"\"\"\n+        Args:\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            self_attention_cache = DynamicCache()\n+            cross_attention_cache = DynamicCache()\n+            past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    encoder_hidden_states,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+MOONSHINE_MODEL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n+            Float values of the raw speech waveform. Raw speech waveform can be\n+            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n+            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+            `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n+            and conversion into a tensor of type `torch.FloatTensor`.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor`)`, *optional*):\n+            Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n+            but it is not used.\n+        decoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `decoder_input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `decoder_position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Moonshine Model outputting raw hidden-states without any specific head on top.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshineModel(WhisperModel):\n+    @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[EncoderDecoderCache, Tuple[torch.FloatTensor]]] = None,\n+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n+        decoder_position_ids: Optional[Tuple[torch.LongTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n+        r\"\"\"\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoFeatureExtractor, MoonshineModel\n+        >>> from datasets import load_dataset\n+\n+        >>> model = MoonshineModel.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> inputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n+        >>> input_values = inputs.input_values\n+        >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n+        >>> last_hidden_state = model(input_values, decoder_input_ids=decoder_input_ids).last_hidden_state\n+        >>> list(last_hidden_state.shape)\n+        [1, 2, 288]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_values,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n+        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            past_key_values=past_key_values,\n+            inputs_embeds=decoder_inputs_embeds,\n+            position_ids=decoder_position_ids,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        if not return_dict:\n+            return decoder_outputs + encoder_outputs\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"The Moonshine Model with a language modeling head. Can be used for automatic speech recognition.\",\n+    MOONSHINE_START_DOCSTRING,\n+)\n+class MoonshineForConditionalGeneration(MoonshinePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"proj_out.weight\"]\n+\n+    def __init__(self, config: MoonshineConfig):\n+        super().__init__(config)\n+        self.model = MoonshineModel(config)\n+        self.proj_out = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_output_embeddings(self):\n+        return self.proj_out\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.proj_out = new_embeddings\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.model.get_input_embeddings()\n+\n+    @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[EncoderDecoderCache, Tuple[torch.FloatTensor]]] = None,\n+        decoder_inputs_embeds: Optional[Tuple[torch.FloatTensor]] = None,\n+        decoder_position_ids: Optional[Tuple[torch.LongTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n+            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is\n+            only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoProcessor, MoonshineForConditionalGeneration\n+        >>> from datasets import load_dataset\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        >>> model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n+        >>> input_values = inputs.input_values\n+\n+        >>> generated_ids = model.generate(input_values, max_new_tokens=100)\n+\n+        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+        >>> transcription\n+        'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if labels is not None:\n+            if decoder_input_ids is None and decoder_inputs_embeds is None:\n+                decoder_input_ids = shift_tokens_right(\n+                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n+                )\n+\n+        outputs = self.model(\n+            input_values,\n+            decoder_input_ids=decoder_input_ids,\n+            encoder_outputs=encoder_outputs,\n+            decoder_attention_mask=decoder_attention_mask,\n+            past_key_values=past_key_values,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            decoder_position_ids=decoder_position_ids,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+        logits = self.proj_out(outputs[0])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+        )\n+\n+    def generate(self, *args, **kwargs):\n+        # TODO: @eustlb do it rather with a custom logits processor\n+        token_limit_factor = 6.5 / 16000.0  # Maximum of 6.5 tokens per second\n+        if kwargs.get(\"max_new_tokens\") is None and kwargs.get(\"max_length\") is None:\n+            if kwargs.get(\"attention_mask\") is not None:\n+                seq_lens = kwargs[\"attention_mask\"].sum(dim=-1)\n+            else:\n+                seq_lens = kwargs[\"input_values\"].shape[-1]\n+            max_length = int(seq_lens.max().item() * token_limit_factor)\n+            logger.warning_once(\n+                f\"Based on the input length, Moonshine will generate up to {max_length} tokens (ratio of 6.5 tokens/second). \"\n+                \"To specify a different length, set either `max_new_tokens` or `max_length`.\"\n+            )\n+            kwargs[\"max_length\"] = max_length\n+\n+        return super().generate(*args, **kwargs)\n+\n+\n+__all__ = [\n+    \"MoonshineConfig\",\n+    \"MoonshineModel\",\n+    \"MoonshinePreTrainedModel\",\n+    \"MoonshineForConditionalGeneration\",\n+]"
        },
        {
            "sha": "8c05d6093f521dfe01419ca8d80422d7d20becc9",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -6523,6 +6523,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class MoonshineForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class MoonshineModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class MoonshinePreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MoshiForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/moonshine/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/tests%2Fmodels%2Fmoonshine%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/tests%2Fmodels%2Fmoonshine%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2F__init__.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79"
        },
        {
            "sha": "99bb3dec21fd5226624b9650b9492a5328b69e8e",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "added",
            "additions": 620,
            "deletions": 0,
            "changes": 620,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f087d1335e327187ab4edec746d3d5339a68a79/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f087d1335e327187ab4edec746d3d5339a68a79/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=5f087d1335e327187ab4edec746d3d5339a68a79",
            "patch": "@@ -0,0 +1,620 @@\n+# coding=utf-8\n+# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Moonshine model.\"\"\"\n+\n+import copy\n+import unittest\n+\n+from transformers import MoonshineConfig, is_torch_available\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    floats_tensor,\n+    random_attention_mask,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoProcessor,\n+        MoonshineForConditionalGeneration,\n+        MoonshineModel,\n+    )\n+\n+from datasets import load_dataset\n+\n+\n+class MoonshineModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,  # need batch_size != num_hidden_layers\n+        seq_length=1000,\n+        is_training=False,\n+        use_labels=False,\n+        vocab_size=147,\n+        hidden_size=8,\n+        intermediate_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        num_key_value_heads=2,\n+        encoder_hidden_act=\"gelu\",\n+        decoder_hidden_act=\"silu\",\n+        decoder_start_token_id=85,\n+        bos_token_id=98,\n+        eos_token_id=98,\n+        pad_token_id=0,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.encoder_hidden_act = encoder_hidden_act\n+        self.decoder_hidden_act = decoder_hidden_act\n+        self.decoder_start_token_id = decoder_start_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+\n+    def prepare_config_and_inputs(self):\n+        input_values = floats_tensor([self.batch_size, self.seq_length], scale=1.0)\n+        attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n+\n+        decoder_input_ids = torch.tensor(self.batch_size * [[self.decoder_start_token_id]], device=torch_device)\n+        decoder_attention_mask = decoder_input_ids.ne(self.pad_token_id)\n+\n+        config = self.get_config()\n+\n+        return config, input_values, attention_mask, decoder_input_ids, decoder_attention_mask\n+\n+    def get_config(self):\n+        return MoonshineConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            encoder_num_hidden_layers=self.num_hidden_layers,\n+            decoder_num_hidden_layers=self.num_hidden_layers,\n+            encoder_num_attention_heads=self.num_attention_heads,\n+            decoder_num_attention_heads=self.num_attention_heads,\n+            encoder_num_key_value_heads=self.num_key_value_heads,\n+            decoder_num_key_value_heads=self.num_key_value_heads,\n+            encoder_hidden_act=self.encoder_hidden_act,\n+            decoder_hidden_act=self.decoder_hidden_act,\n+            decoder_start_token_id=self.decoder_start_token_id,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+        )\n+\n+    def create_and_check_model(self, config, input_values, attention_mask):\n+        model = MoonshineModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_values, attention_mask=attention_mask)\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n+        )\n+\n+    def create_and_check_batch_inference(self, config, input_values, *args):\n+        # test does not pass for models making use of `group_norm`\n+        # check: https://github.com/pytorch/fairseq/issues/3227\n+        model = MoonshineModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        input_values = input_values[:3]\n+        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n+\n+        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n+\n+        # pad input\n+        for i in range(len(input_lengths)):\n+            input_values[i, input_lengths[i] :] = 0.0\n+            attention_mask[i, input_lengths[i] :] = 0.0\n+\n+        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n+\n+        for i in range(input_values.shape[0]):\n+            input_slice = input_values[i : i + 1, : input_lengths[i]]\n+            output = model(input_slice).last_hidden_state\n+\n+            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n+            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n+\n+    def check_output_attentions(self, config, input_values, attention_mask):\n+        model = MoonshineModel(config=config)\n+        model.config.layerdrop = 1.0\n+        model.to(torch_device)\n+        model.train()\n+\n+        outputs = model(input_values, attention_mask=attention_mask, output_attentions=True)\n+        self.parent.assertTrue(len(outputs.attentions) > 0)\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_values, attention_mask, decoder_input_ids, decoder_attention_mask = (\n+            self.prepare_config_and_inputs()\n+        )\n+        inputs_dict = {\n+            \"input_values\": input_values,\n+            \"attention_mask\": attention_mask,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class MoonshineModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (MoonshineModel, MoonshineForConditionalGeneration) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"automatic-speech-recognition\": MoonshineForConditionalGeneration,\n+            \"feature-extraction\": MoonshineModel,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_pruning = False\n+    test_headmasking = False\n+\n+    def setUp(self):\n+        self.model_tester = MoonshineModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=MoonshineConfig)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+        decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", 1)\n+        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n+        decoder_key_length = getattr(self.model_tester, \"decoder_key_length\", 1)\n+        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            subsampled_encoder_seq_length = model._get_feat_extract_output_lengths(encoder_seq_length)\n+            subsampled_encoder_key_length = model._get_feat_extract_output_lengths(encoder_key_length)\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, subsampled_encoder_seq_length, subsampled_encoder_key_length],\n+            )\n+            out_len = len(outputs)\n+\n+            correct_outlen = 5\n+\n+            # loss is at first position\n+            if \"labels\" in inputs_dict:\n+                correct_outlen += 1  # loss is added to beginning\n+            if \"past_key_values\" in outputs:\n+                correct_outlen += 1  # past_key_values have been returned\n+\n+            self.assertEqual(out_len, correct_outlen)\n+\n+            # decoder attentions\n+            decoder_attentions = outputs.decoder_attentions\n+            self.assertIsInstance(decoder_attentions, (list, tuple))\n+            self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(decoder_attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, decoder_seq_length, decoder_key_length],\n+            )\n+\n+            # cross attentions\n+            cross_attentions = outputs.cross_attentions\n+            self.assertIsInstance(cross_attentions, (list, tuple))\n+            self.assertEqual(len(cross_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(cross_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    decoder_seq_length,\n+                    subsampled_encoder_key_length,\n+                ],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            added_hidden_states = 2\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, subsampled_encoder_seq_length, subsampled_encoder_key_length],\n+            )\n+\n+    # Copied from tests.models.whisper.test_modeling_whisper.WhisperModelTest.test_hidden_states_output\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            if hasattr(self.model_tester, \"encoder_seq_length\"):\n+                seq_length = self.model_tester.encoder_seq_length\n+            else:\n+                seq_length = self.model_tester.seq_length\n+\n+            subsampled_seq_length = model._get_feat_extract_output_lengths(seq_length)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [subsampled_seq_length, self.model_tester.hidden_size],\n+            )\n+\n+            if config.is_encoder_decoder:\n+                hidden_states = outputs.decoder_hidden_states\n+\n+                self.assertIsInstance(hidden_states, (list, tuple))\n+                self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+                decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", 1)\n+\n+                self.assertListEqual(\n+                    list(hidden_states[0].shape[-2:]),\n+                    [decoder_seq_length, self.model_tester.hidden_size],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    # Copied from tests.models.whisper.test_modeling_whisper.WhisperModelTest.test_inputs_embeds\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+\n+            decoder_input_ids = inputs.pop(\"decoder_input_ids\", None)\n+            inputs.pop(\"decoder_attention_mask\", None)\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"decoder_inputs_embeds\"] = wte(decoder_input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)[0]\n+\n+    # Copied from tests.models.whisper.test_modeling_whisper.WhisperModelTest.test_resize_tokens_embeddings\n+    def test_resize_tokens_embeddings(self):\n+        (\n+            original_config,\n+            inputs_dict,\n+        ) = self.model_tester.prepare_config_and_inputs_for_common()\n+        if not self.test_resize_embeddings:\n+            self.skipTest(reason=\"test_resize_embeddings is False\")\n+\n+        for model_class in self.all_model_classes:\n+            config = copy.deepcopy(original_config)\n+            model = model_class(config)\n+            model.to(torch_device)\n+\n+            if self.model_tester.is_training is False:\n+                model.eval()\n+\n+            model_vocab_size = config.vocab_size\n+            # Retrieve the embeddings and clone theme\n+            model_embed = model.resize_token_embeddings(model_vocab_size)\n+            cloned_embeddings = model_embed.weight.clone()\n+\n+            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n+            model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n+            self.assertEqual(model.config.vocab_size, model_vocab_size + 10)\n+            # Check that it actually resizes the embeddings matrix\n+            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n+            model_embed = model.resize_token_embeddings(model_vocab_size - 15)\n+            self.assertEqual(model.config.vocab_size, model_vocab_size - 15)\n+            # Check that it actually resizes the embeddings matrix\n+            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] - 15)\n+\n+            # make sure that decoder_input_ids are resized\n+            if \"decoder_input_ids\" in inputs_dict:\n+                inputs_dict[\"decoder_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that adding and removing tokens has not modified the first part of the embedding matrix.\n+            models_equal = True\n+            for p1, p2 in zip(cloned_embeddings, model_embed.weight):\n+                if p1.data.ne(p2.data).sum() > 0:\n+                    models_equal = False\n+\n+            self.assertTrue(models_equal)\n+\n+    # Copied from tests.models.whisper.test_modeling_whisper.WhisperModelTest.test_resize_embeddings_untied\n+    def test_resize_embeddings_untied(self):\n+        (\n+            original_config,\n+            inputs_dict,\n+        ) = self.model_tester.prepare_config_and_inputs_for_common()\n+        if not self.test_resize_embeddings:\n+            self.skipTest(reason=\"test_resize_embeddings is False\")\n+\n+        original_config.tie_word_embeddings = False\n+\n+        # if model cannot untied embeddings -> leave test\n+        if original_config.tie_word_embeddings:\n+            self.skipTest(reason=\"Model cannot untie embeddings\")\n+\n+        for model_class in self.all_model_classes:\n+            config = copy.deepcopy(original_config)\n+            model = model_class(config).to(torch_device)\n+\n+            # if no output embeddings -> leave test\n+            if model.get_output_embeddings() is None:\n+                continue\n+\n+            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n+            model_vocab_size = config.vocab_size\n+            model.resize_token_embeddings(model_vocab_size + 10)\n+            self.assertEqual(model.config.vocab_size, model_vocab_size + 10)\n+            output_embeds = model.get_output_embeddings()\n+            self.assertEqual(output_embeds.weight.shape[0], model_vocab_size + 10)\n+            # Check bias if present\n+            if output_embeds.bias is not None:\n+                self.assertEqual(output_embeds.bias.shape[0], model_vocab_size + 10)\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n+            model.resize_token_embeddings(model_vocab_size - 15)\n+            self.assertEqual(model.config.vocab_size, model_vocab_size - 15)\n+            # Check that it actually resizes the embeddings matrix\n+            output_embeds = model.get_output_embeddings()\n+            self.assertEqual(output_embeds.weight.shape[0], model_vocab_size - 15)\n+            # Check bias if present\n+            if output_embeds.bias is not None:\n+                self.assertEqual(output_embeds.bias.shape[0], model_vocab_size - 15)\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            if \"decoder_input_ids\" in inputs_dict:\n+                inputs_dict[\"decoder_input_ids\"].clamp_(max=model_vocab_size - 15 - 1)\n+            # Check that the model can still do a forward pass successfully (every parameter should be resized)\n+            model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+\n+@require_torch\n+class MoonshineModelIntegrationTests(unittest.TestCase):\n+    def setUp(self):\n+        self.processor_tiny = AutoProcessor.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        self.processor_base = AutoProcessor.from_pretrained(\"UsefulSensors/moonshine-base\")\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def _load_datasamples(self, num_samples):\n+        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        # automatic decoding with librispeech\n+        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @slow\n+    def test_tiny_logits_single(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        model.to(torch_device)\n+\n+        inputs = self.processor_tiny(self._load_datasamples(1), return_tensors=\"pt\")\n+        inputs.to(torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_logits=True)\n+\n+        # fmt: off\n+        EXPECTED_LOGITS = torch.tensor([\n+            -9.1107,  4.5538,  6.3902, -6.8141, -7.2459, -7.9077, -7.2842, -7.6045, -8.0387, -7.8354,\n+            -7.3870, -7.2453, -7.7423, -7.3914, -7.3869, -7.6982, -7.6422, -7.0507, -7.3982, -7.2486,\n+            -8.0799, -7.3303, -7.3675, -6.8769, -7.6879, -7.2684, -6.9868, -6.7459, -7.6858, -7.3052,\n+        ])\n+        # fmt: on\n+        self.assertTrue(torch.allclose(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+\n+    @slow\n+    def test_base_logits_single(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-base\")\n+        model.to(torch_device)\n+\n+        inputs = self.processor_base(self._load_datasamples(1), return_tensors=\"pt\")\n+        inputs.to(torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_logits=True)\n+\n+        # fmt: off\n+        EXPECTED_LOGITS = torch.tensor([\n+            -6.7340,  1.9483,  5.2449, -8.0277, -7.9167, -7.8956, -7.9649, -7.9348, -8.1312, -8.0616,\n+            -8.1070, -7.7696, -7.8809, -7.9451, -8.1013, -7.8177, -7.8598, -7.8257, -7.8729, -7.9657,\n+            -7.9310, -8.1024, -7.8698, -7.8231, -8.0752, -7.9764, -7.8127, -8.0536, -7.9492, -7.9289,\n+        ])\n+        # fmt: on\n+        self.assertTrue(torch.allclose(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+\n+    @slow\n+    def test_tiny_logits_batch(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        model.to(torch_device)\n+\n+        inputs = self.processor_tiny(self._load_datasamples(4), return_tensors=\"pt\", padding=True)\n+        inputs.to(torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_logits=True)\n+        # fmt: off\n+        EXPECTED_LOGITS = torch.tensor([\n+            [-8.0098,  5.0239,  4.5986, -6.8125, -7.1676, -7.8782, -7.2152, -7.5188, -7.9078, -7.7394],\n+            [-4.4394, -1.4429,  6.6715, -6.8927, -7.3748, -7.0967, -6.5255, -7.0255, -7.2583, -7.0007],\n+            [-10.0088, 3.2862,  0.7342, -6.5558, -6.8514, -6.5309, -6.4173, -6.9485, -6.6215, -6.6230],\n+            [-10.8083, 4.0034, -0.0635, -5.0501, -5.3903, -5.4587, -5.2416, -5.4742, -5.2662, -5.3154]\n+        ])\n+        # fmt: on\n+        self.assertTrue(torch.allclose(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+\n+    @slow\n+    def test_base_logits_batch(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-base\")\n+        model.to(torch_device)\n+\n+        inputs = self.processor_base(self._load_datasamples(4), return_tensors=\"pt\", padding=True)\n+        inputs.to(torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_logits=True)\n+\n+        # fmt: off\n+        EXPECTED_LOGITS = torch.tensor([\n+            [-7.7288,  1.4636,  5.2273, -7.7310, -7.6249, -7.6009, -7.6786, -7.6438, -7.8450, -7.7546],\n+            [-6.2161, -0.5891,  7.9489, -7.0693, -6.9996, -6.9980, -7.0952, -7.0830, -7.1685, -7.0136],\n+            [-7.3186,  3.1192,  3.8938, -5.7208, -5.8429, -5.7610, -5.9997, -5.8213, -5.8616, -5.8720],\n+            [-9.5488,  1.0147,  4.1174, -5.9972, -6.0616, -6.0331, -6.2105, -6.0320, -6.0791, -6.0875]\n+        ])\n+\n+        # fmt: on\n+        self.assertTrue(torch.allclose(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+\n+    @slow\n+    def test_tiny_generation_single(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        model.to(torch_device)\n+\n+        audio_array = self._load_datasamples(1)\n+        inputs = self.processor_tiny(audio_array, return_tensors=\"pt\")\n+        inputs.to(torch_device)\n+        generated_ids = model.generate(**inputs, max_new_tokens=20)\n+        transcript = self.processor_tiny.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+\n+        EXPECTED_TRANSCRIPT = \"Mr. Quilter is the apostle of the middle classes, and we are glad to welcome\"\n+        self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n+\n+    @slow\n+    def test_base_generation_single(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-base\")\n+        model.to(torch_device)\n+\n+        audio_array = self._load_datasamples(1)\n+        inputs = self.processor_base(audio_array, return_tensors=\"pt\")\n+        inputs.to(torch_device)\n+        generated_ids = model.generate(**inputs, max_new_tokens=20)\n+        transcript = self.processor_base.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+\n+        EXPECTED_TRANSCRIPT = \"Mr. Quilter is the apostle of the middle classes, and we are glad to welcome\"\n+        self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n+\n+    @slow\n+    def test_tiny_generation_batch(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-tiny\")\n+        model.to(torch_device)\n+\n+        audio_array = self._load_datasamples(4)\n+        inputs = self.processor_tiny(audio_array, return_tensors=\"pt\", padding=True)\n+        inputs.to(torch_device)\n+        generated_ids = model.generate(**inputs, max_new_tokens=20)\n+        transcript = self.processor_tiny.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        # fmt: off\n+        EXPECTED_TRANSCRIPT = [\n+            \"Mr. Quilter is the apostle of the middle classes, and we are glad to welcome\",\n+            \"Nor is Mr. Quilter's manner less interesting than his matter.\",\n+            \"He tells us that at this festive season of the year, with Christmas and Rose beef lo\",\n+            \"He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\",\n+        ]\n+        # fmt: on\n+\n+        self.assertListEqual(transcript, EXPECTED_TRANSCRIPT)\n+\n+    @slow\n+    def test_base_generation_batch(self):\n+        model = MoonshineForConditionalGeneration.from_pretrained(\"UsefulSensors/moonshine-base\")\n+        model.to(torch_device)\n+\n+        audio_array = self._load_datasamples(4)\n+        inputs = self.processor_base(audio_array, return_tensors=\"pt\", padding=True)\n+        inputs.to(torch_device)\n+        generated_ids = model.generate(**inputs, max_new_tokens=20)\n+        transcript = self.processor_base.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        # fmt: off\n+        EXPECTED_TRANSCRIPT = [\n+            \"Mr. Quilter is the apostle of the middle classes, and we are glad to welcome\",\n+            \"Nor is Mr. Quilter's manner less interesting than his matter.\",\n+            \"He tells us that at this festive season of the year, with Christmas and rose beef lo\",\n+            \"He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\",\n+        ]\n+        # fmt: on\n+\n+        self.assertListEqual(transcript, EXPECTED_TRANSCRIPT)"
        }
    ],
    "stats": {
        "total": 3854,
        "additions": 3852,
        "deletions": 2
    }
}