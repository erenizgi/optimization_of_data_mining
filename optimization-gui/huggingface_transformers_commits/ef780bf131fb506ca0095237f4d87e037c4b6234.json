{
    "author": "remi-or",
    "message": "Add torch compile to CB (#42516)\n\n* transfer commit\n\n* Allow fullgraph and little fixes\n\n* fix when no measurements\n\n* CB is better at handling compile. Also can be benched.\n\n* Style\n\n* Add sumarrized by default\n\n* Doc and better CG logic\n\n* CG logic rollback\n\n* Update to Fa3 thx to Anton\n\n* style",
    "sha": "ef780bf131fb506ca0095237f4d87e037c4b6234",
    "files": [
        {
            "sha": "9d051e9a99ec580f3cc8f1db333d72c6e25ae20a",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 51,
            "deletions": 30,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -5,6 +5,7 @@\n from functools import lru_cache\n from typing import Any\n \n+from transformers.generation.configuration_utils import CompileConfig\n from transformers.utils.import_utils import is_flash_attn_2_available, is_kernels_available\n \n \n@@ -61,8 +62,7 @@ def __init__(\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n         attn_implementation: str = \"eager\",\n-        compile_mode: str | None = None,\n-        compile_options: dict[str, Any] | None = None,\n+        compile_kwargs: dict[str, Any] | None = None,\n         kernelize: bool = False,\n         name: str | None = None,\n         skip_validity_check: bool = False,\n@@ -79,8 +79,11 @@ def __init__(\n         # Generation parameters\n         self.attn_implementation = attn_implementation\n         # Optimization parameters\n-        self.compile_mode = compile_mode\n-        self.compile_options = compile_options if compile_options is not None else {}\n+        if compile_kwargs is None:\n+            self.compile_config = None\n+        else:\n+            compile_kwargs[\"fullgraph\"] = compile_kwargs.get(\"fullgraph\", True)\n+            self.compile_config = CompileConfig(**compile_kwargs)\n         self.kernelize = kernelize\n         # Constant parameters\n         self.dtype = \"torch.bfloat16\"\n@@ -92,22 +95,41 @@ def __init__(\n     def check_validity(self, skip_validity_check: bool = False) -> None:\n         if skip_validity_check:\n             return\n-        # Check FA is installed\n-        is_fa = self.attn_implementation == \"flash_attention_2\"\n-        if is_fa and not is_fa2_or_kernel_available():\n-            logger.warning(\"Flash attention is not available. Defaulting to SDPA.\")\n+\n+        # If flash_attention_2 is selected but not available, default to SDPA\n+        if self.attn_implementation == \"flash_attention_2\" and not is_fa2_or_kernel_available():\n+            logger.error(\"Flash attention is not available. Defaulting to SDPA.\")\n             self.attn_implementation = \"sdpa\"\n-        # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n-        if is_fa and self.compile_mode is not None:\n-            logger.warning(\"Flash attention does not support compile mode. Turning off compile mode.\")\n-            self.compile_mode = None\n-        # Handle continuous batching cases\n-        if self.continuous_batching:\n-            if self.attn_implementation == \"flex_attention\":\n-                logger.error(\n-                    \"Disabling continuous batching because of invalid configuration: flex attention is not supported.\"\n-                )\n-                self.continuous_batching = False\n+\n+        # The combination of flash_attention_2, compile and generate is not supported # FIXME: support it\n+        if (\n+            not self.continuous_batching\n+            and self.attn_implementation == \"flash_attention_2\"\n+            and self.compile_config is not None\n+        ):\n+            logger.error(\n+                \"The combination of flash_attention_2, compile and generate is not supported. Turning off compile.\"\n+            )\n+            self.compile_config = None\n+\n+        # Continuous batching does not support flex attention as an attention implementation # FIXME: support it\n+        if self.attn_implementation == \"flex_attention\" and self.continuous_batching:\n+            logger.error(\n+                \"Disabling continuous batching because of invalid configuration: flex attention is not supported.\"\n+            )\n+            self.continuous_batching = False\n+\n+        # Continuous batching supports compile mode \"default\" or \"max-autotune-no-cudagraphs\"\n+        if (\n+            self.continuous_batching\n+            and self.compile_config is not None\n+            and self.compile_config.mode not in [\"default\", \"max-autotune-no-cudagraphs\"]\n+        ):\n+            logger.error(\n+                f\"You have continuous batching and compile enabled, but {self.compile_config.mode = } is not supported.\"\n+                \" Supported modes are: default, max-autotune-no-cudagraphs. Changing to default.\"\n+            )\n+            self.compile_config.mode = \"default\"\n \n     @property\n     def hash(self) -> str:\n@@ -120,7 +142,7 @@ def infer_name(self, compact: bool = True) -> str:\n             gpu_monitor_str = \"monitored\" if self.gpu_monitoring else \"unmonitored\"\n             dimensions_str = f\"b{self.batch_size}_s{self.sequence_length}_n{self.num_tokens_to_generate}\"\n             attn_code = self.attn_implementation\n-            compile_str = f\"compiled_{self.compile_mode}\" if self.compile_mode is not None else \"uncompiled\"\n+            compile_str = f\"compiled_{self.compile_config.mode}\" if self.compile_config is not None else \"uncompiled\"\n             kernelize_str = \"kernelized\" if self.kernelize else \"unkernelized\"\n             continuous_batching_str = \"cb\" if self.continuous_batching else \"generate\"\n             sep = \"-\"\n@@ -129,7 +151,7 @@ def infer_name(self, compact: bool = True) -> str:\n             gpu_monitor_str = (\"with\" if self.gpu_monitoring else \"no\") + \" GPU monitoring\"\n             dimensions_str = f\"batch size {self.batch_size}, sequence length {self.sequence_length}, {self.num_tokens_to_generate} generated tokens\"\n             attn_code = f\"{self.attn_implementation} attention\"\n-            compile_str = \"compiled\" if self.compile_mode is not None else \"not compiled\"\n+            compile_str = \"compiled\" if self.compile_config is not None else \"not compiled\"\n             kernelize_str = \"kernelized\" if self.kernelize else \"not kernelized\"\n             continuous_batching_str = \"continuous batching\" if self.continuous_batching else \"regular generate\"\n             sep = \", \"\n@@ -148,8 +170,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"sequence_length\": self.sequence_length,\n             \"num_tokens_to_generate\": self.num_tokens_to_generate,\n             \"attn_implementation\": self.attn_implementation,\n-            \"compile_mode\": self.compile_mode,\n-            \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n+            \"compile_kwargs\": self.compile_config.to_dict() if self.compile_config is not None else None,\n             \"kernelize\": self.kernelize,\n         }\n \n@@ -164,8 +185,7 @@ def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"\n             sequence_length=data.get(\"sequence_length\", 128),\n             num_tokens_to_generate=data.get(\"num_tokens_to_generate\", 128),\n             attn_implementation=data.get(\"attn_implementation\", \"eager\"),\n-            compile_mode=data.get(\"compile_mode\"),\n-            compile_options=data.get(\"compile_options\"),\n+            compile_kwargs=data.get(\"compile_kwargs\"),\n             kernelize=data.get(\"kernelize\", False),\n             name=data.get(\"name\"),\n             skip_validity_check=skip_validity_check,\n@@ -218,27 +238,28 @@ def get_config_by_level(level: int) -> list[BenchmarkConfig]:\n             # Usually there is not much to gain by compiling with other modes, but we allow it for level 4\n             compile_modes = BenchmarkConfig.all_compiled_modes if level >= 4 else [None, \"default\"]\n             for cm in compile_modes:\n+                compile_kwargs = {\"mode\": cm} if cm is not None else None\n                 for kernelize_on in {False, KERNELIZATION_AVAILABLE}:\n                     for cb_on in [False, True]:\n                         configs.append(\n                             BenchmarkConfig(\n                                 attn_implementation=attn_implementation,\n-                                compile_mode=cm,\n+                                compile_kwargs=compile_kwargs,\n                                 kernelize=kernelize_on,\n                                 continuous_batching=cb_on,\n                             )\n                         )\n         return configs\n     # Otherwise, we add the configs for the given level\n     if level >= 0:\n-        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\"))\n+        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_kwargs={}))\n     if level >= 1:\n         configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\"))\n-        configs.append(BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\"))\n+        configs.append(BenchmarkConfig(attn_implementation=\"eager\", compile_kwargs={}))\n         configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\", continuous_batching=True))\n     if level >= 2:\n-        configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", compile_mode=\"default\"))\n-        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", kernelize=True))\n+        configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", compile_kwargs={}))\n+        configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_kwargs={}, kernelize=True))\n         configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\", kernelize=True))\n         configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", continuous_batching=True))\n     return configs"
        },
        {
            "sha": "fe7c7175062e67ccaa7ba19d93544a6f1749931e",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 79,
            "deletions": 73,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -18,7 +18,6 @@\n from transformers import (\n     AutoModelForCausalLM,\n     AutoTokenizer,\n-    CompileConfig,\n     GenerationConfig,\n     GenerationMixin,\n )\n@@ -78,23 +77,26 @@ def get_git_revision() -> str:\n         return git_hash.readline().strip()\n \n \n-def flush_memory():\n-    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n+def flush_memory(flush_compile: bool = True) -> None:\n+    \"\"\"Flush GPU memory and run garbage collection. If the flush_compile flag is set, we also clear the everything\n+    related to compile cache.\"\"\"\n     gc.collect()\n-    # Dynamo resets\n-    torch._dynamo.reset()\n-    torch._dynamo.reset_code_caches()\n-    if hasattr(torch._inductor, \"codecache\"):\n-        # Clear FX graph cache\n-        if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n-            torch._inductor.codecache.FxGraphCache.clear()\n-        # Clear PyCodeCache\n-        if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n-            torch._inductor.codecache.PyCodeCache.cache_clear()\n-        # Clear TritonFuture cache (for async compilation)\n-        if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n-            if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n-                torch._inductor.codecache.TritonFuture._compile_cache.clear()\n+    # If needed, flush everything related to torch.compile\n+    if flush_compile:\n+        # Dynamo resets\n+        torch._dynamo.reset()\n+        torch._dynamo.reset_code_caches()\n+        if hasattr(torch._inductor, \"codecache\"):\n+            # Clear FX graph cache\n+            if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n+                torch._inductor.codecache.FxGraphCache.clear()\n+            # Clear PyCodeCache\n+            if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n+                torch._inductor.codecache.PyCodeCache.cache_clear()\n+            # Clear TritonFuture cache (for async compilation)\n+            if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n+                if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n+                    torch._inductor.codecache.TritonFuture._compile_cache.clear()\n     # Clear CUDA cache\n     if torch.cuda.is_available():\n         torch.cuda.empty_cache()\n@@ -179,63 +181,57 @@ def setup_benchmark(self, model_id: str, config: BenchmarkConfig) -> None:\n         self.inputs[\"use_cache\"] = True\n \n         # Prepare generation config\n-        gen_config = GenerationConfig(\n-            do_sample=False, top_p=1.0, temperature=1.0, max_new_tokens=config.num_tokens_to_generate\n-        )\n+        generation_config_kwargs = {\n+            \"do_sample\": False,\n+            \"max_new_tokens\": config.num_tokens_to_generate,\n+        }\n+\n+        # Add compile config if found\n+        if config.compile_config is not None:\n+            generation_config_kwargs.update(compile_config=config.compile_config)\n+            # To trigger compile in generate, we need to set the cache to static\n+            if not config.continuous_batching:\n+                generation_config_kwargs.update(cache_implementation=\"static\")\n \n-        # Prepare compile config\n-        if config.compile_mode is not None:\n-            gen_config.compile_config = CompileConfig(mode=config.compile_mode, options=config.compile_options)\n-            gen_config.cache_implementation = \"static\"\n+        generation_config = GenerationConfig(**generation_config_kwargs)\n \n         # Load model\n         self.logger.debug(f\"Loading model {model_id} on device {config.device}...\")\n         dtype = getattr(torch, config.dtype.removeprefix(\"torch.\"))\n         self.model = AutoModelForCausalLM.from_pretrained(\n-            model_id, dtype=dtype, attn_implementation=config.attn_implementation, generation_config=gen_config\n+            model_id, dtype=dtype, attn_implementation=config.attn_implementation, generation_config=generation_config\n         )\n         self.model = self.model.eval().to(config.device)\n \n         # Kernelize the model if needed\n         if config.kernelize and kernelize is not None and Mode is not None:\n             self.model = kernelize(self.model, mode=Mode.INFERENCE)\n \n-    def run_benchmark(\n-        self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n-    ) -> dict[str, Any] | None:\n+    def run_benchmark(self, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> BenchmarkResult | None:\n         \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n         with torch.no_grad():\n             self.logger.info(f\"Running benchmark scenario: {config.name}\")\n+            self.logger.debug(f\"Full config: {config.to_dict()}\")\n \n             # Quick validation: try one measurement first to see if this scenario works\n             flush_memory()\n-            e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics = self.time_generate(\n-                max_new_tokens=config.num_tokens_to_generate,\n-                use_continuous_batching=config.continuous_batching,\n-                gpu_monitor=None,\n-            )\n+            e2e_latency = self.time_generate(config, warmup=True)[0]\n             if e2e_latency < 0:\n-                self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = } (no GPU monitoring)\")\n+                self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = }\")\n                 return None\n \n             # Warmup runs\n             self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n             for _ in trange(config.warmup_iterations, desc=\"Warmup\"):\n-                _ = self.time_generate(\n-                    max_new_tokens=config.num_tokens_to_generate,\n-                    use_continuous_batching=config.continuous_batching,\n-                    gpu_monitor=None,\n-                )\n+                self.time_generate(config, warmup=True)\n             self.logger.info(\"Warmup over.\")\n \n             # Measurement runs\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations, desc=\"Benchmarking\"):\n                 e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics = self.time_generate(\n-                    max_new_tokens=config.num_tokens_to_generate,\n-                    use_continuous_batching=config.continuous_batching,\n-                    gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n+                    config, warmup=False\n                 )\n                 result.accumulate(e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n@@ -244,52 +240,45 @@ def run_benchmark(\n             if num_tokens_to_profile > 0:\n                 self.profile_generate(num_tokens_to_profile, config.name)\n \n-            return {\n-                \"metadata\": BenchmarkMetadata(\n-                    model_id=model_id,\n-                    branch_name=self.branch_name,\n-                    commit_id=self.commit_id,\n-                    commit_message=self.commit_message,\n-                ),\n-                \"measurements\": result,\n-                \"config\": config,\n-            }\n+            return result\n \n     def time_generate(\n-        self,\n-        max_new_tokens: int,\n-        use_continuous_batching: bool = False,\n-        gpu_monitor: GPUMonitor | None = None,\n+        self, config: BenchmarkConfig, warmup: bool\n     ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n         # Prepare gpu monitoring if needed\n-        if gpu_monitor is not None:\n+        if config.gpu_monitoring and not warmup:\n+            gpu_monitor = GPUMonitor(logger=self.logger)\n             gpu_monitor.start()\n+        else:\n+            gpu_monitor = None\n \n         # Generate and time\n-        if use_continuous_batching:\n+        if config.continuous_batching:\n             inputs = self.inputs[\"input_ids\"].tolist()\n             wall_time_0 = time.perf_counter()\n-            results = self.model.generate_batch(inputs, allow_prefix_sharing=False, record_timestamps=True)\n+            outputs = self.model.generate_batch(inputs, allow_prefix_sharing=False, record_timestamps=True)\n         else:\n             streamer = BenchmarkStreamer()\n             wall_time_0 = time.perf_counter()\n-            results = self.model.generate(**self.inputs, streamer=streamer)\n+            outputs = self.model.generate(**self.inputs, streamer=streamer)\n \n         wall_time_1 = time.perf_counter()\n         gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n \n         # Retrieve timestamps and results in a way that allows similar post-processing\n         input_tokens = self.inputs[\"input_ids\"].size(-1)\n-        if use_continuous_batching:\n-            timestamps = [result.timestamps for result in results.values()]\n-            results = torch.tensor([result.generated_tokens for result in results.values()])\n+        if config.continuous_batching:\n+            timestamps = [output.timestamps[:] for output in outputs.values()]\n+            results = torch.tensor([output.generated_tokens[:] for output in outputs.values()])\n         else:\n             timestamps = [streamer.timestamps[1:]]  # skip the first timestamp because it's the input tokens\n-            results = results[:, input_tokens:]\n+            results = outputs[:, input_tokens:]\n+        outputs = None\n+        flush_memory(flush_compile=False)\n \n         # Check if generation had the right number of tokens\n-        if results.size(-1) != max_new_tokens:\n-            raise RuntimeError(f\"Generated {results.size(-1)} tokens, expected {max_new_tokens}\")\n+        if results.size(-1) != config.num_tokens_to_generate:\n+            raise RuntimeError(f\"Generated {results.size(-1)} tokens, expected {config.num_tokens_to_generate}\")\n \n         # Decode outputs\n         decoded_output = self.tokenizer.decode(results[0], skip_special_tokens=True)\n@@ -298,6 +287,9 @@ def time_generate(\n         # Compute metrics\n         e2e_latency = wall_time_1 - wall_time_0\n         timestamps = torch.tensor(timestamps).sub(wall_time_0).tolist()\n+        self.logger.info(\n+            f\"Time generate done in {e2e_latency:.2f} seconds. Memory usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\"\n+        )\n         return e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n@@ -316,12 +308,14 @@ def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None\n             os.makedirs(self.profile_dir, exist_ok=True)\n         prof.export_chrome_trace(f\"{self.profile_dir}/{config_name}.json\")\n \n+    @torch.inference_mode()\n     def run_benchmarks(\n         self,\n         model_id: str,\n         benchmark_configs: list[BenchmarkConfig],\n         num_tokens_to_profile: int = 0,\n         pretty_print_summary: bool = True,\n+        summarized: bool = True,\n     ) -> tuple[str, dict[str, Any]]:\n         \"\"\"Run multiple benchmarks for the given model ID and list of benchmark configs.\"\"\"\n         all_results = {}\n@@ -343,15 +337,27 @@ def run_benchmarks(\n \n             # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n             try:\n-                results = self.run_benchmark(model_id, config, num_tokens_to_profile)\n-                if results is not None:\n-                    all_results[config.hash] = results\n-\n+                result = self.run_benchmark(config, num_tokens_to_profile)\n             except Exception as e:\n                 self.logger.error(f\"Error running with scenario: {config.name}:\\n{repr(e)}\")\n+                result = None\n+\n+            # Memoize\n+            all_results[config.hash] = {\n+                \"metadata\": BenchmarkMetadata(\n+                    model_id=model_id,\n+                    branch_name=self.branch_name,\n+                    commit_id=self.commit_id,\n+                    commit_message=self.commit_message,\n+                    success=result is not None,\n+                ),\n+                \"measurements\": result if result is not None else BenchmarkResult(),\n+                \"config\": config,\n+            }\n+\n             # Cleanup model and save results\n             self.cleanup()\n-            self.save_results(model_id, all_results, timestamp=timestamp)\n+            self.save_results(model_id, all_results, timestamp=timestamp, summarized=summarized)\n \n         if len(all_results) < 1:\n             raise RuntimeError(\"No benchmark was run successfully\")\n@@ -378,7 +384,7 @@ def run_benchmarks(\n \n         return (timestamp, all_results)\n \n-    def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n+    def save_results(self, model_name: str, results: dict, timestamp: str = \"\", summarized: bool = True) -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n         # Create model-specific subdirectory\n         model_name = model_name.replace(\"/\", \"_\")\n@@ -395,7 +401,7 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> s\n         for cfg_hash in results.keys():\n             converted_results[cfg_hash] = {\n                 \"metadata\": results[cfg_hash][\"metadata\"].to_dict(),\n-                \"measurements\": results[cfg_hash][\"measurements\"].to_dict(),\n+                \"measurements\": results[cfg_hash][\"measurements\"].to_dict(summarized=summarized),\n                 \"config\": results[cfg_hash][\"config\"].to_dict(),\n             }\n "
        },
        {
            "sha": "0707a664445bd02c691ca069249fd59bdd9478ec",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -9,12 +9,12 @@\n \n def compute_basic_statistics(measurements: list[float]) -> dict[str, float]:\n     return {\n-        \"avg\": np.mean(measurements),\n-        \"std\": np.std(measurements),\n-        \"min\": np.min(measurements),\n-        \"med\": np.median(measurements),\n-        \"max\": np.max(measurements),\n-        \"p95\": np.percentile(measurements, 95),\n+        \"avg\": np.mean(measurements) if measurements else 0,\n+        \"std\": np.std(measurements) if measurements else 0,\n+        \"min\": np.min(measurements) if measurements else 0,\n+        \"med\": np.median(measurements) if measurements else 0,\n+        \"max\": np.max(measurements) if measurements else 0,\n+        \"p95\": np.percentile(measurements, 95) if measurements else 0,\n     }\n \n \n@@ -64,14 +64,18 @@ class BenchmarkMetadata:\n     commit_id: str\n     commit_message: str\n     hardware_info: HardwareInfo\n+    success: bool\n \n-    def __init__(self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\") -> None:\n+    def __init__(\n+        self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\", success: bool = True\n+    ) -> None:\n         self.model_id = model_id\n         self.timestamp = datetime.now(timezone.utc).isoformat()\n         self.branch_name = branch_name\n         self.commit_id = commit_id\n         self.commit_message = commit_message\n         self.hardware_info = HardwareInfo()\n+        self.success = success\n \n     def to_dict(self) -> dict[str, Any]:\n         return {\n@@ -81,6 +85,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"commit_id\": self.commit_id,\n             \"commit_message\": self.commit_message,\n             \"hardware_info\": self.hardware_info.to_dict(),\n+            \"success\": self.success,\n         }\n \n "
        },
        {
            "sha": "003e7646389c7dff1bbbf41c4f4e9edea725b62b",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 27,
            "deletions": 7,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -25,7 +25,7 @@\n from torch.profiler import ProfilerActivity, profile\n from tqdm import tqdm\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, CompileConfig\n from transformers.generation import GenerationConfig\n from transformers.generation.continuous_batching.requests import logger\n \n@@ -172,7 +172,7 @@ def batch_generate(\n \n     # Model parameters\n     parser.add_argument(\"--sliding-window\", type=int, default=0)\n-    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn2\", help=\"Attention implementation\")\n+    parser.add_argument(\"--attn\", type=str, default=None, help=\"Attention implementation\")\n \n     # Performance parameters\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n@@ -182,6 +182,8 @@ def batch_generate(\n \n     # Benchmark parameters\n     parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n+    parser.add_argument(\"--max-new-tokens\", type=int, default=512, help=\"Maximum number of new tokens to generate\")\n+\n     parser.add_argument(\"--add-prefix\", action=\"store_true\", help=\"Add a prefix to the samples\")\n     parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare CB generation with classic generate\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n@@ -195,6 +197,18 @@ def batch_generate(\n \n     args = parser.parse_args()\n \n+    # Choose attention implementation\n+    if args.attn is None:\n+        if args.compile:\n+            args.attn = \"kernels-community/flash-attn3@fake-ops-return-probs\"\n+            logger.warning(\n+                \"No attention implementation was provided and compile is enabled. Using experimental kernel: \"\n+                \"kernels-community/flash-attn3@fake-ops-return-probs because compile is not supported on main. Change \"\n+                \"this when main supports it.\"  # TODO: cf comment\n+            )\n+        else:\n+            args.attn = \"kernels-community/flash-attn3\"\n+\n     # Create model\n     model_id = \"google/gemma-2-2b-it\" if args.sliding_window > 0 else \"meta-llama/Llama-3.1-8B-Instruct\"\n     has_system_role = args.sliding_window == 0\n@@ -221,9 +235,6 @@ def batch_generate(\n         \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n     }[cuda_graph_arg]  # fmt: skip\n \n-    if args.compile:\n-        model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-\n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n \n@@ -251,11 +262,12 @@ def batch_generate(\n                 question = prefix + \"\\n\\n\" + question\n         messages.append({\"role\": \"user\", \"content\": question})\n         inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n-        batched_inputs.append(inputs[\"input_ids\"])\n+        inputs = inputs if isinstance(inputs, list) else inputs[\"input_ids\"]\n+        batched_inputs.append(inputs)\n \n     # Prepare generation config\n     generation_cfg = GenerationConfig(\n-        max_new_tokens=512,\n+        max_new_tokens=args.max_new_tokens,\n         use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if args.force_max_length else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n@@ -266,6 +278,13 @@ def batch_generate(\n         max_batch_tokens=args.max_batch_tokens,\n     )\n \n+    # Add a compile config if requested\n+    if args.compile:\n+        generation_cfg.compile_config = CompileConfig(\n+            fullgraph=True,\n+            mode=\"max-autotune-no-cudagraphs\",\n+        )\n+\n     # If we need to compare, we need to generate the reference outputs\n     if args.compare:\n         expected_outputs = generate_without_cb(\n@@ -313,3 +332,4 @@ def batch_generate(\n # Example usage:\n # python examples/pytorch/continuous_batching.py --attn sdpa --add-prefix --samples 10 --compare\n # python examples/pytorch/continuous_batching.py --attn flash_attention_2 -mp none --add-prefix --samples 500\n+# python examples/pytorch/continuous_batching.py -mp none -cg yes --samples 10 --max-new-tokens 32 --profile profile_wip.json"
        },
        {
            "sha": "a30497ed548393238a26b1c1ed199428f3cb77d8",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 129,
            "deletions": 69,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -29,7 +29,7 @@\n from tqdm.contrib.logging import logging_redirect_tqdm\n \n from ...configuration_utils import PretrainedConfig\n-from ...generation.configuration_utils import GenerationConfig\n+from ...generation.configuration_utils import CompileConfig, GenerationConfig\n from ...generation.logits_process import LogitsProcessor\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -45,17 +45,20 @@\n - the number of keys/values tokens (KV), which grows as the cache does\n \n To solve this, we slice along those dimensions to fixed lengths. The size of the slices is controlled by the variables\n-below: NUM_X_CUDA_GRAPHS means that we create at most NUM_X_CUDA_GRAPHS graphs for the X dimension. So if the maximum\n-number of queries tokens is 1000, and NUM_Q_CUDA_GRAPHS is 4, we will slice the number of queries token by intervals of\n-1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n+num_x_padding_intervals: NUM_X_PADDING_INTERVALS means that we create at most NUM_X_PADDING_INTERVALS graphs for the X\n+dimension. So if the maximum number of queries tokens is 1000, and NUM_Q_PADDING_INTERVALS is 4, we will slice the\n+number of queries token by intervals of 1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n \n Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n number of tokens cached, dimension of KV is usually much larger than the dimension of Q. So we have more granularity\n for the KV dimension than the query dimension.\n+\n+This variable used to be called NUM_X_CUDA_GRAPHS, but we renamed it to NUM_X_PADDING_INTERVALS because it is used for\n+padding in the case of cuda graphs AND torch.compile.\n \"\"\"\n-NUM_Q_CUDA_GRAPHS = 4\n-NUM_KV_CUDA_GRAPHS = 8\n+NUM_Q_PADDING_INTERVALS = 4\n+NUM_KV_PADDING_INTERVALS = 8\n \n \n def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n@@ -188,6 +191,8 @@ def __init__(\n         scheduler: Scheduler,\n         manual_eviction: bool,\n         use_cuda_graph: bool,\n+        q_padding_intervals: int,\n+        kv_padding_intervals: int,\n     ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -221,7 +226,14 @@ def __init__(\n         # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n         # Cuda graphs for the generation step\n+        self.q_padding_intervals = q_padding_intervals\n+        self.kv_padding_intervals = kv_padding_intervals\n         self._graphs: dict[tuple[int, int], torch.cuda.CUDAGraph] | None = {} if use_cuda_graph else None\n+        # Compile-related arguments\n+        self.compile_config: CompileConfig | None = getattr(generation_config, \"compile_config\", None)\n+        self._forward_process_and_sample_is_compiled = False\n+\n+        self._pad_inputs = use_cuda_graph or (self.compile_config is not None and not self.compile_config.dynamic)\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n@@ -627,28 +639,39 @@ def fail_all_requests(self, error: Exception) -> None:\n     def _generation_step(self, model: nn.Module, logit_processor: LogitsProcessor, do_sample: bool) -> None:\n         \"\"\"Perform a single generation step.\"\"\"\n \n-        # If cuda graphs are disabled, we just use the actual size\n+        # If a compile config is specified, we compile the forward pass once in a wrapper\n+        if self.compile_config is not None and not self._forward_process_and_sample_is_compiled:\n+            self._forward_process_and_sample = torch.compile(\n+                self._forward_process_and_sample,\n+                fullgraph=self.compile_config.fullgraph,\n+                mode=self.compile_config.mode,\n+                dynamic=self.compile_config.dynamic,\n+                backend=self.compile_config.backend,\n+                options=self.compile_config.options,\n+            )\n+            self._forward_process_and_sample_is_compiled = True\n+\n+        # If inputs are static sized, we find the padded sizes of the queries and keys/values\n+        if self._pad_inputs:\n+            padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, self.q_padding_intervals)\n+            max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n+            padded_read_index_size = pad_by_intervals(\n+                max_read_index_size - self.max_batch_tokens,\n+                self.cache.num_blocks * self.cache.block_size,\n+                self.kv_padding_intervals,\n+            )\n+        else:\n+            padded_q, padded_read_index_size = 0, 0\n+        # Retrieve the model kwargs with or without padding\n+        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n+\n+        # If we are not using cuda graphs, we perform the generation step and return\n         if self._graphs is None:\n-            batch_data = self.get_model_kwargs()\n             self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n             return None\n \n-        # Determine the padded size of the queries and keys/values\n-        padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, NUM_Q_CUDA_GRAPHS)\n-\n-        max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n-        padded_read_index_size = pad_by_intervals(\n-            max_read_index_size - self.max_batch_tokens,\n-            self.cache.num_blocks * self.cache.block_size,\n-            NUM_KV_CUDA_GRAPHS,\n-        )\n-\n-        # Get the batch data and the associated graph\n-        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n-\n-        graph = self._graphs.get((padded_q, padded_read_index_size))\n-\n         # If we have a graph that fits, we replay it\n+        graph = self._graphs.get((padded_q, padded_read_index_size))\n         if graph is not None:\n             graph.replay()\n             return None\n@@ -673,7 +696,6 @@ def _forward_process_and_sample(\n     ) -> None:\n         \"\"\"This function performs the forward pass, logits processing, and sampling; which are broken down into smaller\n         function to be easier to trace with OpenTelemetry.\"\"\"\n-        # with torch.no_grad():\n         logits = self._model_forward(model, batch_data)\n         # if self.log_prob_generation:    batch_processor.output_probs.copy_(logits)  # TODO\n         probs = self._process_logit(batch_data, logits, logit_processor)\n@@ -727,8 +749,8 @@ def __init__(\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        num_q_cuda_graphs: int = 0,\n-        num_kv_cuda_graphs: int = 0,\n+        num_q_padding_intervals: int = 0,\n+        num_kv_padding_intervals: int = 0,\n         allow_prefix_sharing: bool = True,\n     ) -> None:\n         \"\"\"Initialize the continuous batching manager.\n@@ -737,8 +759,8 @@ def __init__(\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n-            num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n-            num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n+            num_q_padding_intervals: (optional) Number of intervals used to pad the query dimension\n+            num_kv_padding_intervals: (optional) Number of intervals used to pad the keys/values dimension\n             allow_prefix_sharing: (optional) Whether to allow prefix sharing if the model has only full attention layers\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n@@ -764,38 +786,69 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        use_cuda_graph: bool | None = getattr(generation_config, \"use_cuda_graph\", None)\n         self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: ContinuousBatchProcessor | None = None\n-\n         self._allow_prefix_sharing = allow_prefix_sharing\n \n-        # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n-        if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n-            self.use_cuda_graph = True\n-        # If use_cuda_graph is specified, we follow the user's choice\n-        elif use_cuda_graph is not None:\n-            self.use_cuda_graph = use_cuda_graph\n-        # If the use of cuda graphs is not specified, we follow the user's choice, otherwise we have a default heuristic\n-        else:\n-            # Attention implementations where an attention mask is needed suffer a lot more from the padding associated\n-            # with cuda graphs, so default is to turn cuda graphs off for those implementations\n-            self.use_cuda_graph = not attn_mask_is_needed(self.model.config)\n-            logger.warning(\n-                f\"No behavior specified for use_cuda_graph, defaulting to {self.use_cuda_graph = } because \"\n-                f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n-                \"they can improve performances.\"\n-            )\n+        self.use_cuda_graph = self._decide_use_cuda_graphs(\n+            use_cuda_graph=getattr(generation_config, \"use_cuda_graph\", None),\n+            num_q_padding_intervals=num_q_padding_intervals,\n+            num_kv_padding_intervals=num_kv_padding_intervals,\n+            compile_config=getattr(generation_config, \"compile_config\", None),\n+        )\n \n-        # If cuda graphs are activated, we set the number of cuda graphs for Q and KV if not specified\n-        if self.use_cuda_graph:\n-            self.num_q_cuda_graphs = num_q_cuda_graphs if num_q_cuda_graphs > 0 else NUM_Q_CUDA_GRAPHS\n-            self.num_kv_cuda_graphs = num_kv_cuda_graphs if num_kv_cuda_graphs > 0 else NUM_KV_CUDA_GRAPHS\n+        # We set the number of padding intervals for Q and KV\n+        self.q_padding_intervals = num_q_padding_intervals if num_q_padding_intervals > 0 else NUM_Q_PADDING_INTERVALS\n+        self.kv_padding_intervals = (\n+            num_kv_padding_intervals if num_kv_padding_intervals > 0 else NUM_KV_PADDING_INTERVALS\n+        )\n \n         if self.log_prob_generation:\n             raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n+    def _decide_use_cuda_graphs(\n+        self,\n+        use_cuda_graph: bool | None,\n+        num_q_padding_intervals: int,\n+        num_kv_padding_intervals: int,\n+        compile_config: CompileConfig | None,\n+    ) -> bool:\n+        \"\"\"Returns whether or not to use cuda graphs for continuous batching, depending on the following criteria:\n+        - (use_cuda_graph) which is the user choice\n+        - (num_q_padding_intervals) or (num_kv_padding_intervals) which is used to pad inputs: if it was specified by\n+            the user, it's probable they want to use cuda graphs so inputs need to be padded\n+        - (compile_config): if compile is on, turn on cuda graphs unless the compile mode uses its own cudagraphs\n+        If none of the above criteria are met, we use a default heuristic based on the attention implementation: we turn\n+        on cuda graphs if and only if no attention mask is needed.\n+        \"\"\"\n+        # If use_cuda_graph is specified, we follow the user's choice\n+        if use_cuda_graph is not None:\n+            return use_cuda_graph\n+        # If a number of padding intervals was specified for either Q or KV, we activate cuda graphs\n+        if num_q_padding_intervals > 0 or num_kv_padding_intervals > 0:\n+            return True\n+        # If a compile config was found, turn off cuda graphs if the compile config already uses them\n+        if compile_config is not None:\n+            options = torch._inductor.list_mode_options().get(compile_config.mode, compile_config.options)\n+            compile_uses_cudagraphs = options.get(\"triton.cudagraphs\", False)\n+            if compile_uses_cudagraphs:\n+                logger.warning(\n+                    f\"Compile config {compile_config.mode = } uses cudagraphs, which usually does not work well with \"\n+                    \"continuous batching. We recommend using mode 'default' or 'max-autotune-no-cudagraphs' instead.\"\n+                )\n+            return not compile_uses_cudagraphs  # TODO: should this also match the dynamic shapes?\n+        # Otherwise we have a default heuristic based on the attention implementation:\n+        # attention implementations where an attention mask is needed suffer a lot more from the padding associated\n+        # with cuda graphs, so default is to turn cuda graphs off for those implementations\n+        use_cuda_graph = not attn_mask_is_needed(self.model.config)\n+        logger.warning(\n+            f\"No behavior specified for use_cuda_graph, defaulting to {use_cuda_graph = } because \"\n+            f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n+            \"they can improve performances.\"\n+        )\n+        return use_cuda_graph\n+\n     @traced\n     def start(self) -> None:\n         \"\"\"Start the background generation thread.\"\"\"\n@@ -999,6 +1052,8 @@ def _run_generation_loop(self) -> None:\n                 scheduler=scheduler(paged_attention_cache, self.manual_eviction),\n                 manual_eviction=self.manual_eviction,\n                 use_cuda_graph=self.use_cuda_graph,\n+                q_padding_intervals=self.q_padding_intervals,\n+                kv_padding_intervals=self.kv_padding_intervals,\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n@@ -1024,12 +1079,15 @@ def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> N\n         # Debug logging of the current memory usage\n         if logger.level <= logging.DEBUG:\n             device, total, reserved, allocated = get_device_and_memory_breakdown()\n-            logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n+            available_memory = total - max(allocated, reserved)\n+            logger.debug(\n+                f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}, Available: {available_memory}\"\n+            )\n \n         self._generation_step()\n \n         if torch.cuda.is_available():\n-            torch.cuda.synchronize()\n+            torch.cuda.synchronize()  # FIXME: why is this needed?\n         # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n@@ -1099,18 +1157,19 @@ def init_continuous_batching(\n         generation_config: GenerationConfig | None = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        num_q_cuda_graphs: int = 0,\n-        num_kv_cuda_graphs: int = 0,\n+        num_q_padding_intervals: int = 0,\n+        num_kv_padding_intervals: int = 0,\n         allow_prefix_sharing: bool = True,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n-            generation_config: Custom generation configuration\n+            generation_config: An optional generation configuration, which may contain a CompileConfig object\n             manual_eviction: Whether to manually evict requests from the cache\n             max_queue_size: Maximum size of the input request queue\n-            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n-            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n+            num_q_padding_intervals: Number of intervals used to pad the query dimension\n+            num_kv_padding_intervals: Number of intervals used to pad the keys/values dimension\n+            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -1132,8 +1191,8 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            num_q_cuda_graphs=num_q_cuda_graphs,\n-            num_kv_cuda_graphs=num_kv_cuda_graphs,\n+            num_q_padding_intervals=num_q_padding_intervals,\n+            num_kv_padding_intervals=num_kv_padding_intervals,\n             allow_prefix_sharing=allow_prefix_sharing,\n         )\n \n@@ -1144,26 +1203,27 @@ def generate_batch(\n         self,\n         inputs: list[list[int]],\n         generation_config: GenerationConfig | None = None,\n-        progress_bar: bool = True,\n-        num_q_cuda_graphs: int = 0,\n-        num_kv_cuda_graphs: int = 0,\n+        num_q_padding_intervals: int = 0,\n+        num_kv_padding_intervals: int = 0,\n         allow_prefix_sharing: bool = True,\n         record_timestamps: bool = False,\n+        progress_bar: bool = True,\n         **kwargs,\n     ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n             inputs: List of input token sequences (prompts)\n             generation_config: Optional generation configuration\n-            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n-            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n+            num_q_padding_intervals: Number of intervals used to pad the query dimension\n+            num_kv_padding_intervals: Number of intervals used to pad the keys/values dimension\n+            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers\n+            record_timestamps: If set to true, the requests will have a timestamp for each token generated\n+            progress_bar: If set to true, a progress bar will be displayed\n             **kwargs: Additional generation parameters\n \n         Returns:\n-            `list[list[int]]`: A list containing the generated sequences (including prompt tokens\n-                                if not handled otherwise) for each input prompt, in the same order.\n-                                Returns an empty list `[]` for requests that failed.\n+            `dict[str, GenerationOutput]`: a dictionary of request ids to GenerationOutput objects\n         \"\"\"\n         if not inputs:\n             return {}\n@@ -1177,8 +1237,8 @@ def generate_batch(\n         with (\n             self.continuous_batching_context_manager(\n                 generation_config=generation_config,\n-                num_q_cuda_graphs=num_q_cuda_graphs,\n-                num_kv_cuda_graphs=num_kv_cuda_graphs,\n+                num_q_cuda_graphs=num_q_padding_intervals,\n+                num_kv_cuda_graphs=num_kv_padding_intervals,\n                 allow_prefix_sharing=allow_prefix_sharing,\n                 block=True,\n                 timeout=5,"
        },
        {
            "sha": "4fc86c206928460187f6b993b8f94867c20f3f8d",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef780bf131fb506ca0095237f4d87e037c4b6234/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef780bf131fb506ca0095237f4d87e037c4b6234/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=ef780bf131fb506ca0095237f4d87e037c4b6234",
            "patch": "@@ -1301,11 +1301,20 @@ def is_jit_tracing() -> bool:\n         return False\n \n \n+def is_cuda_stream_capturing() -> bool:\n+    try:\n+        import torch\n+\n+        return torch.cuda.is_current_stream_capturing()\n+    except Exception:\n+        return False\n+\n+\n def is_tracing(tensor=None) -> bool:\n-    \"\"\"Checks whether we are tracing a graph with dynamo (compile or export), torch.jit, or torch.fx\"\"\"\n+    \"\"\"Checks whether we are tracing a graph with dynamo (compile or export), torch.jit, torch.fx or CUDA stream capturing\"\"\"\n     # Note that `is_torchdynamo_compiling` checks both compiling and exporting (the export check is stricter and\n     # only checks export)\n-    _is_tracing = is_torchdynamo_compiling() or is_jit_tracing()\n+    _is_tracing = is_torchdynamo_compiling() or is_jit_tracing() or is_cuda_stream_capturing()\n     if tensor is not None:\n         _is_tracing |= is_torch_fx_proxy(tensor)\n     return _is_tracing"
        }
    ],
    "stats": {
        "total": 497,
        "additions": 309,
        "deletions": 188
    }
}