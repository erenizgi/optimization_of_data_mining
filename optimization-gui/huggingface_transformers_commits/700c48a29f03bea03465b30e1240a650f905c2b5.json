{
    "author": "yaogang2060",
    "message": "fix qwen2vl/qwen3vl video processor temporal padding when num_frames%temporal_patch_size!=1 (#42083)\n\n* qwen3vl video process padding video frames\n\n* add two video processor test cases\n\n* fix typo\n\n* down test image size\n\n* fix qwen2vl video processor t padding\n\n* delete padding when num_frames < temporal_patch_size\n\n* to default format\n\n* fix smart_resize in qwen3vl",
    "sha": "700c48a29f03bea03465b30e1240a650f905c2b5",
    "files": [
        {
            "sha": "c0ae21ecd84e03ac3767235c9fdea9868fd7f37f",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/700c48a29f03bea03465b30e1240a650f905c2b5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/700c48a29f03bea03465b30e1240a650f905c2b5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=700c48a29f03bea03465b30e1240a650f905c2b5",
            "patch": "@@ -232,9 +232,10 @@ def _preprocess(\n             patches = stacked_videos\n \n             # Check that videos have `num_frames` divisible by `temporal_patch_size`\n-            if patches.shape[1] % temporal_patch_size != 0:\n-                repeats = patches[:, -1:].repeat(1, self.temporal_patch_size - 1, 1, 1, 1)\n-                patches = torch.cat([patches, repeats], dim=1)\n+            T = patches.shape[1]\n+            if pad := -T % temporal_patch_size:\n+                repeats = patches[:, -1:].expand(-1, pad, -1, -1, -1)\n+                patches = torch.cat((patches, repeats), dim=1)\n \n             batch_size, grid_t, channel = patches.shape[:3]\n             grid_t = grid_t // temporal_patch_size"
        },
        {
            "sha": "90d7dd0abfb9feb76e7cc04c239a9ce075b5aa02",
            "filename": "src/transformers/models/qwen3_vl/video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/700c48a29f03bea03465b30e1240a650f905c2b5/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/700c48a29f03bea03465b30e1240a650f905c2b5/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py?ref=700c48a29f03bea03465b30e1240a650f905c2b5",
            "patch": "@@ -40,8 +40,6 @@ def smart_resize(\n     min_pixels: int = 128 * 128,\n     max_pixels: int = 16 * 16 * 2 * 2 * 2 * 6144,\n ):\n-    if num_frames < temporal_factor:\n-        raise ValueError(f\"t:{num_frames} must be larger than temporal_factor:{temporal_factor}\")\n     if height < factor or width < factor:\n         raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n     elif max(height, width) / min(height, width) > 200:\n@@ -50,7 +48,7 @@ def smart_resize(\n         )\n     h_bar = round(height / factor) * factor\n     w_bar = round(width / factor) * factor\n-    t_bar = round(num_frames / temporal_factor) * temporal_factor\n+    t_bar = math.ceil(num_frames / temporal_factor) * temporal_factor\n \n     if t_bar * h_bar * w_bar > max_pixels:\n         beta = math.sqrt((num_frames * height * width) / max_pixels)\n@@ -232,9 +230,10 @@ def _preprocess(\n             patches = stacked_videos\n \n             # Check that videos have `num_frames` divisible by `temporal_patch_size`\n-            if patches.shape[1] % temporal_patch_size != 0:\n-                repeats = patches[:, -1:].repeat(1, temporal_patch_size - 1, 1, 1, 1)\n-                patches = torch.cat([patches, repeats], dim=1)\n+            T = patches.shape[1]\n+            if pad := -T % temporal_patch_size:\n+                repeats = patches[:, -1:].expand(-1, pad, -1, -1, -1)\n+                patches = torch.cat((patches, repeats), dim=1)\n             batch_size, grid_t, channel = patches.shape[:3]\n             grid_t = grid_t // temporal_patch_size\n             grid_h, grid_w = resized_height // patch_size, resized_width // patch_size"
        },
        {
            "sha": "0ccffca73fa7610ebf7da25ecf73447bc942b3e0",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/700c48a29f03bea03465b30e1240a650f905c2b5/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/700c48a29f03bea03465b30e1240a650f905c2b5/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=700c48a29f03bea03465b30e1240a650f905c2b5",
            "patch": "@@ -343,3 +343,22 @@ def test_call_sample_frames(self):\n \n             # Assign back the actual num frames in tester\n             self.video_processor_tester.num_frames = prev_num_frames\n+\n+    def test_num_frames_equal_temporal_patch_size_plus_two(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processor_dict = self.video_processor_dict.copy()\n+            video_processor_dict[\"size\"] = {\"longest_edge\": 5 * 28 * 28, \"shortest_edge\": 28 * 28}\n+            video_processor_dict[\"do_sample_frames\"] = False\n+            temporal_patch_size = 3\n+            video_processor_dict[\"temporal_patch_size\"] = temporal_patch_size\n+            video_processing = video_processing_class(**video_processor_dict)\n+\n+            n, w, h = 5, 28, 28\n+            video_inputs = [(np.random.randint(0, 256, (h, w, 3), dtype=np.uint8)) for _ in range(n)]\n+\n+            video_processed = video_processing(video_inputs, return_tensors=\"pt\")\n+            encoded_videos = video_processed[self.input_name]\n+            self.assertEqual(list(encoded_videos.shape), [8, temporal_patch_size * 3 * 14 * 14])\n+\n+            video_grid_thw = video_processed[\"video_grid_thw\"]\n+            self.assertEqual(video_grid_thw.tolist(), [[2, 2, 2]])"
        },
        {
            "sha": "d3b9423030c2be7cea70e9e1f1237207347ba4e2",
            "filename": "tests/models/qwen3_vl/test_video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/700c48a29f03bea03465b30e1240a650f905c2b5/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/700c48a29f03bea03465b30e1240a650f905c2b5/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py?ref=700c48a29f03bea03465b30e1240a650f905c2b5",
            "patch": "@@ -328,3 +328,22 @@ def test_call_sample_frames(self):\n                 self.video_processor_tester.min_resolution = prev_min_resolution\n             if prev_max_resolution is not None:\n                 self.video_processor_tester.max_resolution = prev_max_resolution\n+\n+    def test_num_frames_equal_temporal_patch_size_plus_two(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processor_dict = self.video_processor_dict.copy()\n+            video_processor_dict[\"size\"] = {\"longest_edge\": 5 * 32 * 32, \"shortest_edge\": 32 * 32}\n+            video_processor_dict[\"do_sample_frames\"] = False\n+            temporal_patch_size = 3\n+            video_processor_dict[\"temporal_patch_size\"] = temporal_patch_size\n+            video_processing = video_processing_class(**video_processor_dict)\n+\n+            n, w, h = 5, 32, 32\n+            video_inputs = [(np.random.randint(0, 256, (h, w, 3), dtype=np.uint8)) for _ in range(n)]\n+\n+            video_processed = video_processing(video_inputs, return_tensors=\"pt\")\n+            encoded_videos = video_processed[self.input_name]\n+            self.assertEqual(list(encoded_videos.shape), [8, temporal_patch_size * 3 * 16 * 16])\n+\n+            video_grid_thw = video_processed[\"video_grid_thw\"]\n+            self.assertEqual(video_grid_thw.tolist(), [[2, 2, 2]])"
        }
    ],
    "stats": {
        "total": 56,
        "additions": 47,
        "deletions": 9
    }
}