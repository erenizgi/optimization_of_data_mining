{
    "author": "Cyrilvallez",
    "message": "ğŸš¨ Remove BetterTransformer (#41367)\n\nremove",
    "sha": "4903cd40874b53b40c53a2e7c7ba54bee16e811f",
    "files": [
        {
            "sha": "687c63ba27bd8f18e2924005c1c75381def0e8f3",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 168,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -329,174 +329,6 @@ $$ \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\\n Ù„Ù†Ù„Ù‚Ù Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ.\n \n \n-ÙŠØ­ØµÙ„ Ù†Ù…ÙˆØ°Ø¬ OctoCoder Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ù…ÙˆØ¬Ù‡ Ø¥Ø¯Ø®Ø§Ù„ Ø£Ø·ÙˆÙ„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙŠØªØ¶Ù…Ù† Ù…Ø§ ÙŠØ³Ù…Ù‰ *Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…*. ØªÙØ³ØªØ®Ø¯Ù… Ù…ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ù„ØªÙˆØ¬ÙŠÙ‡ LLM Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯ Ø£ÙØ¶Ù„ Ù…ØµÙ…Ù… Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.\n-ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙŠ Ø³ÙŠØ¬Ø¹Ù„ OctoCoder Ù…Ø³Ø§Ø¹Ø¯ ØªØ±Ù…ÙŠØ² Ø£ÙØ¶Ù„.\n-\n-```python\n-system_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\n-The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\n-The assistant is happy to help with code questions and will do their best to understand exactly what is needed.\n-It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\n-That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n-\n-The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\n-The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n------\n-\n-Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n-\n-Answer: Sure. Here is a function that does that.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(len(list1)):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   return results\n-\n-Question: Can you write some test cases for this function?\n-\n-Answer: Sure, here are some tests.\n-\n-assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\n-assert alternating([True, False], [4, 5]) == [True, 4, False, 5]\n-assert alternating([], []) == []\n-\n-Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n-\n-Answer: Here is the modified function.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(min(len(list1), len(list2))):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   if len(list1) > len(list2):\n-       results.extend(list1[i+1:])\n-   else:\n-       results.extend(list2[i+1:])\n-   return results\n------\n-\"\"\"\n-```\n-Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ØŒ Ø³Ù†ÙƒØ±Ø± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ø´Ø± Ù…Ø±Ø§Øª Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ Ù„Ù…Ù„Ø§Ø­Ø¸Ø© ÙˆÙÙˆØ±Ø§Øª Ø°Ø§ÙƒØ±Ø© Flash Attention.\n-Ù†Ø¶ÙŠÙ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ \"Ø³Ø¤Ø§Ù„: ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© ÙˆØ¸ÙŠÙØ© ÙÙŠ Python ØªÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª Ø¥Ù„Ù‰ Ø¬ÙŠØ¬Ø§ Ø¨Ø§ÙŠØª.\n-\n-```python\n-long_prompt = 10 * system_prompt + prompt\n-```\n-\n-Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¯Ù‚Ø© bfloat16.\n-\n-```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n-tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n-\n-pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n-```\n-\n-Ø¯Ø¹Ù†Ø§ Ø§Ù„Ø¢Ù† Ù†Ù‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø«Ù„Ù…Ø§ ÙƒØ§Ù† Ù…Ù† Ù‚Ø¨Ù„ *Ø¨Ø¯ÙˆÙ† Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´ÙŠ* ÙˆÙ‚ÙŠØ§Ø³ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø°Ø§ÙƒØ±Ø© GPU ÙˆÙ‚Øª Ø§Ù„Ø°Ø±ÙˆØ© ÙˆÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n-\n-```python\n-import time\n-\n-start_time = time.time()\n-result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬**:\n-```\n-ØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠ 10.96854019165039 Ø«Ø§Ù†ÙŠØ©.\n-Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯. Ø¥Ù„ÙŠÙƒ ÙˆØ¸ÙŠÙØ© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ.\n-\n-def bytes_to_giga(bytes):\n-return bytes / 1024 / 1024 / 1024\n-\n-Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯. Ø¥Ù„ÙŠÙƒ ÙˆØ¸ÙŠÙØ© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ.\n-\n-Ø¯ÙŠÙ\n-```\n-\n-Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ ÙƒÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ù‚Ø¨Ù„ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø©ØŒ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø¯Ø© Ù…Ø±Ø§Øª Ø­ØªÙ‰ ÙŠØªÙ… Ù‚Ø·Ø¹Ù‡Ø§ Ø¹Ù†Ø¯ 60 Ø±Ù…Ø²Ù‹Ø§. Ù„ÙŠØ³ Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ø¨ Ø£Ù†Ù†Ø§ ÙƒØ±Ø±Ù†Ø§ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ø´Ø± Ù…Ø±Ø§Øª Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ù‚Ù…Ù†Ø§ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙƒØ±Ø§Ø± Ù†ÙØ³Ù‡.\n-\n-**Ù…Ù„Ø§Ø­Ø¸Ø©** Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ ØªÙƒØ±Ø§Ø± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ø´Ø± Ù…Ø±Ø§Øª ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© - Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙƒØ§ÙÙŠØ©!\n-\n-Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙŠØ³ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø°Ø§ÙƒØ±Ø© GPU ÙˆÙ‚Øª Ø§Ù„Ø°Ø±ÙˆØ©.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬**:\n-```\n-37.668193340301514\n-```\n-\n-ÙƒÙ…Ø§ Ù†Ø±Ù‰ØŒ ÙØ¥Ù† Ù…ØªØ·Ù„Ø¨Ø§Øª Ø°Ø§ÙƒØ±Ø© GPU ÙˆÙ‚Øª Ø§Ù„Ø°Ø±ÙˆØ© Ø£Ø¹Ù„Ù‰ Ø¨ÙƒØ«ÙŠØ± Ù…Ù…Ø§ ÙƒØ§Ù†Øª Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ ÙˆÙ‡Ùˆ Ù…Ø§ ÙŠØ±Ø¬Ø¹ Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ± Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£Ø·ÙˆÙ„. Ø£ÙŠØ¶Ù‹Ø§ØŒ ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø£ÙƒØ«Ø± Ù…Ù† Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ù‚Ù„ÙŠÙ„ Ø§Ù„Ø¢Ù†.\n-\n-Ù†Ø³ØªØ¯Ø¹ÙŠ `flush()` Ù„ØªØ­Ø±ÙŠØ± Ø°Ø§ÙƒØ±Ø© GPU Ù„ØªØ¬Ø±Ø¨ØªÙ†Ø§ Ø§Ù„ØªØ§Ù„ÙŠØ©.\n-\n-```python\n-flush()\n-```\n-\n-Ù„Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ù‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ù†ÙØ³ Ø§Ù„Ø¯Ø§Ù„Ø©ØŒ ÙˆÙ„ÙƒÙ† ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´ Ø¨Ø¯Ù„Ø§ Ù…Ù† Ø°Ù„Ùƒ.\n-Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ [BetterTransformer](Https://huggingface.co/docs/optimum/bettertransformer/overview) ÙˆÙ…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ ØªÙ…ÙƒÙŠÙ† PyTorch's [SDPA self-attention](Https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) ÙˆØ§Ù„ØªÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´.\n-\n-```python\n-model.to_bettertransformer()\n-```\n-\n-Ø§Ù„Ø¢Ù† Ù†Ù‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ù†ÙØ³ Ù…Ù‚ØªØ·Ù Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ù‚Ø¨Ù„ ÙˆØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡ Ø³ÙˆÙ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´.\n-\n-```py\n-start_time = time.time()\n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n-    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬**:\n-```\n-ØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠ 3.0211617946624756 Ø«Ø§Ù†ÙŠØ©.\n-Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯. Ø¥Ù„ÙŠÙƒ ÙˆØ¸ÙŠÙØ© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ.\n-\n-def bytes_to_giga(bytes):\n-return bytes / 1024 / 1024 / 1024\n-\n-Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯. Ø¥Ù„ÙŠÙƒ ÙˆØ¸ÙŠÙØ© Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ.\n-\n-Ø¯ÙŠÙ\n-```\n-\n-Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ù‚Ø¨Ù„ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ù„Ø§Ø­Ø¸Ø© ØªØ³Ø±ÙŠØ¹ ÙƒØ¨ÙŠØ± Ø¨ÙØ¶Ù„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´.\n-\n-Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙŠØ³ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ø¢Ø®Ø± Ù…Ø±Ø©.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬**:\n-```\n-32.617331981658936\n-```\n-\n-ÙˆÙ†Ø­Ù† ØªÙ‚Ø±ÙŠØ¨Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø°Ø±ÙˆØ© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ø¯ÙŠÙ†Ø§ 29GB.\n-\n-ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø­ÙˆØ§Ù„ÙŠ 100 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø°Ø§ÙƒØ±Ø© GPU Ø¹Ù†Ø¯ ØªÙ…Ø±ÙŠØ± ØªØ³Ù„Ø³Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ù‹Ø§ Ù…Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ„Ø§Ø´ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ØªÙ…Ø±ÙŠØ± ØªØ³Ù„Ø³Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ù‚ØµÙŠØ± ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©.\n-\n-```py\n-flush()\n-```\n-\n-Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Flash AttentionØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ [ØµÙØ­Ø© doc Ù‡Ø°Ù‡](Https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2).\n-\n ## 3. Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©\n \n Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†ØŒ Ù†Ø¸Ø±Ù†Ø§ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù† Ø®Ù„Ø§Ù„:"
        },
        {
            "sha": "8abd152ba6c9d8b60c68b5cf5d7790d92fc474bf",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 163,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -338,169 +338,6 @@ Essentially, Flash Attention makes sure that all intermediate write and read ope\n \n In practice, there is currently absolutely no reason to **not** use Flash Attention if available. The algorithm gives mathematically the same outputs, and is both faster and more memory-efficient.\n \n-Let's look at a practical example.\n-\n-Our OctoCoder model now gets a significantly longer input prompt which includes a so-called *system prompt*. System prompts are used to steer the LLM into a better assistant that is tailored to the users' task.\n-In the following, we use a system prompt that will make OctoCoder a better coding assistant.\n-\n-```python\n-system_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\n-The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\n-The assistant is happy to help with code questions and will do their best to understand exactly what is needed.\n-It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\n-That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n-\n-The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\n-The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n-\n------\n-\n-Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n-\n-Answer: Sure. Here is a function that does that.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(len(list1)):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   return results\n-\n-Question: Can you write some test cases for this function?\n-\n-Answer: Sure, here are some tests.\n-\n-assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\n-assert alternating([True, False], [4, 5]) == [True, 4, False, 5]\n-assert alternating([], []) == []\n-\n-Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n-\n-Answer: Here is the modified function.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(min(len(list1), len(list2))):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   if len(list1) > len(list2):\n-       results.extend(list1[i+1:])\n-   else:\n-       results.extend(list2[i+1:])\n-   return results\n-\n------\n-\"\"\"\n-```\n-\n-For demonstration purposes, we duplicate the system prompt by ten so that the input length is long enough to observe Flash Attention's memory savings.\n-We append the original text prompt `\"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"`\n-\n-```python\n-long_prompt = 10 * system_prompt + prompt\n-```\n-\n-We instantiate our model again in bfloat16 precision.\n-\n-```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n-tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n-\n-pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n-```\n-\n-Let's now run the model just like before *without Flash Attention* and measure the peak GPU memory requirement and inference time.\n-\n-```python\n-import time\n-\n-start_time = time.time()\n-result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**Output**:\n-\n-```text\n-Generated in 10.96854019165039 seconds.\n-Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n-````\n-\n-We're getting the same output as before, however this time, the model repeats the answer multiple times until it's 60 tokens cut-off. This is not surprising as we've repeated the system prompt ten times for demonstration purposes and thus cued the model to repeat itself.\n-\n-**Note** that the system prompt should not be repeated ten times in real-world applications - one time is enough!\n-\n-Let's measure the peak GPU memory requirement.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**Output**:\n-\n-```text\n-37.668193340301514\n-```\n-\n-As we can see the peak GPU memory requirement is now significantly higher than in the beginning, which is largely due to the longer input sequence. Also the generation takes a little over a minute now.\n-\n-We call `flush()` to free GPU memory for our next experiment.\n-\n-```python\n-flush()\n-```\n-\n-For comparison, let's run the same function, but enable Flash Attention instead.\n-To do so, we convert the model to [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) and by doing so enabling PyTorch's [SDPA self-attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) which in turn is able to use Flash Attention.\n-\n-```python\n-model.to_bettertransformer()\n-```\n-\n-Now we run the exact same code snippet as before and under the hood Transformers will make use of Flash Attention.\n-\n-```py\n-start_time = time.time()\n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n-    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**Output**:\n-\n-```text\n-Generated in 3.0211617946624756 seconds.\n- Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n-```\n-\n-We're getting the exact same result as before, but can observe a very significant speed-up thanks to Flash Attention.\n-\n-Let's measure the memory consumption one last time.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**Output**:\n-\n-```text\n-32.617331981658936\n-```\n-\n-And we're almost back to our original 29GB peak GPU memory from the beginning.\n-\n-We can observe that we only use roughly 100MB more GPU memory when passing a very long input sequence with Flash Attention compared to passing a short input sequence as done in the beginning.\n-\n-```py\n-flush()\n-```\n-\n-For more information on how to use Flash Attention, please have a look at [this doc page](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2).\n-\n ## 3. Architectural Innovations\n \n So far we have looked into improving computational and memory efficiency by:"
        },
        {
            "sha": "53adaed38fe054a62a3fe633f69fd5ec647819cf",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 3,
            "deletions": 16,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -62,24 +62,13 @@ model.enable_cpu_offload()\n \n Note that ğŸ¤— Accelerate must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/accelerate/basic_tutorials/install)\n \n-#### Using Better Transformer\n-\n-Better Transformer is an ğŸ¤— Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to ğŸ¤— Better Transformer:\n-\n-```python\n-model =  model.to_bettertransformer()\n-```\n-\n-Note that ğŸ¤— Optimum must be installed before using this feature. [Here's how to install it.](https://huggingface.co/docs/optimum/installation)\n-\n #### Using Flash Attention 2\n \n Flash Attention 2 is an even faster, optimized version of the previous optimization.\n \n ##### Installation\n \n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n-\n+First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n \n ```bash\n@@ -96,19 +85,17 @@ model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16, attn_i\n \n ##### Performance comparison\n \n-The following diagram shows the latency for the native attention implementation (no optimisation) against Better Transformer and Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1. Flash Attention 2 is also consistently faster than Better Transformer, and its performance improves even more as batch sizes increase:\n+The following diagram shows the latency for the native attention implementation (no optimisation) against Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1:\n \n <div style=\"text-align: center\">\n <img src=\"https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/Bark%20Optimization%20Benchmark.png\">\n </div>\n \n To put this into perspective, on an NVIDIA A100 and when generating 400 semantic tokens with a batch size of 16, you can get 17 times the [throughput](https://huggingface.co/blog/optimizing-bark#throughput) and still be 2 seconds faster than generating sentences one by one with the native model implementation. In other words, all the samples will be generated 17 times faster.\n \n-At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Better Transformer, and at batch size 16, 25%.\n-\n #### Combining optimization techniques\n \n-You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or ğŸ¤— Better Transformer) all at once.\n+You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 all at once.\n \n ```python\n from transformers import BarkModel, infer_device"
        },
        {
            "sha": "517a0b78f387e401d861c0b850ac9941a14112a4",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -73,7 +73,7 @@ Flash Attention 2 is an faster, optimized version of the model.\n \n ### Installation\n \n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n+First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n \n Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n "
        },
        {
            "sha": "45dbc0fc1c5985eba5d8a3a4e8e62b98d0a8d119",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -55,7 +55,7 @@ Flash Attention 2 is an faster, optimized version of the model.\n \n ### Installation\n \n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n+First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features).\n \n Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n "
        },
        {
            "sha": "455d18e3f27fe614d8d2a01474e0a8b58845d8cb",
            "filename": "docs/source/en/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_cpu.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -39,25 +39,6 @@ pred = onnx_qa(question, context)\n > [!TIP]\n > Optimum includes an [Intel](https://hf.co/docs/optimum/intel/index) extension that provides additional optimizations such as quantization, pruning, and knowledge distillation for Intel CPUs. This extension also includes tools to convert models to [OpenVINO](https://hf.co/docs/optimum/intel/inference), a toolkit for optimizing and deploying models, for even faster inference.\n \n-### BetterTransformer\n-\n-[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) is a *fastpath* execution of specialized Transformers functions directly on the hardware level such as a CPU. There are two main components of the fastpath execution.\n-\n-- fusing multiple operations into a single kernel for faster and more efficient execution\n-- skipping unnecessary computation of padding tokens with nested tensors\n-\n-> [!WARNING]\n-> BetterTransformer isn't supported for all models. Check this [list](https://hf.co/docs/optimum/bettertransformer/overview#supported-models) to see whether a model supports BetterTransformer.\n-\n-BetterTransformer is available through Optimum with [`~PreTrainedModel.to_bettertransformer`].\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")\n-model = model.to_bettertransformer()\n-```\n-\n ## TorchScript\n \n [TorchScript](https://pytorch.org/docs/stable/jit.html) is an intermediate PyTorch model format that can be run in non-Python environments, like C++, where performance is critical. Train a PyTorch model and convert it to a TorchScript function or module with [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html). This function optimizes the model with just-in-time (JIT) compilation, and compared to the default eager mode, JIT-compiled models offer better inference performance."
        },
        {
            "sha": "e4f28973326d66908dbf26f15b50e8546f953a5c",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -137,34 +137,6 @@ result = pipeline(\"Both the music and visual were astounding, not to mention the\n \n Learn more details about using ORT with Optimum in the [Accelerated inference on NVIDIA GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) and [Accelerated inference on AMD GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus) guides.\n \n-### BetterTransformer\n-\n-[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) is a *fastpath* execution of specialized Transformers functions directly on the hardware level such as a GPU. There are two main components of the fastpath execution.\n-\n-- fusing multiple operations into a single kernel for faster and more efficient execution\n-- skipping unnecessary computation of padding tokens with nested tensors\n-\n-> [!WARNING]\n-> Some BetterTransformer features are being upstreamed to Transformers with default support for native [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA). BetterTransformer has a wider coverage than the Transformers SDPA integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n-\n-BetterTransformer is available through Optimum with [`~PreTrainedModel.to_bettertransformer`].\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")\n-model = model.to_bettertransformer()\n-```\n-\n-Call [`~PreTrainedModel.reverse_bettertransformer`] and save it first to return the model to the original Transformers model.\n-\n-```py\n-model = model.reverse_bettertransformer()\n-model.save_pretrained(\"saved_model\")\n-```\n-\n-Refer to the benchmarks in [Out of the box acceleration and memory savings of ğŸ¤— decoder models with PyTorch 2.0](https://pytorch.org/blog/out-of-the-box-acceleration/) for BetterTransformer and scaled dot product attention performance. The [BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) blog post also discusses fastpath execution in greater detail if you're interested in learning more.\n-\n ## Scaled dot product attention (SDPA)\n \n PyTorch's [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) is a native implementation of the scaled dot product attention mechanism. SDPA is a more efficient and optimized version of the attention mechanism used in transformer models."
        },
        {
            "sha": "ba7591ffe0f7d97c84790710f636e6fe684e3897",
            "filename": "docs/source/it/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_cpu.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -17,10 +17,6 @@ rendered properly in your Markdown viewer.\n \n Questa guida si concentra sull'inferenza di modelli di grandi dimensioni in modo efficiente sulla CPU.\n \n-## `BetterTransformer` per inferenza piÃ¹ rapida\n-\n-Abbiamo integrato di recente `BetterTransformer` per fare inferenza piÃ¹ rapidamente con modelli per testi, immagini e audio. Visualizza la documentazione sull'integrazione [qui](https://huggingface.co/docs/optimum/bettertransformer/overview) per maggiori dettagli.\n-\n ## PyTorch JIT-mode (TorchScript)\n \n TorchScript Ã¨ un modo di creare modelli serializzabili e ottimizzabili da codice PyTorch. Ogni programma TorchScript puÃ² esere salvato da un processo Python  e caricato in un processo dove non ci sono dipendenze Python."
        },
        {
            "sha": "1baa9f630bdca5f170eada01a58e6176725a1582",
            "filename": "docs/source/it/perf_infer_gpu_many.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_gpu_many.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -22,7 +22,3 @@ Questo documento contiene informazioni su come fare inferenza in maniera efficie\n Nota: Un setup con GPU multiple puÃ² utilizzare la maggior parte delle strategie descritte nella [sezione con GPU singola](./perf_infer_gpu_one). Tuttavia, Ã¨ necessario conoscere delle tecniche semplici che possono essere utilizzate per un risultato migliore.\n \n </Tip>\n-\n-## `BetterTransformer` per inferenza piÃ¹ rapida\n-\n-Abbiamo recentemente integrato `BetterTransformer` per inferenza piÃ¹ rapida su multi-GPU per modelli su testo, immagini e audio. Controlla il documento con queste integrazioni [qui](https://huggingface.co/docs/optimum/bettertransformer/overview) per maggiori dettagli."
        },
        {
            "sha": "dd49d6ba4f2f7676ad6c05f96f672e0de96564ef",
            "filename": "docs/source/it/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -17,10 +17,6 @@ rendered properly in your Markdown viewer.\n \n Questo documento sarÃ  presto completato con informazioni su come effetture l'inferenza su una singola GPU. Nel frattempo Ã¨ possibile consultare [la guida per l'addestramento su una singola GPU](perf_train_gpu_one) e [la guida per l'inferenza su CPU](perf_infer_cpu).\n \n-## `BetterTransformer` per l'inferenza piÃ¹ veloce\n-\n-Abbiamo recentemente integrato `BetterTransformer` per velocizzare l'inferenza su GPU per modelli di testo, immagini e audio. Per maggiori dettagli, consultare la documentazione su questa integrazione [qui](https://huggingface.co/docs/optimum/bettertransformer/overview).\n-\n ## Integrazione di `bitsandbytes` per Int8 mixed-precision matrix decomposition\n \n <Tip>"
        },
        {
            "sha": "99467f2321bbfe1366aa3c61a95ba6f3d428670f",
            "filename": "docs/source/ja/model_doc/bark.md",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -42,16 +42,6 @@ device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n ```\n \n-#### Using ğŸ¤— Better Transformer\n-\n-Better Transformer ã¯ã€å†…éƒ¨ã§ã‚«ãƒ¼ãƒãƒ«èåˆã‚’å®Ÿè¡Œã™ã‚‹ ğŸ¤— æœ€é©ãªæ©Ÿèƒ½ã§ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãªãã€é€Ÿåº¦ã‚’ 20% ï½ 30% å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Better Transformer ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã®ã«å¿…è¦ãªã‚³ãƒ¼ãƒ‰ã¯ 1 è¡Œã ã‘ã§ã™ã€‚\n-\n-```python\n-model =  model.to_bettertransformer()\n-```\n-\n-ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã« ğŸ¤— Optimum ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ã“ã¡ã‚‰](https://huggingface.co/docs/optimum/installation)\n-\n #### Using CPU offload\n \n å‰è¿°ã—ãŸã‚ˆã†ã«ã€Bark ã¯ 4 ã¤ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆä¸­ã«é †ç•ªã«å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚è¨€ã„æ›ãˆã‚Œã°ã€1 ã¤ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹é–“ã€ä»–ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã¯ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚\n@@ -64,27 +54,6 @@ model.enable_cpu_offload()\n \n ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ğŸ¤— Accelerate ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ã“ã¡ã‚‰](https://huggingface.co/docs/accelerate/basic_tutorials/install)\n \n-#### Combining optimization techniques\n-\n-æœ€é©åŒ–æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã€åŠç²¾åº¦ã€ğŸ¤— Better Transformer ã‚’ã™ã¹ã¦ä¸€åº¦ã«ä½¿ç”¨ã§ãã¾ã™ã€‚\n-\n-```python\n-from transformers import BarkModel\n-import torch\n-\n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n-# load in fp16\n-model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n-\n-# convert to bettertransformer\n-model = BetterTransformer.transform(model, keep_original_model=False)\n-\n-# enable CPU offload\n-model.enable_cpu_offload()\n-```\n-\n-æ¨è«–æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](https://huggingface.co/docs/transformers/perf_infer_gpu_one) ã‚’ã”è¦§ãã ã•ã„ã€‚\n \n ### Tips\n "
        },
        {
            "sha": "114bc901b8d1ca94628e0a90caec982813e77711",
            "filename": "docs/source/ja/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_cpu.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -18,10 +18,6 @@ rendered properly in your Markdown viewer.\n \n ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€CPUä¸Šã§å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªæ¨è«–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n \n-## `BetterTransformer` for faster inference\n-\n-æœ€è¿‘ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ãŠã‚ˆã³éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®CPUä¸Šã§ã®é«˜é€Ÿãªæ¨è«–ã®ãŸã‚ã«`BetterTransformer`ã‚’çµ±åˆã—ã¾ã—ãŸã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã®çµ±åˆã«é–¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’[ã“ã¡ã‚‰](https://huggingface.co/docs/optimum/bettertransformer/overview)ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n ## PyTorch JITãƒ¢ãƒ¼ãƒ‰ï¼ˆTorchScriptï¼‰\n TorchScriptã¯ã€PyTorchã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã§æœ€é©åŒ–å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚ä»»æ„ã®TorchScriptãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€Pythonä¾å­˜æ€§ã®ãªã„ãƒ—ãƒ­ã‚»ã‚¹ã§ä¿å­˜ãŠã‚ˆã³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¤ãƒ¼ã‚¬ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã¨æ¯”è¼ƒã—ã¦ã€PyTorchã®jitãƒ¢ãƒ¼ãƒ‰ã¯é€šå¸¸ã€ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ãªã©ã®æœ€é©åŒ–æ‰‹æ³•ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«æ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚"
        },
        {
            "sha": "839dadf97113c8059343fee9343a8d29a69f2344",
            "filename": "docs/source/ja/perf_infer_gpu_many.md",
            "status": "modified",
            "additions": 0,
            "deletions": 100,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -25,103 +25,3 @@ rendered properly in your Markdown viewer.\n ## Flash Attention 2\n \n Flash Attention 2ã®çµ±åˆã¯ã€è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](./perf_infer_gpu_one#Flash-Attention-2)ã®é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-## BetterTransformer\n-\n-[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿå®Ÿè¡Œãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€ãã®ä¸‹ã§Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚\n-\n-BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€GPUãŠã‚ˆã³è¤‡æ•°GPUã§ã®é«˜é€Ÿæ¨è«–ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n-<Tip>\n-\n-Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16 dtypeã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-### Decoder models\n-\n-ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã®å ´åˆã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’[`torch.nn.functional.scaled_dot_product_attention`ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã¯PyTorch 2.0ä»¥é™ã§ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ï¼š\n-\n-```python\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n-# convert the model to BetterTransformer\n-model.to_bettertransformer()\n-\n-# Use it for training or inference\n-```\n-\n-SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºãªã©ã®ç‰¹å®šã®è¨­å®šã§[Flash Attention](https://huggingface.co/papers/2205.14135)ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã®ã‚µã‚¤ã‚ºï¼‰ã§åˆ©ç”¨å¯èƒ½ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[`torch.nn.kernel.sdpa_kernel`](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html)ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-\n-```diff\n-import torch\n-+ from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(\"cuda\")\n-# convert the model to BetterTransformer\n-model.to_bettertransformer()\n-\n-input_text = \"Hello my dog is cute and\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n-\n-+ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-```\n-\n-ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã§æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆï¼š\n-\n-\n-```bash\n-RuntimeError: No available kernel.  Aborting execution.\n-```\n-\n-å½“æ—¥ã€Flash Attentionã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒåºƒç¯„å›²ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹PyTorch Nightlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã™ã‚ˆã†ã«ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-```bash\n-pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n-```\n-\n-[ã“ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://pytorch.org/blog/out-of-the-box-acceleration/)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€BetterTransformer + SDPA APIã§å¯èƒ½ãªã“ã¨ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã—ã‚‡ã†ã€‚\n-\n-### Encoder Models\n-\n-æ¨è«–ä¸­ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€BetterTransformerã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®forwardå‘¼ã³å‡ºã—ã‚’ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®[`torch.nn.TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)ã®ç›¸å½“ã™ã‚‹ã‚‚ã®ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é«˜é€Ÿå®Ÿè£…ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚\n-\n-`torch.nn.TransformerEncoderLayer`ã®é«˜é€Ÿå®Ÿè£…ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ä»£ã‚ã‚Šã«`torch.nn.functional.scaled_dot_product_attention`ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’æ´»ç”¨ã—ãªã„Flash Attentionã¾ãŸã¯Memory-Efficient Attentionã®èåˆã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n-\n-BetterTransformerã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã‚’ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚ã¾ãŸã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ç”¨ã®BetterTransformerã«ã¤ã„ã¦ã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)ã§è©³ã—ãå­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-\n-## Advanced usage: mixing FP4 (or Int8) and BetterTransformer\n-\n-ãƒ¢ãƒ‡ãƒ«ã®æœ€è‰¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã§èª¬æ˜ã—ãŸç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–+Flash Attentionã‚’ä½¿ç”¨ã—ãŸBetterTransformerã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-\n-```py\n-import torch\n-from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n-\n-quantization_config = BitsAndBytesConfig(\n-    load_in_4bit=True,\n-    bnb_4bit_compute_dtype=torch.float16\n-)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=quantization_config)\n-\n-input_text = \"Hello my dog is cute and\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n-\n-with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "762e8f50bce8649d4fc4b333d9672e977b01e0fa",
            "filename": "docs/source/ja/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -19,12 +19,6 @@ rendered properly in your Markdown viewer.\n \n ## Flash Attention 2\n \n-<Tip>\n-\n-ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ã§ã‚ã‚Šã€å°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å¤§å¹…ã«å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€Flash Attention 2 APIã¯è¿‘ã„å°†æ¥`BetterTransformer` APIã«ç§»è¡Œã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n-\n-</Tip>\n-\n Flash Attention 2ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–é€Ÿåº¦ã‚’å¤§å¹…ã«é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚Flash Attention 2ã¯ã€Tri Daoæ°ã«ã‚ˆã£ã¦[å…¬å¼ã®Flash Attentionãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Dao-AILab/flash-attention)ã§å°å…¥ã•ã‚Œã¾ã—ãŸã€‚Flash Attentionã«é–¢ã™ã‚‹ç§‘å­¦è«–æ–‡ã¯[ã“ã¡ã‚‰](https://huggingface.co/papers/2205.14135)ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n \n Flash Attention 2ã‚’æ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰ã«å¾“ã£ã¦ãã ã•ã„ã€‚\n@@ -34,8 +28,6 @@ Flash Attention 2ã‚’æ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€ä¸Šè¨˜ã®ãƒªãƒã‚¸\n - Llama\n - Falcon\n \n-ã•ã‚‰ã«å¤šãã®ãƒ¢ãƒ‡ãƒ«ã«Flash Attention 2ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’GitHubã§ææ¡ˆã™ã‚‹ã“ã¨ã‚‚ã§ãã€å¤‰æ›´ã‚’çµ±åˆã™ã‚‹ãŸã‚ã«ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ãã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å«ã‚€ã€æ¨è«–ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã§ãã¾ã™ï¼ˆç¾åœ¨ã®`BetterTransformer` APIã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ï¼‰ã€‚\n-\n <Tip>\n \n Flash Attention 2ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®dtypeãŒ`fp16`ã¾ãŸã¯`bf16`ã®å ´åˆã«ã®ã¿ä½¿ç”¨ã§ãã€NVIDIA-GPUãƒ‡ãƒã‚¤ã‚¹ã§ã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n@@ -164,98 +156,6 @@ model.add_adapter(lora_config)\n ... # train your model\n ```\n \n-## BetterTransformer\n-\n-[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿãƒ‘ã‚¹å®Ÿè¡Œã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ãŒå†…éƒ¨ã§å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚\n-\n-BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ãŠã‚ˆã³ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ãŠã‚ˆã³ãƒãƒ«ãƒGPUã§ã®é«˜é€Ÿãªæ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n-\n-<Tip>\n-\n-Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16ã®dtypeã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-### Encoder models\n-\n-PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®[`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜é€Ÿãƒ‘ã‚¹ã€BetterTransformerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã¯ã€[ğŸ¤— Optimumãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://huggingface.co/docs/optimum/bettertransformer/overview)ã®çµ±åˆã‚’é€šã˜ã¦Transformersã¨ä¸€ç·’ã«ä½¿ç”¨ã§ãã¾ã™ã€‚\n-\n-PyTorchã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜é€Ÿãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚«ãƒ¼ãƒãƒ«ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ã¨[ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«](https://pytorch.org/docs/stable/nested.html)ã®ä½¿ç”¨ã«ã‚ˆã‚Šã€æ¨è«–ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚è©³ç´°ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æƒ…å ±ã¯[ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã«ã‚ã‚Šã¾ã™ã€‚\n-\n-[`optimum`](https://github.com/huggingface/optimum)ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€æ¨è«–ä¸­ã«Better Transformerã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€é–¢é€£ã™ã‚‹å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™[`~PreTrainedModel.to_bettertransformer`]:\n-\n-\n-```python\n-model = model.to_bettertransformer()\n-```\n-\n-ãƒ¡ã‚½ãƒƒãƒ‰ [`~PreTrainedModel.reverse_bettertransformer`] ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹å‰ã«ä½¿ç”¨ã™ã¹ãã§ã€æ¨™æº–ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ï¼š\n-\n-```python\n-model = model.reverse_bettertransformer()\n-model.save_pretrained(\"saved_model\")\n-```\n-\n-BetterTransformer APIã‚’ä½¿ã£ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å¯èƒ½æ€§ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚‹ã«ã¯ã€[ã“ã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-### Decoder models\n-\n-ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã«ã¨ã£ã¦ã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’[`torch.nn.functional.scaled_dot_product_attention`ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯PyTorch 2.0ä»¥é™ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n-\n-```python\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n-# convert the model to BetterTransformer\n-model.to_bettertransformer()\n-\n-# Use it for training or inference\n-```\n-\n-SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦[Flash Attention](https://huggingface.co/papers/2205.14135)ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã‚µã‚¤ã‚ºï¼‰ã§ä½¿ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[`torch.nn.attention.sdpa_kernel`](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html)ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-\n-```diff\n-import torch\n-+ from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", dtype=torch.float16).to(\"cuda\")\n-# convert the model to BetterTransformer\n-model.to_bettertransformer()\n-\n-input_text = \"Hello my dog is cute and\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n-\n-+ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-```\n-\n-ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã«ãƒã‚°ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆ\n-\n-```bash\n-RuntimeError: No available kernel.  Aborting execution.\n-```\n-\n-Flash Attention ã®åºƒç¯„ãªã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æŒã¤ã‹ã‚‚ã—ã‚Œãªã„ PyTorch ã®ãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-```bash\n-pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n-```\n-\n-Or make sure your model is correctly casted in float16 or bfloat16\n-\n-ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãfloat16ã¾ãŸã¯bfloat16ã«ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-Have a look at [this detailed blogpost](https://pytorch.org/blog/out-of-the-box-acceleration/) to read more about what is possible to do with `BetterTransformer` + SDPA API.\n-\n-`BetterTransformer` + SDPA APIã‚’ä½¿ç”¨ã—ã¦ä½•ãŒå¯èƒ½ã‹ã«ã¤ã„ã¦è©³ã—ãèª­ã‚€ã«ã¯ã€[ã“ã®è©³ç´°ãªãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://pytorch.org/blog/out-of-the-box-acceleration/)ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n ## `bitsandbytes` integration for FP4 mixed-precision inference\n \n FP4æ··åˆç²¾åº¦æ¨è«–ã®ãŸã‚ã®`bitsandbytes`çµ±åˆ\n@@ -415,29 +315,3 @@ In this example, the first GPU will use 1GB of memory and the second 2GB.\n \n [![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)\n \n-## Advanced usage: mixing FP4 (or Int8) and BetterTransformer\n-\n-ç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€BetterTransformerã‚’ä½¿ç”¨ã—ã¦FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–ã¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-\n-```py\n-import torch\n-from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n-\n-quantization_config = BitsAndBytesConfig(\n-    load_in_4bit=True,\n-    bnb_4bit_compute_dtype=torch.float16\n-)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=quantization_config)\n-\n-input_text = \"Hello my dog is cute and\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n-\n-with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "46d99cd613282ee42d5a3f1ce967161ba0b12dd2",
            "filename": "docs/source/ja/perf_train_gpu_one.md",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_train_gpu_one.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -57,7 +57,6 @@ rendered properly in your Markdown viewer.\n ã“ã‚Œã‚‰ã®æ–¹æ³•ãŒååˆ†ãªåˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã•ãªã„å ´åˆã€ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¤œè¨ã§ãã¾ã™ï¼š\n * [åŠ¹ç‡çš„ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã‚’å‚™ãˆãŸã‚«ã‚¹ã‚¿ãƒ Dockerã‚³ãƒ³ãƒ†ãƒŠã®ä½œæˆ](#efficient-software-prebuilds)\n * [Mixture of Expertsï¼ˆMoEï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨](#mixture-of-experts)\n-* [ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã—ã¦ã€PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ´»ç”¨](#using-pytorch-native-attention)\n \n æœ€å¾Œã«ã€ã“ã‚Œã‚‰ã®æ–¹æ³•ãŒã¾ã ååˆ†ã§ãªã„å ´åˆã€A100ãªã©ã®ã‚µãƒ¼ãƒãƒ¼ã‚°ãƒ¬ãƒ¼ãƒ‰GPUã«åˆ‡ã‚Šæ›¿ãˆã¦ã‚‚ã€ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒãƒ«ãƒGPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚æœ‰åŠ¹ã§ã‚ã‚Šã€[ãƒãƒ«ãƒGPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](perf_train_gpu_many)ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹è¿½åŠ ã®ä¸¦åˆ—åŒ–æŠ€è¡“ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚\n \n@@ -412,26 +411,3 @@ PyTorchã®[pipã¨condaãƒ“ãƒ«ãƒ‰](https://pytorch.org/get-started/locally/#start-\n \n Pytorchã«ã¯DeepSpeedãŒæ§‹ç¯‰ã—ãŸã‚‚ã®ã‚‚ã‚ã‚Šã¾ã™: [DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://huggingface.co/papers/2201.05596)ã€[Mixture of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - ãƒ–ãƒ­ã‚°è¨˜äº‹: [1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)ã€[2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/)ã€å¤§è¦æ¨¡ãªTransformerãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶è¨€èªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…·ä½“çš„ãªå±•é–‹ã«ã¤ã„ã¦ã¯ã€[ãƒ–ãƒ­ã‚°è¨˜äº‹](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html)ã€[Megatron-Deepspeedãƒ–ãƒ©ãƒ³ãƒ](https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n \n-\n-## PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨Flash Attentionã®ä½¿ç”¨\n-\n-PyTorch 2.0ã§ã¯ã€ãƒã‚¤ãƒ†ã‚£ãƒ–ã®[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)ï¼ˆSDPAï¼‰ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã€[ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®é«˜ã„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³](https://huggingface.co/papers/2112.05682)ã‚„[ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³](https://huggingface.co/papers/2205.14135)ãªã©ã®èåˆã•ã‚ŒãŸGPUã‚«ãƒ¼ãƒãƒ«ã®ä½¿ç”¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚\n-\n-[`optimum`](https://github.com/huggingface/optimum)ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€é–¢é€£ã™ã‚‹å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç½®ãæ›ãˆã¦ã€PyTorchã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ï¼š\n-\n-\n-```python\n-model = model.to_bettertransformer()\n-```\n-\n-å¤‰æ›å¾Œã€é€šå¸¸é€šã‚Šãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚\n-\n-<Tip warning={true}>\n-\n-PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®`scaled_dot_product_attention`æ¼”ç®—å­ã¯ã€`attention_mask`ãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã«ã®ã¿Flash Attentionã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§ãã¾ã™ã€‚\n-\n-ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§BetterTransformerçµ±åˆã¯ãƒã‚¹ã‚¯ã‚µãƒãƒ¼ãƒˆã‚’å‰Šé™¤ã—ã€ãƒãƒƒãƒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ãŒå¿…è¦ãªã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã—ã‹ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã€ä¾‹ãˆã°ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªã€ãƒãƒƒãƒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ãŒä¸è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆã«è©²å½“ã—ã¾ã™ã€‚BetterTransformerã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚\n-\n-</Tip>\n-\n-SDPAã‚’ä½¿ç”¨ã—ãŸã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ¡ãƒ¢ãƒªã®ç¯€ç´„ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°è¨˜äº‹](https://pytorch.org/blog/out-of-the-box-acceleration/)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "sha": "c4a396604ae8036a0c63ce545205e77f476a35ba",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 154,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -320,160 +320,6 @@ $$ \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\\n \n ì‹¤ì œ ì˜ˆë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n \n-ìš°ë¦¬ì˜ OctoCoder ëª¨ë¸ì€ ì´ì œ *ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸*ê°€ í¬í•¨ëœ í›¨ì”¬ ë” ê¸´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©ìì˜ ì‘ì—…ì— ë§ì¶˜ ë” ë‚˜ì€ ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ ìœ ë„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì œì—ì„œëŠ” OctoCoderë¥¼ ë” ë‚˜ì€ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n-\n-```python\n-system_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\n-The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\n-The assistant is happy to help with code questions and will do their best to understand exactly what is needed.\n-It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\n-That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n-\n-The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\n-The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n-\n------\n-\n-Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n-\n-Answer: Sure. Here is a function that does that.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(len(list1)):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   return results\n-\n-Question: Can you write some test cases for this function?\n-\n-Answer: Sure, here are some tests.\n-\n-assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\n-assert alternating([True, False], [4, 5]) == [True, 4, False, 5]\n-assert alternating([], []) == []\n-\n-Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n-\n-Answer: Here is the modified function.\n-\n-def alternating(list1, list2):\n-   results = []\n-   for i in range(min(len(list1), len(list2))):\n-       results.append(list1[i])\n-       results.append(list2[i])\n-   if len(list1) > len(list2):\n-       results.extend(list1[i+1:])\n-   else:\n-       results.extend(list2[i+1:])\n-   return results\n-\n------\n-\"\"\"\n-```\n-ì‹œì—°ì„ ìœ„í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ 10ë²ˆ ì¤‘ë³µí•˜ì—¬ ì¦ê°€ì‹œì¼œ í”Œë˜ì‹œ ì–´í…ì…˜ì˜ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ë¥¼ ê´€ì°°í•  ìˆ˜ ìˆì„ ë§Œí¼ ì…ë ¥ ê¸¸ì´ë¥¼ ì¶©ë¶„íˆ ê¸¸ê²Œ ë§Œë“­ë‹ˆë‹¤. ì›ë˜ì˜ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ê°€í•©ë‹ˆë‹¤. `\"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"`\n-\n-```python\n-long_prompt = 10 * system_prompt + prompt\n-```\n-\n-ëª¨ë¸ì„ ë‹¤ì‹œ bfloat16 ì •ë°€ë„ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n-\n-```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n-tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n-\n-pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n-```\n-\n-ì´ì œ í”Œë˜ì‹œ ì–´í…ì…˜ì„ *ì‚¬ìš©í•˜ì§€ ì•Šê³ * ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ê³¼ ì¶”ë¡  ì‹œê°„ì„ ì¸¡ì •í•´ ë´…ì‹œë‹¤.\n-\n-```python\n-import time\n-\n-start_time = time.time()\n-result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**ì¶œë ¥**:\n-```\n-Generated in 10.96854019165039 seconds.\n-Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n-````\n-\n-ì´ì „ê³¼ ë™ì¼í•œ ì¶œë ¥ì„ ì–»ê³  ìˆì§€ë§Œ, ì´ë²ˆì—ëŠ” ëª¨ë¸ì´ ë‹µë³€ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ 60ê°œì˜ í† í°ì´ ì˜ë¦´ ë•Œê¹Œì§€ ê³„ì†ë©ë‹ˆë‹¤. ì‹œì—°ì„ ìœ„í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ 10ë²ˆ ë°˜ë³µí–ˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ë°˜ë³µí•˜ë„ë¡ ìœ ë„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ëŠ” ë†€ë¼ìš´ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.\n-\n-**ì°¸ê³ ** ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ 10ë²ˆ ë°˜ë³µí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤!\n-\n-ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì„ ì¸¡ì •í•´ ë´…ì‹œë‹¤.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**ì¶œë ¥**:\n-```bash\n-37.668193340301514\n-```\n-\n-ë³´ì‹œë‹¤ì‹œí”¼ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ ì²˜ìŒë³´ë‹¤ ìƒë‹¹íˆ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ë¡œ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì¡Œê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë˜í•œ ìƒì„± ì‹œê°„ì´ ì´ì œ 1ë¶„ì„ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n-\n-ë‹¤ìŒ ì‹¤í—˜ì„ ìœ„í•´ `flush()`ë¥¼ í˜¸ì¶œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n-\n-```python\n-flush()\n-```\n-\n-ë¹„êµë¥¼ ìœ„í•´, ë™ì¼í•œ ê¸°ëŠ¥ì„ ì‹¤í–‰í•˜ë˜ í”Œë˜ì‹œ ì–´í…ì…˜ì„ í™œì„±í™”í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n-ì´ë¥¼ ìœ„í•´ ëª¨ë¸ì„ [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ í†µí•´ PyTorchì˜ [SDPA self-attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ì„ í™œì„±í™”í•˜ë©´ í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-```python\n-model.to_bettertransformer()\n-```\n-\n-ì´ì œ ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì‹¤í–‰í•˜ë©´, ë‚´ë¶€ì ìœ¼ë¡œ Transformersê°€ í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n-\n-```py\n-start_time = time.time()\n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n-    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n-\n-print(f\"Generated in {time.time() - start_time} seconds.\")\n-result\n-```\n-\n-**ì¶œë ¥**:\n-```\n-Generated in 3.0211617946624756 seconds.\n- Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n-```\n-\n-ì´ì „ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆì§€ë§Œ, í”Œë˜ì‹œ ì–´í…ì…˜ ë•ë¶„ì— ë§¤ìš° í° ì†ë„ í–¥ìƒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ë©”ëª¨ë¦¬ ì†Œë¹„ëŸ‰ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ì¸¡ì •í•´ ë´…ì‹œë‹¤.\n-\n-```python\n-bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n-```\n-\n-**ì¶œë ¥**:\n-```\n-32.617331981658936\n-```\n-\n-ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ì²˜ìŒì— ë³´ì•˜ë˜ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì¸ 29GBë¡œ ëŒì•„ì™”ìŠµë‹ˆë‹¤.\n-\n-í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê¸´ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í•  ë•Œ ì²˜ìŒì— ì§§ì€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í–ˆì„ ë•Œì™€ ë¹„êµí•˜ì—¬ ì•½ 100MB ì •ë„ì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ë” ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-```py\n-flush()\n-```\n-\n-í”Œë˜ì‹œ ì–´í…ì…˜ ì‚¬ìš©ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” [ì´ ë¬¸ì„œ í˜ì´ì§€](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2)ë¥¼ ì°¸ì¡°í•´ ì£¼ì„¸ìš”.\n-\n ## 3. ì•„í‚¤í…ì²˜ í˜ì‹  [[3-architectural-innovations]]\n \n ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ê³„ì‚° ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤:"
        },
        {
            "sha": "8415207f4e0f504d71ab2ead8071ae4a283dd830",
            "filename": "docs/source/ko/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_cpu.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -17,10 +17,6 @@ rendered properly in your Markdown viewer.\n \n ì´ ê°€ì´ë“œëŠ” CPUì—ì„œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì— ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.\n \n-## ë” ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•œ `BetterTransformer` [[bettertransformer-for-faster-inference]]\n-\n-ìš°ë¦¬ëŠ” ìµœê·¼ CPUì—ì„œ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë° ì˜¤ë””ì˜¤ ëª¨ë¸ì˜ ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ `BetterTransformer`ë¥¼ í†µí•©í–ˆìŠµë‹ˆë‹¤. ì´ í†µí•©ì— ëŒ€í•œ ë” ìì„¸í•œ ë‚´ìš©ì€ [ì´ ë¬¸ì„œ](https://huggingface.co/docs/optimum/bettertransformer/overview)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n ## PyTorch JIT ëª¨ë“œ (TorchScript) [[pytorch-jitmode-torchscript]]\n TorchScriptëŠ” PyTorch ì½”ë“œì—ì„œ ì§ë ¬í™”ì™€ ìµœì í™”ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìƒì„±í• ë•Œ ì“°ì…ë‹ˆë‹¤. TorchScriptë¡œ ë§Œë“¤ì–´ì§„ í”„ë¡œê·¸ë¨ì€ ê¸°ì¡´ Python í”„ë¡œì„¸ìŠ¤ì—ì„œ ì €ì¥í•œ ë’¤, ì¢…ì†ì„±ì´ ì—†ëŠ” ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. PyTorchì˜ ê¸°ë³¸ ì„¤ì •ì¸ `eager` ëª¨ë“œì™€ ë¹„êµí–ˆì„ë•Œ, `jit` ëª¨ë“œëŠ” ì—°ì‚°ì ê²°í•©ê³¼ ê°™ì€ ìµœì í™” ë°©ë²•ë¡ ì„ í†µí•´ ëª¨ë¸ ì¶”ë¡ ì—ì„œ ëŒ€ë¶€ë¶„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n "
        },
        {
            "sha": "b3431838d7601296ff964e821906376827a77c63",
            "filename": "docs/source/ko/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -17,27 +17,6 @@ rendered properly in your Markdown viewer.\n \n ì´ ê°€ì´ë“œ ì™¸ì—ë„, [ë‹¨ì¼ GPUì—ì„œì˜ í›ˆë ¨ ê°€ì´ë“œ](perf_train_gpu_one)ì™€ [CPUì—ì„œì˜ ì¶”ë¡  ê°€ì´ë“œ](perf_infer_cpu)ì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n-## Better Transformer: PyTorch ë„¤ì´í‹°ë¸Œ Transformer íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ [[better-transformer-pytorchnative-transformer-fastpath]]\n-\n-PyTorch ë„¤ì´í‹°ë¸Œ [`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ì¸ BetterTransformerëŠ” [ğŸ¤— Optimum ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/docs/optimum/bettertransformer/overview)ì˜ í†µí•©ì„ í†µí•´ Transformersì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-PyTorchì˜ ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ëŠ” ì»¤ë„ í“¨ì „ê³¼ [ì¤‘ì²©ëœ í…ì„œ](https://pytorch.org/docs/stable/nested.html)ì˜ ì‚¬ìš©ì„ í†µí•´ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë²¤ì¹˜ë§ˆí¬ëŠ” [ì´ ë¸”ë¡œê·¸ ê¸€](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-[`optimum`](https://github.com/huggingface/optimum) íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ í›„ì—ëŠ” ì¶”ë¡  ì¤‘ Better Transformerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [`~PreTrainedModel.to_bettertransformer`]ë¥¼ í˜¸ì¶œí•˜ì—¬ ê´€ë ¨ ë‚´ë¶€ ëª¨ë“ˆì„ ëŒ€ì²´í•©ë‹ˆë‹¤:\n-\n-```python\n-model = model.to_bettertransformer()\n-```\n-\n-[`~PreTrainedModel.reverse_bettertransformer`] ë©”ì†Œë“œëŠ” ì •ê·œí™”ëœ transformers ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ì „ ì›ë˜ì˜ ëª¨ë¸ë§ìœ¼ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤:\n-\n-```python\n-model = model.reverse_bettertransformer()\n-model.save_pretrained(\"saved_model\")\n-```\n-\n-PyTorch 2.0ë¶€í„°ëŠ” ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ê°€ ì¸ì½”ë”ì™€ ë””ì½”ë” ëª¨ë‘ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ì•„í‚¤í…ì²˜ ëª©ë¡ì€ [ì—¬ê¸°](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n ## FP4 í˜¼í•© ì •ë°€ë„ ì¶”ë¡ ì„ ìœ„í•œ `bitsandbytes` í†µí•© [[bitsandbytes-integration-for-fp4-mixedprecision-inference]]\n \n `bitsandbytes`ë¥¼ ì„¤ì¹˜í•˜ë©´ GPUì—ì„œ ì†ì‰½ê²Œ ëª¨ë¸ì„ ì••ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. FP4 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ì›ë˜ì˜ ì „ì²´ ì •ë°€ë„ ë²„ì „ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ìµœëŒ€ 8ë°° ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”."
        },
        {
            "sha": "a9742d9bd7042cb5509ed948e1487ee187d4198a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 83,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/4903cd40874b53b40c53a2e7c7ba54bee16e811f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4903cd40874b53b40c53a2e7c7ba54bee16e811f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4903cd40874b53b40c53a2e7c7ba54bee16e811f",
            "patch": "@@ -97,7 +97,6 @@\n     is_flash_attn_3_available,\n     is_kernels_available,\n     is_offline_mode,\n-    is_optimum_available,\n     is_peft_available,\n     is_remote_url,\n     is_torch_flex_attn_available,\n@@ -2269,9 +2268,9 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n         Args:\n             is_init_check (`bool`, *optional*):\n                 Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n-                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n-                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n-                before instantiating the full models if we know that the model does not support the requested attention.\n+                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n+                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n+                if we know that the model does not support the requested attention.\n         \"\"\"\n         dtype = self.config.dtype\n \n@@ -2329,11 +2328,6 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n \n         # With the early check, the parameters are not yet initialized correctly\n         if not is_init_check:\n-            if getattr(self, \"use_bettertransformer\", False):\n-                raise ValueError(\n-                    \"Flash Attention 2 and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n-                )\n-\n             param_devices = list({param.device for param in self.parameters()})\n             if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n                 if torch.cuda.is_available():\n@@ -2363,9 +2357,9 @@ def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n         Args:\n             is_init_check (`bool`, *optional*):\n                 Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n-                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n-                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n-                before instantiating the full models if we know that the model does not support the requested attention.\n+                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n+                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n+                if we know that the model does not support the requested attention.\n         \"\"\"\n         dtype = self.config.dtype\n \n@@ -2440,9 +2434,9 @@ def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n         Args:\n             is_init_check (`bool`, *optional*):\n                 Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n-                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n-                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n-                before instantiating the full models if we know that the model does not support the requested attention.\n+                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n+                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n+                if we know that the model does not support the requested attention.\n         \"\"\"\n         if not self._supports_sdpa:\n             raise ValueError(\n@@ -2461,12 +2455,6 @@ def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n             )\n             torch.backends.cuda.enable_flash_sdp(False)\n \n-        if not is_init_check:\n-            if getattr(self, \"use_bettertransformer\", False):\n-                raise ValueError(\n-                    \"SDPA and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n-                )\n-\n         return True\n \n     def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n@@ -2476,9 +2464,9 @@ def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n         Args:\n             is_init_check (`bool`, *optional*):\n                 Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n-                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n-                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n-                before instantiating the full models if we know that the model does not support the requested attention.\n+                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n+                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n+                if we know that the model does not support the requested attention.\n         \"\"\"\n         if not self._supports_flex_attn:\n             raise ValueError(\n@@ -2493,12 +2481,6 @@ def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n             )\n \n-        if not is_init_check:\n-            if getattr(self, \"use_bettertransformer\", False):\n-                raise ValueError(\n-                    \"FlexAttention and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n-                )\n-\n         # If no error raise by this point, we can return `True`\n         return True\n \n@@ -2514,9 +2496,9 @@ def _check_and_adjust_attn_implementation(\n                 The attention implementation to check for existence/validity.\n             is_init_check (`bool`, *optional*):\n                 Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n-                fully instantiated. This is needed as we also check the devices of the weights, and/or if the model uses\n-                BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n-                before instantiating the full models if we know that the model does not support the requested attention.\n+                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n+                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n+                if we know that the model does not support the requested attention.\n \n         Returns:\n             `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n@@ -5409,56 +5391,6 @@ def register_for_auto_class(cls, auto_class=\"AutoModel\"):\n \n         cls._auto_class = auto_class\n \n-    def to_bettertransformer(self) -> \"PreTrainedModel\":\n-        \"\"\"\n-        Converts the model to use [PyTorch's native attention\n-        implementation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), integrated to\n-        Transformers through [Optimum library](https://huggingface.co/docs/optimum/bettertransformer/overview). Only a\n-        subset of all Transformers models are supported.\n-\n-        PyTorch's attention fastpath allows to speed up inference through kernel fusions and the use of [nested\n-        tensors](https://pytorch.org/docs/stable/nested.html). Detailed benchmarks can be found in [this blog\n-        post](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2).\n-\n-        Returns:\n-            [`PreTrainedModel`]: The model converted to BetterTransformer.\n-        \"\"\"\n-        if not is_optimum_available():\n-            raise ImportError(\"The package `optimum` is required to use Better Transformer.\")\n-\n-        from optimum.version import __version__ as optimum_version\n-\n-        if version.parse(optimum_version) < version.parse(\"1.7.0\"):\n-            raise ImportError(\n-                f\"Please install optimum>=1.7.0 to use Better Transformer. The version {optimum_version} was found.\"\n-            )\n-\n-        from optimum.bettertransformer import BetterTransformer\n-\n-        return BetterTransformer.transform(self)\n-\n-    def reverse_bettertransformer(self):\n-        \"\"\"\n-        Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original modeling is\n-        used, for example in order to save the model.\n-\n-        Returns:\n-            [`PreTrainedModel`]: The model converted back to the original modeling.\n-        \"\"\"\n-        if not is_optimum_available():\n-            raise ImportError(\"The package `optimum` is required to use Better Transformer.\")\n-\n-        from optimum.version import __version__ as optimum_version\n-\n-        if version.parse(optimum_version) < version.parse(\"1.7.0\"):\n-            raise ImportError(\n-                f\"Please install optimum>=1.7.0 to use Better Transformer. The version {optimum_version} was found.\"\n-            )\n-\n-        from optimum.bettertransformer import BetterTransformer\n-\n-        return BetterTransformer.reverse(self)\n-\n     def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask):\n         \"\"\"\n         Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given."
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/bettertransformer/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5700c497ecb34887d158782389071cd02594b5c/tests%2Fbettertransformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5700c497ecb34887d158782389071cd02594b5c/tests%2Fbettertransformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fbettertransformer%2F__init__.py?ref=a5700c497ecb34887d158782389071cd02594b5c"
        },
        {
            "sha": "9a3d9c3721baa6b22b072bcff4b6aec86327d6fa",
            "filename": "tests/bettertransformer/test_integration.py",
            "status": "removed",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5700c497ecb34887d158782389071cd02594b5c/tests%2Fbettertransformer%2Ftest_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5700c497ecb34887d158782389071cd02594b5c/tests%2Fbettertransformer%2Ftest_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fbettertransformer%2Ftest_integration.py?ref=a5700c497ecb34887d158782389071cd02594b5c",
            "patch": "@@ -1,85 +0,0 @@\n-# Copyright 2023 The HuggingFace Team Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import tempfile\n-import unittest\n-\n-from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-from transformers.testing_utils import (\n-    is_torch_available,\n-    require_optimum,\n-    require_torch,\n-    slow,\n-)\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-@require_torch\n-@require_optimum\n-@slow\n-class BetterTransformerIntegrationTest(unittest.TestCase):\n-    # refer to the full test suite in Optimum library:\n-    # https://github.com/huggingface/optimum/tree/main/tests/bettertransformer\n-\n-    def test_transform_and_reverse(self):\n-        r\"\"\"\n-        Classic tests to simply check if the conversion has been successful.\n-        \"\"\"\n-        model_id = \"hf-internal-testing/tiny-random-t5\"\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n-\n-        inp = tokenizer(\"This is me\", return_tensors=\"pt\")\n-\n-        model = model.to_bettertransformer()\n-\n-        self.assertTrue(any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model.named_modules()))\n-\n-        output = model.generate(**inp)\n-\n-        model = model.reverse_bettertransformer()\n-\n-        self.assertFalse(any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model.named_modules()))\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            model.save_pretrained(tmpdirname)\n-\n-            model_reloaded = AutoModelForSeq2SeqLM.from_pretrained(tmpdirname)\n-\n-            self.assertFalse(\n-                any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model_reloaded.named_modules())\n-            )\n-\n-            output_from_pretrained = model_reloaded.generate(**inp)\n-            torch.testing.assert_close(output, output_from_pretrained)\n-\n-    def test_error_save_pretrained(self):\n-        r\"\"\"\n-        The save_pretrained method should raise a ValueError if the model is in BetterTransformer mode.\n-        All should be good if the model is reversed.\n-        \"\"\"\n-        model_id = \"hf-internal-testing/tiny-random-t5\"\n-        model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n-\n-        model = model.to_bettertransformer()\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            with self.assertRaises(ValueError):\n-                model.save_pretrained(tmpdirname)\n-\n-            model = model.reverse_bettertransformer()\n-            model.save_pretrained(tmpdirname)"
        }
    ],
    "stats": {
        "total": 1060,
        "additions": 20,
        "deletions": 1040
    }
}