{
    "author": "gante",
    "message": "[tests] remove flax-pt equivalence and cross tests (#36283)",
    "sha": "99adc7446236b6654c8949df29d4c80a141f4683",
    "files": [
        {
            "sha": "156765d30f30a41331df433f819f6245ba7a2e31",
            "filename": ".circleci/create_circleci_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/.circleci%2Fcreate_circleci_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/.circleci%2Fcreate_circleci_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fcreate_circleci_config.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -28,7 +28,6 @@\n     \"TRANSFORMERS_IS_CI\": True,\n     \"PYTEST_TIMEOUT\": 120,\n     \"RUN_PIPELINE_TESTS\": False,\n-    \"RUN_PT_FLAX_CROSS_TESTS\": False,\n }\n # Disable the use of {\"s\": None} as the output is way too long, causing the navigation on CircleCI impractical\n COMMON_PYTEST_OPTIONS = {\"max-worker-restart\": 0, \"dist\": \"loadfile\", \"vvv\": None, \"rsfE\":None}\n@@ -176,14 +175,6 @@ def job_name(self):\n \n \n # JOBS\n-torch_and_flax_job = CircleCIJob(\n-    \"torch_and_flax\",\n-    additional_env={\"RUN_PT_FLAX_CROSS_TESTS\": True},\n-    docker_image=[{\"image\":\"huggingface/transformers-torch-jax-light\"}],\n-    marker=\"is_pt_flax_cross_test\",\n-    pytest_options={\"rA\": None, \"durations\": 0},\n-)\n-\n torch_job = CircleCIJob(\n     \"torch\",\n     docker_image=[{\"image\": \"huggingface/transformers-torch-light\"}],\n@@ -343,7 +334,7 @@ def job_name(self):\n     pytest_num_workers=1,\n )\n \n-REGULAR_TESTS = [torch_and_flax_job, torch_job, tf_job, flax_job, hub_job, onnx_job, tokenization_job, processor_job, generate_job, non_model_job] # fmt: skip\n+REGULAR_TESTS = [torch_job, tf_job, flax_job, hub_job, onnx_job, tokenization_job, processor_job, generate_job, non_model_job] # fmt: skip\n EXAMPLES_TESTS = [examples_torch_job, examples_tensorflow_job]\n PIPELINE_TESTS = [pipelines_torch_job, pipelines_tf_job]\n REPO_UTIL_TESTS = [repo_utils_job]"
        },
        {
            "sha": "5606668531da95a9839921e7a79be0f81d2191a3",
            "filename": ".github/workflows/build-ci-docker-images.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/.github%2Fworkflows%2Fbuild-ci-docker-images.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/.github%2Fworkflows%2Fbuild-ci-docker-images.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbuild-ci-docker-images.yml?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -26,19 +26,19 @@ jobs:\n \n     strategy:\n       matrix:\n-        file: [\"quality\", \"consistency\", \"custom-tokenizers\", \"torch-light\", \"tf-light\", \"exotic-models\", \"torch-tf-light\", \"torch-jax-light\", \"jax-light\", \"examples-torch\",  \"examples-tf\"]\n+        file: [\"quality\", \"consistency\", \"custom-tokenizers\", \"torch-light\", \"tf-light\", \"exotic-models\", \"torch-tf-light\", \"jax-light\", \"examples-torch\",  \"examples-tf\"]\n     continue-on-error: true\n \n     steps:\n       -\n         name: Set tag\n         run: |\n               if ${{contains(github.event.head_commit.message, '[build-ci-image]')}}; then\n-                  echo \"TAG=huggingface/transformers-${{ matrix.file }}:dev\" >> \"$GITHUB_ENV\" \n+                  echo \"TAG=huggingface/transformers-${{ matrix.file }}:dev\" >> \"$GITHUB_ENV\"\n                   echo \"setting it to DEV!\"\n               else\n                   echo \"TAG=huggingface/transformers-${{ matrix.file }}\" >> \"$GITHUB_ENV\"\n-                  \n+\n               fi\n       -\n         name: Set up Docker Buildx"
        },
        {
            "sha": "c6047b0e1cc1ac2a9d7dd6222f4ee28dd0156910",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -343,7 +343,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n \n Like the slow tests, there are other environment variables available which are not enabled by default during testing:\n - `RUN_CUSTOM_TOKENIZERS`: Enables tests for custom tokenizers.\n-- `RUN_PT_FLAX_CROSS_TESTS`: Enables tests for PyTorch + Flax integration.\n \n More environment variables and additional information can be found in the [testing_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/testing_utils.py).\n "
        },
        {
            "sha": "ef52038044337a73cc92034927b2c1addcdc2720",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -84,9 +84,6 @@\n \n \n def pytest_configure(config):\n-    config.addinivalue_line(\n-        \"markers\", \"is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested\"\n-    )\n     config.addinivalue_line(\"markers\", \"is_pipeline_test: mark test to run only when pipelines are tested\")\n     config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n     config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")"
        },
        {
            "sha": "61ee8c3fc4e3472c1a94017d77915deae28e6dd3",
            "filename": "docs/source/de/contributing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fde%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fde%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fcontributing.md?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -283,7 +283,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n Wie bei den langsamen Tests gibt es auch andere Umgebungsvariablen, die standardmäßig beim Testen nicht gesetzt sind:\n \n * `RUN_CUSTOM_TOKENIZERS`: Aktiviert Tests für benutzerdefinierte Tokenizer.\n-* `RUN_PT_FLAX_CROSS_TESTS`: Aktiviert Tests für die Integration von PyTorch + Flax.\n \n Weitere Umgebungsvariablen und zusätzliche Informationen finden Sie in der [testing_utils.py](src/transformers/testing_utils.py).\n "
        },
        {
            "sha": "f1c0a84ef32d848467026a812eaf776c2222a7fe",
            "filename": "docs/source/ko/contributing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fko%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fko%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcontributing.md?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -282,7 +282,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n \n 느린 테스트와 마찬가지로, 다음과 같이 테스트 중에 기본적으로 활성화되지 않는 다른 환경 변수도 있습니다:\n - `RUN_CUSTOM_TOKENIZERS`: 사용자 정의 토크나이저 테스트를 활성화합니다.\n-- `RUN_PT_FLAX_CROSS_TESTS`: PyTorch + Flax 통합 테스트를 활성화합니다.\n \n 더 많은 환경 변수와 추가 정보는 [testing_utils.py](src/transformers/testing_utils.py)에서 찾을 수 있습니다.\n "
        },
        {
            "sha": "045b58af086168f107cac485e032e1034db54d90",
            "filename": "docs/source/zh/contributing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fzh%2Fcontributing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/docs%2Fsource%2Fzh%2Fcontributing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcontributing.md?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -281,7 +281,6 @@ RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/t\n \n 和时间较长的测试一样，还有其他环境变量在测试过程中，在默认情况下是未启用的：\n - `RUN_CUSTOM_TOKENIZERS`: 启用自定义分词器的测试。\n-- `RUN_PT_FLAX_CROSS_TESTS`: 启用 PyTorch + Flax 整合的测试。\n \n 更多环境变量和额外信息可以在 [testing_utils.py](src/transformers/testing_utils.py) 中找到。\n "
        },
        {
            "sha": "9c207fb8161fd16c0eb3c8388249357528f74a8a",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -473,7 +473,6 @@ def run(self):\n extras[\"tests_torch\"] = deps_list()\n extras[\"tests_tf\"] = deps_list()\n extras[\"tests_flax\"] = deps_list()\n-extras[\"tests_torch_and_flax\"] = deps_list()\n extras[\"tests_hub\"] = deps_list()\n extras[\"tests_pipelines_torch\"] = deps_list()\n extras[\"tests_pipelines_tf\"] = deps_list()"
        },
        {
            "sha": "1d575ad4a3a73477f495894e5d7df78ce06dc5ea",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -230,10 +230,8 @@ def parse_int_from_env(key, default=None):\n \n \n _run_slow_tests = parse_flag_from_env(\"RUN_SLOW\", default=False)\n-_run_pt_flax_cross_tests = parse_flag_from_env(\"RUN_PT_FLAX_CROSS_TESTS\", default=True)\n _run_custom_tokenizers = parse_flag_from_env(\"RUN_CUSTOM_TOKENIZERS\", default=False)\n _run_staging = parse_flag_from_env(\"HUGGINGFACE_CO_STAGING\", default=False)\n-_tf_gpu_memory_limit = parse_int_from_env(\"TF_GPU_MEMORY_LIMIT\", default=None)\n _run_pipeline_tests = parse_flag_from_env(\"RUN_PIPELINE_TESTS\", default=True)\n _run_agent_tests = parse_flag_from_env(\"RUN_AGENT_TESTS\", default=False)\n _run_third_party_device_tests = parse_flag_from_env(\"RUN_THIRD_PARTY_DEVICE_TESTS\", default=False)\n@@ -250,25 +248,6 @@ def get_device_count():\n     return num_devices\n \n \n-def is_pt_flax_cross_test(test_case):\n-    \"\"\"\n-    Decorator marking a test as a test that control interactions between PyTorch and Flax\n-\n-    PT+FLAX tests are skipped by default and we can run only them by setting RUN_PT_FLAX_CROSS_TESTS environment\n-    variable to a truthy value and selecting the is_pt_flax_cross_test pytest mark.\n-\n-    \"\"\"\n-    if not _run_pt_flax_cross_tests or not is_torch_available() or not is_flax_available():\n-        return unittest.skip(reason=\"test is PT+FLAX test\")(test_case)\n-    else:\n-        try:\n-            import pytest  # We don't need a hard dependency on pytest in the main library\n-        except ImportError:\n-            return test_case\n-        else:\n-            return pytest.mark.is_pt_flax_cross_test()(test_case)\n-\n-\n def is_staging_test(test_case):\n     \"\"\"\n     Decorator marking a test as a staging test."
        },
        {
            "sha": "ffad4459ec9612ee5525a4208d999cddb29bbee3",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -624,15 +624,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    # overwrite from common in order to skip the check on `attentions`\n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=1e-5, name=\"outputs\", attributes=None):\n-        # `bigbird_block_sparse_attention` in `FlaxBigBird` returns `attention_probs = None`, while in PyTorch version,\n-        # an effort was done to return `attention_probs` (yet to be verified).\n-        if name.startswith(\"outputs.attentions\"):\n-            return\n-        else:\n-            super().check_pt_flax_outputs(fx_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n \n @require_torch\n @slow"
        },
        {
            "sha": "fe1790bf75c09ecbf8bef081a500d5a424057417",
            "filename": "tests/models/big_bird/test_modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -212,12 +212,3 @@ def model_jitted(input_ids, attention_mask=None, **kwargs):\n                 self.assertEqual(len(outputs), len(jitted_outputs))\n                 for jitted_output, output in zip(jitted_outputs, outputs):\n                     self.assertEqual(jitted_output.shape, output.shape)\n-\n-    # overwrite from common in order to skip the check on `attentions`\n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=1e-5, name=\"outputs\", attributes=None):\n-        # `bigbird_block_sparse_attention` in `FlaxBigBird` returns `attention_probs = None`, while in PyTorch version,\n-        # an effort was done to return `attention_probs` (yet to be verified).\n-        if name.startswith(\"outputs.attentions\"):\n-            return\n-        else:\n-            super().check_pt_flax_outputs(fx_outputs, pt_outputs, model_class, tol, name, attributes)"
        },
        {
            "sha": "63723bfe3b98058f4edd2e11ba9477ea92e01631",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -25,11 +25,8 @@\n from parameterized import parameterized\n from pytest import mark\n \n-import transformers\n from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from transformers.testing_utils import (\n-    is_flax_available,\n-    is_pt_flax_cross_test,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -82,15 +79,6 @@\n     from transformers import CLIPProcessor\n \n \n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n-\n class CLIPVisionModelTester:\n     def __init__(\n         self,\n@@ -883,126 +871,6 @@ def test_load_vision_text_config(self):\n             text_config = CLIPTextConfig.from_pretrained(tmp_dir_name)\n             self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n \n-    # overwrite from common since FlaxCLIPModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                pt_model.to(torch_device)\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-                fx_outputs = fx_model(**fx_inputs).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = fx_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**fx_inputs).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n-\n-    # overwrite from common since FlaxCLIPModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # load corresponding PyTorch class\n-                pt_model = model_class(config).eval()\n-\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-                pt_model.to(torch_device)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                fx_outputs = fx_model(**fx_inputs).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = model_class.from_pretrained(tmpdirname, from_flax=True)\n-                    pt_model_loaded.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"openai/clip-vit-base-patch32\""
        },
        {
            "sha": "d499f4bf7dcb06dbbcb998cb59dd0da7496da60a",
            "filename": "tests/models/clip/test_modeling_flax_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 127,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -4,31 +4,22 @@\n \n import numpy as np\n \n-import transformers\n-from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig, is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow\n+from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig, is_flax_available\n+from transformers.testing_utils import require_flax, slow\n \n from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n \n \n if is_flax_available():\n     import jax\n-    import jax.numpy as jnp\n \n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.models.clip.modeling_flax_clip import (\n         FlaxCLIPModel,\n         FlaxCLIPTextModel,\n         FlaxCLIPTextModelWithProjection,\n         FlaxCLIPVisionModel,\n     )\n \n-if is_torch_available():\n-    import torch\n-\n \n class FlaxCLIPVisionModelTester:\n     def __init__(\n@@ -223,21 +214,6 @@ def test_save_load_from_base(self):\n     def test_save_load_to_base(self):\n         pass\n \n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:\n@@ -333,21 +309,6 @@ def test_save_load_from_base(self):\n     def test_save_load_to_base(self):\n         pass\n \n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:\n@@ -472,92 +433,6 @@ def test_model_from_pretrained(self):\n             outputs = model(input_ids=np.ones((1, 1)), pixel_values=np.ones((1, 3, 224, 224)))\n             self.assertIsNotNone(outputs)\n \n-    # overwrite from common since FlaxCLIPModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 4e-2)\n-\n-    # overwrite from common since FlaxCLIPModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n     # overwrite from common since FlaxCLIPModel returns nested output\n     # which is not supported in the common test\n     def test_from_pretrained_save_pretrained(self):"
        },
        {
            "sha": "a17b2b6a4fea399daec358662c6f8c51ed7f9f92",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 129,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -22,11 +22,8 @@\n import numpy as np\n import requests\n \n-import transformers\n from transformers import CLIPSegConfig, CLIPSegProcessor, CLIPSegTextConfig, CLIPSegVisionConfig\n from transformers.testing_utils import (\n-    is_flax_available,\n-    is_pt_flax_cross_test,\n     require_torch,\n     require_vision,\n     slow,\n@@ -57,15 +54,6 @@\n     from PIL import Image\n \n \n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n-\n class CLIPSegVisionModelTester:\n     def __init__(\n         self,\n@@ -635,123 +623,6 @@ def test_load_vision_text_config(self):\n             text_config = CLIPSegTextConfig.from_pretrained(tmp_dir_name)\n             self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n \n-    # overwrite from common since FlaxCLIPSegModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-                fx_outputs = fx_model(**fx_inputs).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = fx_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**fx_inputs).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 4e-2)\n-\n-    # overwrite from common since FlaxCLIPSegModel returns nested output\n-    # which is not supported in the common test\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # load corresponding PyTorch class\n-                pt_model = model_class(config).eval()\n-\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                fx_outputs = fx_model(**fx_inputs).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n-\n     def test_training(self):\n         if not self.model_tester.is_training:\n             self.skipTest(reason=\"Training test is skipped as the model was not trained\")"
        },
        {
            "sha": "4b7e24a23ed6c1a2061d870fb58898e4b444fba6",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -22,7 +22,7 @@\n \n from tests.test_modeling_common import floats_tensor, ids_tensor, random_attention_mask\n from transformers import Data2VecAudioConfig, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init\n@@ -442,16 +442,6 @@ def test_resize_tokens_embeddings(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @is_pt_flax_cross_test\n-    # non-robust architecture does not exist in Flax\n-    def test_equivalence_flax_to_pt(self):\n-        pass\n-\n-    @is_pt_flax_cross_test\n-    # non-robust architecture does not exist in Flax\n-    def test_equivalence_pt_to_flax(self):\n-        pass\n-\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.output_hidden_states = True"
        },
        {
            "sha": "36298182002446a177d6a1ca80f0dc54c5cd254e",
            "filename": "tests/models/encoder_decoder/test_modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 107,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -19,8 +19,8 @@\n \n import numpy as np\n \n-from transformers import is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow, torch_device\n+from transformers import is_flax_available\n+from transformers.testing_utils import require_flax, slow\n \n from ...test_modeling_flax_common import ids_tensor\n from ..bart.test_modeling_flax_bart import FlaxBartStandaloneDecoderModelTester\n@@ -38,15 +38,6 @@\n         FlaxEncoderDecoderModel,\n         FlaxGPT2LMHeadModel,\n     )\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n-if is_torch_available():\n-    import torch\n-\n-    from transformers import EncoderDecoderModel\n \n \n @require_flax\n@@ -291,68 +282,6 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n         generated_sequences = generated_output.sequences\n         self.assertEqual(generated_sequences.shape, (input_ids.shape[0],) + (decoder_config.max_length,))\n \n-    def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = EncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n-\n-    def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = EncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxEncoderDecoderModel(encoder_decoder_config)\n-\n-        fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-        fx_model.params = fx_state\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n-    def check_equivalence_flax_to_pt(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = EncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxEncoderDecoderModel(encoder_decoder_config)\n-\n-        pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n     def test_encoder_decoder_model_from_pretrained_configs(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model_from_pretrained_configs(**input_ids_dict)\n@@ -385,40 +314,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n \n-    @is_pt_flax_cross_test\n-    def test_pt_flax_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        config = config_inputs_dict.pop(\"config\")\n-        decoder_config = config_inputs_dict.pop(\"decoder_config\")\n-\n-        inputs_dict = config_inputs_dict\n-        # `encoder_hidden_states` is not used in model call/forward\n-        del inputs_dict[\"encoder_hidden_states\"]\n-\n-        # Avoid the case where a sequence has no place to attend (after combined with the causal attention mask)\n-        batch_size = inputs_dict[\"decoder_attention_mask\"].shape[0]\n-        inputs_dict[\"decoder_attention_mask\"] = np.concatenate(\n-            [np.ones(shape=(batch_size, 1)), inputs_dict[\"decoder_attention_mask\"][:, 1:]], axis=1\n-        )\n-\n-        # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-        # So we disable `use_cache` here for PyTorch model.\n-        decoder_config.use_cache = False\n-\n-        self.assertTrue(decoder_config.cross_attention_hidden_size is None)\n-\n-        # check without `enc_to_dec_proj` projection\n-        decoder_config.hidden_size = config.hidden_size\n-        self.assertTrue(config.hidden_size == decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n-        # check `enc_to_dec_proj` work as expected\n-        decoder_config.hidden_size = decoder_config.hidden_size * 2\n-        self.assertTrue(config.hidden_size != decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2 = self.get_pretrained_model()"
        },
        {
            "sha": "5beb43d90e69747ef3ce31203e6f9958e039a6b0",
            "filename": "tests/models/gpt2/test_modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 110,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -13,14 +13,12 @@\n # limitations under the License.\n \n \n-import tempfile\n import unittest\n \n import numpy as np\n \n-import transformers\n-from transformers import GPT2Config, GPT2Tokenizer, is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow\n+from transformers import GPT2Config, GPT2Tokenizer, is_flax_available\n+from transformers.testing_utils import require_flax, slow\n \n from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n \n@@ -29,15 +27,8 @@\n     import jax\n     import jax.numpy as jnp\n \n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.models.gpt2.modeling_flax_gpt2 import FlaxGPT2LMHeadModel, FlaxGPT2Model\n \n-if is_torch_available():\n-    import torch\n-\n \n class FlaxGPT2ModelTester:\n     def __init__(\n@@ -255,105 +246,6 @@ def test_batch_generation(self):\n \n         self.assertListEqual(output_string, expected_string)\n \n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-                    self.assert_almost_equals(fx_output_loaded[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs_loaded):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:"
        },
        {
            "sha": "170a261b41217ba32988b230b2d3c8d6118812f7",
            "filename": "tests/models/gpt_neo/test_modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 110,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -13,14 +13,12 @@\n # limitations under the License.\n \n \n-import tempfile\n import unittest\n \n import numpy as np\n \n-import transformers\n-from transformers import GPT2Tokenizer, GPTNeoConfig, is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow\n+from transformers import GPT2Tokenizer, GPTNeoConfig, is_flax_available\n+from transformers.testing_utils import require_flax, slow\n \n from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n \n@@ -29,15 +27,8 @@\n     import jax\n     import jax.numpy as jnp\n \n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.models.gpt_neo.modeling_flax_gpt_neo import FlaxGPTNeoForCausalLM, FlaxGPTNeoModel\n \n-if is_torch_available():\n-    import torch\n-\n \n class FlaxGPTNeoModelTester:\n     def __init__(\n@@ -224,105 +215,6 @@ def test_batch_generation(self):\n \n         self.assertListEqual(output_string, expected_string)\n \n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-                    self.assert_almost_equals(fx_output_loaded[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs_loaded):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:"
        },
        {
            "sha": "305a86ece1e5b8c67b82c4f61bea60317f35f5b5",
            "filename": "tests/models/gptj/test_modeling_flax_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 110,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -13,14 +13,12 @@\n # limitations under the License.\n \n \n-import tempfile\n import unittest\n \n import numpy as np\n \n-import transformers\n-from transformers import GPT2Tokenizer, GPTJConfig, is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, tooslow\n+from transformers import GPT2Tokenizer, GPTJConfig, is_flax_available\n+from transformers.testing_utils import require_flax, tooslow\n \n from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n \n@@ -29,15 +27,8 @@\n     import jax\n     import jax.numpy as jnp\n \n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.models.gptj.modeling_flax_gptj import FlaxGPTJForCausalLM, FlaxGPTJModel\n \n-if is_torch_available():\n-    import torch\n-\n \n class FlaxGPTJModelTester:\n     def __init__(\n@@ -221,105 +212,6 @@ def test_batch_generation(self):\n \n         self.assertListEqual(output_string, expected_string)\n \n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-                    self.assert_almost_equals(fx_output_loaded[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs_loaded):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n     @tooslow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:"
        },
        {
            "sha": "ec5432f9efd14f9748559035c047b9745641a235",
            "filename": "tests/models/longt5/test_modeling_flax_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -17,11 +17,9 @@\n \n import numpy as np\n \n-import transformers\n from transformers import is_flax_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import (\n-    is_pt_flax_cross_test,\n     require_flax,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -46,7 +44,6 @@\n     from flax.traverse_util import flatten_dict\n \n     from transformers import FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING, FLAX_MODEL_MAPPING, AutoTokenizer, LongT5Config\n-    from transformers.modeling_flax_pytorch_utils import load_flax_weights_in_pytorch_model\n     from transformers.models.longt5.modeling_flax_longt5 import (\n         FlaxLongT5ForConditionalGeneration,\n         FlaxLongT5Model,\n@@ -467,95 +464,6 @@ def test_attention_outputs(self):\n                 [self.model_tester.num_attention_heads, block_len, 3 * block_len],\n             )\n \n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = base_class(config)\n-            base_params = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, base_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                # save pt model\n-                pt_model.save_pretrained(tmpdirname)\n-                head_model = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_param_from_head = flatten_dict(unfreeze(head_model.params))\n-\n-                for key in base_param_from_head.keys():\n-                    max_diff = (base_params[key] - base_param_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            model.params = model.to_bf16(model.params)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n \n class FlaxLongT5TGlobalModelTest(FlaxLongT5ModelTest):\n     def setUp(self):"
        },
        {
            "sha": "5348315c7c84c9dbf84037818a19992deaf5c2ca",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 302,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -19,7 +19,7 @@\n import numpy as np\n \n from transformers import is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow, torch_device\n+from transformers.testing_utils import require_flax, slow\n \n from ...test_modeling_flax_common import floats_tensor, ids_tensor, random_attention_mask\n from ..bart.test_modeling_flax_bart import FlaxBartStandaloneDecoderModelTester\n@@ -43,14 +43,8 @@\n         SpeechEncoderDecoderConfig,\n     )\n     from transformers.modeling_flax_outputs import FlaxBaseModelOutput\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n \n if is_torch_available():\n-    import torch\n-\n     from transformers import SpeechEncoderDecoderModel\n \n \n@@ -406,68 +400,6 @@ def compute_loss(\n         for grad, grad_frozen in zip(grads, grads_frozen):\n             self.assertTrue((grad == grad_frozen).all())\n \n-    def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxSpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = SpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n-\n-    def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = SpeechEncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxSpeechEncoderDecoderModel(encoder_decoder_config)\n-\n-        fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-        fx_model.params = fx_state\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n-    def check_equivalence_flax_to_pt(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = SpeechEncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxSpeechEncoderDecoderModel(encoder_decoder_config)\n-\n-        pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n     def test_encoder_decoder_model_from_pretrained_configs(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model_from_pretrained_configs(**input_ids_dict)\n@@ -504,46 +436,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n \n-    @is_pt_flax_cross_test\n-    def test_pt_flax_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        config = config_inputs_dict.pop(\"config\")\n-        decoder_config = config_inputs_dict.pop(\"decoder_config\")\n-\n-        inputs_dict = config_inputs_dict\n-        # `encoder_hidden_states` is not used in model call/forward\n-        del inputs_dict[\"encoder_hidden_states\"]\n-\n-        # Avoid the case where a sequence has no place to attend (after combined with the causal attention mask)\n-        batch_size = inputs_dict[\"decoder_attention_mask\"].shape[0]\n-        inputs_dict[\"decoder_attention_mask\"] = np.concatenate(\n-            [np.ones(shape=(batch_size, 1)), inputs_dict[\"decoder_attention_mask\"][:, 1:]], axis=1\n-        )\n-\n-        # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-        # So we disable `use_cache` here for PyTorch model.\n-        decoder_config.use_cache = False\n-\n-        self.assertTrue(decoder_config.cross_attention_hidden_size is None)\n-\n-        # check without `enc_to_dec_proj` projection\n-        decoder_config.hidden_size = config.hidden_size\n-        self.assertTrue(config.hidden_size == decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n-        # check `enc_to_dec_proj` work as expected\n-        decoder_config.hidden_size = decoder_config.hidden_size * 2\n-        self.assertTrue(config.hidden_size != decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n-        # check `add_adapter` works as expected\n-        config.add_adapter = True\n-        self.assertTrue(config.add_adapter)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2 = self.get_pretrained_model()\n@@ -625,71 +517,6 @@ def prepare_config_and_inputs(self):\n             \"encoder_hidden_states\": encoder_hidden_states,\n         }\n \n-    @slow\n-    def test_flaxwav2vec2gpt2_pt_flax_equivalence(self):\n-        pt_model = SpeechEncoderDecoderModel.from_pretrained(\"jsnfly/wav2vec2-large-xlsr-53-german-gpt2\")\n-        fx_model = FlaxSpeechEncoderDecoderModel.from_pretrained(\n-            \"jsnfly/wav2vec2-large-xlsr-53-german-gpt2\", from_pt=True\n-        )\n-\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        batch_size = 13\n-        input_values = floats_tensor([batch_size, 512], scale=1.0)\n-        attention_mask = random_attention_mask([batch_size, 512])\n-        decoder_input_ids = ids_tensor([batch_size, 4], fx_model.config.decoder.vocab_size)\n-        decoder_attention_mask = random_attention_mask([batch_size, 4])\n-        inputs_dict = {\n-            \"inputs\": input_values,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-        }\n-\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs)\n-        pt_logits = pt_outputs.logits\n-        pt_outputs = pt_outputs.to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict)\n-        fx_logits = fx_outputs.logits\n-        fx_outputs = fx_outputs.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits.numpy(), 4e-2)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxSpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict)\n-        fx_logits_loaded = fx_outputs_loaded.logits\n-        fx_outputs_loaded = fx_outputs_loaded.to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits_loaded, pt_logits.numpy(), 4e-2)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = SpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-        pt_logits_loaded = pt_outputs_loaded.logits\n-        pt_outputs_loaded = pt_outputs_loaded.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits_loaded.numpy(), 4e-2)\n-\n \n @require_flax\n class FlaxWav2Vec2BartModelTest(FlaxEncoderDecoderMixin, unittest.TestCase):\n@@ -742,71 +569,6 @@ def prepare_config_and_inputs(self):\n             \"encoder_hidden_states\": encoder_hidden_states,\n         }\n \n-    @slow\n-    def test_flaxwav2vec2bart_pt_flax_equivalence(self):\n-        pt_model = SpeechEncoderDecoderModel.from_pretrained(\"patrickvonplaten/wav2vec2-2-bart-large\")\n-        fx_model = FlaxSpeechEncoderDecoderModel.from_pretrained(\n-            \"patrickvonplaten/wav2vec2-2-bart-large\", from_pt=True\n-        )\n-\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        batch_size = 13\n-        input_values = floats_tensor([batch_size, 512], scale=1.0)\n-        attention_mask = random_attention_mask([batch_size, 512])\n-        decoder_input_ids = ids_tensor([batch_size, 4], fx_model.config.decoder.vocab_size)\n-        decoder_attention_mask = random_attention_mask([batch_size, 4])\n-        inputs_dict = {\n-            \"inputs\": input_values,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-        }\n-\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs)\n-        pt_logits = pt_outputs.logits\n-        pt_outputs = pt_outputs.to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict)\n-        fx_logits = fx_outputs.logits\n-        fx_outputs = fx_outputs.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits.numpy(), 4e-2)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxSpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict)\n-        fx_logits_loaded = fx_outputs_loaded.logits\n-        fx_outputs_loaded = fx_outputs_loaded.to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits_loaded, pt_logits.numpy(), 4e-2)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = SpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-        pt_logits_loaded = pt_outputs_loaded.logits\n-        pt_outputs_loaded = pt_outputs_loaded.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits_loaded.numpy(), 4e-2)\n-\n \n @require_flax\n class FlaxWav2Vec2BertModelTest(FlaxEncoderDecoderMixin, unittest.TestCase):\n@@ -858,66 +620,3 @@ def prepare_config_and_inputs(self):\n             \"decoder_attention_mask\": decoder_attention_mask,\n             \"encoder_hidden_states\": encoder_hidden_states,\n         }\n-\n-    @slow\n-    def test_flaxwav2vec2bert_pt_flax_equivalence(self):\n-        pt_model = SpeechEncoderDecoderModel.from_pretrained(\"speech-seq2seq/wav2vec2-2-bert-large\")\n-        fx_model = FlaxSpeechEncoderDecoderModel.from_pretrained(\"speech-seq2seq/wav2vec2-2-bert-large\", from_pt=True)\n-\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        batch_size = 13\n-        input_values = floats_tensor([batch_size, 512], fx_model.config.encoder.vocab_size)\n-        attention_mask = random_attention_mask([batch_size, 512])\n-        decoder_input_ids = ids_tensor([batch_size, 4], fx_model.config.decoder.vocab_size)\n-        decoder_attention_mask = random_attention_mask([batch_size, 4])\n-        inputs_dict = {\n-            \"inputs\": input_values,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-        }\n-\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs)\n-        pt_logits = pt_outputs.logits\n-        pt_outputs = pt_outputs.to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict)\n-        fx_logits = fx_outputs.logits\n-        fx_outputs = fx_outputs.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits.numpy(), 4e-2)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxSpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict)\n-        fx_logits_loaded = fx_outputs_loaded.logits\n-        fx_outputs_loaded = fx_outputs_loaded.to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits_loaded, pt_logits.numpy(), 4e-2)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = SpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-        pt_logits_loaded = pt_outputs_loaded.logits\n-        pt_outputs_loaded = pt_outputs_loaded.to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        self.assert_almost_equals(fx_logits, pt_logits_loaded.numpy(), 4e-2)"
        },
        {
            "sha": "dc372c57694d4ef59a06e0052604cfb10b0ec7d6",
            "filename": "tests/models/t5/test_modeling_flax_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 187,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -17,15 +17,8 @@\n \n import numpy as np\n \n-import transformers\n from transformers import is_flax_available\n-from transformers.testing_utils import (\n-    is_pt_flax_cross_test,\n-    require_flax,\n-    require_sentencepiece,\n-    require_tokenizers,\n-    slow,\n-)\n+from transformers.testing_utils import require_flax, require_sentencepiece, require_tokenizers, slow\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor\n@@ -47,7 +40,6 @@\n     from flax.traverse_util import flatten_dict\n \n     from transformers import FLAX_MODEL_MAPPING, ByT5Tokenizer, T5Config, T5Tokenizer\n-    from transformers.modeling_flax_pytorch_utils import load_flax_weights_in_pytorch_model\n     from transformers.models.t5.modeling_flax_t5 import (\n         FlaxT5EncoderModel,\n         FlaxT5ForConditionalGeneration,\n@@ -373,95 +365,6 @@ def test_save_load_to_base(self):\n                     max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n                     self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n \n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = base_class(config)\n-            base_params = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, base_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                # save pt model\n-                pt_model.save_pretrained(tmpdirname)\n-                head_model = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_param_from_head = flatten_dict(unfreeze(head_model.params))\n-\n-                for key in base_param_from_head.keys():\n-                    max_diff = (base_params[key] - base_param_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            model.params = model.to_bf16(model.params)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n \n class FlaxT5EncoderOnlyModelTester:\n     def __init__(\n@@ -663,95 +566,6 @@ def test_save_load_to_base(self):\n                     max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n                     self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n \n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = base_class(config)\n-            base_params = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, base_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                # save pt model\n-                pt_model.save_pretrained(tmpdirname)\n-                head_model = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_param_from_head = flatten_dict(unfreeze(head_model.params))\n-\n-                for key in base_param_from_head.keys():\n-                    max_diff = (base_params[key] - base_param_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite since special base model prefix is used\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            model.params = model.to_bf16(model.params)\n-            base_params_from_head = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n \n @require_sentencepiece\n @require_tokenizers"
        },
        {
            "sha": "bec79869ae31794400eea7ee7a73a2cd664508c3",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 105,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -19,8 +19,8 @@\n \n import numpy as np\n \n-from transformers import is_flax_available, is_torch_available, is_vision_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, require_vision, slow, torch_device\n+from transformers import is_flax_available, is_vision_available\n+from transformers.testing_utils import require_flax, require_vision, slow\n \n from ...test_modeling_flax_common import floats_tensor, ids_tensor\n from ..gpt2.test_modeling_flax_gpt2 import FlaxGPT2ModelTester\n@@ -35,15 +35,7 @@\n         FlaxViTModel,\n         VisionEncoderDecoderConfig,\n     )\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n-if is_torch_available():\n-    import torch\n \n-    from transformers import VisionEncoderDecoderModel\n \n if is_vision_available():\n     from PIL import Image\n@@ -235,68 +227,6 @@ def check_encoder_decoder_model_generate(self, pixel_values, config, decoder_con\n         generated_sequences = generated_output.sequences\n         self.assertEqual(generated_sequences.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n \n-    def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxVisionEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = VisionEncoderDecoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n-\n-    def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = VisionEncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxVisionEncoderDecoderModel(encoder_decoder_config)\n-\n-        fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-        fx_model.params = fx_state\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n-    def check_equivalence_flax_to_pt(self, config, decoder_config, inputs_dict):\n-        encoder_decoder_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-\n-        pt_model = VisionEncoderDecoderModel(encoder_decoder_config)\n-        fx_model = FlaxVisionEncoderDecoderModel(encoder_decoder_config)\n-\n-        pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n     def test_encoder_decoder_model_from_pretrained_configs(self):\n         config_inputs_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model_from_pretrained_configs(**config_inputs_dict)\n@@ -325,39 +255,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n \n-    @is_pt_flax_cross_test\n-    def test_pt_flax_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        config = config_inputs_dict.pop(\"config\")\n-        decoder_config = config_inputs_dict.pop(\"decoder_config\")\n-\n-        inputs_dict = config_inputs_dict\n-        # `encoder_hidden_states` is not used in model call/forward\n-        del inputs_dict[\"encoder_hidden_states\"]\n-\n-        # Avoid the case where a sequence has no place to attend (after combined with the causal attention mask)\n-        batch_size = inputs_dict[\"decoder_attention_mask\"].shape[0]\n-        inputs_dict[\"decoder_attention_mask\"] = np.concatenate(\n-            [np.ones(shape=(batch_size, 1)), inputs_dict[\"decoder_attention_mask\"][:, 1:]], axis=1\n-        )\n-\n-        # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-        # So we disable `use_cache` here for PyTorch model.\n-        decoder_config.use_cache = False\n-\n-        self.assertTrue(decoder_config.cross_attention_hidden_size is None)\n-\n-        # check without `enc_to_dec_proj` projection\n-        self.assertTrue(config.hidden_size == decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n-        # check `enc_to_dec_proj` work as expected\n-        decoder_config.hidden_size = decoder_config.hidden_size * 2\n-        self.assertTrue(config.hidden_size != decoder_config.hidden_size)\n-        self.check_equivalence_pt_to_flax(config, decoder_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(config, decoder_config, inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2 = self.get_pretrained_model()"
        },
        {
            "sha": "115cdf444fe46531c3a21b46a767a5435dd5e0c1",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 91,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -20,15 +20,8 @@\n \n import numpy as np\n \n-from transformers.testing_utils import (\n-    is_pt_flax_cross_test,\n-    require_flax,\n-    require_torch,\n-    require_vision,\n-    slow,\n-    torch_device,\n-)\n-from transformers.utils import is_flax_available, is_torch_available, is_vision_available\n+from transformers.testing_utils import require_flax, require_torch, require_vision, slow\n+from transformers.utils import is_flax_available, is_vision_available\n \n from ...test_modeling_flax_common import floats_tensor, ids_tensor, random_attention_mask\n from ..bert.test_modeling_flax_bert import FlaxBertModelTester\n@@ -45,16 +38,7 @@\n         VisionTextDualEncoderConfig,\n         VisionTextDualEncoderProcessor,\n     )\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n \n-if is_torch_available():\n-    import torch\n-\n-    from transformers import VisionTextDualEncoderModel\n \n if is_vision_available():\n     from PIL import Image\n@@ -154,68 +138,6 @@ def check_vision_text_output_attention(\n             (text_config.num_attention_heads, input_ids.shape[-1], input_ids.shape[-1]),\n         )\n \n-    def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-        fx_outputs = fx_model(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxVisionTextDualEncoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = VisionTextDualEncoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output_loaded in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 4e-2)\n-\n-    def check_equivalence_pt_to_flax(self, vision_config, text_config, inputs_dict):\n-        config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)\n-\n-        pt_model = VisionTextDualEncoderModel(config)\n-        fx_model = FlaxVisionTextDualEncoderModel(config)\n-\n-        fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-        fx_model.params = fx_state\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n-    def check_equivalence_flax_to_pt(self, vision_config, text_config, inputs_dict):\n-        config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)\n-\n-        pt_model = VisionTextDualEncoderModel(config)\n-        fx_model = FlaxVisionTextDualEncoderModel(config)\n-\n-        pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, inputs_dict)\n-\n     def test_model_from_pretrained_configs(self):\n         inputs_dict = self.prepare_config_and_inputs()\n         self.check_model_from_pretrained_configs(**inputs_dict)\n@@ -232,17 +154,6 @@ def test_vision_text_output_attention(self):\n         inputs_dict = self.prepare_config_and_inputs()\n         self.check_vision_text_output_attention(**inputs_dict)\n \n-    @is_pt_flax_cross_test\n-    def test_pt_flax_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        vision_config = config_inputs_dict.pop(\"vision_config\")\n-        text_config = config_inputs_dict.pop(\"text_config\")\n-\n-        inputs_dict = config_inputs_dict\n-\n-        self.check_equivalence_pt_to_flax(vision_config, text_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(vision_config, text_config, inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2, inputs = self.get_pretrained_model_and_inputs()"
        },
        {
            "sha": "ab10f3e93fac36e0f15306d281c411bbc2f89bb3",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 86,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -20,8 +20,8 @@\n \n import numpy as np\n \n-from transformers.testing_utils import is_pt_flax_cross_test, require_torch, require_vision, slow, torch_device\n-from transformers.utils import is_flax_available, is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_modeling_common import floats_tensor, ids_tensor, random_attention_mask\n from ..bert.test_modeling_bert import BertModelTester\n@@ -44,12 +44,6 @@\n         ViTModel,\n     )\n \n-if is_flax_available():\n-    from transformers import FlaxVisionTextDualEncoderModel\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n \n if is_vision_available():\n     from PIL import Image\n@@ -172,69 +166,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n \n-    def check_pt_flax_equivalence(self, pt_model, fx_model, input_ids, attention_mask, pixel_values, **kwargs):\n-        pt_model.to(torch_device)\n-        pt_model.eval()\n-\n-        # prepare inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values}\n-        pt_inputs = inputs_dict\n-        flax_inputs = {k: v.numpy(force=True) for k, v in pt_inputs.items()}\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-        fx_outputs = fx_model(**flax_inputs).to_tuple()\n-        self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n-\n-        # PT -> Flax\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            pt_model.save_pretrained(tmpdirname)\n-            fx_model_loaded = FlaxVisionTextDualEncoderModel.from_pretrained(tmpdirname, from_pt=True)\n-\n-        fx_outputs_loaded = fx_model_loaded(**flax_inputs).to_tuple()\n-        self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n-\n-        # Flax -> PT\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            fx_model.save_pretrained(tmpdirname)\n-            pt_model_loaded = VisionTextDualEncoderModel.from_pretrained(tmpdirname, from_flax=True)\n-\n-        pt_model_loaded.to(torch_device)\n-        pt_model_loaded.eval()\n-\n-        with torch.no_grad():\n-            pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-        self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n-        for fx_output, pt_output_loaded in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 4e-2)\n-\n-    def check_equivalence_pt_to_flax(self, vision_config, text_config, inputs_dict):\n-        config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)\n-\n-        pt_model = VisionTextDualEncoderModel(config)\n-        fx_model = FlaxVisionTextDualEncoderModel(config)\n-\n-        fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-        fx_model.params = fx_state\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, **inputs_dict)\n-\n-    def check_equivalence_flax_to_pt(self, vision_config, text_config, inputs_dict):\n-        config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)\n-\n-        pt_model = VisionTextDualEncoderModel(config)\n-        fx_model = FlaxVisionTextDualEncoderModel(config)\n-\n-        pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-        self.check_pt_flax_equivalence(pt_model, fx_model, **inputs_dict)\n-\n     def test_vision_text_dual_encoder_model(self):\n         inputs_dict = self.prepare_config_and_inputs()\n         self.check_vision_text_dual_encoder_model(**inputs_dict)\n@@ -255,17 +186,6 @@ def test_vision_text_output_attention(self):\n         inputs_dict = self.prepare_config_and_inputs()\n         self.check_vision_text_output_attention(**inputs_dict)\n \n-    @is_pt_flax_cross_test\n-    def test_pt_flax_equivalence(self):\n-        config_inputs_dict = self.prepare_config_and_inputs()\n-        vision_config = config_inputs_dict.pop(\"vision_config\")\n-        text_config = config_inputs_dict.pop(\"text_config\")\n-\n-        inputs_dict = config_inputs_dict\n-\n-        self.check_equivalence_pt_to_flax(vision_config, text_config, inputs_dict)\n-        self.check_equivalence_flax_to_pt(vision_config, text_config, inputs_dict)\n-\n     @slow\n     def test_real_model_save_load_from_pretrained(self):\n         model_2, inputs = self.get_pretrained_model_and_inputs()\n@@ -429,10 +349,6 @@ def prepare_config_and_inputs(self):\n             \"text_choice_labels\": choice_labels,\n         }\n \n-    @unittest.skip(reason=\"DeiT is not available in Flax\")\n-    def test_pt_flax_equivalence(self):\n-        pass\n-\n \n @require_torch\n class CLIPVisionBertModelTest(VisionTextDualEncoderMixin, unittest.TestCase):"
        },
        {
            "sha": "e888cc5ff3cb52be5cda0e2c2fc3509fc24af865",
            "filename": "tests/models/wav2vec2/test_modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -24,9 +24,7 @@\n from transformers import Wav2Vec2Config, is_flax_available\n from transformers.testing_utils import (\n     CaptureLogger,\n-    is_flaky,\n     is_librosa_available,\n-    is_pt_flax_cross_test,\n     is_pyctcdecode_available,\n     require_flax,\n     require_librosa,\n@@ -350,11 +348,6 @@ def test_model_from_pretrained(self):\n             outputs = model(np.ones((1, 1024), dtype=\"f4\"))\n             self.assertIsNotNone(outputs)\n \n-    @is_pt_flax_cross_test\n-    @is_flaky()\n-    def test_equivalence_pt_to_flax(self):\n-        super().test_equivalence_pt_to_flax()\n-\n \n @require_flax\n class FlaxWav2Vec2UtilsTest(unittest.TestCase):"
        },
        {
            "sha": "e71c2d677a77210e5da03ca7c7847318c03cef34",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -31,7 +31,6 @@\n     CaptureLogger,\n     cleanup,\n     is_flaky,\n-    is_pt_flax_cross_test,\n     is_pyctcdecode_available,\n     is_torchaudio_available,\n     require_flash_attn,\n@@ -569,16 +568,6 @@ def test_resize_tokens_embeddings(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @is_pt_flax_cross_test\n-    @unittest.skip(reason=\"Non-rubst architecture does not exist in Flax\")\n-    def test_equivalence_flax_to_pt(self):\n-        pass\n-\n-    @is_pt_flax_cross_test\n-    @unittest.skip(reason=\"Non-rubst architecture does not exist in Flax\")\n-    def test_equivalence_pt_to_flax(self):\n-        pass\n-\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.output_hidden_states = True"
        },
        {
            "sha": "fc563df8f195b53bd820a85308a89aa4b031dadd",
            "filename": "tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -21,7 +21,6 @@\n \n from transformers import Wav2Vec2BertConfig, is_torch_available\n from transformers.testing_utils import (\n-    is_pt_flax_cross_test,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -559,18 +558,6 @@ def test_resize_tokens_embeddings(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # Ignore copy\n-    @unittest.skip(reason=\"non-robust architecture does not exist in Flax\")\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        pass\n-\n-    # Ignore copy\n-    @unittest.skip(reason=\"non-robust architecture does not exist in Flax\")\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        pass\n-\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.output_hidden_states = True"
        },
        {
            "sha": "0c406bfbc820f1fc09371eaa02b1582ba481abd5",
            "filename": "tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -24,7 +24,6 @@\n from transformers import Wav2Vec2ConformerConfig, is_torch_available\n from transformers.testing_utils import (\n     is_flaky,\n-    is_pt_flax_cross_test,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -535,16 +534,6 @@ def test_resize_tokens_embeddings(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @is_pt_flax_cross_test\n-    @unittest.skip(reason=\"Non-robust architecture does not exist in Flax\")\n-    def test_equivalence_flax_to_pt(self):\n-        pass\n-\n-    @is_pt_flax_cross_test\n-    @unittest.skip(reason=\"Non-robust architecture does not exist in Flax\")\n-    def test_equivalence_pt_to_flax(self):\n-        pass\n-\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.output_hidden_states = True"
        },
        {
            "sha": "274582be23356c34fef2cbe2d4e3acdfc1757576",
            "filename": "tests/models/whisper/test_modeling_flax_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 111,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -17,9 +17,8 @@\n import tempfile\n import unittest\n \n-import transformers\n from transformers import WhisperConfig, is_flax_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, slow\n+from transformers.testing_utils import require_flax, slow\n from transformers.utils import cached_property\n from transformers.utils.import_utils import is_datasets_available\n \n@@ -45,7 +44,6 @@\n         WhisperFeatureExtractor,\n         WhisperProcessor,\n     )\n-    from transformers.modeling_flax_pytorch_utils import load_flax_weights_in_pytorch_model\n     from transformers.models.whisper.modeling_flax_whisper import sinusoidal_embedding_init\n \n \n@@ -245,99 +243,6 @@ def model_jitted(input_features, decoder_input_ids, **kwargs):\n                 for jitted_output, output in zip(jitted_outputs, outputs):\n                     self.assertEqual(jitted_output.shape, output.shape)\n \n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as test recently became flaky\n-        super().check_pt_flax_outputs(fx_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n-    # overwrite because of `input_features`\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = make_partial_class(FLAX_MODEL_MAPPING[config.__class__], input_shape=self.init_shape)\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == base_class.__name__:\n-                continue\n-\n-            model = model_class(config)\n-            model.params = model.to_bf16(model.params)\n-            base_params_from_head = flatten_dict(unfreeze(model.params[model.base_model_prefix]))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite because of `input_features`\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = make_partial_class(FLAX_MODEL_MAPPING[config.__class__], input_shape=self.init_shape)\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == base_class.__name__:\n-                continue\n-\n-            model = base_class(config)\n-            base_params = flatten_dict(unfreeze(model.params))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, base_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                # save pt model\n-                pt_model.save_pretrained(tmpdirname)\n-                head_model = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_param_from_head = flatten_dict(unfreeze(head_model.params[head_model.base_model_prefix]))\n-\n-                for key in base_param_from_head.keys():\n-                    max_diff = (base_params[key] - base_param_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    # overwrite because of `input_features`\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = make_partial_class(FLAX_MODEL_MAPPING[config.__class__], input_shape=self.init_shape)\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == base_class.__name__:\n-                continue\n-\n-            model = model_class(config)\n-            base_params_from_head = flatten_dict(unfreeze(model.params[model.base_model_prefix]))\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = flatten_dict(unfreeze(base_model.params))\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n     # overwrite because of `input_features`\n     def test_save_load_from_base(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -899,18 +804,3 @@ def test_save_load_to_base(self):\n     # WhisperEncoder does not have any base model\n     def test_save_load_from_base(self):\n         pass\n-\n-    # WhisperEncoder does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        pass\n-\n-    # WhisperEncoder does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        pass\n-\n-    # WhisperEncoder does not have any base model\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        pass"
        },
        {
            "sha": "1a38b5b225f6040ee0ed6e4694cc15815c6ad038",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 317,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -32,7 +32,6 @@\n from transformers import WhisperConfig\n from transformers.testing_utils import (\n     is_flaky,\n-    is_pt_flax_cross_test,\n     require_flash_attn,\n     require_non_xpu,\n     require_torch,\n@@ -44,7 +43,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_flax_available, is_torch_available, is_torchaudio_available\n+from transformers.utils import cached_property, is_torch_available, is_torchaudio_available\n from transformers.utils.import_utils import is_datasets_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -155,15 +154,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n     import torchaudio\n \n \n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n-\n def prepare_whisper_inputs_dict(\n     config,\n     input_features,\n@@ -1069,161 +1059,6 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n \n             self.assertTrue(models_equal)\n \n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n-        # We override with a slightly higher tol value, as test recently became flaky\n-        super().check_pt_flax_outputs(fx_outputs, pt_outputs, model_class, tol, name, attributes)\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        init_shape = (1,) + inputs_dict[\"input_features\"].shape[1:]\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, input_shape=init_shape, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = fx_model_class.from_pretrained(tmpdirname, input_shape=init_shape, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs_loaded.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs_loaded, pt_outputs, model_class)\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        init_shape = (1,) + inputs_dict[\"input_features\"].shape[1:]\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, input_shape=init_shape, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                # send pytorch model to the correct device\n-                pt_model_loaded.to(torch_device)\n-                pt_model_loaded.eval()\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs_loaded.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs_loaded, model_class)\n-\n     def test_mask_feature_prob(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.mask_feature_prob = 0.2\n@@ -3622,157 +3457,6 @@ def test_model_get_set_embeddings(self):\n     def test_resize_tokens_embeddings(self):\n         pass\n \n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        init_shape = (1,) + inputs_dict[\"input_features\"].shape[1:]\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"Flax model does not exist\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, input_shape=init_shape, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = fx_model_class.from_pretrained(tmpdirname, input_shape=init_shape, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs_loaded.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs_loaded, pt_outputs, model_class)\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        init_shape = (1,) + inputs_dict[\"input_features\"].shape[1:]\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(\"Flax model does not exist\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, input_shape=init_shape, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                # send pytorch model to the correct device\n-                pt_model_loaded.to(torch_device)\n-                pt_model_loaded.eval()\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs_loaded.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs_loaded, model_class)\n-\n \n class WhisperStandaloneDecoderModelTester:\n     def __init__("
        },
        {
            "sha": "0eaf5d46af9e36a88eb60467a43a21e65d371708",
            "filename": "tests/models/xglm/test_modeling_flax_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 114,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -14,12 +14,10 @@\n # limitations under the License.\n \n \n-import tempfile\n import unittest\n \n-import transformers\n-from transformers import XGLMConfig, XGLMTokenizer, is_flax_available, is_torch_available\n-from transformers.testing_utils import is_pt_flax_cross_test, require_flax, require_sentencepiece, slow\n+from transformers import XGLMConfig, XGLMTokenizer, is_flax_available\n+from transformers.testing_utils import require_flax, require_sentencepiece, slow\n \n from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n \n@@ -29,17 +27,9 @@\n     import jax.numpy as jnp\n     import numpy as np\n \n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.models.xglm.modeling_flax_xglm import FlaxXGLMForCausalLM, FlaxXGLMModel\n \n \n-if is_torch_available():\n-    import torch\n-\n-\n @require_flax\n class FlaxXGLMModelTester:\n     def __init__(\n@@ -220,108 +210,6 @@ def test_batch_generation(self):\n \n         self.assertListEqual(output_string, expected_string)\n \n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-                pt_model = pt_model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-                fx_model = model_class(config, dtype=jnp.float32)\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(\n-                    len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-                    self.assert_almost_equals(fx_output_loaded[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-    # overwrite from common since `attention_mask` in combination\n-    # with `causal_mask` behaves slighly differently\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist()) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                pt_model.config.use_cache = False\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-                batch_size, seq_length = pt_inputs[\"input_ids\"].shape\n-                rnd_start_indices = np.random.randint(0, seq_length - 1, size=(batch_size,))\n-                for batch_idx, start_index in enumerate(rnd_start_indices):\n-                    pt_inputs[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    pt_inputs[\"attention_mask\"][batch_idx, start_index:] = 1\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, :start_index] = 0\n-                    prepared_inputs_dict[\"attention_mask\"][batch_idx, start_index:] = 1\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs).to_tuple()\n-\n-                fx_outputs = fx_model(**prepared_inputs_dict).to_tuple()\n-                self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(tmpdirname, from_flax=True)\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n-\n-                self.assertEqual(\n-                    len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n-                )\n-                for fx_output, pt_output in zip(fx_outputs, pt_outputs_loaded):\n-                    self.assert_almost_equals(fx_output[:, -1], pt_output[:, -1].numpy(), 4e-2)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         for model_class_name in self.all_model_classes:"
        },
        {
            "sha": "8fc7a409a454b29a8d36e7cc0e13d7143d312181",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 284,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -32,7 +32,6 @@\n from parameterized import parameterized\n from pytest import mark\n \n-import transformers\n from transformers import (\n     AutoModel,\n     AutoModelForCausalLM,\n@@ -75,7 +74,6 @@\n from transformers.testing_utils import (\n     CaptureLogger,\n     is_flaky,\n-    is_pt_flax_cross_test,\n     require_accelerate,\n     require_bitsandbytes,\n     require_deepspeed,\n@@ -100,14 +98,12 @@\n     GENERATION_CONFIG_NAME,\n     SAFE_WEIGHTS_NAME,\n     is_accelerate_available,\n-    is_flax_available,\n-    is_tf_available,\n     is_torch_bf16_available_on_device,\n     is_torch_fp16_available_on_device,\n     is_torch_fx_available,\n     is_torch_sdpa_available,\n )\n-from transformers.utils.generic import ContextManagers, ModelOutput\n+from transformers.utils.generic import ContextManagers\n \n \n if is_accelerate_available():\n@@ -126,19 +122,6 @@\n     from transformers.modeling_utils import load_state_dict, no_init_weights\n     from transformers.pytorch_utils import id_tensor_storage\n \n-\n-if is_tf_available():\n-    pass\n-\n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from tests.utils.test_modeling_flax_utils import check_models_equal\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n-\n if is_torch_fx_available():\n     from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n \n@@ -2552,249 +2535,6 @@ def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n         diff = np.abs((a - b)).max()\n         self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n \n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        \"\"\"\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, ..., etc.\n-            Currently unused, but it could make debugging easier and faster.\n-\n-            names: A string, or a list of strings. These specify what fx_outputs/pt_outputs represent in the model outputs.\n-                Currently unused, but in the future, we could use this information to make the error message clearer\n-                by giving the name(s) of the output tensor(s) with large difference(s) between PT and Flax.\n-        \"\"\"\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(fx_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `fx_outputs` is\",\n-            )\n-\n-            fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-            pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-            self.assertEqual(fx_keys, pt_keys, f\"{name}: Output keys differ between Flax and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `name`\n-            attributes = tuple([f\"{name}.{k}\" for k in fx_keys])\n-            self.check_pt_flax_outputs(\n-                fx_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(fx_outputs) in [tuple, list]:\n-            self.assertEqual(\n-                type(fx_outputs), type(pt_outputs), f\"{name}: Output types differ between Flax and PyTorch\"\n-            )\n-            self.assertEqual(\n-                len(fx_outputs), len(pt_outputs), f\"{name}: Output lengths differ between Flax and PyTorch\"\n-            )\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(fx_outputs),\n-                    f\"{name}: The tuple `attributes` should have the same length as `fx_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(fx_outputs))])\n-\n-            for fx_output, pt_output, attr in zip(fx_outputs, pt_outputs, attributes):\n-                self.check_pt_flax_outputs(fx_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(fx_outputs, jnp.ndarray):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `fx_outputs` is\"\n-            )\n-\n-            # Using `np.asarray` gives `ValueError: assignment destination is read-only` at the line `fx_outputs[fx_nans] = 0`.\n-            fx_outputs = np.array(fx_outputs)\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                fx_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between Flax and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(fx_outputs):\n-                fx_outputs = np.array([fx_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            fx_nans = np.isnan(fx_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[fx_nans] = 0\n-            fx_outputs[fx_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            fx_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(fx_outputs - pt_outputs))\n-            self.assertLessEqual(\n-                max_diff, tol, f\"{name}: Difference between PyTorch and Flax is {max_diff} (>= {tol}).\"\n-            )\n-        else:\n-            raise ValueError(\n-                \"`fx_outputs` should be an instance of `ModelOutput`, a `tuple`, or an instance of `jnp.ndarray`. Got\"\n-                f\" {type(fx_outputs)} instead.\"\n-            )\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = fx_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs_loaded.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs_loaded, pt_outputs, model_class)\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                fx_model_class_name = \"Flax\" + model_class.__name__\n-\n-                if not hasattr(transformers, fx_model_class_name):\n-                    self.skipTest(reason=\"No Flax model exists for this class\")\n-\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                fx_model_class = getattr(transformers, fx_model_class_name)\n-\n-                # load PyTorch class\n-                pt_model = model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-\n-                # load Flax class\n-                fx_model = fx_model_class(config, dtype=jnp.float32)\n-\n-                # make sure only flax inputs are forward that actually exist in function args\n-                fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n-\n-                # prepare inputs\n-                pt_inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                # remove function args that don't exist in Flax\n-                pt_inputs = {k: v for k, v in pt_inputs.items() if k in fx_input_keys}\n-\n-                # send pytorch inputs to the correct device\n-                pt_inputs = {\n-                    k: v.to(device=torch_device) if isinstance(v, torch.Tensor) else v for k, v in pt_inputs.items()\n-                }\n-\n-                # convert inputs to Flax\n-                fx_inputs = {k: np.array(v.to(\"cpu\")) for k, v in pt_inputs.items() if torch.is_tensor(v)}\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**fx_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = model_class.from_pretrained(\n-                        tmpdirname, from_flax=True, attn_implementation=fx_model.config._attn_implementation\n-                    )\n-\n-                # send pytorch model to the correct device\n-                pt_model_loaded.to(torch_device)\n-                pt_model_loaded.eval()\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs_loaded.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs_loaded, model_class)\n-\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -4413,29 +4153,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 tol = torch.finfo(torch.float16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n-    @is_pt_flax_cross_test\n-    def test_flax_from_pt_safetensors(self):\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            flax_model_class_name = \"Flax\" + model_class.__name__  # Add the \"Flax at the beginning\n-            if not hasattr(transformers, flax_model_class_name):\n-                self.skipTest(reason=\"transformers does not have this model in Flax version yet\")\n-\n-            flax_model_class = getattr(transformers, flax_model_class_name)\n-\n-            pt_model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname, safe_serialization=True)\n-                flax_model_1 = flax_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                pt_model.save_pretrained(tmpdirname, safe_serialization=False)\n-                flax_model_2 = flax_model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                # Check models are equal\n-                self.assertTrue(check_models_equal(flax_model_1, flax_model_2))\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        },
        {
            "sha": "ab126357f58d87291d7eb746c3fc4a2d171c4583",
            "filename": "tests/test_modeling_flax_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 316,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_flax_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_flax_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_flax_common.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -21,13 +21,10 @@\n \n import numpy as np\n \n-import transformers\n-from transformers import is_flax_available, is_torch_available\n-from transformers.cache_utils import DynamicCache\n+from transformers import is_flax_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import CaptureLogger, is_pt_flax_cross_test, require_flax, torch_device\n+from transformers.testing_utils import CaptureLogger, require_flax\n from transformers.utils import CONFIG_NAME, GENERATION_CONFIG_NAME, logging\n-from transformers.utils.generic import ModelOutput\n \n \n if is_flax_available():\n@@ -47,17 +44,10 @@\n         FlaxAutoModelForSequenceClassification,\n         FlaxBertModel,\n     )\n-    from transformers.modeling_flax_pytorch_utils import (\n-        convert_pytorch_state_dict_to_flax,\n-        load_flax_weights_in_pytorch_model,\n-    )\n     from transformers.modeling_flax_utils import FLAX_WEIGHTS_INDEX_NAME, FLAX_WEIGHTS_NAME\n \n     os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.12\"  # assumed parallelism: 8\n \n-if is_torch_available():\n-    import torch\n-\n \n def ids_tensor(shape, vocab_size, rng=None):\n     \"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"\n@@ -184,216 +174,6 @@ def recursive_check(tuple_object, dict_object):\n             dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n             check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n \n-    # (Copied from tests.test_modeling_common.ModelTesterMixin.check_pt_flax_outputs)\n-    def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=1e-4, name=\"outputs\", attributes=None):\n-        \"\"\"\n-        Args:\n-            model_class: The class of the model that is currently testing. For example, ..., etc.\n-            Currently unused, but it could make debugging easier and faster.\n-\n-            names: A string, or a list of strings. These specify what fx_outputs/pt_outputs represent in the model outputs.\n-                Currently unused, but in the future, we could use this information to make the error message clearer\n-                by giving the name(s) of the output tensor(s) with large difference(s) between PT and Flax.\n-        \"\"\"\n-        self.assertEqual(type(name), str)\n-        if attributes is not None:\n-            self.assertEqual(type(attributes), tuple, f\"{name}: The argument `attributes` should be a `tuple`\")\n-\n-        # Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).\n-        if isinstance(fx_outputs, ModelOutput):\n-            self.assertTrue(\n-                isinstance(pt_outputs, ModelOutput),\n-                f\"{name}: `pt_outputs` should an instance of `ModelOutput` when `fx_outputs` is\",\n-            )\n-\n-            fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-            pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-            self.assertEqual(fx_keys, pt_keys, f\"{name}: Output keys differ between Flax and PyTorch\")\n-\n-            # convert to the case of `tuple`\n-            # appending each key to the current (string) `name`\n-            attributes = tuple([f\"{name}.{k}\" for k in fx_keys])\n-            self.check_pt_flax_outputs(\n-                fx_outputs.to_tuple(), pt_outputs.to_tuple(), model_class, tol=tol, name=name, attributes=attributes\n-            )\n-\n-        # Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)\n-        elif type(fx_outputs) in [tuple, list]:\n-            self.assertEqual(\n-                type(fx_outputs), type(pt_outputs), f\"{name}: Output types differ between Flax and PyTorch\"\n-            )\n-            self.assertEqual(\n-                len(fx_outputs), len(pt_outputs), f\"{name}: Output lengths differ between Flax and PyTorch\"\n-            )\n-\n-            if attributes is not None:\n-                # case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)\n-                self.assertEqual(\n-                    len(attributes),\n-                    len(fx_outputs),\n-                    f\"{name}: The tuple `attributes` should have the same length as `fx_outputs`\",\n-                )\n-            else:\n-                # case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`\n-                attributes = tuple([f\"{name}_{idx}\" for idx in range(len(fx_outputs))])\n-\n-            for fx_output, pt_output, attr in zip(fx_outputs, pt_outputs, attributes):\n-                if isinstance(pt_output, DynamicCache):\n-                    pt_output = pt_output.to_legacy_cache()\n-                self.check_pt_flax_outputs(fx_output, pt_output, model_class, tol=tol, name=attr)\n-\n-        elif isinstance(fx_outputs, jnp.ndarray):\n-            self.assertTrue(\n-                isinstance(pt_outputs, torch.Tensor), f\"{name}: `pt_outputs` should a tensor when `fx_outputs` is\"\n-            )\n-\n-            # Using `np.asarray` gives `ValueError: assignment destination is read-only` at the line `fx_outputs[fx_nans] = 0`.\n-            fx_outputs = np.array(fx_outputs)\n-            pt_outputs = pt_outputs.detach().to(\"cpu\").numpy()\n-\n-            self.assertEqual(\n-                fx_outputs.shape, pt_outputs.shape, f\"{name}: Output shapes differ between Flax and PyTorch\"\n-            )\n-\n-            # deal with NumPy's scalars to make replacing nan values by 0 work.\n-            if np.isscalar(fx_outputs):\n-                fx_outputs = np.array([fx_outputs])\n-                pt_outputs = np.array([pt_outputs])\n-\n-            fx_nans = np.isnan(fx_outputs)\n-            pt_nans = np.isnan(pt_outputs)\n-\n-            pt_outputs[fx_nans] = 0\n-            fx_outputs[fx_nans] = 0\n-            pt_outputs[pt_nans] = 0\n-            fx_outputs[pt_nans] = 0\n-\n-            max_diff = np.amax(np.abs(fx_outputs - pt_outputs))\n-            self.assertLessEqual(\n-                max_diff, tol, f\"{name}: Difference between PyTorch and Flax is {max_diff} (>= {tol}).\"\n-            )\n-        else:\n-            raise ValueError(\n-                \"`fx_outputs` should be an instance of `ModelOutput`, a `tuple`, or an instance of `jnp.ndarray`. Got\"\n-                f\" {type(fx_outputs)} instead.\"\n-            )\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_pt_to_flax(self):\n-        # It might be better to put this inside the for loop below (because we modify the config there).\n-        # But logically, it is fine.\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist(), device=torch_device) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                fx_state = convert_pytorch_state_dict_to_flax(pt_model.state_dict(), fx_model)\n-                fx_model.params = fx_state\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**prepared_inputs_dict)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    pt_model.save_pretrained(tmpdirname)\n-                    fx_model_loaded = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                fx_outputs_loaded = fx_model_loaded(**prepared_inputs_dict)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs_loaded.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs_loaded, pt_outputs, model_class)\n-\n-    @is_pt_flax_cross_test\n-    def test_equivalence_flax_to_pt(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                # Output all for aggressive testing\n-                config.output_hidden_states = True\n-                config.output_attentions = self.has_attentions\n-\n-                # prepare inputs\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                pt_inputs = {k: torch.tensor(v.tolist(), device=torch_device) for k, v in prepared_inputs_dict.items()}\n-\n-                # load corresponding PyTorch class\n-                pt_model_class_name = model_class.__name__[4:]  # Skip the \"Flax\" at the beginning\n-                pt_model_class = getattr(transformers, pt_model_class_name)\n-\n-                pt_model = pt_model_class(config).eval()\n-                # Flax models don't use the `use_cache` option and cache is not returned as a default.\n-                # So we disable `use_cache` here for PyTorch model.\n-                pt_model.config.use_cache = False\n-                fx_model = model_class(config, dtype=jnp.float32)\n-\n-                pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n-\n-                # make sure weights are tied in PyTorch\n-                pt_model.tie_weights()\n-\n-                # send pytorch model to the correct device\n-                pt_model.to(torch_device)\n-\n-                with torch.no_grad():\n-                    pt_outputs = pt_model(**pt_inputs)\n-                fx_outputs = fx_model(**prepared_inputs_dict)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs, model_class)\n-\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    fx_model.save_pretrained(tmpdirname)\n-                    pt_model_loaded = pt_model_class.from_pretrained(\n-                        tmpdirname, from_flax=True, attn_implementation=fx_model.config._attn_implementation\n-                    )\n-\n-                # send pytorch model to the correct device\n-                pt_model_loaded.to(torch_device)\n-                pt_model_loaded.eval()\n-\n-                with torch.no_grad():\n-                    pt_outputs_loaded = pt_model_loaded(**pt_inputs)\n-\n-                fx_keys = tuple([k for k, v in fx_outputs.items() if v is not None])\n-                pt_keys = tuple([k for k, v in pt_outputs_loaded.items() if v is not None])\n-\n-                self.assertEqual(fx_keys, pt_keys)\n-                self.check_pt_flax_outputs(fx_outputs, pt_outputs_loaded, model_class)\n-\n     def test_from_pretrained_save_pretrained(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -474,92 +254,6 @@ def test_save_load_to_base(self):\n                     max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n                     self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n \n-    @is_pt_flax_cross_test\n-    def test_save_load_from_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = base_class(config)\n-            base_params = get_params(model.params)\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, base_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                # save pt model\n-                pt_model.save_pretrained(tmpdirname)\n-                head_model = model_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_param_from_head = get_params(head_model.params, from_head_prefix=head_model.base_model_prefix)\n-\n-                for key in base_param_from_head.keys():\n-                    max_diff = (base_params[key] - base_param_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    @is_pt_flax_cross_test\n-    def test_save_load_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            base_params_from_head = get_params(model.params, from_head_prefix=model.base_model_prefix)\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = get_params(base_model.params)\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n-    @is_pt_flax_cross_test\n-    def test_save_load_bf16_to_base_pt(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        base_class = FLAX_MODEL_MAPPING[config.__class__]\n-\n-        for model_class in self.all_model_classes:\n-            if model_class == base_class:\n-                continue\n-\n-            model = model_class(config)\n-            model.params = model.to_bf16(model.params)\n-            base_params_from_head = get_params(model.params, from_head_prefix=model.base_model_prefix)\n-\n-            # convert Flax model to PyTorch model\n-            pt_model_class = getattr(transformers, model_class.__name__[4:])  # Skip the \"Flax\" at the beginning\n-            pt_model = pt_model_class(config).eval()\n-            pt_model = load_flax_weights_in_pytorch_model(pt_model, model.params)\n-\n-            # check that all base model weights are loaded correctly\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pt_model.save_pretrained(tmpdirname)\n-                base_model = base_class.from_pretrained(tmpdirname, from_pt=True)\n-\n-                base_params = get_params(base_model.params)\n-\n-                for key in base_params_from_head.keys():\n-                    max_diff = (base_params[key] - base_params_from_head[key]).sum().item()\n-                    self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")\n-\n     def test_jit_compilation(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -1119,14 +813,6 @@ def test_checkpoint_sharding_local(self):\n                 for p1, p2 in zip(flatten_dict(model.params).values(), flatten_dict(new_model.params).values()):\n                     self.assertTrue(np.allclose(np.array(p1), np.array(p2)))\n \n-    @is_pt_flax_cross_test\n-    def test_from_sharded_pt(self):\n-        model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-sharded\", from_pt=True)\n-        ref_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-fx-only\")\n-        for key, ref_val in flatten_dict(ref_model.params).items():\n-            val = flatten_dict(model.params)[key]\n-            assert np.allclose(np.array(val), np.array(ref_val))\n-\n     def test_gradient_checkpointing(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "fb6860bcc31ee0dd0faef40bfbc1a7e2f31c4dcf",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 17,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -31,13 +31,11 @@\n \n from transformers import is_tf_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import (  # noqa: F401\n+from transformers.testing_utils import (\n     CaptureLogger,\n-    _tf_gpu_memory_limit,\n     require_tf,\n     require_tf2onnx,\n     slow,\n-    torch_device,\n )\n from transformers.utils import CONFIG_NAME, GENERATION_CONFIG_NAME, logging\n from transformers.utils.generic import ModelOutput\n@@ -73,20 +71,6 @@\n \n     tf.config.experimental.enable_tensor_float_32_execution(False)\n \n-    if _tf_gpu_memory_limit is not None:\n-        gpus = tf.config.list_physical_devices(\"GPU\")\n-        for gpu in gpus:\n-            # Restrict TensorFlow to only allocate x GB of memory on the GPUs\n-            try:\n-                tf.config.set_logical_device_configuration(\n-                    gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=_tf_gpu_memory_limit)]\n-                )\n-                logical_gpus = tf.config.list_logical_devices(\"GPU\")\n-                print(\"Logical GPUs\", logical_gpus)\n-            except RuntimeError as e:\n-                # Virtual devices must be set before GPUs have been initialized\n-                print(e)\n-\n \n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)"
        },
        {
            "sha": "7a2c516132b8e720617802aa084c33f8751b9b7f",
            "filename": "tests/utils/test_modeling_flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 112,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_flax_utils.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -18,16 +18,14 @@\n import numpy as np\n from huggingface_hub import HfFolder, snapshot_download\n \n-from transformers import BertConfig, BertModel, is_flax_available, is_torch_available\n+from transformers import BertConfig, is_flax_available\n from transformers.testing_utils import (\n     TOKEN,\n     CaptureLogger,\n     TemporaryHubRepo,\n-    is_pt_flax_cross_test,\n     is_staging_test,\n     require_flax,\n     require_safetensors,\n-    require_torch,\n )\n from transformers.utils import FLAX_WEIGHTS_NAME, SAFE_WEIGHTS_NAME, logging\n \n@@ -42,9 +40,6 @@\n \n     os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.12\"  # assumed parallelism: 8\n \n-if is_torch_available():\n-    import torch\n-\n \n @require_flax\n @is_staging_test\n@@ -205,23 +200,6 @@ def test_safetensors_save_and_load(self):\n \n         self.assertTrue(check_models_equal(model, new_model))\n \n-    @require_flax\n-    @require_torch\n-    @is_pt_flax_cross_test\n-    def test_safetensors_save_and_load_pt_to_flax(self):\n-        model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\", from_pt=True)\n-        pt_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            pt_model.save_pretrained(tmp_dir)\n-\n-            # Check we have a model.safetensors file\n-            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_NAME)))\n-\n-            new_model = FlaxBertModel.from_pretrained(tmp_dir)\n-\n-        # Check models are equal\n-        self.assertTrue(check_models_equal(model, new_model))\n-\n     @require_safetensors\n     def test_safetensors_load_from_hub(self):\n         \"\"\"\n@@ -248,58 +226,6 @@ def test_safetensors_load_from_local(self):\n \n         self.assertTrue(check_models_equal(flax_model, safetensors_model))\n \n-    @require_safetensors\n-    @is_pt_flax_cross_test\n-    def test_safetensors_load_from_hub_from_safetensors_pt(self):\n-        \"\"\"\n-        This test checks that we can load safetensors from a checkpoint that only has those on the Hub.\n-        saved in the \"pt\" format.\n-        \"\"\"\n-        flax_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-msgpack\")\n-\n-        # Can load from the PyTorch-formatted checkpoint\n-        safetensors_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-safetensors\")\n-        self.assertTrue(check_models_equal(flax_model, safetensors_model))\n-\n-    @require_safetensors\n-    @require_torch\n-    @is_pt_flax_cross_test\n-    def test_safetensors_load_from_hub_from_safetensors_pt_bf16(self):\n-        \"\"\"\n-        This test checks that we can load safetensors from a checkpoint that only has those on the Hub.\n-        saved in the \"pt\" format.\n-        \"\"\"\n-        import torch\n-\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-safetensors\")\n-        model.to(torch.bfloat16)\n-\n-        with tempfile.TemporaryDirectory() as tmp:\n-            model.save_pretrained(tmp)\n-            flax_model = FlaxBertModel.from_pretrained(tmp)\n-\n-        # Can load from the PyTorch-formatted checkpoint\n-        safetensors_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-safetensors-bf16\")\n-        self.assertTrue(check_models_equal(flax_model, safetensors_model))\n-\n-    @require_safetensors\n-    @is_pt_flax_cross_test\n-    def test_safetensors_load_from_local_from_safetensors_pt(self):\n-        \"\"\"\n-        This test checks that we can load safetensors from a checkpoint that only has those on the Hub.\n-        saved in the \"pt\" format.\n-        \"\"\"\n-        with tempfile.TemporaryDirectory() as tmp:\n-            location = snapshot_download(\"hf-internal-testing/tiny-bert-msgpack\", cache_dir=tmp)\n-            flax_model = FlaxBertModel.from_pretrained(location)\n-\n-        # Can load from the PyTorch-formatted checkpoint\n-        with tempfile.TemporaryDirectory() as tmp:\n-            location = snapshot_download(\"hf-internal-testing/tiny-bert-pt-safetensors\", cache_dir=tmp)\n-            safetensors_model = FlaxBertModel.from_pretrained(location)\n-\n-        self.assertTrue(check_models_equal(flax_model, safetensors_model))\n-\n     @require_safetensors\n     def test_safetensors_load_from_hub_msgpack_before_safetensors(self):\n         \"\"\"\n@@ -328,19 +254,6 @@ def test_safetensors_flax_from_flax(self):\n \n         self.assertTrue(check_models_equal(model, new_model))\n \n-    @require_safetensors\n-    @require_torch\n-    @is_pt_flax_cross_test\n-    def test_safetensors_flax_from_torch(self):\n-        hub_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-flax-only\")\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n-            new_model = FlaxBertModel.from_pretrained(tmp_dir)\n-\n-        self.assertTrue(check_models_equal(hub_model, new_model))\n-\n     @require_safetensors\n     def test_safetensors_flax_from_sharded_msgpack_with_sharded_safetensors_local(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -370,27 +283,3 @@ def test_safetensors_from_pt_bf16(self):\n             \"Some of the weights of FlaxBertModel were initialized in bfloat16 precision from the model checkpoint\"\n             in cl.out\n         )\n-\n-    @require_torch\n-    @require_safetensors\n-    @is_pt_flax_cross_test\n-    def test_from_pt_bf16(self):\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n-        model.to(torch.bfloat16)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=False)\n-\n-            logger = logging.get_logger(\"transformers.modeling_flax_utils\")\n-\n-            with CaptureLogger(logger) as cl:\n-                new_model = FlaxBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-safetensors-bf16\")\n-\n-            self.assertTrue(\n-                \"Some of the weights of FlaxBertModel were initialized in bfloat16 precision from the model checkpoint\"\n-                in cl.out\n-            )\n-\n-            flat_params_1 = flatten_dict(new_model.params)\n-            for value in flat_params_1.values():\n-                self.assertEqual(value.dtype, \"bfloat16\")"
        },
        {
            "sha": "c1cfc469697b1bdda02019fe24cf87c4609f598c",
            "filename": "tests/utils/test_modeling_tf_core.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_tf_core.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_tf_core.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_tf_core.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -24,7 +24,7 @@\n \n from transformers import is_tf_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import _tf_gpu_memory_limit, require_tf, slow\n+from transformers.testing_utils import require_tf, slow\n \n from ..test_modeling_tf_common import ids_tensor\n \n@@ -48,20 +48,6 @@\n     )\n     from transformers.modeling_tf_utils import keras\n \n-    if _tf_gpu_memory_limit is not None:\n-        gpus = tf.config.list_physical_devices(\"GPU\")\n-        for gpu in gpus:\n-            # Restrict TensorFlow to only allocate x GB of memory on the GPUs\n-            try:\n-                tf.config.set_logical_device_configuration(\n-                    gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=_tf_gpu_memory_limit)]\n-                )\n-                logical_gpus = tf.config.list_logical_devices(\"GPU\")\n-                print(\"Logical GPUs\", logical_gpus)\n-            except RuntimeError as e:\n-                # Virtual devices must be set before GPUs have been initialized\n-                print(e)\n-\n \n @require_tf\n class TFCoreModelTesterMixin:"
        },
        {
            "sha": "116af748e6f9b6fc301dcd7459e429ea5503de39",
            "filename": "tests/utils/test_modeling_tf_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_tf_utils.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -33,7 +33,6 @@\n     USER,\n     CaptureLogger,\n     TemporaryHubRepo,\n-    _tf_gpu_memory_limit,\n     is_staging_test,\n     require_safetensors,\n     require_tf,\n@@ -68,20 +67,6 @@\n \n     tf.config.experimental.enable_tensor_float_32_execution(False)\n \n-    if _tf_gpu_memory_limit is not None:\n-        gpus = tf.config.list_physical_devices(\"GPU\")\n-        for gpu in gpus:\n-            # Restrict TensorFlow to only allocate x GB of memory on the GPUs\n-            try:\n-                tf.config.set_logical_device_configuration(\n-                    gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=_tf_gpu_memory_limit)]\n-                )\n-                logical_gpus = tf.config.list_logical_devices(\"GPU\")\n-                print(\"Logical GPUs\", logical_gpus)\n-            except RuntimeError as e:\n-                # Virtual devices must be set before GPUs have been initialized\n-                print(e)\n-\n \n @require_tf\n class TFModelUtilsTest(unittest.TestCase):"
        },
        {
            "sha": "3543b18f9c22e2d93054b7db7c274ecd11c73062",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99adc7446236b6654c8949df29d4c80a141f4683/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99adc7446236b6654c8949df29d4c80a141f4683/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=99adc7446236b6654c8949df29d4c80a141f4683",
            "patch": "@@ -1148,7 +1148,6 @@ def parse_commit_message(commit_message: str) -> Dict[str, bool]:\n \n \n JOB_TO_TEST_FILE = {\n-    \"tests_torch_and_flax\": r\"tests/models/.*/test_modeling_(?:flax|(?!tf)).*\",\n     \"tests_tf\": r\"tests/models/.*/test_modeling_tf_.*\",\n     \"tests_torch\": r\"tests/models/.*/test_modeling_(?!(?:flax_|tf_)).*\",\n     \"tests_generate\": r\"tests/models/.*/test_modeling_(?!(?:flax_|tf_)).*\","
        }
    ],
    "stats": {
        "total": 3136,
        "additions": 33,
        "deletions": 3103
    }
}