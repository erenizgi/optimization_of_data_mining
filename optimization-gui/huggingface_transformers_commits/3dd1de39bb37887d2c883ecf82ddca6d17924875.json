{
    "author": "zucchini-nlp",
    "message": "Paligemma: fix generation with Gemma2 (#36044)\n\n* fix paligemma\n\n* nit\n\n* use `kwargs` in models that can load any LM",
    "sha": "3dd1de39bb37887d2c883ecf82ddca6d17924875",
    "files": [
        {
            "sha": "36f212e76844e7b480acbdfcb9ebe4c15011b737",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -425,6 +425,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -520,6 +521,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "06e1cc63940faf3531f1cf470ea0cd8772e8ac1f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -794,6 +794,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -896,6 +897,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "f62824947ddf342540319602d8319b129bf43484",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -829,6 +829,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -991,6 +992,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "b2e06c337c1b4c3ee0a64835080a58b7c52c1b28",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -360,6 +360,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -522,6 +523,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "ed584bda7f5dbdcebb470891a217f59a3ae3735e",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -619,6 +619,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -766,6 +767,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "b6dab1830c85385c2bbcceddec8e3019aad5b3f5",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -342,8 +342,7 @@ def _update_causal_mask(\n         token_type_ids,\n         past_key_values,\n         cache_position,\n-        input_ids=None,\n-        inputs_embeds=None,\n+        input_tensor,\n         is_training: bool = False,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n@@ -353,8 +352,7 @@ def _update_causal_mask(\n \n         using_static_cache = isinstance(past_key_values, StaticCache)\n         min_dtype = torch.finfo(self.dtype).min\n-        inputs_lead_dim = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]\n-        sequence_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         elif isinstance(past_key_values, HybridCache):\n@@ -432,6 +430,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -524,7 +523,7 @@ def forward(\n             labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n \n         causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n+            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n         )\n         outputs = self.language_model(\n             attention_mask=causal_mask,\n@@ -537,6 +536,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs.logits\n@@ -612,10 +612,12 @@ def prepare_inputs_for_generation(\n             model_inputs[\"pixel_values\"] = pixel_values\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n             causal_mask = self._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n+                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "d8da974b9862cfa9e35145dcacbd8a98e9e764ec",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -463,6 +463,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -616,6 +617,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "71201db2098ec828b69e923d64354662425759a3",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -400,6 +400,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n     ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -490,6 +491,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/paligemma2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/tests%2Fmodels%2Fpaligemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/tests%2Fmodels%2Fpaligemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2F__init__.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875"
        },
        {
            "sha": "4a87eb329ddc2063983e4dfc1b6ca3b87a693f6e",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "added",
            "additions": 350,
            "deletions": 0,
            "changes": 350,
            "blob_url": "https://github.com/huggingface/transformers/blob/3dd1de39bb37887d2c883ecf82ddca6d17924875/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3dd1de39bb37887d2c883ecf82ddca6d17924875/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=3dd1de39bb37887d2c883ecf82ddca6d17924875",
            "patch": "@@ -0,0 +1,350 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch PaliGemma model.\"\"\"\n+\n+import unittest\n+\n+from transformers import (\n+    PaliGemmaConfig,\n+    PaliGemmaForConditionalGeneration,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    pass\n+\n+\n+class PaliGemma2VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        image_token_index=0,\n+        projector_hidden_act=\"gelu\",\n+        seq_length=25,\n+        vision_feature_select_strategy=\"default\",\n+        vision_feature_layer=-1,\n+        projection_dim=32,\n+        text_config={\n+            \"model_type\": \"gemma2\",\n+            \"seq_length\": 128,\n+            \"is_training\": True,\n+            # \"use_input_mask\": True,\n+            \"use_token_type_ids\": False,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 1,\n+            \"head_dim\": 8,\n+            \"intermediate_size\": 37,\n+            \"hidden_activation\": \"gelu_pytorch_tanh\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 512,\n+            \"type_vocab_size\": 16,\n+            \"type_sequence_label_size\": 2,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"num_choices\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"use_labels\": True,\n+            \"image_size\": 20,\n+            \"patch_size\": 5,\n+            \"num_image_tokens\": 4,\n+            \"num_channels\": 3,\n+            \"is_training\": True,\n+            \"hidden_size\": 32,\n+            \"projection_dim\": 32,\n+            \"num_key_value_heads\": 1,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"dropout\": 0.1,\n+            \"attention_dropout\": 0.1,\n+            \"initializer_range\": 0.02,\n+        },\n+        use_cache=False,\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        # `image_token_index` is set to 0 to pass \"resize_embeddings\" test, do not modify\n+        self.image_token_index = image_token_index\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.seq_length = seq_length\n+        self.projection_dim = projection_dim\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_channels = vision_config[\"num_channels\"]\n+        self.image_size = vision_config[\"image_size\"]\n+        self.encoder_seq_length = seq_length\n+        self.use_cache = use_cache\n+\n+    def get_config(self):\n+        return PaliGemmaConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            ignore_index=self.ignore_index,\n+            image_token_index=self.image_token_index,\n+            projector_hidden_act=self.projector_hidden_act,\n+            projection_dim=self.projection_dim,\n+            vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            vision_feature_layer=self.vision_feature_layer,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.vision_config[\"num_channels\"],\n+                self.vision_config[\"image_size\"],\n+                self.vision_config[\"image_size\"],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n+\n+        # set the 16 first tokens to be image, and ensure that no other tokens are image tokens\n+        # do not change this unless you modified image size or patch size\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, :16] = config.image_token_index\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"labels\": input_ids,\n+            \"token_type_ids\": torch.zeros_like(input_ids),\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class PaliGemma2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `PaliGemmaForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n+    fx_compatible = False\n+    test_pruning = False\n+    test_torchscript = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = PaliGemma2VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=PaliGemmaConfig, has_text_modality=False)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_model_parallelism(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"PaliGemmma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n+    )\n+    def test_initialization(self):\n+        pass\n+\n+    # TODO extend valid outputs to include this test @Molbap\n+    @unittest.skip(reason=\"PaliGemma has currently one output format.\")\n+    def test_model_outputs_equivalence(self):\n+        pass\n+\n+    # TODO fix the loss = nan in the testing configuration chosen @Molbap\n+    @unittest.skip(reason=\"Edge case giving loss nan values in testing configuration.\")\n+    def test_determinism(self):\n+        pass\n+\n+    @unittest.skip(reason=\"PaliGemma does not use feedforward chunking.\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n+    def test_save_load_low_cpu_mem_usage(self):\n+        pass\n+\n+    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n+    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n+        pass\n+\n+    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n+    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"VLMs doen't accept inputs embeds and pixel values at the same time. So if the test passed for bacbone LM, it passes for VLM also\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n+    @unittest.skip(\"PaliGemma is not compatible with end-to-end generation compilation\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n+    @unittest.skip(\"Low memory will be removed soon so no need to fix it\")\n+    def test_beam_search_low_memory(self):\n+        pass"
        }
    ],
    "stats": {
        "total": 378,
        "additions": 372,
        "deletions": 6
    }
}