{
    "author": "zucchini-nlp",
    "message": "Single config attribute for weight tying (#42815)\n\n* merge two attr into one\n\n* delete tie encoder decoder\n\n* one more\n\n* mt5\n\n* skip tests when tying is hardcoded\n\n* change test value to True, so we don't have to adjust hardcoded configs\n\n* awful decision in t5 to support two variants\n\n* delete my comment\n\n* not copied anymore\n\n* skip\n\n* they all had a shared embedding which was hardcoded, force it\n\n* force it in umt5 also, my model won't work otherwise :(\n\n* skip the key\n\n* dont't force if official weights set it to True\n\n* skip one test and fix teh other",
    "sha": "9a90500b0b7a3f481c005a1398ab50539146dfbc",
    "files": [
        {
            "sha": "544f6f54ad18eec7f790fac30357d7e0dbf088d4",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -125,9 +125,6 @@ class PreTrainedConfig(PushToHubMixin, RotaryEmbeddingConfigMixin):\n             Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n             that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models\n             in `AUTO_MODELS_FOR_CAUSAL_LM`.\n-        tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n-            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n-            and decoder model to have the exact same parameter names.\n         chunk_size_feed_forward (`int`, *optional*, defaults to `0`):\n             The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that\n             the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <\n@@ -217,7 +214,6 @@ def __init__(\n         is_decoder: bool = False,\n         cross_attention_hidden_size: Optional[int] = None,\n         add_cross_attention: bool = False,\n-        tie_encoder_decoder: bool = False,\n         # Fine-tuning task arguments\n         architectures: Optional[list[str]] = None,\n         finetuning_task: Optional[str] = None,\n@@ -281,6 +277,10 @@ def __init__(\n         self._output_attentions = output_attentions  # has public property\n \n         # Less common kwargs, only used by some models\n+        if \"tie_encoder_decoder\" in kwargs:\n+            tie_encoder_decoder = kwargs.pop(\"tie_encoder_decoder\")\n+            tie_word_embeddings = tie_encoder_decoder or tie_word_embeddings\n+\n         self.tie_word_embeddings = tie_word_embeddings\n         self.chunk_size_feed_forward = chunk_size_feed_forward\n \n@@ -289,7 +289,6 @@ def __init__(\n         self.is_decoder = is_decoder  # used in encoder-decoder models to differentiate encoder from decoder\n         self.cross_attention_hidden_size = cross_attention_hidden_size\n         self.add_cross_attention = add_cross_attention\n-        self.tie_encoder_decoder = tie_encoder_decoder\n \n         # Fine-tuning task attributes\n         self.architectures = architectures"
        },
        {
            "sha": "1e5c0a7cf8cc03c80af026eb9578834a05d3689c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -2350,7 +2350,7 @@ def get_expanded_tied_weights_keys(self, all_submodels: bool = False) -> dict:\n \n         tied_mapping = self._tied_weights_keys\n         # If the config does not specify any tying, return empty dict\n-        if not self.config.tie_word_embeddings and not self.config.tie_encoder_decoder:\n+        if not self.config.tie_word_embeddings:\n             return {}\n         # If None, return empty dict\n         elif tied_mapping is None:"
        },
        {
            "sha": "758d02aee47b4fffc5a1637ae823e9d9b0bbc433",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -157,7 +157,6 @@ def __init__(\n             decoder_start_token_id=decoder_start_token_id,\n             **kwargs,\n         )\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"BartConfig\"]"
        },
        {
            "sha": "21c36cc0197d7900fbbfc77cdd203f3ac7d6e9e1",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -394,8 +394,8 @@ def __init__(\n             raise ValueError(\n                 f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n             )\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"DFineConfig\"]"
        },
        {
            "sha": "70f17817705ffb4257c803791c13abe59c1f0db2",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -414,8 +414,8 @@ def __init__(\n             raise ValueError(\n                 f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n             )\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n class DFineMultiscaleDeformableAttention(nn.Module):"
        },
        {
            "sha": "8d3c03fe9e95580b7f37c177a2c39a4b26aacde4",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -255,8 +255,8 @@ def __init__(\n         self.temperature_height = temperature_height\n         self.sine_position_embedding_scale = sine_position_embedding_scale\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True  # weights have to be tied for this model\n \n \n __all__ = [\"DabDetrConfig\"]"
        },
        {
            "sha": "c53b256e2a8214957b9cea39776f2847662cd785",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -269,8 +269,8 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         self.focal_alpha = focal_alpha\n         self.disable_custom_kernels = disable_custom_kernels\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"DeformableDetrConfig\"]"
        },
        {
            "sha": "7452c7d9616bcead5992c31eaa36db2b70bd3655",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -285,9 +285,8 @@ def __init__(\n         self.positional_embedding_temperature = positional_embedding_temperature\n         self.init_std = init_std\n         self.layer_norm_eps = layer_norm_eps\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"GroundingDinoConfig\"]"
        },
        {
            "sha": "4065e91eee39cf5268ae0a324a1d9a9a243e7835",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -1583,12 +1583,10 @@ def __init__(self, config: LongT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = LongT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = LongT5Stack(decoder_config)\n \n@@ -1746,12 +1744,10 @@ def __init__(self, config: LongT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = LongT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = LongT5Stack(decoder_config)\n "
        },
        {
            "sha": "6307f114ea5127b1abf464dc92fc4daf0688a0f1",
            "filename": "src/transformers/models/marian/configuration_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -147,7 +147,7 @@ def __init__(\n         self.num_hidden_layers = encoder_layers\n         self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n         self.share_encoder_decoder_embeddings = share_encoder_decoder_embeddings\n-        kwargs[\"tie_encoder_decoder\"] = share_encoder_decoder_embeddings\n+        kwargs[\"tie_word_embeddings\"] = share_encoder_decoder_embeddings\n         super().__init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "78460183741906a2428f07826406b8b1b45076ff",
            "filename": "src/transformers/models/mbart/configuration_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -147,6 +147,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.num_hidden_layers = encoder_layers\n         self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "59e0e1d4fc449b2a9a16c61e541f01cc5ebb4b53",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -280,7 +280,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n \n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"MMGroundingDinoConfig\"]"
        },
        {
            "sha": "0897859d78098c5114f4f735a740a565372bdab7",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -293,7 +293,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n \n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n class MMGroundingDinoContrastiveEmbedding(GroundingDinoContrastiveEmbedding):"
        },
        {
            "sha": "15c9dc9e16169bd3b5d97f9140a8501944073b54",
            "filename": "src/transformers/models/mt5/configuration_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -133,17 +133,16 @@ def __init__(\n         if feed_forward_proj == \"gated-gelu\":\n             self.dense_act_fn = \"gelu_new\"\n \n+        # Force because official weights have False serialized, but we have to tie always\n+        kwargs[\"tie_word_embeddings\"] = True\n         super().__init__(\n             is_encoder_decoder=is_encoder_decoder,\n             tokenizer_class=tokenizer_class,\n-            tie_word_embeddings=tie_word_embeddings,\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,\n             decoder_start_token_id=decoder_start_token_id,\n             **kwargs,\n         )\n-        # TODO: Mt5 never supported not tying encoder decoder so this has to be true.\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"MT5Config\"]"
        },
        {
            "sha": "e456f35c82652184a1a7fdd6704cbc73eac62d2e",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -860,12 +860,10 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config)\n \n@@ -1043,12 +1041,10 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config)\n \n@@ -1066,7 +1062,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.decoder.set_input_embeddings(new_embeddings)\n \n     @auto_docstring\n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1184,9 +1179,6 @@ def forward(\n \n         sequence_output = decoder_outputs[0]\n \n-        if self.config.tie_word_embeddings:\n-            sequence_output = sequence_output * (self.model_dim**-0.5)\n-\n         lm_logits = self.lm_head(sequence_output)\n \n         loss = None\n@@ -1551,12 +1543,10 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config)\n "
        },
        {
            "sha": "f0d45f40c702ed6c1cea842ca0ff5ff795139455",
            "filename": "src/transformers/models/nllb_moe/configuration_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconfiguration_nllb_moe.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -206,6 +206,7 @@ def __init__(\n         self.moe_eval_capacity_token_fraction = moe_eval_capacity_token_fraction\n         self.moe_token_dropout = moe_token_dropout\n         self.output_router_logits = output_router_logits\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "3ca7c6730344497aadb54b4a8c10ce1d1cf74a5d",
            "filename": "src/transformers/models/pegasus/configuration_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -143,6 +143,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.num_hidden_layers = encoder_layers\n         self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "f877976748c9f0d8d64b6ab385e953b201b8df29",
            "filename": "src/transformers/models/plbart/configuration_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -151,6 +151,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.num_hidden_layers = encoder_layers\n         self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "135b4185d162de90d98367d46411de71425ef9a1",
            "filename": "src/transformers/models/pop2piano/configuration_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -122,7 +122,6 @@ def __init__(\n             is_encoder_decoder=is_encoder_decoder,\n             **kwargs,\n         )\n-        self.tie_encoder_decoder = True  # forcing it\n \n \n __all__ = [\"Pop2PianoConfig\"]"
        },
        {
            "sha": "cd5cc27bd21e833db73a373c259c52a4f5f2423d",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -356,8 +356,8 @@ def __init__(\n         self.decoder_n_levels = decoder_n_levels\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n __all__ = [\"RTDetrV2Config\"]"
        },
        {
            "sha": "6a9d8a4ab85bfc24b0c61167e9cf6332d5959c7e",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -368,8 +368,8 @@ def __init__(\n         self.decoder_n_levels = decoder_n_levels\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n-        self.tie_encoder_decoder = True\n \n \n def multi_scale_deformable_attention_v2("
        },
        {
            "sha": "17bce4b29933f2596afbe507610e4390c8de7eaf",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -922,12 +922,10 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         self.decoder = SwitchTransformersStack(decoder_config)\n \n         # Initialize weights and apply final processing\n@@ -1070,12 +1068,10 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = SwitchTransformersStack(decoder_config)\n "
        },
        {
            "sha": "0c9bd113b878b1ff643dd93bfc05adf758e22f5c",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -678,12 +678,10 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         self.decoder = SwitchTransformersStack(decoder_config)\n \n         # Initialize weights and apply final processing\n@@ -761,12 +759,10 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = SwitchTransformersStack(decoder_config)\n "
        },
        {
            "sha": "62b6925d2065f1fbdf0006e0d2eab22d165acebc",
            "filename": "src/transformers/models/t5/configuration_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -131,13 +131,19 @@ def __init__(\n         if feed_forward_proj == \"gated-gelu\":\n             self.dense_act_fn = \"gelu_new\"\n \n+        # Super weird feature of T5 because we support T5 and T51.1 from the same\n+        # model code. Original T5 always scaled outputs, but the 1.1v does not.\n+        # The model code was relying on saved configs where `tie_word_embeddings` is\n+        # set to `False` in 1.1v and using it as indicator of whether to scale or not\n+        # But in fact we tie weights always and force it to be `True`\n+        self.scale_decoder_outputs = kwargs.get(\"tie_word_embeddings\") is not False\n+        kwargs[\"tie_word_embeddings\"] = True\n         super().__init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,\n             is_encoder_decoder=is_encoder_decoder,\n             **kwargs,\n         )\n-        self.tie_encoder_decoder = True  # T5 is always tied, has always been like that.\n \n \n __all__ = [\"T5Config\"]"
        },
        {
            "sha": "a1806cb22f1ff6e072990a6064b19573cc7626db",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -844,12 +844,10 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config)\n \n@@ -1007,12 +1005,10 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config)\n \n@@ -1147,7 +1143,7 @@ def forward(\n \n         sequence_output = decoder_outputs[0]\n \n-        if self.config.tie_word_embeddings:\n+        if self.config.scale_decoder_outputs:\n             sequence_output = sequence_output * (self.model_dim**-0.5)\n \n         lm_logits = self.lm_head(sequence_output)\n@@ -1487,12 +1483,10 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config)\n "
        },
        {
            "sha": "b8369e053b415cfa1ab7a5b709905e91ac3bc1f2",
            "filename": "src/transformers/models/udop/configuration_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fudop%2Fconfiguration_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fudop%2Fconfiguration_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fconfiguration_udop.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -149,6 +149,7 @@ def __init__(\n                 \"'gated-gelu' or 'relu'\"\n             )\n \n+        kwargs[\"tie_word_embeddings\"] = True\n         super().__init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "5d31f68daa6cd176ff0fe14d70c478293ea1374f",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -1436,12 +1436,10 @@ def __init__(self, config):\n         encoder_config = deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_word_embeddings = True\n         self.encoder = UdopStack(encoder_config)\n \n         decoder_config = deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_word_embeddings = True\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UdopStack(decoder_config)\n \n@@ -1611,12 +1609,10 @@ def __init__(self, config):\n         encoder_config = deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = UdopStack(encoder_config)\n \n         decoder_config = deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UdopStack(decoder_config)\n "
        },
        {
            "sha": "797d39a340b5fa65274c0f11e9b50abfd1b41d6e",
            "filename": "src/transformers/models/umt5/configuration_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -94,7 +94,6 @@ def __init__(\n         is_encoder_decoder=True,\n         use_cache=True,\n         tokenizer_class=\"T5Tokenizer\",\n-        tie_word_embeddings=True,\n         pad_token_id=0,\n         eos_token_id=1,\n         decoder_start_token_id=0,\n@@ -133,10 +132,11 @@ def __init__(\n         if feed_forward_proj == \"gated-gelu\":\n             self.dense_act_fn = \"gelu_new\"\n \n+        # Force because official weights have False serialized, but we have to tie always\n+        kwargs[\"tie_word_embeddings\"] = True\n         super().__init__(\n             is_encoder_decoder=is_encoder_decoder,\n             tokenizer_class=tokenizer_class,\n-            tie_word_embeddings=tie_word_embeddings,\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,\n             decoder_start_token_id=decoder_start_token_id,"
        },
        {
            "sha": "812c22f515e6c9df0c0d098293a83e1e2ecf1ed6",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -929,12 +929,10 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config)\n \n@@ -1108,12 +1106,10 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config)\n \n@@ -1614,12 +1610,10 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config)\n "
        },
        {
            "sha": "25c2509d78a1c317b38c18308604e152331fc219",
            "filename": "src/transformers/models/vilt/configuration_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -115,7 +115,7 @@ def __init__(\n         num_channels=3,\n         qkv_bias=True,\n         max_image_length=-1,\n-        tie_word_embeddings=False,\n+        tie_word_embeddings=True,\n         num_images=-1,\n         **kwargs,\n     ):\n@@ -142,7 +142,7 @@ def __init__(\n         self.qkv_bias = qkv_bias\n         self.max_image_length = max_image_length\n         self.num_images = num_images\n-        self.tie_encoder_decoder = True\n+        self.tie_word_embeddings = True  # force it\n \n \n __all__ = [\"ViltConfig\"]"
        },
        {
            "sha": "6d10016fe310cfe2e0fcee9f07c0b5311a79592a",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -363,6 +363,10 @@ def test_resize_tokens_embeddings(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n+    @unittest.skip(reason=\"Weight tying is hardcoded (module_x = module_y) and always `True`\")\n+    def test_load_save_without_tied_weights(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "233e274931ad2ec6fbcd570740a3c18e82a55273",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -15,6 +15,7 @@\n \n import inspect\n import math\n+import tempfile\n import unittest\n from functools import cached_property\n \n@@ -29,6 +30,7 @@\n if is_torch_available():\n     import torch\n     import torch.nn.functional as F\n+    from safetensors import safe_open\n \n     from transformers import (\n         DabDetrForObjectDetection,\n@@ -229,6 +231,44 @@ def test_dab_detr_object_detection_head_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_dab_detr_object_detection_head_model(*config_and_inputs)\n \n+    def test_load_save_without_tied_weights(self):\n+        # DabDetrForObjectDetection forces `bbox_embed` to be tied by `self.x = y`\n+        # Run only DabDetrModel by overriding\n+        for model_class in [DabDetrModel]:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            config.tie_word_embeddings = False\n+            try:\n+                config.get_text_config().tie_word_embeddings = False\n+            except Exception as _:\n+                pass\n+\n+            model = model_class(config)  # we init the model without tie\n+            # if this test fails later on, it means init tied the weights\n+            with tempfile.TemporaryDirectory() as d:\n+                model.save_pretrained(d)\n+                with safe_open(f\"{d}/model.safetensors\", framework=\"pt\") as f:\n+                    serialized_keys = f.keys()\n+\n+                    model_reloaded, infos = model_class.from_pretrained(d, output_loading_info=True)\n+                    # Checking the state dicts are correct\n+\n+                    reloaded_state = model_reloaded.state_dict()\n+                    for k, v in model.state_dict().items():\n+                        with self.subTest(k):\n+                            torch.testing.assert_close(\n+                                v,\n+                                reloaded_state[k],\n+                                msg=lambda x: f\"{model_class.__name__}: Tensor {k}: {x}. Key {k} was serialized: {k in serialized_keys}. If `False`, this means it was probably aliased and safetensors removed it. If `True` it means `_init_weights` overwrote that key\",\n+                            )\n+\n+                # Checking there was no complain of missing weights\n+                self.assertEqual(\n+                    infos[\"missing_keys\"],\n+                    set(),\n+                    \"Given that the loaded weights are the same, the issue is in `tie_weights`: it tied these keys and removed them from serialization. But because of tiying (hardcoded or not) the previous check is fine.\\\n+                        This can happen if `save_pretrained` remove the targets and not the keys from serialiazation, or you hardcoded `self.xxx = yyy` thus forcing to always tie -> they are removed from serialization.\",\n+                )\n+\n     # TODO: check if this works again for PyTorch 2.x.y\n     @unittest.skip(reason=\"Got `CUDA error: misaligned address` with PyTorch 2.0.0.\")\n     def test_multi_gpu_data_parallel_forward(self):"
        },
        {
            "sha": "a08cbc1996925ff3ddaeb33bf1f490018554c415",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 81,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -509,82 +509,6 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n         )\n         self.assertEqual(generated_output.shape, (input_ids.shape[0],) + (enc_dec_model.generation_config.max_length,))\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        labels,\n-        **kwargs,\n-    ):\n-        torch.manual_seed(0)\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        model = EncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-        model.to(torch_device)\n-        model.eval()\n-        # load state dict copies weights but does not tie them\n-        decoder_state_dict = model.decoder._modules[model.decoder.base_model_prefix].state_dict()\n-        model.encoder.load_state_dict(decoder_state_dict, strict=False)\n-\n-        torch.manual_seed(0)\n-        tied_encoder_model, tied_decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        config = EncoderDecoderConfig.from_encoder_decoder_configs(\n-            tied_encoder_model.config, tied_decoder_model.config, tie_encoder_decoder=True\n-        )\n-        tied_model = EncoderDecoderModel(encoder=tied_encoder_model, decoder=tied_decoder_model, config=config)\n-        tied_model.to(torch_device)\n-        tied_model.eval()\n-\n-        model_result = model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-        )\n-\n-        tied_model_result = tied_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-        )\n-\n-        random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-        # check that outputs are equal\n-        self.assertTrue(\n-            torch.allclose(\n-                model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-            )\n-        )\n-\n-        # check that outputs after saving and loading are equal\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            tied_model.save_pretrained(tmpdirname)\n-            tied_model = EncoderDecoderModel.from_pretrained(tmpdirname)\n-            tied_model.to(torch_device)\n-            tied_model.eval()\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            # check that outputs are equal\n-            self.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n     def test_encoder_decoder_model(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model(**input_ids_dict)\n@@ -629,11 +553,6 @@ def test_encoder_decoder_model_generate(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n         self.check_encoder_decoder_model_generate(**input_ids_dict)\n \n-    @unittest.skip(\"This is no longer FORCED, it was just not working before.\")\n-    def test_encoder_decoder_model_shared_weights(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.create_and_check_encoder_decoder_shared_weights(**input_ids_dict)\n-\n     def test_training_gradient_checkpointing(self):\n         inputs_dict = self.prepare_config_and_inputs()\n         encoder_model, decoder_model = self.get_encoder_decoder_model("
        },
        {
            "sha": "3896294719da093b389f5fcd53bf24ac033c38c8",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -318,6 +318,10 @@ def test_resize_tokens_embeddings(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n+    @unittest.skip(reason=\"Weight tying is hardcoded (module_x = module_y) and always `True`\")\n+    def test_load_save_without_tied_weights(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "fe7ab28b2032c9a33bcabb004a654736d5398135",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \n \n-import copy\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -396,74 +394,6 @@ def create_and_check_generate_with_past_key_values(\n         output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n         self.parent.assertTrue(torch.all(output_with_past_cache == output_without_past_cache))\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        for model_class in [LongT5Model, LongT5ForConditionalGeneration]:\n-            torch.manual_seed(0)\n-            model = model_class(config=config).to(torch_device).eval()\n-            # load state dict copies weights but does not tie them\n-            model.encoder.load_state_dict(model.decoder.state_dict(), strict=False)\n-\n-            torch.manual_seed(0)\n-            tied_config = copy.deepcopy(config)\n-            tied_config.tie_encoder_decoder = True\n-            tied_model = model_class(config=tied_config).to(torch_device).eval()\n-\n-            model_result = model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            # check that outputs are equal\n-            self.parent.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n-            # check that outputs after saving and loading are equal\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                tied_model.save_pretrained(tmpdirname)\n-                tied_model = model_class.from_pretrained(tmpdirname)\n-                tied_model.to(torch_device)\n-                tied_model.eval()\n-\n-                random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-                tied_model_result = tied_model(\n-                    input_ids=input_ids,\n-                    decoder_input_ids=decoder_input_ids,\n-                    attention_mask=attention_mask,\n-                    decoder_attention_mask=decoder_attention_mask,\n-                )\n-\n-                # check that outputs are equal\n-                self.parent.assertTrue(\n-                    torch.allclose(\n-                        model_result[0][0, :, random_slice_idx],\n-                        tied_model_result[0][0, :, random_slice_idx],\n-                        atol=1e-4,\n-                    )\n-                )\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         (\n@@ -600,10 +530,6 @@ def test_generate_with_past_key_values(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n \n-    def test_encoder_decoder_shared_weights(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"google/long-t5-local-base\""
        },
        {
            "sha": "3d126de5b597d38bb0ccc673e1abfb6dc5319e14",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -272,7 +272,6 @@ def test_share_encoder_decoder_embeddings(self):\n         # check if embeddings are shared by default\n         for model_class in self.all_model_classes:\n             config.share_encoder_decoder_embeddings = True\n-            config.tie_encoder_decoder = True\n             model = model_class(config)\n             self.assertIs(\n                 model.get_encoder().embed_tokens.weight,\n@@ -282,7 +281,6 @@ def test_share_encoder_decoder_embeddings(self):\n \n         # check if embeddings are not shared when config.share_encoder_decoder_embeddings = False\n         config.share_encoder_decoder_embeddings = False\n-        config.tie_encoder_decoder = False\n         config.tie_word_embeddings = False\n         for model_class in self.all_model_classes:\n             model = model_class(config)"
        },
        {
            "sha": "3ae6cc9c834a6895d1b83d2dfea4b43400fea3c2",
            "filename": "tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -323,6 +323,10 @@ def test_resize_tokens_embeddings(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n+    @unittest.skip(reason=\"Weight tying is hardcoded (module_x = module_y) and always `True`\")\n+    def test_load_save_without_tied_weights(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "a8dbf5d8b0317bdaedaddf64fa89c76feda241ab",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import copy\n-import tempfile\n import unittest\n \n from transformers import MT5Config, is_torch_available\n@@ -422,74 +421,6 @@ def create_and_check_model_fp16_forward(\n         output = model(input_ids, decoder_input_ids=input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n         self.parent.assertFalse(torch.isnan(output).any().item())\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        for model_class in [MT5Model, MT5ForConditionalGeneration]:\n-            torch.manual_seed(0)\n-            model = model_class(config=config).to(torch_device).eval()\n-            # load state dict copies weights but does not tie them\n-            model.encoder.load_state_dict(model.decoder.state_dict(), strict=False)\n-\n-            torch.manual_seed(0)\n-            tied_config = copy.deepcopy(config)\n-            tied_config.tie_encoder_decoder = True\n-            tied_model = model_class(config=tied_config).to(torch_device).eval()\n-\n-            model_result = model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            # check that outputs are equal\n-            self.parent.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n-            # check that outputs after saving and loading are equal\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                tied_model.save_pretrained(tmpdirname)\n-                tied_model = model_class.from_pretrained(tmpdirname)\n-                tied_model.to(torch_device)\n-                tied_model.eval()\n-\n-                random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-                tied_model_result = tied_model(\n-                    input_ids=input_ids,\n-                    decoder_input_ids=decoder_input_ids,\n-                    attention_mask=attention_mask,\n-                    decoder_attention_mask=decoder_attention_mask,\n-                )\n-\n-                # check that outputs are equal\n-                self.parent.assertTrue(\n-                    torch.allclose(\n-                        model_result[0][0, :, random_slice_idx],\n-                        tied_model_result[0][0, :, random_slice_idx],\n-                        atol=1e-4,\n-                    )\n-                )\n-\n     def check_resize_embeddings_t5_v1_1(\n         self,\n         config,\n@@ -716,10 +647,6 @@ def test_generate_with_past_key_values(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n \n-    def test_encoder_decoder_shared_weights(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n-\n     @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "030aa230d4c6255ab1e9ef4324065adec7875d37",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Pop2Piano model.\"\"\"\n \n-import copy\n-import tempfile\n import unittest\n \n import numpy as np\n@@ -370,74 +368,6 @@ def create_and_check_model_fp16_forward(\n         ]\n         self.parent.assertFalse(torch.isnan(output).any().item())\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        for model_class in [Pop2PianoForConditionalGeneration]:\n-            torch.manual_seed(0)\n-            model = model_class(config=config).to(torch_device).eval()\n-            # load state dict copies weights but does not tie them\n-            model.encoder.load_state_dict(model.decoder.state_dict(), strict=False)\n-\n-            torch.manual_seed(0)\n-            tied_config = copy.deepcopy(config)\n-            tied_config.tie_encoder_decoder = True\n-            tied_model = model_class(config=tied_config).to(torch_device).eval()\n-\n-            model_result = model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            # check that outputs are equal\n-            self.parent.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n-            # check that outputs after saving and loading are equal\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                tied_model.save_pretrained(tmpdirname)\n-                tied_model = model_class.from_pretrained(tmpdirname)\n-                tied_model.to(torch_device)\n-                tied_model.eval()\n-\n-                random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-                tied_model_result = tied_model(\n-                    input_ids=input_ids,\n-                    decoder_input_ids=decoder_input_ids,\n-                    attention_mask=attention_mask,\n-                    decoder_attention_mask=decoder_attention_mask,\n-                )\n-\n-                # check that outputs are equal\n-                self.parent.assertTrue(\n-                    torch.allclose(\n-                        model_result[0][0, :, random_slice_idx],\n-                        tied_model_result[0][0, :, random_slice_idx],\n-                        atol=1e-4,\n-                    )\n-                )\n-\n     def check_resize_embeddings_pop2piano_v1_1(\n         self,\n         config,\n@@ -558,10 +488,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_encoder_decoder_shared_weights(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n-\n     @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "8f9b24ecf01ab6ed1e53e52fba5b7e560fff8d72",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -330,6 +330,10 @@ def test_resize_tokens_embeddings(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n+    @unittest.skip(reason=\"Weight tying is hardcoded (module_x = module_y) and always `True`\")\n+    def test_load_save_without_tied_weights(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "7afa29b8dffe5de97e57cdcb10c0abc0657fd0cf",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \n \n-import copy\n-import tempfile\n import unittest\n \n from transformers import SwitchTransformersConfig, is_torch_available\n@@ -439,74 +437,6 @@ def create_and_check_model_fp16_forward(\n         output = model(input_ids, decoder_input_ids=input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n         self.parent.assertFalse(torch.isnan(output).any().item())\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        for model_class in [SwitchTransformersModel, SwitchTransformersForConditionalGeneration]:\n-            torch.manual_seed(0)\n-            model = model_class(config=config).to(torch_device).eval()\n-            # load state dict copies weights but does not tie them\n-            model.encoder.load_state_dict(model.decoder.state_dict(), strict=False)\n-\n-            torch.manual_seed(0)\n-            tied_config = copy.deepcopy(config)\n-            tied_config.tie_encoder_decoder = True\n-            tied_model = model_class(config=tied_config).to(torch_device).eval()\n-\n-            model_result = model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            # check that outputs are equal\n-            self.parent.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n-            # check that outputs after saving and loading are equal\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                tied_model.save_pretrained(tmpdirname)\n-                tied_model = model_class.from_pretrained(tmpdirname)\n-                tied_model.to(torch_device)\n-                tied_model.eval()\n-\n-                random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-                tied_model_result = tied_model(\n-                    input_ids=input_ids,\n-                    decoder_input_ids=decoder_input_ids,\n-                    attention_mask=attention_mask,\n-                    decoder_attention_mask=decoder_attention_mask,\n-                )\n-\n-                # check that outputs are equal\n-                self.parent.assertTrue(\n-                    torch.allclose(\n-                        model_result[0][0, :, random_slice_idx],\n-                        tied_model_result[0][0, :, random_slice_idx],\n-                        atol=1e-4,\n-                    )\n-                )\n-\n     def check_resize_embeddings_switch_transformers_v1_1(\n         self,\n         config,\n@@ -679,10 +609,6 @@ def test_generate_with_past_key_values(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n \n-    def test_encoder_decoder_shared_weights(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n-\n     @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "3382c7bfd83a66967d53d52ee9890637f06c8554",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import copy\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -432,74 +431,6 @@ def create_and_check_model_fp16_forward(\n         output = model(input_ids, decoder_input_ids=input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n         self.parent.assertFalse(torch.isnan(output).any().item())\n \n-    def create_and_check_encoder_decoder_shared_weights(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        for model_class in [T5Model, T5ForConditionalGeneration]:\n-            torch.manual_seed(0)\n-            model = model_class(config=config).to(torch_device).eval()\n-            # load state dict copies weights but does not tie them\n-            model.encoder.load_state_dict(model.decoder.state_dict(), strict=False)\n-\n-            torch.manual_seed(0)\n-            tied_config = copy.deepcopy(config)\n-            tied_config.tie_encoder_decoder = True\n-            tied_model = model_class(config=tied_config).to(torch_device).eval()\n-\n-            model_result = model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            tied_model_result = tied_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-\n-            random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-            # check that outputs are equal\n-            self.parent.assertTrue(\n-                torch.allclose(\n-                    model_result[0][0, :, random_slice_idx], tied_model_result[0][0, :, random_slice_idx], atol=1e-4\n-                )\n-            )\n-\n-            # check that outputs after saving and loading are equal\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                tied_model.save_pretrained(tmpdirname)\n-                tied_model = model_class.from_pretrained(tmpdirname)\n-                tied_model.to(torch_device)\n-                tied_model.eval()\n-\n-                random_slice_idx = ids_tensor((1,), model_result[0].shape[-1]).item()\n-\n-                tied_model_result = tied_model(\n-                    input_ids=input_ids,\n-                    decoder_input_ids=decoder_input_ids,\n-                    attention_mask=attention_mask,\n-                    decoder_attention_mask=decoder_attention_mask,\n-                )\n-\n-                # check that outputs are equal\n-                self.parent.assertTrue(\n-                    torch.allclose(\n-                        model_result[0][0, :, random_slice_idx],\n-                        tied_model_result[0][0, :, random_slice_idx],\n-                        atol=1e-4,\n-                    )\n-                )\n-\n     def check_resize_embeddings_t5_v1_1(\n         self,\n         config,\n@@ -725,10 +656,6 @@ def test_generate_with_past_key_values(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n \n-    def test_encoder_decoder_shared_weights(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n-\n     @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "f87c611fee79742f548829092837af3230982eef",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -2296,7 +2296,6 @@ def test_load_save_without_tied_weights(self):\n             except Exception as _:\n                 pass\n \n-            # config.tie_encoder_decoder = False\n             model = model_class(config)  # we init the model without tie\n             # if this test fails later on, it means init tied the weights\n             with tempfile.TemporaryDirectory() as d:"
        },
        {
            "sha": "3e40796aad3b99e26a86226aa721d0c95a9ec637",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -38,11 +38,10 @@\n     \"output_hidden_states\": True,\n     \"output_attentions\": True,\n     \"dtype\": \"float16\",\n-    \"tie_word_embeddings\": False,\n+    # \"tie_word_embeddings\": True, # attribute is hardcoded in many models, hard to test\n     \"is_decoder\": True,\n     \"cross_attention_hidden_size\": 128,\n     \"add_cross_attention\": True,\n-    \"tie_encoder_decoder\": True,\n     \"chunk_size_feed_forward\": 5,\n     \"architectures\": [\"BertModel\"],\n     \"finetuning_task\": \"translation\",\n@@ -159,6 +158,7 @@ def test_config_common_kwargs_is_complete(self):\n             missing_keys,\n             [\n                 \"_output_attentions\",\n+                \"tie_word_embeddings\",  # was omitted in purpose and will be deleted from base config soon\n                 \"is_encoder_decoder\",\n                 \"_name_or_path\",\n                 \"_commit_hash\","
        },
        {
            "sha": "6b86f03e39278975b60542df396412d51597c938",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a90500b0b7a3f481c005a1398ab50539146dfbc/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a90500b0b7a3f481c005a1398ab50539146dfbc/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=9a90500b0b7a3f481c005a1398ab50539146dfbc",
            "patch": "@@ -415,6 +415,7 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"initializer_range\",\n         \"init_std\",\n         \"initializer_factor\",\n+        \"tie_word_embeddings\",\n         \"bos_index\",\n         \"eos_index\",\n         \"pad_index\","
        }
    ],
    "stats": {
        "total": 611,
        "additions": 91,
        "deletions": 520
    }
}