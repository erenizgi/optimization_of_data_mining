{
    "author": "Rocketknight1",
    "message": "Fix exploitable regexes in Nougat and GPTSan/GPTJNeoXJapanese (#36121)\n\n* Fix potential regex catastrophic backtracking in NougatTokenizerFast\n\nThe original regex pattern in tokenization_nougat_fast.py was vulnerable to\ncatastrophic backtracking due to greedy quantifiers and nested alternations.\nThis commit replaces it with a more efficient pattern that:\n\n1. Uses explicit character classes instead of dot (.)\n2. Handles whitespace more precisely\n3. Avoids unnecessary backtracking\n4. Supports both lowercase and uppercase roman numerals\n5. Maintains the same functionality while being more robust\n\n* Try another regex\n\n* Trying deepseek's answer\n\n* Start with a simplification\n\n* Another simplification\n\n* Just rewrite the whole function myself\n\n* Fix gptneox and gptsan\n\n* Simplify the regex even further\n\n* Tighten up the price regex a little\n\n* Add possessive version of the regex\n\n* Fix regex\n\n* Much cleaner regexes\n\n---------\n\nCo-authored-by: openhands <openhands@all-hands.dev>",
    "sha": "92c5ca9dd70de3ade2af2eb835c96215cc50e815",
    "files": [
        {
            "sha": "58031bffba029dc7a7bfa06031e4d2ca1afe1686",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=92c5ca9dd70de3ade2af2eb835c96215cc50e815",
            "patch": "@@ -18,6 +18,7 @@\n import json\n import os\n import re\n+import sys\n from typing import List, Optional, Tuple, Union\n \n import numpy as np\n@@ -407,9 +408,23 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n         self.content_repatter5 = re.compile(\n             r\"(明治|大正|昭和|平成|令和|㍾|㍽|㍼|㍻|\\u32ff)\\d{1,2}年(0?[1-9]|1[0-2])月(0?[1-9]|[12][0-9]|3[01])日(\\d{1,2}|:|\\d{1,2}時|\\d{1,2}分|\\(日\\)|\\(月\\)|\\(火\\)|\\(水\\)|\\(木\\)|\\(金\\)|\\(土\\)|㈰|㈪|㈫|㈬|㈭|㈮|㈯)*\"\n         )\n-        self.content_repatter6 = re.compile(\n-            r\"((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*億)*((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*万)*((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*千)*(0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*(千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+(\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n-        )\n+        # The original version of this regex displays catastrophic backtracking behaviour. We avoid this using\n+        # possessive quantifiers in Py >= 3.11. In versions below this, we avoid the vulnerability using a slightly\n+        # different regex that should generally have the same behaviour in most non-pathological cases.\n+        if sys.version_info >= (3, 11):\n+            self.content_repatter6 = re.compile(\n+                r\"(?:\\d,\\d{3}|[\\d億])*+\"\n+                r\"(?:\\d,\\d{3}|[\\d万])*+\"\n+                r\"(?:\\d,\\d{3}|[\\d千])*+\"\n+                r\"(?:千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+\"\n+                r\"(?:\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n+            )\n+        else:\n+            self.content_repatter6 = re.compile(\n+                r\"(?:\\d,\\d{3}|[\\d億万千])*\"\n+                r\"(?:千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+\"\n+                r\"(?:\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n+            )\n         keisen = \"─━│┃┄┅┆┇┈┉┊┋┌┍┎┏┐┑┒┓└┕┖┗┘┙┚┛├┝┞┟┠┡┢┣┤┥┦┧┨┩┪┫┬┭┮┯┰┱┲┳┴┵┶┷┸┹┺┻┼┽┾┿╀╁╂╃╄╅╆╇╈╉╊╋╌╍╎╏═║╒╓╔╕╖╗╘╙╚╛╜╝╞╟╠╡╢╣╤╥╦╧╨╩╪╫╬╭╮╯╰╱╲╳╴╵╶╷╸╹╺╻╼╽╾╿\"\n         blocks = \"▀▁▂▃▄▅▆▇█▉▊▋▌▍▎▏▐░▒▓▔▕▖▗▘▙▚▛▜▝▞▟\"\n         self.content_trans1 = str.maketrans({k: \"<BLOCK>\" for k in keisen + blocks})"
        },
        {
            "sha": "1030c93397e26754aa1e34eb491c531d3385f91d",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=92c5ca9dd70de3ade2af2eb835c96215cc50e815",
            "patch": "@@ -18,6 +18,7 @@\n import json\n import os\n import re\n+import sys\n from typing import Optional, Tuple\n \n import numpy as np\n@@ -230,9 +231,23 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n         self.content_repatter5 = re.compile(\n             r\"(明治|大正|昭和|平成|令和|㍾|㍽|㍼|㍻|\\u32ff)\\d{1,2}年(0?[1-9]|1[0-2])月(0?[1-9]|[12][0-9]|3[01])日(\\d{1,2}|:|\\d{1,2}時|\\d{1,2}分|\\(日\\)|\\(月\\)|\\(火\\)|\\(水\\)|\\(木\\)|\\(金\\)|\\(土\\)|㈰|㈪|㈫|㈬|㈭|㈮|㈯)*\"\n         )\n-        self.content_repatter6 = re.compile(\n-            r\"((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*億)*((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*万)*((0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*千)*(0|[1-9]\\d*|[1-9]\\d{0,2}(,\\d{3})+)*(千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+(\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n-        )\n+        # The original version of this regex displays catastrophic backtracking behaviour. We avoid this using\n+        # possessive quantifiers in Py >= 3.11. In versions below this, we avoid the vulnerability using a slightly\n+        # different regex that should generally have the same behaviour in most non-pathological cases.\n+        if sys.version_info >= (3, 11):\n+            self.content_repatter6 = re.compile(\n+                r\"(?:\\d,\\d{3}|[\\d億])*+\"\n+                r\"(?:\\d,\\d{3}|[\\d万])*+\"\n+                r\"(?:\\d,\\d{3}|[\\d千])*+\"\n+                r\"(?:千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+\"\n+                r\"(?:\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n+            )\n+        else:\n+            self.content_repatter6 = re.compile(\n+                r\"(?:\\d,\\d{3}|[\\d億万千])*\"\n+                r\"(?:千円|万円|千万円|円|千ドル|万ドル|千万ドル|ドル|千ユーロ|万ユーロ|千万ユーロ|ユーロ)+\"\n+                r\"(?:\\(税込\\)|\\(税抜\\)|\\+tax)*\"\n+            )\n         keisen = \"─━│┃┄┅┆┇┈┉┊┋┌┍┎┏┐┑┒┓└┕┖┗┘┙┚┛├┝┞┟┠┡┢┣┤┥┦┧┨┩┪┫┬┭┮┯┰┱┲┳┴┵┶┷┸┹┺┻┼┽┾┿╀╁╂╃╄╅╆╇╈╉╊╋╌╍╎╏═║╒╓╔╕╖╗╘╙╚╛╜╝╞╟╠╡╢╣╤╥╦╧╨╩╪╫╬╭╮╯╰╱╲╳╴╵╶╷╸╹╺╻╼╽╾╿\"\n         blocks = \"▀▁▂▃▄▅▆▇█▉▊▋▌▍▎▏▐░▒▓▔▕▖▗▘▙▚▛▜▝▞▟\"\n         self.content_trans1 = str.maketrans({k: \"<BLOCK>\" for k in keisen + blocks})"
        },
        {
            "sha": "b4bc76f613a8b2b157da52ce80cc025f0ddcd46c",
            "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 24,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/92c5ca9dd70de3ade2af2eb835c96215cc50e815/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py?ref=92c5ca9dd70de3ade2af2eb835c96215cc50e815",
            "patch": "@@ -113,26 +113,17 @@ def normalize_list_like_lines(generation):\n         normalization adjusts the bullet point style and nesting levels based on the captured patterns.\n     \"\"\"\n \n-    # This matches lines starting with - or *, not followed by - or * (lists)\n-    # that are then numbered by digits \\d or roman numerals (one or more)\n-    # and then, optional additional numbering of this line is captured\n-    # this is then fed to re.finditer.\n-    pattern = r\"(?:^)(-|\\*)?(?!-|\\*) ?((?:\\d|[ixv])+ )?.+? (-|\\*) (((?:\\d|[ixv])+)\\.(\\d|[ixv]) )?.*(?:$)\"\n-\n-    for match in reversed(list(re.finditer(pattern, generation, flags=re.I | re.M))):\n-        start, stop = match.span()\n-        delim = match.group(3) + \" \"\n-        splits = match.group(0).split(delim)\n+    lines = generation.split(\"\\n\")\n+    output_lines = []\n+    for line_no, line in enumerate(lines):\n+        match = re.search(r\". ([-*]) \", line)\n+        if not match or line[0] not in (\"-\", \"*\"):\n+            output_lines.append(line)\n+            continue  # Doesn't fit the pattern we want, no changes\n+        delim = match.group(1) + \" \"\n+        splits = line.split(delim)[1:]\n         replacement = \"\"\n-\n-        if match.group(1) is not None:\n-            splits = splits[1:]\n-            delim1 = match.group(1) + \" \"\n-        else:\n-            delim1 = \"\"\n-            continue  # Skip false positives\n-\n-        pre, post = generation[:start], generation[stop:]\n+        delim1 = line[0] + \" \"\n \n         for i, item in enumerate(splits):\n             level = 0\n@@ -144,15 +135,15 @@ def normalize_list_like_lines(generation):\n                 level = potential_numeral.count(\".\")\n \n             replacement += (\n-                (\"\\n\" if i > 0 else \"\") + (\"\\t\" * level) + (delim if i > 0 or start == 0 else delim1) + item.strip()\n+                (\"\\n\" if i > 0 else \"\") + (\"\\t\" * level) + (delim if i > 0 or line_no == 0 else delim1) + item.strip()\n             )\n \n-        if post == \"\":\n-            post = \"\\n\"\n+        if line_no == len(lines) - 1:  # If this is the last line in the generation\n+            replacement += \"\\n\"  # Add an empty line to the end of the generation\n \n-        generation = pre + replacement + post\n+        output_lines.append(replacement)\n \n-    return generation\n+    return \"\\n\".join(output_lines)\n \n \n def find_next_punctuation(text: str, start_idx=0):"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 51,
        "deletions": 30
    }
}