{
    "author": "gante",
    "message": "fix: providing a tensor to cache_position in model.generate kwargs always crashes because of boolean test (#39300)\n\n* fix: cache_position: RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\n* test cache_position\n\n* move test\n\n* propagate changes\n\n---------\n\nCo-authored-by: Masataro Asai <guicho2.71828@gmail.com>",
    "sha": "4f93cc91745181d95cf63f65e50bb9523111bf5a",
    "files": [
        {
            "sha": "36210b398906004cb51148796a0a5d59a1d40e46",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -1800,7 +1800,7 @@ def _prepare_generation_config(\n     def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         \"\"\"Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length\"\"\"\n         # `torch.compile`-friendly `torch.arange` from a shape -- the lines below are equivalent to `torch.arange`\n-        if \"cache_position\" in model_kwargs and model_kwargs[\"cache_position\"]:\n+        if \"cache_position\" in model_kwargs and model_kwargs[\"cache_position\"] is not None:\n             return model_kwargs\n         if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n             cache_position = torch.ones_like(model_kwargs[\"inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1"
        },
        {
            "sha": "0f7966a9c9ebfbfb2e016c6dc93bc846b8b8fcbe",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 139,
            "deletions": 38,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -168,34 +168,6 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n \n         return config, filtered_inputs_dict\n \n-    def _check_similar_generate_outputs(self, output_1, output_2, atol=1e-5, rtol=1e-5):\n-        \"\"\"\n-        Checks whether a pair of generate outputs are similar. Two `generate` call outputs are considered similar in\n-        the following situations:\n-        1. The sequences are the same\n-        2. The sequences are different, but the scores up to (and including) the first mismatch are nearly identical\n-        \"\"\"\n-        # scores doesn't include data regarding decoder input tokens\n-        decoder_input_length = output_1.sequences.shape[1] - len(output_1.scores)\n-        output_matches = output_1.sequences == output_2.sequences\n-        has_matching_outputs = output_matches.all()\n-        has_matching_scores = None\n-        if not has_matching_outputs:\n-            for batch_idx in range(output_1.sequences.shape[0]):\n-                batch_matches = output_matches[batch_idx]\n-                if batch_matches.all():\n-                    continue\n-                first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n-                first_mismatch_idx -= decoder_input_length\n-                output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n-                output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n-                has_matching_scores = torch.allclose(\n-                    output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n-                )\n-                if not has_matching_scores:\n-                    break\n-        self.assertTrue(has_matching_outputs or has_matching_scores)\n-\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {\n             \"bad_words_ids\": [[1, 0]],\n@@ -1094,7 +1066,7 @@ def test_contrastive_generate_low_memory(self):\n \n             low_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=True)\n             high_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=False)\n-            self._check_similar_generate_outputs(low_output, high_output)\n+            self.assertTrue(has_similar_generate_outputs(low_output, high_output))\n \n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n@@ -1176,7 +1148,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n \n             # The two outputs must match and their shape must be as expected\n-            self._check_similar_generate_outputs(output_greedy, output_assisted)\n+            self.assertTrue(has_similar_generate_outputs(output_greedy, output_assisted))\n             for output in (output_greedy, output_assisted):\n                 self._check_generate_outputs(output, model.config, use_cache=True)\n \n@@ -1259,7 +1231,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             output_prompt_lookup = model.generate(**generation_kwargs, **inputs_dict)\n \n             # The two outputs must match and their shape must be as expected\n-            self._check_similar_generate_outputs(output_greedy, output_prompt_lookup)\n+            self.assertTrue(has_similar_generate_outputs(output_greedy, output_prompt_lookup))\n             for output in (output_greedy, output_prompt_lookup):\n                 self._check_generate_outputs(output, model.config, use_cache=True)\n \n@@ -1745,7 +1717,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 input_ids=input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n             )\n             if not has_complex_embeds_computation:\n-                self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n+                self.assertTrue(has_similar_generate_outputs(outputs_from_ids, outputs_from_embeds))\n \n             # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n             # be the same\n@@ -1754,7 +1726,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                     inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n                 )\n                 outputs_from_embeds.sequences = outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :]\n-                self._check_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds)\n+                self.assertTrue(has_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds))\n \n     @pytest.mark.generate\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n@@ -1896,7 +1868,7 @@ def test_generate_continue_from_past_key_values(self):\n             outputs_cached.scores = full_cached_scores\n \n             # The two sets of generated text and past kv should be equal to each other\n-            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n             for layer_idx in range(len(outputs_cached.past_key_values)):\n                 for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                     self.assertTrue(\n@@ -1923,7 +1895,7 @@ def test_generate_continue_from_inputs_embeds(self):\n             if config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder\")\n             # TODO (joao, raushan): the correct line below is `if not hasattr(config.get_text_config(), \"use_cache\")`,\n-            # but it breaks a few models. Fix and then apply `_check_similar_generate_outputs` pattern\n+            # but it breaks a few models. Fix and then apply `has_similar_generate_outputs` pattern\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n@@ -2038,7 +2010,7 @@ def test_generate_with_static_cache(self):\n \n                 # Check 2: The outputs must be similar to the case with dynamic cache\n                 dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n-                self._check_similar_generate_outputs(dynamic_cache_generation, static_cache_generation)\n+                self.assertTrue(has_similar_generate_outputs(dynamic_cache_generation, static_cache_generation))\n \n     @require_optimum_quanto\n     @pytest.mark.generate\n@@ -2192,7 +2164,7 @@ def test_generate_compile_model_forward(self):\n                 )\n \n             for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n-                self._check_similar_generate_outputs(dynamic_result, compiled_result)\n+                self.assertTrue(has_similar_generate_outputs(dynamic_result, compiled_result))\n \n     @pytest.mark.generate\n     def test_generate_compilation_all_outputs(self):\n@@ -2381,7 +2353,7 @@ def _test_attention_implementation(self, attn_implementation):\n                 del model_attn\n                 gc.collect()\n \n-                self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n+                self.assertTrue(has_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3))\n \n     @pytest.mark.generate\n     @require_torch_sdpa\n@@ -5087,6 +5059,97 @@ def test_custom_generate_local_directory(self):\n             )\n             assert value == \"success\"\n \n+    @pytest.mark.generate\n+    def test_generate_custom_cache_position(self):\n+        \"\"\"\n+        Regression test for #39261. Tests that we can continue generating from past key values, returned from a\n+        previous `generate` call, without the tokens that correspond to the cached part. This is achieved by passing\n+        manually creating `cache_position` -- this tests that it is piped correctly.\n+        \"\"\"\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=\"auto\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+\n+        model_inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n+        generate_kwargs = {\n+            \"use_cache\": True,\n+            \"do_sample\": False,\n+            \"return_dict_in_generate\": True,\n+            \"output_scores\": True,\n+        }\n+\n+        # Traditional way to continue generating text using kv cache\n+        #                 output2\n+        # /~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\n+        #            input2\n+        # /~~~~~~~~~~~~~~~~~~~~~~~~\\\n+        #      output1\n+        # /~~~~~~~~~~~~~~~~\\\n+        #  input1\n+        # /~~~~~~\\\n+        # IIIIIIIIOOOOOOOOOOIIIIIIIIOOOOOOOOOOOOOOOOOO\n+        inputs_1a = model_inputs\n+        outputs_1a = model.generate(**inputs_1a, **generate_kwargs, max_new_tokens=2)\n+        inputs_2a = {**model_inputs}\n+        inputs_2a[\"input_ids\"] = torch.cat((outputs_1a.sequences, model_inputs[\"input_ids\"]), dim=1)\n+        inputs_2a[\"attention_mask\"] = torch.nn.functional.pad(\n+            inputs_1a[\"attention_mask\"],\n+            (0, inputs_2a[\"input_ids\"].shape[1] - inputs_1a[\"input_ids\"].shape[1]),\n+            mode=\"constant\",\n+            value=1,\n+        )\n+        inputs_2a[\"past_key_values\"] = outputs_1a.past_key_values\n+        outputs_2a = model.generate(**inputs_2a, **generate_kwargs, max_new_tokens=2)\n+        # Keep only the part of the output related to the second output + last token from the first output, for future\n+        # comparison\n+        traditional_outputs = copy.deepcopy(outputs_2a)\n+        traditional_outputs.sequences = traditional_outputs.sequences[:, outputs_1a.sequences.shape[1] - 1 :]\n+\n+        # Continue generating text using kv cache, but without providing the cached part of the input in the input_ids.\n+        #                    cache_position\n+        #                   /~~~~~~~\\\n+        #  inputs2[\"attention_mask\"]\n+        # /~~~~~~~~~~~~~~~~~~~~~~~~~\\\n+        #      output1               output2\n+        # /~~~~~~~~~~~~~~~~\\/~~~~~~~~~~~~~~~~~~~~~~~~~\\\n+        #  input1             input2\n+        # /~~~~~~\\          /~~~~~~~\\\n+        # IIIIIIIIOOOOOOOOOOIIIIIIIIIOOOOOOOOOOOOOOOOOO\n+        #\n+        inputs_1b = model_inputs\n+        outputs_1b = model.generate(**inputs_1b, **generate_kwargs, max_new_tokens=2)\n+        inputs_2b = {**model_inputs}\n+        # The last output token isn't cached, so it needs to be included in the new input\n+        inputs_2b[\"input_ids\"] = torch.cat((outputs_1b.sequences[:, -1:], model_inputs[\"input_ids\"]), dim=1)\n+        inputs_2b[\"attention_mask\"] = torch.nn.functional.pad(\n+            inputs_1b[\"attention_mask\"],\n+            (0, outputs_1b.sequences.shape[1]),\n+            mode=\"constant\",\n+            value=1,\n+        )\n+        inputs_2b[\"past_key_values\"] = outputs_1b.past_key_values\n+        cache_length_1b = outputs_1b.past_key_values[0][0].shape[-2]\n+        inputs_2b[\"cache_position\"] = torch.arange(\n+            cache_length_1b,\n+            cache_length_1b + inputs_2b[\"input_ids\"].shape[1],\n+            dtype=torch.int64,\n+            device=model.device,\n+        )\n+        outputs_2b = model.generate(**inputs_2b, **generate_kwargs, max_new_tokens=2)\n+        incremental_outputs = outputs_2b\n+\n+        # The two sets of generated text and past kv should be equal to each other\n+        self.assertTrue(has_similar_generate_outputs(traditional_outputs, incremental_outputs))\n+        for layer_idx in range(len(traditional_outputs.past_key_values)):\n+            for kv_idx in range(len(traditional_outputs.past_key_values[layer_idx])):\n+                self.assertTrue(\n+                    torch.allclose(\n+                        traditional_outputs.past_key_values[layer_idx][kv_idx],\n+                        incremental_outputs.past_key_values[layer_idx][kv_idx],\n+                    )\n+                )\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):\n@@ -5281,3 +5344,41 @@ def test_update_candidate_strategy_with_matches_1(self, sklearn_available):\n             self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n         else:\n             self.assert_no_sklearn()\n+\n+\n+def has_similar_generate_outputs(output_1, output_2, atol=1e-5, rtol=1e-5) -> bool:\n+    \"\"\"\n+    Returns a boolean indicating whether a pair of generate outputs are similar. Two `generate` call outputs are\n+    considered similar in the following situations:\n+    1. The sequences are the same\n+    2. The sequences are different, but the scores up to (and including) the first mismatch are nearly identical\n+\n+    Args:\n+        output_1 (`GenerateOutput`): The first `generate` call output.\n+        output_2 (`GenerateOutput`): The second `generate` call output.\n+        atol (`float`, *optional*, defaults to 1e-5): The absolute tolerance for the scores.\n+        rtol (`float`, *optional*, defaults to 1e-5): The relative tolerance for the scores.\n+\n+    Returns:\n+        A boolean indicating whether the two generate outputs are similar.\n+    \"\"\"\n+    # scores doesn't include data regarding decoder input tokens\n+    decoder_input_length = output_1.sequences.shape[1] - len(output_1.scores)\n+    output_matches = output_1.sequences == output_2.sequences\n+    has_matching_outputs = output_matches.all()\n+    has_matching_scores = None\n+    if not has_matching_outputs:\n+        for batch_idx in range(output_1.sequences.shape[0]):\n+            batch_matches = output_matches[batch_idx]\n+            if batch_matches.all():\n+                continue\n+            first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n+            first_mismatch_idx -= decoder_input_length\n+            output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n+            output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n+            has_matching_scores = torch.allclose(\n+                output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n+            )\n+            if not has_matching_scores:\n+                break\n+    return has_matching_outputs or has_matching_scores"
        },
        {
            "sha": "ecd4cf3a42fb4faf195ea3bf02ad2e3e45232ad1",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -33,7 +33,7 @@\n from transformers.utils import is_soundfile_available, is_torch_available, is_torchaudio_available\n from transformers.utils.import_utils import is_datasets_available\n \n-from ...generation.test_utils import GenerationTesterMixin\n+from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -512,7 +512,7 @@ def test_generate_continue_from_past_key_values(self):\n             outputs_cached.scores = full_cached_scores\n \n             # The two sets of generated text and past kv should be equal to each other\n-            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n             for layer_idx in range(len(outputs_cached.past_key_values)):\n                 for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                     self.assertTrue("
        },
        {
            "sha": "28aaa8025414f49c13a50c8454f6f0cacf92d424",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -40,7 +40,7 @@\n     is_vision_available,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n+from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n@@ -650,15 +650,15 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n             outputs_from_embeds = model.generate(\n                 input_ids=input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n             )\n-            self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n+            self.assertTrue(has_similar_generate_outputs(outputs_from_ids, outputs_from_embeds))\n \n             # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n             # be the same\n             outputs_from_embeds_wo_ids = model.generate(\n                 inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n             )\n             outputs_from_embeds.sequences = outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :]\n-            self._check_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds)\n+            self.assertTrue(has_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds))\n \n \n # We will verify our results on an image of cute cats"
        },
        {
            "sha": "cb58eea2618af95cd8fb1bfe6b25ee12718a61b8",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -38,7 +38,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n+from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n@@ -527,7 +527,7 @@ def test_generate_continue_from_past_key_values(self):\n             outputs_cached.scores = full_cached_scores\n \n             # The two sets of generated text and past kv should be equal to each other\n-            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n             for layer_idx in range(len(outputs_cached.past_key_values)):\n                 for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                     self.assertTrue(\n@@ -613,7 +613,7 @@ def _test_attention_implementation(self, attn_implementation):\n                 del model_attn\n                 gc.collect()\n \n-                self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n+                self.assertTrue(has_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3))\n \n \n @require_torch"
        },
        {
            "sha": "95279fae5bc038b965e35ff1e6d37a74e51ae90a",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f93cc91745181d95cf63f65e50bb9523111bf5a/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=4f93cc91745181d95cf63f65e50bb9523111bf5a",
            "patch": "@@ -29,7 +29,7 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n+from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -1196,7 +1196,7 @@ def test_generate_continue_from_past_key_values(self):\n             outputs_cached.scores = full_cached_scores\n \n             # The two sets of generated text and past kv should be equal to each other\n-            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n             for layer_idx in range(len(outputs_cached.past_key_values)):\n                 for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                     self.assertTrue("
        }
    ],
    "stats": {
        "total": 199,
        "additions": 150,
        "deletions": 49
    }
}