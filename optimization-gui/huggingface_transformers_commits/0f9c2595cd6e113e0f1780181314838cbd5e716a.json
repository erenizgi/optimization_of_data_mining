{
    "author": "Anil-Red",
    "message": "updated visualBERT modelcard (#40057)\n\n* updated visualBERT modelcard\n\n* fix: Review for VisualBERT card",
    "sha": "0f9c2595cd6e113e0f1780181314838cbd5e716a",
    "files": [
        {
            "sha": "9e4376d0d4c42dfaf0e4b098af03dd0a3455ea90",
            "filename": "docs/source/en/model_doc/visual_bert.md",
            "status": "modified",
            "additions": 102,
            "deletions": 73,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c2595cd6e113e0f1780181314838cbd5e716a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c2595cd6e113e0f1780181314838cbd5e716a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md?ref=0f9c2595cd6e113e0f1780181314838cbd5e716a",
            "patch": "@@ -14,87 +14,116 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# VisualBERT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n-VisualBERT is a neural network trained on a variety of (image, text) pairs.\n-\n-The abstract from the paper is the following:\n-\n-*We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks.\n-VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an\n-associated input image with self-attention. We further propose two visually-grounded language model objectives for\n-pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2,\n-and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly\n-simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any\n-explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between\n-verbs and image regions corresponding to their arguments.*\n-\n-This model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/uclanlp/visualbert).\n-\n-## Usage tips\n-\n-1. Most of the checkpoints provided work with the [`VisualBertForPreTraining`] configuration. Other\n-   checkpoints provided are the fine-tuned checkpoints for down-stream tasks - VQA ('visualbert-vqa'), VCR\n-   ('visualbert-vcr'), NLVR2 ('visualbert-nlvr2'). Hence, if you are not working on these downstream tasks, it is\n-   recommended that you use the pretrained checkpoints.\n-\n-2. For the VCR task, the authors use a fine-tuned detector for generating visual embeddings, for all the checkpoints.\n-   We do not provide the detector and its weights as a part of the package, but it will be available in the research\n-   projects, and the states can be loaded directly into the detector provided.\n-\n-VisualBERT is a multi-modal vision and language model. It can be used for visual question answering, multiple choice,\n-visual reasoning and region-to-phrase correspondence tasks. VisualBERT uses a BERT-like transformer to prepare\n-embeddings for image-text pairs. Both the text and visual features are then projected to a latent space with identical\n-dimension.\n-\n-To feed images to the model, each image is passed through a pre-trained object detector and the regions and the\n-bounding boxes are extracted. The authors use the features generated after passing these regions through a pre-trained\n-CNN like ResNet as visual embeddings. They also add absolute position embeddings, and feed the resulting sequence of\n-vectors to a standard BERT model. The text input is concatenated in the front of the visual embeddings in the embedding\n-layer, and is expected to be bound by [CLS] and a [SEP] tokens, as in BERT. The segment IDs must also be set\n-appropriately for the textual and visual parts.\n-\n-The [`BertTokenizer`] is used to encode the text. A custom detector/image processor must be used\n-to get the visual embeddings. The following example notebooks show how to use VisualBERT with Detectron-like models:\n-\n-- [VisualBERT VQA demo notebook](https://github.com/huggingface/transformers-research-projects/tree/main/visual_bert) : This notebook\n-  contains an example on VisualBERT VQA.\n+# VisualBERT\n \n-- [Generate Embeddings for VisualBERT (Colab Notebook)](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing) : This notebook contains\n-  an example on how to generate visual embeddings.\n+[VisualBERT](https://huggingface.co/papers/1908.03557) is a vision-and-language model. It uses an approach called \"early fusion\", where inputs are fed together into a single Transformer stack initialized from [BERT](./bert). Self-attention implicitly aligns words with their corresponding image objects. It processes text with visual features from object-detector regions instead of raw pixels.\n+\n+You can find all the original VisualBERT checkpoints under the [UCLA NLP](https://huggingface.co/uclanlp/models?search=visualbert) organization.\n+\n+\n+> [!TIP]\n+> This model was contributed by [gchhablani](https://huggingface.co/gchhablani).\n+> Click on the VisualBERT models in the right sidebar for more examples of how to apply VisualBERT to different image and language tasks.\n+\n+The example below demonstrates how to answer a question based on an image with the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+import torchvision\n+from PIL import Image\n+import numpy as np\n+from transformers import AutoTokenizer, VisualBertForQuestionAnswering\n+import requests\n+from io import BytesIO\n+\n+def get_visual_embeddings_simple(image, device=None):\n+    \n+    model = torchvision.models.resnet50(pretrained=True)\n+    model = torch.nn.Sequential(*list(model.children())[:-1])\n+    model.to(device)\n+    model.eval()\n+    \n+    transform = torchvision.transforms.Compose([\n+        torchvision.transforms.Resize(256),\n+        torchvision.transforms.CenterCrop(224),\n+        torchvision.transforms.ToTensor(),\n+        torchvision.transforms.Normalize(\n+            mean=[0.485, 0.456, 0.406],\n+            std=[0.229, 0.224, 0.225]\n+        )\n+    ])\n+    \n+    if isinstance(image, str):\n+        image = Image.open(image).convert('RGB')\n+    elif isinstance(image, Image.Image):\n+        image = image.convert('RGB')\n+    else:\n+        raise ValueError(\"Image must be a PIL Image or path to image file\")\n+    \n+    image_tensor = transform(image).unsqueeze(0).to(device)\n+    \n+    with torch.no_grad():\n+        features = model(image_tensor)\n+    \n+    batch_size = features.shape[0]\n+    feature_dim = features.shape[1]\n+    visual_seq_length = 10\n+    \n+    visual_embeds = features.squeeze(-1).squeeze(-1).unsqueeze(1).expand(batch_size, visual_seq_length, feature_dim)\n+    \n+    return visual_embeds\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n+\n+response = requests.get(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+image = Image.open(BytesIO(response.content))\n+    \n+visual_embeds = get_visual_embeddings_simple(image)\n+    \n+inputs = tokenizer(\"What is shown in this image?\", return_tensors=\"pt\")\n+    \n+visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n+visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n+    \n+inputs.update({\n+    \"visual_embeds\": visual_embeds,\n+    \"visual_token_type_ids\": visual_token_type_ids,\n+    \"visual_attention_mask\": visual_attention_mask,\n+})\n+    \n+with torch.no_grad():\n+    outputs = model(**inputs)\n+    logits = outputs.logits\n+    predicted_answer_idx = logits.argmax(-1).item()\n+\n+print(f\"Predicted answer: {predicted_answer_idx}\")\n+```\n \n-The following example shows how to get the last hidden state using [`VisualBertModel`]:\n+</hfoption>\n+</hfoptions>\n \n-```python\n->>> import torch\n->>> from transformers import BertTokenizer, VisualBertModel\n+## Notes\n \n->>> model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n->>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+- Use a fine-tuned checkpoint for downstream tasks, like `visualbert-vqa` for visual question answering. Otherwise, use one of the pretrained checkpoints.\n+- The fine-tuned detector and weights aren't provided (available in the research projects), but the states can be directly loaded into the detector.\n+- The text input is concatenated in front of the visual embeddings in the embedding layer and is expected to be bound by `[CLS]` and [`SEP`] tokens.\n+- The segment ids must be set appropriately for the text and visual parts.\n+- Use [`BertTokenizer`] to encode the text and implement a custom detector/image processor to get the visual embeddings.\n \n->>> inputs = tokenizer(\"What is the man eating?\", return_tensors=\"pt\")\n->>> # this is a custom function that returns the visual embeddings given the image path\n->>> visual_embeds = get_visual_embeddings(image_path)\n+## Resources\n \n->>> visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n->>> visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n->>> inputs.update(\n-...     {\n-...         \"visual_embeds\": visual_embeds,\n-...         \"visual_token_type_ids\": visual_token_type_ids,\n-...         \"visual_attention_mask\": visual_attention_mask,\n-...     }\n-... )\n->>> outputs = model(**inputs)\n->>> last_hidden_state = outputs.last_hidden_state\n-```\n+- Refer to this [notebook](https://github.com/huggingface/transformers-research-projects/tree/main/visual_bert) for an example of using VisualBERT for visual question answering.\n+- Refer to this [notebook](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing) for an example of how to generate visual embeddings.\n \n ## VisualBertConfig\n "
        }
    ],
    "stats": {
        "total": 175,
        "additions": 102,
        "deletions": 73
    }
}