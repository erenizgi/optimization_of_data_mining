{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for LLaVa processor and update docs (#32858)\n\n* Uniformize kwargs for LlaVa and update docs\r\n\r\n* Change order of processor inputs in docstring\r\n\r\n* Improve BC support for reversed images and text inputs\r\n\r\n* cleanup llava processor call docstring\r\n\r\n* Add encoded inputs as valid text inputs in reverse input check, add deprecation version in warning\r\n\r\n* Put function check reversed images text outside base processor class\r\n\r\n* Refactor _validate_images_text_input_order\r\n\r\n* Add ProcessingUtilTester\r\n\r\n* fix processing and test_processing",
    "sha": "2f62146f0e916c3e6752b59d34853be6df0506f2",
    "files": [
        {
            "sha": "eb1c55341b0784846f2e7d7959e18ab8977a5237",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f62146f0e916c3e6752b59d34853be6df0506f2/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f62146f0e916c3e6752b59d34853be6df0506f2/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=2f62146f0e916c3e6752b59d34853be6df0506f2",
            "patch": "@@ -405,7 +405,7 @@ def forward(\n         >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_new_tokens=15)"
        },
        {
            "sha": "28a9410e6cbf0b3afc1f77f540603717d4dfcdda",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 39,
            "deletions": 34,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f62146f0e916c3e6752b59d34853be6df0506f2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f62146f0e916c3e6752b59d34853be6df0506f2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=2f62146f0e916c3e6752b59d34853be6df0506f2",
            "patch": "@@ -16,18 +16,33 @@\n Processor class for Llava.\n \"\"\"\n \n-from typing import List, Optional, Union\n+import sys\n+from typing import List, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n+if sys.version_info >= (3, 11):\n+    from typing import Unpack\n+else:\n+    from typing_extensions import Unpack\n+\n logger = logging.get_logger(__name__)\n \n \n+class LlavaProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n class LlavaProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Llava processor which wraps a Llava image processor and a Llava tokenizer into a single processor.\n@@ -73,12 +88,11 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         images: ImageInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length=None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[LlavaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -88,29 +102,15 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n-\n                 - `'tf'`: Return TensorFlow `tf.constant` objects.\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return NumPy `np.ndarray` objects.\n@@ -125,8 +125,19 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You have to specify at least one of `images` or `text`.\")\n+\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            LlavaProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         if images is not None:\n-            image_inputs = self.image_processor(images, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n \n@@ -158,13 +169,7 @@ def __call__(\n                     \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n                 )\n \n-        text_inputs = self.tokenizer(\n-            prompt_strings,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n         return BatchFeature(data={**text_inputs, **image_inputs})\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama"
        },
        {
            "sha": "305fc9e9a84cdb42a8ba06f2eda734f6260f27d9",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f62146f0e916c3e6752b59d34853be6df0506f2/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f62146f0e916c3e6752b59d34853be6df0506f2/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=2f62146f0e916c3e6752b59d34853be6df0506f2",
            "patch": "@@ -274,7 +274,7 @@ def test_small_model_integration_test(self):\n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n         image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(prompt, raw_image, return_tensors=\"pt\")\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\")\n \n         EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n         self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n@@ -299,7 +299,7 @@ def test_small_model_integration_test_llama_single(self):\n         prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? ASSISTANT:\"\n         image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n         EXPECTED_DECODED_TEXT = \"USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Finally, be respectful of the environment and other visitors, and follow any posted rules or guidelines for the area.\"  # fmt: skip\n@@ -325,7 +325,7 @@ def test_small_model_integration_test_llama_batched(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = processor(prompts, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -349,7 +349,7 @@ def test_small_model_integration_test_batch(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = self.processor(prompts, images=[image1, image2], return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -381,7 +381,7 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n-        inputs = processor(prompts, images=[image1, image2, image1], return_tensors=\"pt\", padding=True)\n+        inputs = processor(images=[image1, image2, image1], text=prompts, return_tensors=\"pt\", padding=True)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n@@ -409,8 +409,8 @@ def test_batched_generation(self):\n         image2 = Image.open(requests.get(url2, stream=True).raw)\n \n         inputs = processor(\n-            text=[prompt1, prompt2, prompt3],\n             images=[image1, image2, image1, image2],\n+            text=[prompt1, prompt2, prompt3],\n             return_tensors=\"pt\",\n             padding=True,\n         ).to(torch_device)\n@@ -444,7 +444,7 @@ def test_llava_index_error_bug(self):\n         image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n \n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         # Make sure that `generate` works\n         _ = model.generate(**inputs, max_new_tokens=20)\n@@ -510,7 +510,7 @@ def test_generation_no_images(self):\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         # Prepare inputs with no images\n-        inputs = processor(\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n+        inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n \n         # Make sure that `generate` works\n         _ = model.generate(**inputs, max_new_tokens=20)\n@@ -554,13 +554,13 @@ def test_expansion_in_processing(self):\n         # check processing with expansion of inputs\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n-        inputs_expanded = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n-        inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n         self.assertTrue(inputs.input_ids.shape[-1] == 18)\n \n         # generate exactly 20 tokens"
        },
        {
            "sha": "5b05a8b92ea513f4812f662d698a989ab8cdb1d3",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 54,
            "deletions": 3,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f62146f0e916c3e6752b59d34853be6df0506f2/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f62146f0e916c3e6752b59d34853be6df0506f2/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=2f62146f0e916c3e6752b59d34853be6df0506f2",
            "patch": "@@ -11,18 +11,43 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import shutil\n+import tempfile\n import unittest\n \n-from transformers.testing_utils import require_vision\n+from transformers import AutoProcessor, AutoTokenizer, LlamaTokenizerFast, LlavaProcessor\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n-    from transformers import AutoTokenizer, LlavaProcessor\n+    from transformers import CLIPImageProcessor\n \n \n @require_vision\n-class LlavaProcessorTest(unittest.TestCase):\n+class LlavaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = LlavaProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = CLIPImageProcessor(do_center_crop=False)\n+        tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n+\n+        processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n     def test_can_load_various_tokenizers(self):\n         for checkpoint in [\"Intel/llava-gemma-2b\", \"llava-hf/llava-1.5-7b-hf\"]:\n             processor = LlavaProcessor.from_pretrained(checkpoint)\n@@ -45,3 +70,29 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            images=image_input,\n+            text=input_str,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 5)"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 104,
        "deletions": 48
    }
}