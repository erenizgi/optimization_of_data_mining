{
    "author": "SunMarc",
    "message": "Fix tests fsdp (#41422)\n\n* Fix tests\n\n* fix !\n\n* fix",
    "sha": "add4df62ba7b68ed26e9828c02aa6f886e61caa1",
    "files": [
        {
            "sha": "e8972109fc40d9f5688bacadbeff1bcae481c83e",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/add4df62ba7b68ed26e9828c02aa6f886e61caa1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add4df62ba7b68ed26e9828c02aa6f886e61caa1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=add4df62ba7b68ed26e9828c02aa6f886e61caa1",
            "patch": "@@ -473,7 +473,7 @@ class TrainingArguments:\n             When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n             stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n             can take a long time) but will not yield the same results as the interrupted training would have.\n-        fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `[]`):\n+        fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `None`):\n             Use PyTorch Distributed Parallel Training (in distributed training only).\n \n             A list of options along the following:\n@@ -1146,8 +1146,8 @@ class TrainingArguments:\n             )\n         },\n     )\n-    fsdp: Union[list[FSDPOption], str, bool] = field(\n-        default_factory=list,\n+    fsdp: Optional[Union[list[FSDPOption], str]] = field(\n+        default=None,\n         metadata={\n             \"help\": (\n                 \"Whether or not to use PyTorch Fully Sharded Data Parallel (FSDP) training (in distributed training\"\n@@ -1734,10 +1734,13 @@ def __post_init__(self):\n         if not isinstance(self.warmup_steps, int) or self.warmup_steps < 0:\n             raise ValueError(\"warmup_steps must be of type int and must be 0 or a positive integer.\")\n \n-        if isinstance(self.fsdp, bool):\n-            self.fsdp = [FSDPOption.FULL_SHARD] if self.fsdp else \"\"\n-        if isinstance(self.fsdp, str):\n+        if self.fsdp is None:\n+            self.fsdp = []\n+        elif self.fsdp is True:\n+            self.fsdp = [FSDPOption.FULL_SHARD]\n+        elif isinstance(self.fsdp, str):\n             self.fsdp = [FSDPOption(s) for s in self.fsdp.split()]\n+\n         if self.fsdp == [FSDPOption.OFFLOAD]:\n             raise ValueError(\n                 \"`--fsdp offload` can't work on its own. It needs to be added to `--fsdp full_shard` or \""
        },
        {
            "sha": "93b4adcd8a8bd64aa44d06a03d4f9c3599b7c65b",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 19,
            "deletions": 11,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/add4df62ba7b68ed26e9828c02aa6f886e61caa1/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add4df62ba7b68ed26e9828c02aa6f886e61caa1/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=add4df62ba7b68ed26e9828c02aa6f886e61caa1",
            "patch": "@@ -258,8 +258,12 @@ def test_fsdp_config_transformers_auto_wrap(self, sharding_strategy, dtype):\n     def test_basic_run(self, sharding_strategy, dtype):\n         launcher = get_launcher(distributed=True, use_accelerate=False)\n         output_dir = self.get_auto_remove_tmp_dir()\n+        fsdp_config = '{\"fsdp_transformer_layer_cls_to_wrap\": \"BertLayer\"}'\n         args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\"]\n-        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n+        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_config\", f\"{fsdp_config}\"]\n+        if dtype == \"fp16\":\n+            # fp16 + fsdp + fused adamw torch breaks so we switch optimizers\n+            fsdp_args += [\"--optim\", \"adamw_torch\"]\n         script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n         cmd = launcher + script + args + fsdp_args\n         execute_subprocess_async(cmd, env=self.get_env())\n@@ -271,8 +275,12 @@ def test_basic_run(self, sharding_strategy, dtype):\n     def test_basic_run_with_gradient_accumulation(self, sharding_strategy, dtype):\n         launcher = get_launcher(distributed=True, use_accelerate=False)\n         output_dir = self.get_auto_remove_tmp_dir()\n+        fsdp_config = '{\"fsdp_transformer_layer_cls_to_wrap\": \"BertLayer\"}'\n         args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\", \"--gradient_accumulation_steps\", \"2\"]\n-        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n+        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_config\", f\"{fsdp_config}\"]\n+        if dtype == \"fp16\":\n+            # fp16 + fsdp + fused adamw torch breaks so we switch optimizers\n+            fsdp_args += [\"--optim\", \"adamw_torch\"]\n         script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n         cmd = launcher + script + args + fsdp_args\n         execute_subprocess_async(cmd, env=self.get_env())\n@@ -285,7 +293,11 @@ def test_basic_run_with_cpu_offload(self, dtype):\n         launcher = get_launcher(distributed=True, use_accelerate=False)\n         output_dir = self.get_auto_remove_tmp_dir()\n         args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\", \"--max_steps\", \"10\"]\n-        fsdp_args = [\"--fsdp\", \"full_shard auto_wrap offload\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n+        fsdp_config = '{\"fsdp_transformer_layer_cls_to_wrap\": \"BertLayer\"}'\n+        fsdp_args = [\"--fsdp\", \"full_shard auto_wrap offload\", \"--fsdp_config\", f\"{fsdp_config}\"]\n+        if dtype == \"fp16\":\n+            # fp16 + fsdp + fused adamw torch breaks so we switch optimizers\n+            fsdp_args += [\"--optim\", \"adamw_torch\"]\n         script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n         cmd = launcher + script + args + fsdp_args\n         execute_subprocess_async(cmd, env=self.get_env())\n@@ -295,7 +307,7 @@ def test_basic_run_with_cpu_offload(self, dtype):\n     @run_first\n     @slow\n     def test_training_and_can_resume_normally(self, state_dict_type):\n-        output_dir = self.get_auto_remove_tmp_dir(\"./xxx\", after=False)\n+        output_dir = self.get_auto_remove_tmp_dir()\n \n         sharding_strategy = \"full_shard\"\n         use_accelerate = state_dict_type == \"SHARDED_STATE_DICT\"\n@@ -351,7 +363,7 @@ def test_fsdp_cpu_offloading(self):\n     @require_fsdp_v2_version\n     @require_accelerate_fsdp2\n     def test_accelerate_fsdp2_integration(self):\n-        output_dir = self.get_auto_remove_tmp_dir(\"./xxx\", after=False)\n+        output_dir = self.get_auto_remove_tmp_dir()\n         sharding_strategy = \"full_shard\"\n         use_accelerate = True\n \n@@ -415,12 +427,8 @@ def test_fsdp2_cpu_offloading(self):\n \n     def run_cmd_and_get_logs(self, use_accelerate, sharding_strategy, launcher, script, args, output_dir):\n         if not use_accelerate:\n-            fsdp_args = [\n-                \"--fsdp\",\n-                f\"{sharding_strategy} auto_wrap\",\n-                \"--fsdp_transformer_layer_cls_to_wrap\",\n-                \"BertLayer\",\n-            ]\n+            fsdp_config = '{\"fsdp_transformer_layer_cls_to_wrap\": \"BertLayer\"}'\n+            fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_config\", f\"{fsdp_config}\"]\n             cmd = launcher + script + args + fsdp_args\n         else:\n             fsdp_config = f\"\"\""
        }
    ],
    "stats": {
        "total": 45,
        "additions": 28,
        "deletions": 17
    }
}