{
    "author": "vasqu",
    "message": "[`FA`] Cleanup loading logic (#41427)\n\n* fix\n\n* style\n\n* fix kernels loading as well\n\n* fix typing\n\n* refactor CB loading logic as well\n\n* fix base fa logic\n\n* rename\n\n* properly lazy load paged fa\n\n* fix\n\n* check if ci is crashing again\n\n* fix fallback\n\n* style\n\n* allow varlen only, e.g. for metal kernel\n\n* fixup new namings from flash-attn to flash-attn2\n\n* make it a bit more explicit\n\n* add comment",
    "sha": "dc7364824d3020a261068618e31e190ba2b473fb",
    "files": [
        {
            "sha": "47ea070da8255b1981defcb4f59efd513a9d5662",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -172,7 +172,7 @@ def batch_generate(\n \n     # Model parameters\n     parser.add_argument(\"--sliding-window\", type=int, default=0)\n-    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n+    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn2\", help=\"Attention implementation\")\n \n     # Performance parameters\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable"
        },
        {
            "sha": "be69894470cd14bbd575e60f6998d4754851afba",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -31,7 +31,7 @@\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n     parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n-    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n+    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn2\", help=\"Attention implementation\")\n     parser.add_argument(\"--samples\", type=int, default=500)\n     parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n "
        },
        {
            "sha": "3c391c432fc0fd9ae5be0a2213e2cb3f4966cdda",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -742,17 +742,14 @@ def __init__(\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+            model.config._attn_implementation = attn_implementation\n \n-            from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-\n-            if attn_implementation not in ALL_ATTENTION_FUNCTIONS._global_mapping:  # when its a kernel\n-                # load_and_register_attn_kernel is imported here to avoid CUDA init\n-                from ...integrations.flash_paged import paged_attention_forward\n-                from ...integrations.hub_kernels import load_and_register_attn_kernel\n+            # lazy loading flash attention including kernel variations\n+            if \"flash\" in attn_implementation:\n+                from ...modeling_flash_attention_utils import lazy_import_paged_flash_attention\n \n-                load_and_register_attn_kernel(attn_implementation, paged_attention_forward)\n+                lazy_import_paged_flash_attention(attn_implementation)\n \n-            model.config._attn_implementation = attn_implementation\n         self.model = model.eval()\n         generation_config = model.generation_config if generation_config is None else generation_config\n         self.generation_config = generation_config"
        },
        {
            "sha": "ee880f865607299a7be5b9754664f2ec224c77dc",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -1,24 +1,7 @@\n import torch\n \n from ..generation.continuous_batching import PagedAttentionCache\n-from ..utils import is_flash_attn_2_available\n-\n-\n-# For some reason, if we dont assign the function to a variable here, it will be garbage collected\n-try:\n-    if is_flash_attn_2_available():\n-        from flash_attn import flash_attn_varlen_func  # noqa: F401\n-\n-        FLASH_ATTN_VARLEN_FUNC = flash_attn_varlen_func\n-    else:\n-        raise RuntimeError(\n-            \"Flash Attention 2 is not installed. Please refer to https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install it\"\n-        )\n-except Exception as e:\n-    msg = repr(e)\n-\n-    def FLASH_ATTN_VARLEN_FUNC(*args, **kwargs):\n-        raise Exception(f\"flash_attn_varlen_func is not available: {msg}\")\n+from ..modeling_flash_attention_utils import lazy_import_paged_flash_attention\n \n \n def paged_attention_forward(\n@@ -32,7 +15,6 @@ def paged_attention_forward(\n     cu_seq_lens_k=None,\n     max_seqlen_q=None,\n     max_seqlen_k=None,\n-    implementation=None,\n     **kwargs,\n ) -> torch.Tensor:\n     r\"\"\"Perform the forward pass of attention with paged key-value cache.\n@@ -57,6 +39,8 @@ def paged_attention_forward(\n         window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n         softcap: float. Anything > 0 activates softcapping attention.\n     \"\"\"\n+    flash_attn_varlen_func = lazy_import_paged_flash_attention(module.config._attn_implementation)\n+\n     sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window - 1, 0)\n     layer_type = \"full_attention\" if sliding_window == (-1, -1) else \"sliding_attention\"\n \n@@ -75,11 +59,6 @@ def paged_attention_forward(\n         cu_seq_lens_k = cu_seq_lens_k[layer_type]\n         max_seqlen_k = max_seqlen_k[layer_type]\n \n-    if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n-        flash_attn_varlen_func = implementation.flash_attn_varlen_func\n-    else:\n-        flash_attn_varlen_func = FLASH_ATTN_VARLEN_FUNC\n-\n     custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n \n     attn_output = flash_attn_varlen_func("
        },
        {
            "sha": "e2065a6f61d9649fd81d6c38ef86b5238b21d62e",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -14,10 +14,8 @@\n import os\n import re\n from collections.abc import Callable\n-from functools import partial\n from types import ModuleType\n \n-from ..modeling_flash_attention_utils import lazy_import_flash_attention\n from ..utils import ENV_VARS_TRUE_VALUES, logging\n from ..utils.import_utils import is_kernels_available\n from .flash_attention import flash_attention_forward\n@@ -217,7 +215,9 @@ def is_kernel(attn_implementation: str | None) -> bool:\n     )\n \n \n-def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: Callable | None = None) -> None:\n+def load_and_register_attn_kernel(\n+    attn_implementation: str, attention_wrapper: Callable | None = None\n+) -> ModuleType | None:\n     \"\"\"\n     Load and register the kernel associated to `attn_implementation`.\n \n@@ -233,7 +233,7 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: C\n \n     actual_attn_name = attn_implementation.split(\"|\")[1] if \"|\" in attn_implementation else attn_implementation\n     if not is_kernel(actual_attn_name):\n-        return\n+        return None\n     if not _kernels_available:\n         raise ImportError(\n             \"`kernels` is either not installed or uses an incompatible version. \"\n@@ -262,14 +262,16 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: C\n     if hasattr(kernel, \"flash_attn_varlen_func\"):\n         if attention_wrapper is None:\n             attention_wrapper = flash_attention_forward\n-        kernel_function = partial(attention_wrapper, implementation=kernel)\n-        lazy_import_flash_attention(kernel, force_import=True)\n+        kernel_function = attention_wrapper\n     elif kernel_name is not None:\n         kernel_function = getattr(kernel, kernel_name)\n+\n     # Register the kernel as a valid attention\n     ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n+    return kernel\n+\n \n def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _KERNEL_MODULE_MAPPING):\n     if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):"
        },
        {
            "sha": "7925bef8bba5650fb23bc206bff920248258a3d7",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 43,
            "deletions": 11,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n import inspect\n import os\n+from collections.abc import Callable\n from functools import partial\n from typing import Optional, TypedDict\n \n@@ -55,6 +56,7 @@ def is_flash_attn_available():\n \n \n # `globals()` is not compatible with dynamo, hence we have do define them in global scope ourselves\n+_loaded_implementation = None\n _flash_fn = None\n _flash_varlen_fn = None\n _pad_fn = None\n@@ -69,7 +71,7 @@ def is_flash_attn_available():\n }\n \n \n-def _lazy_imports(implementation: Optional[str]):\n+def _lazy_imports(implementation: Optional[str], attention_wrapper: Optional[Callable] = None):\n     \"\"\"\n     Lazy loads the respective flash attention implementations.\n \n@@ -85,6 +87,9 @@ def _lazy_imports(implementation: Optional[str]):\n \n     pad_input, unpad_input = _pad_input, _unpad_input\n \n+    is_paged = implementation.startswith(\"paged|\")\n+    implementation = implementation.split(\"|\")[1] if is_paged else implementation\n+\n     if (implementation == \"flash_attention_2\" and is_fa2) or (implementation is None and is_fa2 and not is_fa3):\n         from flash_attn import flash_attn_func, flash_attn_varlen_func\n         from flash_attn.bert_padding import pad_input, unpad_input\n@@ -98,12 +103,24 @@ def _lazy_imports(implementation: Optional[str]):\n             from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n         # Kernels fallback\n         else:\n-            flash_attn_func = getattr(implementation, \"flash_attn_func\", None)\n-            flash_attn_varlen_func = getattr(implementation, \"flash_attn_varlen_func\", None)\n-            if flash_attn_varlen_func is None or flash_attn_func is None:\n+            from .integrations.hub_kernels import load_and_register_attn_kernel\n+\n+            # We want to explicitly register the name with `paged|` if found\n+            kernel_implementation = f\"paged|{implementation}\" if is_paged else implementation\n+            kernel = load_and_register_attn_kernel(kernel_implementation, attention_wrapper)\n+\n+            flash_attn_func = getattr(kernel, \"flash_attn_func\", None)\n+            flash_attn_varlen_func = getattr(kernel, \"flash_attn_varlen_func\", None)\n+            if flash_attn_varlen_func is None:\n                 raise ValueError(\n                     f\"Could not find the currently requested flash attention implementation at `{implementation}`.\"\n-                    f\"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn2`.\"\n+                    \"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn2`.\"\n+                )\n+            if flash_attn_func is None:\n+                logger.warning(\n+                    f\"The loaded flash attention implementation at `{implementation}` only supports varlen, i.e. \"\n+                    \"it can only be used with continous batching and does not support the full functionality for \"\n+                    \"the base transformers generation methods.\"\n                 )\n \n     return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input\n@@ -130,24 +147,39 @@ def _lazy_define_process_function(flash_function):\n     return partial(_process_flash_attention_kwargs, supports_mapping=supports_mapping)\n \n \n-def lazy_import_flash_attention(implementation: Optional[str], force_import: Optional[bool] = False):\n+def lazy_import_flash_attention(implementation: Optional[str], attention_wrapper: Optional[Callable] = None):\n     \"\"\"\n     Lazily import flash attention and return the respective functions + flags.\n \n     NOTE: For fullgraph, this needs to be called before compile, while no fullgraph can\n     work without preloading. See `load_and_register_attn_kernel` in `integrations.hub_kernels`.\n     \"\"\"\n-    global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n-    if force_import or any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):\n-        _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation)\n+    global _loaded_implementation\n+    if implementation is None and _loaded_implementation is None:\n+        raise ValueError(\"Could not find any flash attn implementation based on your environment.\")\n+\n+    global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn, _process_flash_kwargs_fn\n+    if implementation is not None and _loaded_implementation != implementation:\n+        _loaded_implementation = implementation\n \n-    global _process_flash_kwargs_fn\n-    if force_import or _process_flash_kwargs_fn is None:\n+        _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation, attention_wrapper)\n         _process_flash_kwargs_fn = _lazy_define_process_function(_flash_varlen_fn)\n \n     return (_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn), _process_flash_kwargs_fn\n \n \n+def lazy_import_paged_flash_attention(implementation: Optional[str]):\n+    \"\"\"\n+    Same as `lazy_import_flash_attention` but explicitly wrapping it with the paged implementation.\n+    \"\"\"\n+    from .integrations.flash_paged import paged_attention_forward\n+\n+    (_, flash_attn_varlen_func, _, _), _ = lazy_import_flash_attention(\n+        implementation, attention_wrapper=paged_attention_forward\n+    )\n+    return flash_attn_varlen_func\n+\n+\n def _index_first_axis(tensor, indices):\n     \"\"\"\n     A local implementation of the PyTorch indexing operation `tensor[indices]` on the first axis,"
        },
        {
            "sha": "25dc5026002602860cc45e716d33ed78a4aa31fc",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc7364824d3020a261068618e31e190ba2b473fb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=dc7364824d3020a261068618e31e190ba2b473fb",
            "patch": "@@ -70,7 +70,7 @@\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flash_paged import paged_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n-from .integrations.hub_kernels import is_kernel, load_and_register_attn_kernel\n+from .integrations.hub_kernels import is_kernel\n from .integrations.peft import maybe_load_adapters\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n@@ -1790,9 +1790,11 @@ def _check_and_adjust_attn_implementation(\n         applicable_attn_implementation = attn_implementation\n \n         # If FA not installed, do not fail but use kernels instead\n+        requested_original_flash_attn = attn_implementation is not None and (\n+            attn_implementation == \"flash_attention_2\" or attn_implementation == \"flash_attention_3\"\n+        )\n         if (\n-            attn_implementation is not None\n-            and \"flash\" in attn_implementation\n+            requested_original_flash_attn\n             and self._supports_flash_attn\n             and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n             and is_kernels_available()\n@@ -1801,23 +1803,26 @@ def _check_and_adjust_attn_implementation(\n             if attn_implementation.endswith(\"2\"):\n                 applicable_attn_implementation = \"kernels-community/flash-attn2\"\n                 if is_torch_xpu_available():\n-                    # On XPU, kernels library is the native implementation. Rename variable to avoid \"fallback\" warning and irrelevant checks.\n-                    attn_implementation = \"kernels-community/flash-attn2\"\n+                    # On XPU, kernels library is the native implementation\n+                    # Disabling this flag to avoid giving wrong fallbacks on errors and warnings\n+                    requested_original_flash_attn = False\n             else:\n                 applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n \n         if is_kernel(applicable_attn_implementation):\n             try:\n-                load_and_register_attn_kernel(applicable_attn_implementation)\n+                # preload flash attention here to allow compile with fullgraph\n+                lazy_import_flash_attention(applicable_attn_implementation)\n+\n                 # log that we used kernel fallback if successful\n-                if \"flash_\" in attn_implementation:\n+                if requested_original_flash_attn:\n                     logger.warning_once(\n                         f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                         \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n                 # raise the proper exception for requested flash attention\n-                if attn_implementation.startswith(\"flash_\"):\n+                if requested_original_flash_attn:\n                     if attn_implementation.endswith(\"2\"):\n                         self._flash_attn_2_can_dispatch()\n                     else:\n@@ -1829,9 +1834,11 @@ def _check_and_adjust_attn_implementation(\n             applicable_attn_implementation = self.get_correct_attn_implementation(\n                 applicable_attn_implementation, is_init_check\n             )\n+\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_\"):\n-                lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n+            if \"flash\" in applicable_attn_implementation:\n+                lazy_import_flash_attention(applicable_attn_implementation)\n+\n         return applicable_attn_implementation\n \n     def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n@@ -4653,6 +4660,7 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n+        \"paged|flash_attention_3\": paged_attention_forward,\n         \"paged|flash_attention_2\": paged_attention_forward,\n         \"paged|sdpa\": sdpa_attention_paged_forward,\n         \"paged|eager\": eager_paged_attention_forward,"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 79,
        "deletions": 61
    }
}