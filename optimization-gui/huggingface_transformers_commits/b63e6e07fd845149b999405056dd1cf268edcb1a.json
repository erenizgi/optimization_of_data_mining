{
    "author": "ArthurZucker",
    "message": "Update conversion mapping to separate renaming from converting (#42254)\n\n* inital commit\n\n* up\n\n* update unexpected later on\n\n* fix\n\n* update\n\n* simplify our lives\n\n* isolate a bit more\n\n* fixup\n\n* small nits\n\n* style\n\n* nit\n\n* fix common cases\n\n* fix post merge\n\n* bnb needs missing keys\n\n* small fix\n\n* bettrer documentation\n\n* no veradict + base class\n\n* rake review comments\n\n* take all comments\n\n* fix super init\n\n* update doc to be more real\n\n* small nits\n\n* nits\n\n* fix dtype\n\n* fix dtype issue\n\n* remove one unused function\n\n* cleanup and nits\n\n* up\n\n* should be the final fix!\n\n* fixup",
    "sha": "b63e6e07fd845149b999405056dd1cf268edcb1a",
    "files": [
        {
            "sha": "b6aad7e94650fd98d4c3a8905f8b4cfac788b469",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=b63e6e07fd845149b999405056dd1cf268edcb1a",
            "patch": "@@ -15,7 +15,7 @@\n \n from copy import deepcopy\n \n-from .core_model_loading import Concatenate, MergeModulelist, WeightConverter\n+from .core_model_loading import Concatenate, MergeModulelist, WeightConverter, WeightRenaming\n from .utils import is_torch_available\n \n \n@@ -26,6 +26,7 @@\n def _build_checkpoint_conversion_mapping():\n     mapping = {\n         \"mixtral\": [\n+            WeightRenaming(\".block_sparse_moe.gate\", \".mlp.gate\"),\n             WeightConverter(\n                 source_keys=[\n                     \"block_sparse_moe.experts.*.w1.weight\",\n@@ -50,12 +51,6 @@ def _build_checkpoint_conversion_mapping():\n                     ),  # each process has two lists of tensors, we cat each list. -> we end up with 2 tensors\n                 ],  # we want the loading to add this shard operation here. Though we can't shard after concats and merge, needs to be first\n             ),\n-            # WeightConverter(\n-            #     [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],\n-            #     \"self_attn.qkv_proj\",\n-            #     operations=[Concatenate(dim=0)],  # more like stack?\n-            # ),\n-            WeightConverter(\"*.block_sparse_moe.\", \"*.mlp.\"),\n         ],\n         \"qwen2_moe\": [\n             WeightConverter(\n@@ -73,34 +68,34 @@ def _build_checkpoint_conversion_mapping():\n             ),\n         ],\n         \"legacy\": [\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"LayerNorm.gamma\",\n                 target_keys=\"LayerNorm.weight\",\n             ),\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"LayerNorm.beta\",\n                 target_keys=\"LayerNorm.bias\",\n             ),\n         ],\n     }\n     if hasattr(torch.nn.utils.parametrizations, \"weight_norm\"):\n         mapping[\"legacy\"] += [\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"weight_g\",\n                 target_keys=\"parametrizations.weight.original0\",\n             ),\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"weight_v\",\n                 target_keys=\"parametrizations.weight.original1\",\n             ),\n         ]\n     else:\n         mapping[\"legacy\"] += [\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"parametrizations.weight.original0\",\n                 target_keys=\"weight_g\",\n             ),\n-            WeightConverter(\n+            WeightRenaming(\n                 source_keys=\"parametrizations.weight.original1\",\n                 target_keys=\"weight_v\",\n             ),"
        },
        {
            "sha": "f8db2406017cf0c2536d6bad8230fe7029863e90",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 453,
            "deletions": 290,
            "changes": 743,
            "blob_url": "https://github.com/huggingface/transformers/blob/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=b63e6e07fd845149b999405056dd1cf268edcb1a",
            "patch": "@@ -16,16 +16,14 @@\n \n from __future__ import annotations\n \n-import itertools\n import os\n import re\n from abc import abstractmethod\n-from collections import defaultdict\n from collections.abc import MutableMapping, MutableSet, Sequence\n from concurrent.futures import Future, ThreadPoolExecutor\n from contextlib import contextmanager\n+from copy import deepcopy\n from dataclasses import dataclass, field\n-from functools import partial\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n import torch\n@@ -49,71 +47,57 @@\n logger = logging.get_logger(__name__)\n \n \n-def _glob_to_regex_src(glob: str, *, digits_only: bool = True) -> str:\n+def compile_glob_rule(source_glob: str, target_glob: str) -> tuple[re.Pattern, str]:\n     \"\"\"\n-    Convert a glob with '*' into a regex *source* string. We don't use `glob.translate`\n-    '*' matches (\\\\d+) if digits_only else (.+). Inner groups are non-capturing.\n+    Convert a glob-style source + target into a full regex + replacement.\n+\n+    Rules:\n+      - '*' in source_glob  →  (.*) capture group\n+      - '*' in target_glob  →  \\\\1, \\\\2, ... backrefs\n     \"\"\"\n-    star = r\"(\\d+)\" if digits_only else r\"(.+)\"\n-    return glob.replace(r\"\\*\", star)\n+    regex = re.compile(source_glob)\n \n+    counter = 0\n \n-def build_glob_alt(\n-    globs: list[str],\n-) -> tuple[re.Pattern, dict[str, str]]:\n-    r\"\"\"\n-    Build one compiled regex alternation with a named group per glob. This allows to run a single\n-    re.match and get the correct group name to finally get which pattern matched.\n-    Returns (compiled_regex, name->glob map).\n-\n-    Example:\n-\n-    ```py\n-    >>> reg, map_ = build_glob_alt([\"mlp.*.w1\", \"mlp.*.w2\"])\n-    >>> print(reg)\n-    (re.compile(r'(?P<g0>.*mlp\\.(\\d+)\\.w1)|(?P<g1>.*mlp\\.(\\d+)\\.w2)', re.UNICODE),\n-    >>> print(map_)\n-    {'g0': 'mlp.*.w1', 'g1': 'mlp.*.w2'})\n-    >>> match_ = reg.match(\"model.layers.0.mlp.0.w1.weight\")\n-    >>> print(match_.lastgroup)\n-    'g0'\n-    >>> print(map_[match_.lastgroup])\n-    mlp.*.w1\n-    ```\n-    \"\"\"\n-    name_map: dict[str, str] = {}\n-    parts: list[str] = []\n-\n-    for i, g in enumerate(globs):\n-        name = f\"g{i}\"\n-        name_map[name] = g\n-        pat_src = _glob_to_regex_src(g)\n-        prefix_src = \"\"\n-        if pat_src.startswith(\"*\"):\n-            prefix_src = \".\"\n-        elif not pat_src.startswith(r\"\\^\") and not pat_src.startswith(r\".*\"):\n-            prefix_src = \".*\"\n-\n-        parts.append(f\"(?P<{name}>{prefix_src}{pat_src}.*)\")\n-\n-    alt_src = \"|\".join(parts).replace(\"\\\\^\", \"^\").replace(\"\\\\.\", r\"\\.\")\n-    try:\n-        reg = re.compile(alt_src)\n-    except re.error as e:\n-        logger.error(f\"Error compiling regex for alternation: {alt_src}\")\n-        raise e\n+    def _star_to_backref(_: re.Match) -> str:\n+        nonlocal counter\n+        counter += 1\n+        return rf\"\\{counter}\"\n \n-    return reg, name_map\n+    replacement = re.sub(r\"\\*\", _star_to_backref, target_glob)\n+    return regex, replacement\n \n \n-def match_glob(key: str, alt: re.Pattern, name_map: dict[str, str]) -> Optional[str]:\n+def build_glob_alternation(\n+    globs: list[Union[WeightRenaming, WeightConverter, str]],\n+) -> tuple[re.Pattern, dict[str, str], dict[str, str]]:\n     \"\"\"\n-    Match the key against the alternation; return the original glob string that matched.\n+    Build a single alternation regex with one named group per glob.\n     \"\"\"\n-    m = alt.match(key)\n-    if not m:\n-        return None\n-    return name_map.get(m.lastgroup)\n+    src_group_to_glob: dict[str, str] = {}\n+    tgt_group_to_glob: dict[str, str] = {}\n+    branches: list[str] = []\n+    i = 0\n+    for glob in globs:\n+        if isinstance(glob, (WeightRenaming, WeightConverter)):\n+            for src in glob.source_keys:\n+                group_name = f\"g{i}\"\n+                src_group_to_glob[group_name] = src\n+                i += 1\n+                body = src.replace(\"*\", r\".*\")\n+                branches.append(f\"(?P<{group_name}>{body})\")\n+                tgt_group_to_glob[group_name] = glob.target_keys[0]  # we index witht the first target\n+        else:\n+            group_name = f\"g{i}\"\n+            src_group_to_glob[group_name] = glob\n+            i += 1\n+            body = glob\n+            body = body.replace(\"*\", r\".*\")\n+            branches.append(f\"(?P<{group_name}>{body})\")\n+            tgt_group_to_glob[group_name] = glob\n+\n+    alternation = re.compile(\"|\".join(branches))\n+    return alternation, src_group_to_glob, tgt_group_to_glob\n \n \n class ConversionOps:\n@@ -124,8 +108,14 @@ class ConversionOps:\n \n     @abstractmethod\n     def convert(\n-        self, value: Union[dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor], *args, **kwargs\n-    ) -> torch.Tensor:\n+        self,\n+        value: dict[str, Any],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+        **kwargs,\n+    ) -> dict[str, list[torch.Tensor]]:\n         raise NotImplementedError\n \n \n@@ -135,20 +125,24 @@ class Chunk(ConversionOps):\n     reverse_op: type[ConversionOps]\n \n     def __init__(self, dim: int = 0, chunks: Optional[int] = None, sizes: Optional[Sequence[int]] = None):\n-        if chunks is None and sizes is None:\n-            raise ValueError(\"`chunks` or `sizes` must be provided for Chunk operations.\")\n-        if chunks is not None and chunks <= 0:\n-            raise ValueError(\"`chunks` must be a strictly positive integer.\")\n         self.dim = dim\n         self.chunks = chunks\n         self.sizes = list(sizes) if sizes is not None else None\n         self.reverse_op = Concatenate\n \n-    def convert(self, value: torch.Tensor, *args, **kwargs) -> list[torch.Tensor]:\n-        # chunk requires a single tensor input\n-        if len(value) != 1 or len(value[0]) != 1:\n-            raise ValueError(\"Chunk operation requires a single tensor input.\")\n-        return list(torch.chunk(value[0][0], self.chunks, dim=self.dim))\n+    def convert(\n+        self,\n+        value: dict[str, list[torch.Tensor]],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+    ) -> dict[str, list[torch.Tensor]]:\n+        tensors = next(iter(value.values()))\n+        tensor = tensors[0]\n+        sizes = len(target_keys)\n+        chunks = torch.chunk(tensor, sizes, dim=self.dim)\n+        return {full_layer_name.replace(target_keys[0], target): [chunk] for target, chunk in zip(target_keys, chunks)}\n \n \n class Concatenate(ConversionOps):\n@@ -161,14 +155,20 @@ def __init__(self, dim: int = 0):\n         self.reverse_op = Chunk\n \n     @torch.no_grad\n-    def convert(self, value: Sequence[torch.Tensor], *args, **kwargs) -> torch.Tensor:\n-        if isinstance(value[0], list):\n-            value = [v[0] for v in value]\n-        tensors = value\n-        if not tensors:\n-            raise ValueError(\"Fuse requires at least one tensor to concatenate.\")\n+    def convert(\n+        self,\n+        value: dict[str, list[torch.Tensor]],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+    ) -> dict[str, torch.Tensor]:\n+        if len(target_keys) != 1:\n+            raise ValueError(\"Concatenate expects a single target key.\")\n+        if len(value) != len(source_keys):\n+            raise ValueError(\"Concatenate received an unexpected number of tensors compared to source keys.\")\n \n-        return torch.cat(tuple(tensors), dim=self.dim)\n+        return {full_layer_name: torch.cat(tuple(value.values()), dim=self.dim)}\n \n \n class MergeModulelist(Concatenate):\n@@ -183,13 +183,21 @@ def __init__(self, dim: int = 0):\n         self.reverse_op = SplitModulelist\n \n     @torch.no_grad\n-    def convert(self, value: Sequence[torch.Tensor], *args, **kwargs) -> list[torch.Tensor]:\n-        merged = []\n-        for group in value:\n-            if not isinstance(group, Sequence) or len(group) == 0:\n-                raise ValueError(\"MergeModulelist requires non-empty sub-sequences.\")\n-            group = [k for k in group if k.ndim]\n-            merged.append(torch.stack(group, dim=self.dim))\n+    def convert(\n+        self,\n+        value: dict[str, list[torch.Tensor]],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+    ) -> dict[str, torch.Tensor]:\n+        merged: dict[str, torch.Tensor] = {}\n+        for idx, key in enumerate(value.keys()):\n+            tensors = value.get(key, [])\n+            if len(source_keys) == 1:\n+                key = full_layer_name\n+            stacked = torch.stack(tensors, dim=self.dim)\n+            merged[key] = stacked\n         return merged\n \n \n@@ -204,18 +212,24 @@ def __init__(self, sizes: Sequence[Sequence[int]], dim: int = 0):\n         self.reverse_op = MergeModulelist\n \n     @torch.no_grad\n-    def convert(self, value: Sequence[torch.Tensor], *, context: dict[str, Any]) -> list[list[torch.Tensor]]:\n-        if not isinstance(value, Sequence):\n-            raise TypeError(\"SplitModulelist expects a sequence of tensors.\")\n+    def convert(\n+        self,\n+        value: dict[str, list[torch.Tensor]],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+    ) -> dict[str, list[torch.Tensor]]:\n         if len(value) != len(self.sizes):\n-            raise ValueError(\"Number of tensors does not match the provided split specifications.\")\n-\n-        result: list[list[torch.Tensor]] = []\n-        for tensor, split_sizes in zip(value, self.sizes):\n-            if not isinstance(tensor, torch.Tensor):\n+            raise ValueError(\"SplitModulelist received an unexpected number of tensors.\")\n+        result: dict[str, list[torch.Tensor]] = {}\n+        for (key, tensors), split_sizes in zip(value.items(), self.sizes):\n+            if len(tensors) != 1:\n+                raise ValueError(\"SplitModulelist expects exactly one tensor per key.\")\n+            current_tensor = tensors[0]\n+            if not isinstance(current_tensor, torch.Tensor):\n                 raise TypeError(\"SplitModulelist can only split torch.Tensor instances.\")\n-            splits = torch.split(tensor, split_sizes, dim=self.dim)\n-            result.append(list(splits))\n+            result[key] = list(torch.split(current_tensor, split_sizes, dim=self.dim))\n         return result\n \n \n@@ -237,60 +251,112 @@ def _apply(self, tensor: torch.Tensor) -> torch.Tensor:\n \n     @torch.no_grad\n     def convert(\n-        self, value: Union[dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor], config\n-    ) -> Union[dict[str, torch.Tensor], list[torch.Tensor], torch.Tensor]:\n+        self,\n+        value: dict[str, list[torch.Tensor]],\n+        source_keys: list[str],\n+        target_keys: list[str],\n+        full_layer_name: str,\n+        config,\n+    ) -> dict[str, list[torch.Tensor]]:\n         self.config = config\n-        out = [[self._apply(x) for x in inner] if isinstance(inner, list) else self._apply(inner) for inner in value]\n-        return out\n+        output: dict[str, list[torch.Tensor]] = {}\n+        for key, tensors in value.items():\n+            if len(tensors) != 1:\n+                raise ValueError(\"PermuteForRope expects a single tensor per key.\")\n+            output[key] = [self._apply(tensors[0])]\n+        return output\n \n \n @dataclass(slots=True)\n-class WeightConverter:\n-    r\"\"\"\n-    A weight convert that acts on a pattern of source keys.\n-    The keys need to be collected based on the target keys.\n-\n-    With wild card, glob patterns are matched, so you have to be detailed with what to match. If you match:\n-    `model.layers.*.experts.*` -> it will act on all of them\n-    {\"model.layers.*.experts.*\": []}\n-    but\n-    `experts.*.mlp` will be layer specific.\n-    {\"model.layers.1.experts.*\": [], }\n-    - source_keys: str | list[str] (wildcards '*' match digits)\n-    - target_keys: str | list[str] | None\n-    - distributed_operation / operations / quantization_operations are ALWAYS lists.\n-\n-    TODO: for BNB we need to collect model.weight.quant_state_keys\n-    \"\"\"\n-\n-    source_keys: Union[str, list[str]]\n-    target_keys: Optional[Union[str, list[str]]] = None\n-    operations: list[ConversionOps] = field(default_factory=list, repr=False)\n+class WeightTransform:\n+    source_keys: Union[str, list[str]] = field(init=True)\n+    target_keys: Union[str, list[str]] = field(init=True)\n \n     distributed_operation: Optional[TensorParallelLayer] = None\n     quantization_operation: Optional[ConversionOps] = None\n \n+    collected_tensors: dict[str, list[Future]] = field(default_factory=dict, init=False)\n+    layer_targets: dict[str, set[str]] = field(default_factory=dict, init=False)\n+\n     def __post_init__(self):\n-        if not isinstance(self.source_keys, list):\n+        if isinstance(self.source_keys, str):\n             self.source_keys = [self.source_keys]\n-        targets_were_none = False\n-        if not isinstance(self.target_keys, list):\n-            if self.target_keys is None:\n-                self.target_keys = list(self.source_keys)\n-                targets_were_none = True\n-            else:\n-                self.target_keys = [self.target_keys]\n+        if isinstance(self.target_keys, str):\n+            self.target_keys = [self.target_keys]\n \n-        if not targets_were_none and bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n-            raise ValueError(\n-                f\"source keys={self.source_keys}, target_keys={self.target_keys} but you can only have one to many, one to one or many to one.\"\n-            )\n+    def add_tensor(self, target_key: str, source_key: str, source_pattern: str, future: Future):\n+        bucket = self.collected_tensors.setdefault(source_pattern, [])\n+        bucket += [future]\n+\n+        bucket = self.layer_targets.setdefault(target_key, set())\n+        bucket.add(source_key)\n \n \n @dataclass(slots=True)\n-class ConversionEntry:\n-    weight_converter: WeightConverter\n-    collected_tensors: dict = field(default_factory=lambda: defaultdict(dict))\n+class WeightRenaming(WeightTransform):\n+    # Special case of WeightTransform that only renames keys without any conversion.\n+\n+    def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Optional[MutableSet[str]] = None):\n+        misc = {}\n+        for pattern, futures in self.collected_tensors.items():\n+            self.collected_tensors[pattern] = [future.result() for future in futures]\n+\n+        collected_tensors = self.collected_tensors\n+        if quantizer is not None and self.quantization_operation is not None:\n+            with log_to_misc(layer_name, misc, (self.collected_tensors, layer_name), self.quantization_operation):\n+                collected_tensors = self.quantization_operation.convert(\n+                    self.collected_tensors,\n+                    source_keys=self.source_keys,\n+                    target_keys=self.target_keys,\n+                    full_layer_name=layer_name,\n+                    config=config,\n+                    quant_config=quantizer.quantization_config,\n+                    missing_keys=missing_keys,\n+                )\n+\n+        return collected_tensors, misc\n+\n+\n+@dataclass(slots=True)\n+class WeightConverter(WeightTransform):\n+    operations: list[ConversionOps] = field(default_factory=list, repr=False)\n+\n+    def __post_init__(self):\n+        WeightTransform.__post_init__(self)\n+        if bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n+            raise ValueError(\n+                f\"source keys={self.source_keys}, target_keys={self.target_keys} but you can only have one to many, one to one or many to one.\"\n+            )\n+        if not self.operations:\n+            raise ValueError(\"WeightConverter requires at least one operation.\")\n+\n+    def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Optional[MutableSet[str]] = None):\n+        misc = {}\n+        for pattern, futures in self.collected_tensors.items():\n+            self.collected_tensors[pattern] = [future.result() for future in futures]\n+\n+        collected_tensors = self.collected_tensors\n+        for op in self.operations:\n+            with log_to_misc(layer_name, misc, (collected_tensors, layer_name), op):\n+                collected_tensors = op.convert(\n+                    collected_tensors,\n+                    source_keys=self.source_keys,\n+                    target_keys=self.target_keys,\n+                    full_layer_name=layer_name,\n+                    config=config,\n+                )\n+        if quantizer is not None and self.quantization_operation is not None:\n+            with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n+                collected_tensors = self.quantization_operation.convert(\n+                    collected_tensors,\n+                    source_keys=self.source_keys,\n+                    target_keys=self.target_keys,\n+                    full_layer_name=layer_name,\n+                    config=config,\n+                    quant_config=quantizer.quantization_config,\n+                    missing_keys=missing_keys,\n+                )\n+        return collected_tensors, misc\n \n \n GLOBAL_WORKERS = min(16, (os.cpu_count() or 8) * 2)  # NVMe: 8-16; HDD/NFS: 2-4\n@@ -353,7 +419,7 @@ def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) ->\n             values, target_keys = extras\n             descriptor = f\"{op_name} \" if op_name else \"\"\n             misc[layer_name] = (\n-                f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {len(values[0])}\"\n+                f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {len(values)}\"\n             )\n         elif isinstance(extras, str):\n             suffix = f\" via {op_name}\" if op_name else \"\"\n@@ -372,6 +438,7 @@ def set_param_for_module(\n     mismatch_keys: MutableSet[tuple[str, torch.Size, torch.Size]],\n     missing_keys: MutableSet[str],\n     misc: MutableMapping[str, Any],\n+    unexpected_keys: MutableSet[str],\n     distributed_operation: Optional[TensorParallelLayer],\n     hf_quantizer: HfQuantizer,\n ):\n@@ -382,33 +449,38 @@ def set_param_for_module(\n             param_value = param_value[0]\n         elif not isinstance(param_value, torch.nn.Parameter):\n             param_value = param_value[...]\n-        ref = getattr(module_obj, param_name)\n \n-        use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n-        if not isinstance(param_value, torch.nn.Parameter):\n-            if distributed_operation is not None:\n-                param_value = DTensor.from_local(\n-                    param_value,\n-                    distributed_operation.device_mesh,\n-                    getattr(distributed_operation, \"shard\", Replicate()),\n-                    run_check=False,\n-                    shape=ref.size(),\n-                    stride=ref.stride(),\n-                )\n-                if not use_dtensor:\n-                    # we convert to local\n-                    param_value = param_value.to_local()\n-            if param_name not in module_obj._buffers:\n-                param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n-\n-        # Remove from missing keys (it's either mismatched, or all good)\n-        missing_keys.discard(layer_name)\n-        if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n-            mismatch_keys.add((layer_name, param_value.shape, ref.shape))\n-            module_obj.param_name._is_hf_initialized = False  # Needs to be initialized\n+        ref = getattr(module_obj, param_name)\n+        if ref is None:\n+            unexpected_keys.add(layer_name)\n         else:\n-            param_value._is_hf_initialized = True  # super important otherwise _init_weight re-initi if bias is missing\n-            setattr(module_obj, param_name, param_value)\n+            use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n+            if not isinstance(param_value, torch.nn.Parameter):\n+                if distributed_operation is not None:\n+                    param_value = DTensor.from_local(\n+                        param_value,\n+                        distributed_operation.device_mesh,\n+                        getattr(distributed_operation, \"shard\", Replicate()),\n+                        run_check=False,\n+                        shape=ref.size(),\n+                        stride=ref.stride(),\n+                    )\n+                    if not use_dtensor:\n+                        # we convert to local\n+                        param_value = param_value.to_local()\n+                if param_name not in module_obj._buffers:\n+                    param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n+\n+            # Remove from missing keys (it's either mismatched, or all good)\n+            missing_keys.discard(layer_name)\n+            if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n+                mismatch_keys.add((layer_name, param_value.shape, ref.shape))\n+                module_obj.param_name._is_hf_initialized = False  # Needs to be initialized\n+            else:\n+                param_value._is_hf_initialized = (\n+                    True  # super important otherwise _init_weight re-initi if bias is missing\n+                )\n+                setattr(module_obj, param_name, param_value)\n \n \n class SkipLayer(Exception):\n@@ -417,30 +489,131 @@ class SkipLayer(Exception):\n     pass\n \n \n+def repl(m, repl_map: dict[str, str]) -> str:\n+    # Collect all groups that matched\n+    matched_groups = [name for name, val in m.groupdict().items() if val]\n+\n+    if len(matched_groups) == 0:\n+        # Should never happen\n+        return m.group(0)\n+\n+    if len(matched_groups) > 1:\n+        raise ValueError(\n+            \"only a single match should happen, your regex patterns are tangled: \"\n+            f\"groups matched = {matched_groups} for the patternsL {repl_map.keys()}\"\n+        )\n+\n+    # Exactly one match => return replacement\n+    name = matched_groups[0]\n+    return repl_map[name]\n+\n+\n def convert_and_load_state_dict_in_model(\n     model: PreTrainedModel,\n     state_dict: dict[str, Any],\n-    weight_mapping: dict[str, WeightConverter] | None,\n+    weight_mapping: list[WeightConverter | WeightRenaming] | None,\n     tp_plan: dict[str, str] | None,\n     hf_quantizer: HfQuantizer | None,\n     dtype: torch.dtype | None = None,\n     device_map: dict | None = None,\n     dtype_plan: dict | None = None,\n     device_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,\n ):\n-    \"\"\"\n-    Convert a state dict according to a weight mapping (one WeightConverter per glob pattern),\n-    collecting tensors per *layer instance* (the concrete indices captured from '*').\n+    r\"\"\"\n+    We build a mapping from the keys obtained by renaming each of the checkpoint keys according to the weight_mapping rules.\n+    Then we load the tensors into the model, applying any conversion operations as needed.\n+\n+    The `param_name_to_load` will look like this:\n+    {\n+        \"model.layers.0.attention.q.weight\": # Notice here there is only the first key of the target keys\n+            WeightConverter(\n+                source_keys=[\"qkv\"],\n+                target_keys=[\"q\", \"k\",\"v\"],\n+                operations=[Chunk(dim=0, chunks=3)]),\n+                collected_tensors={\n+                    \"qkv\": [Future, Future, Future]},\n+                layer_targets={\n+                    \"model.layers.0.attention.q.weight\": {\"model.layers.0.attention.qkv.weight\"},\n+                    \"model.layers.0.attention.k.weight\": {\"model.layers.0.attention.qkv.weight\"},\n+                    \"model.layers.0.attention.v.weight\": {\"model.layers.0.attention.qkv.weight\"},\n+                }\n+            ),\n+        ...\n+    }\n+\n+    We make sure that the keys are the full keys. The only \"nit\" here is that 1 key can map to multiple target keys (e.g. qkv -> q, k, v).\n+    In that case the weight converter will take care of doing the appropriate renaming.\n+\n+    For example for:\n+    ```python\n+    WeightConverter(\n+        source_keys=[\"mlp.experts.*.gate_proj.weight\",\"mlp.experts.*.up_proj.weight\"],\n+        target_keys=\"mlp.experts.gate_up_proj\",\n+        operations=[MergeModulelist(dim=0), Concatenate(dim=1)],\n+    )\n+    ```\n+    we would have the following collected tensors:\n+    ```python\n+    collected_tensors = {\n+        \"mlp.experts.*.gate_proj.weight\": [Future, Future, Future, Future, Future, Future, Future, Future],\n+        \"mlp.experts.*.up_proj.weight\": [Future, Future, Future, Future, Future, Future, Future, Future],\n+    }\n+    ```\n+    The first op, `MergeModulelist`, would stack the 8 tensors of each source but will not \"rename\" them into the fused target name.\n+    The second op, `Concatenate`, would then rename the fused tensor into the final target name.\n+\n+    If we want to split `qkv` we would have:\n+    ```python\n+    collected_tensors = {\n+        \"attention.qkv.weight\": [Future], # here its the full SOURCE keys.\n+    }\n+    ```\n+    The `Chunk` operation would then split the single tensor into 3 and rename them accordingly and update the collected tensors to:\n+    ```python\n+    realized_values = {\n+        \"attention.q.weight\": [Tensor],\n+        \"attention.k.weight\": [Tensor],\n+        \"attention.v.weight\": [Tensor],\n+    }\n+    ```\n+\n+    Now that this is done, we can quantize / dequantize accordingly the collected_tensors.\n+\n+    For some quantization methods, we need to gather different tensors:\n+\n+    ```python\n+    # for \"medmekk/llama-3.2-1b-float8-torchao\"\n+    WeightConverter(\n+        source_keys=[\":qdata\", \":scale\"],\n+        target_keys=\"\",\n+        operations=[TorchaoDeserialize()],\n+    )\n+    ```\n+    This will collect all tensors that have the same prefix, but end with `:qdata` or `:scale`. This will give us:\n+    ```python\n+    all_weight_mapping = {\n+        \"model.layers.13.self_attn.o_proj.weight\": WeightConverter(\n+            source_keys=[\":qdata\", \":scale\"],\n+            target_keys=\"\",\n+            operations=[TorchaoDeserialize()],\n+            collected_tensors={\n+                \":qdata\": [Future],\n+                \":scale\": [Future],\n+            },\n+        ...\n+    }\n+    ```\n+\n     \"\"\"\n \n     prefix = model.base_model_prefix\n-    tp_plan = tp_plan or {}  # {glob_pattern: plan_obj_or_key}\n-    device_map = device_map or {\"\": \"cpu\"}  # {exact_target_key: device}\n+    tp_plan = tp_plan or {}\n+    device_map = device_map or {\"\": \"cpu\"}\n     device_map_regex = re.compile(\n         \"|\".join(rf\"({k})\" for k in sorted(device_map.keys(), key=lambda x: x.count(\".\"), reverse=True))\n     )\n-    dtype_plan = dtype_plan or {}  # {glob_pattern: dtype}\n-    weight_mapping = weight_mapping or {}  # {glob_pattern: WeightConverter}\n+    dtype_plan = dtype_plan or {}\n+    weight_mapping = weight_mapping or []\n     meta_model_state_dict = model.state_dict()\n     missing_keys = set(meta_model_state_dict.keys())\n \n@@ -450,135 +623,125 @@ def convert_and_load_state_dict_in_model(\n     # Global thread_pool\n     thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n \n-    _patterns = list(itertools.chain.from_iterable([k.source_keys for k in weight_mapping]))\n-    source_to_target = {sk: k for k in weight_mapping for sk in k.source_keys}\n-    weight_pattern_alt, weight_pattern_by_group_name = build_glob_alt(_patterns)\n-    tp_plan_alt, tp_plan_by_group_name = build_glob_alt(list(tp_plan.keys()))\n-    dtype_policy_alt, dtype_policy_by_group_name = build_glob_alt(list(dtype_plan.keys()))\n+    renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n+    converters = [entry for entry in weight_mapping if isinstance(entry, WeightConverter)]\n+    if hf_quantizer:\n+        # We will add the quantizer's deserialization WeightConverter here.\n+        pass\n+\n+    param_name_to_load: dict[str, Union[WeightRenaming | WeightConverter]] = {}\n+\n+    # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n+    # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n+    rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+    if converters != []:\n+        weight_pattern_alt, src_group_to_glob, tgt_group_to_glob = build_glob_alternation(converters)\n+    if tp_plan != {}:\n+        tp_plan_alt, tp_plan_by_group_name, _ = build_glob_alternation(list(tp_plan.keys()))\n+    if dtype_plan != {}:\n+        dtype_policy_alt, dtype_policy_by_group_name, _ = build_glob_alternation(list(dtype_plan.keys()))\n+\n+    pattern_to_converter = {k: converter for converter in converters for k in converter.source_keys}\n \n     state_dict = sorted(state_dict.items(), key=lambda kv: dot_natural_key(kv[0]))\n-    # 1. Create the conversion entries\n-    by_conversion_pattern: dict[str, ConversionEntry] = {}\n     for original_key, tensor in state_dict:\n-        matched_pattern = match_glob(original_key, weight_pattern_alt, weight_pattern_by_group_name)\n-        if matched_pattern is not None:\n-            converter = source_to_target[matched_pattern]  # TODO make sure its the ref\n-            sub_with_extractor = partial(re.sub, matched_pattern.replace(\"*\", r\"(\\d+)\"), string=original_key)\n-            entry_key = \"|\".join(converter.target_keys)\n-            target_key = \"|\".join(map(sub_with_extractor, [k.replace(\"*\", \"\\\\1\") for k in converter.target_keys]))\n-            entry: ConversionEntry = by_conversion_pattern.setdefault(entry_key, ConversionEntry(converter))\n-            converter_key = sub_with_extractor(matched_pattern)\n-        else:\n-            converter = WeightConverter(original_key)\n-            converter_key = entry_key = target_key = original_key\n-            entry = by_conversion_pattern.setdefault(converter_key, ConversionEntry(converter))\n-\n-        _dtype = dtype\n-        new_target_key = []  # test_load_with_mismatched_shapes for AutoModel.from_pretrained(AutoForCausal, vocab=10)\n-        for t in target_key.split(\"|\"):\n-            if t.startswith(prefix) and meta_model_state_dict.get(re.sub(f\"^{prefix}.\", \"\", t, count=1)) is not None:\n-                t = re.sub(f\"^{prefix}.\", \"\", t, count=1)\n-            elif meta_model_state_dict.get(f\"{prefix}.{t}\") is not None:\n-                t = f\"{prefix}.{t}\"\n-            new_target_key.append(t)\n-            empty_param = meta_model_state_dict.get(t)\n-            # If it does not exist, it's unexpected\n-            if empty_param is None:\n-                unexpected_keys.add(t)\n-                continue\n+        # 1. apply all renamings\n+        renamed_key = rename_alt.sub(lambda m: repl(m, rename_by_group), original_key).replace(\"\\\\\", \"\")\n+\n+        # 2. apply 1 weight conversion on the key\n+        matched_pattern = weight_pattern_alt.search(renamed_key) if converters != [] else None\n+        if matched_pattern is not None:  # we have a converter to apply\n+            renamed_key = weight_pattern_alt.sub(lambda m: repl(m, tgt_group_to_glob), renamed_key).replace(\"\\\\\", \"\")\n+\n+        # 3. check if we need to add or remove prefix\n+        if (\n+            renamed_key.startswith(prefix)\n+            and meta_model_state_dict.get(re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)) is not None\n+        ):\n+            renamed_key = re.sub(f\"^{prefix}.\", \"\", renamed_key, count=1)\n+        elif meta_model_state_dict.get(f\"{prefix}.{renamed_key}\") is not None:\n+            renamed_key = f\"{prefix}.{renamed_key}\"\n+\n+        # 4. finally, collect the tensor into the proper converter\n+        if renamed_key in missing_keys:\n+            empty_param = meta_model_state_dict.get(renamed_key)\n+            if matched_pattern:\n+                new_converter = deepcopy(pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]])\n+                # each target key gets its own converter instance\n+                mapping = param_name_to_load.setdefault(renamed_key, new_converter)\n+                source_pattern = src_group_to_glob[matched_pattern.lastgroup]\n+            else:\n+                mapping = param_name_to_load.setdefault(renamed_key, WeightRenaming(renamed_key, renamed_key))\n+                source_pattern = renamed_key\n+\n+            # 5. Handle dtype casting\n+            if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, renamed_key):\n+                mapping.quantization_operation = hf_quantizer.get_quantize_ops()\n \n-            if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, t):\n-                converter.quantization_operation = hf_quantizer.get_quantize_ops()\n             _dtype = dtype\n-            matched_dtype_pattern = match_glob(t, dtype_policy_alt, dtype_policy_by_group_name)\n-            if matched_dtype_pattern is not None:\n-                _dtype = dtype_plan[matched_dtype_pattern]\n-            elif empty_param.dtype != _dtype:\n-                _dtype = empty_param.dtype\n-\n-        first_target_key = new_target_key[0]\n-        target_key = \"|\".join(new_target_key)\n-\n-        future = None\n-        if device_mesh:\n-            if matched_tp_pattern := match_glob(first_target_key, tp_plan_alt, tp_plan_by_group_name):\n-                empty_param = meta_model_state_dict.get(first_target_key)\n-                if getattr(converter, \"distributed_operation\", {}) is None:\n-                    tp_layer = ALL_PARALLEL_STYLES[model.tp_plan[matched_tp_pattern]].__class__\n-                    converter.distributed_operation = tp_layer(\n-                        device_mesh=device_mesh, rank=device_map[\"\"].index, empty_param=empty_param.clone()\n+            if dtype_plan != {} and dtype_policy_alt.search(renamed_key):\n+                matched_dtype_pattern = dtype_policy_alt.search(renamed_key)\n+                if matched_dtype_pattern is not None:\n+                    _dtype = dtype_plan[matched_dtype_pattern.group()]\n+            elif empty_param is not None and empty_param.dtype != _dtype:\n+                _dtype = empty_param.dtype  # usually correct when initializing\n+\n+            # 6. Handle TP sharding or device_map placement -> scheduled materialization\n+            future = None\n+            if device_mesh:\n+                if matched_tp_pattern := tp_plan_alt.search(renamed_key):\n+                    matched_tp_pattern = tp_plan_by_group_name[matched_tp_pattern.lastgroup]\n+                    if getattr(mapping, \"distributed_operation\", None) is None:\n+                        tp_layer = ALL_PARALLEL_STYLES[model.tp_plan[matched_tp_pattern]].__class__\n+                        mapping.distributed_operation = tp_layer(\n+                            device_mesh=device_mesh, rank=device_map[\"\"].index, empty_param=empty_param.clone()\n+                        )\n+                    shard_index = len(mapping.collected_tensors[source_pattern])\n+                    future = spawn_tp_materialize(\n+                        thread_pool,\n+                        tensor,\n+                        _dtype,\n+                        mapping.distributed_operation,\n+                        shard_index,\n                     )\n-                    # VERY IMPORTANT: this tells us wether we collected stuffs or not.\n-                shard_index = len(entry.collected_tensors[target_key].get(converter_key, []))\n-                future = spawn_tp_materialize(\n-                    thread_pool,\n-                    tensor,\n-                    _dtype,\n-                    converter.distributed_operation,\n-                    shard_index,\n-                )\n \n-        if future is None:  # If not TP, async materialize the tensors. TODO handle disk offload?\n-            device_match = device_map_regex.match(first_target_key)\n-            param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n-            future = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n-        entry.collected_tensors[target_key].setdefault(converter_key, []).append(future)\n-\n-    # 2. Actually convert the ckpt\n-    inverse_converters = {}\n-    keys = list(by_conversion_pattern.keys())\n-\n-    with logging.tqdm(total=len(keys), desc=\"Loading weights\") as pbar:\n-        for key in keys[::-1]:  # revert to process simple keys first\n-            group = by_conversion_pattern.pop(key)\n-            converter = group.weight_converter\n-            operations = converter.operations if isinstance(converter.operations, list) else [converter.operations]\n-            for layer_name, tensors_for_this_layer in group.collected_tensors.items():\n-                pbar.update(1)\n-                pbar.set_postfix({\"Materializing param\": layer_name})\n-                pbar.refresh()\n-                concrete_target_keys = layer_name.split(\"|\")\n-                try:\n-                    if bool(set(concrete_target_keys) - unexpected_keys):\n-                        with log_to_misc(layer_name, misc):\n-                            values = [[k.result() for k in inner] for inner in tensors_for_this_layer.values()]\n-\n-                        for op in operations:\n-                            with log_to_misc(layer_name, misc, (values, concrete_target_keys), operations):\n-                                values = op.convert(values, model.config)\n-\n-                        values = [values] if not isinstance(values, list) else values\n-                        with log_to_misc(layer_name, misc, (values, concrete_target_keys), operations):\n-                            realized_value = {\n-                                k: t for k, t in zip(concrete_target_keys, values) if k not in unexpected_keys\n-                            }\n-\n-                        for k in list(realized_value.keys()).copy():\n-                            if op := converter.quantization_operation:\n-                                with log_to_misc(layer_name, misc, op=op):\n-                                    realized_value.update(\n-                                        op.convert({k: realized_value.pop(k)}, model=model, missing_keys=missing_keys)\n-                                    )\n-\n-                        for k, output_value in realized_value.items():\n-                            for src in converter.source_keys:  # what should happen to k when we meet k at saving\n-                                inverse_converters[k] = {src: converter}\n-                            set_param_for_module(\n-                                model,\n-                                k,\n-                                output_value,\n-                                mismatch_keys,\n-                                missing_keys,\n-                                misc,\n-                                converter.distributed_operation,\n-                                hf_quantizer,\n-                            )\n-\n-                except SkipLayer:\n-                    continue\n-            del group\n-\n-    model.inverse_converters = inverse_converters\n+            if future is None:  # TODO handle disk offload\n+                device_match = device_map_regex.match(renamed_key)\n+                param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+                future = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n+\n+            mapping.add_tensor(renamed_key, original_key, source_pattern, future)\n+        elif matched_pattern:  # add all target keys as unexpected\n+            mapping = pattern_to_converter[src_group_to_glob[matched_pattern.lastgroup]]\n+            for k in mapping.target_keys:\n+                unexpected_keys.add(renamed_key.replace(mapping.target_keys[0], k))\n+        else:\n+            unexpected_keys.add(renamed_key)\n+\n+    total_entries = len(param_name_to_load)\n+    with logging.tqdm(total=total_entries, desc=\"Loading weights\") as pbar:\n+        for layer_name, mapping in param_name_to_load.items():\n+            pbar.update(1)\n+            pbar.set_postfix({\"Materializing param\": layer_name})\n+            pbar.refresh()\n+            try:\n+                realized_value, misc = mapping.convert(\n+                    layer_name, config=model.config, quantizer=hf_quantizer, missing_keys=missing_keys\n+                )\n+                for k, output_value in realized_value.items():\n+                    set_param_for_module(\n+                        model,\n+                        k,\n+                        output_value,\n+                        mismatch_keys,\n+                        missing_keys,\n+                        misc,\n+                        unexpected_keys,\n+                        mapping.distributed_operation,\n+                        hf_quantizer,\n+                    )\n+            except SkipLayer:\n+                continue\n     thread_pool.shutdown(wait=False)\n     return missing_keys, unexpected_keys, mismatch_keys, misc\n "
        },
        {
            "sha": "fe52822e1f92b2e8f64bcd933c77fc53f65cafb2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b63e6e07fd845149b999405056dd1cf268edcb1a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b63e6e07fd845149b999405056dd1cf268edcb1a",
            "patch": "@@ -49,6 +49,7 @@\n from .conversion_mapping import get_checkpoint_conversion_mapping\n from .core_model_loading import (\n     WeightConverter,\n+    WeightRenaming,\n     convert_and_load_state_dict_in_model,\n     revert_weight_conversion,\n )\n@@ -3819,14 +3820,16 @@ def from_pretrained(\n             config, quantization_config, dtype, device_map, weights_only, user_agent\n         )\n \n-        weight_conversions: Optional[list[WeightConverter]] = None\n+        weight_conversions: Optional[list[WeightConverter | WeightRenaming]] = None\n         model_type = getattr(config, \"model_type\", None)\n         if model_type is not None:\n             weight_conversions = get_checkpoint_conversion_mapping(model_type)\n             if weight_conversions is None:\n                 weight_conversions = get_checkpoint_conversion_mapping(\"legacy\")\n             if key_mapping is not None:\n-                weight_conversions.extend([WeightConverter(k, v) for k, v in key_mapping.items()])\n+                weight_conversions.extend(\n+                    [WeightRenaming(source_keys=k, target_keys=v) for k, v in key_mapping.items()]\n+                )\n \n         if gguf_file:\n             if hf_quantizer is not None:\n@@ -3997,7 +4000,7 @@ def _load_pretrained_model(\n         hf_quantizer: Optional[HfQuantizer] = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n         weights_only: bool = True,\n-        weight_mapping: Optional[Sequence[WeightConverter]] = None,\n+        weight_mapping: Optional[Sequence[WeightConverter | WeightRenaming]] = None,\n     ):\n         is_quantized = hf_quantizer is not None\n         is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {"
        },
        {
            "sha": "023b9fd0596dbf506f55a14dcc7beec9c8c047f8",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 55,
            "deletions": 22,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/b63e6e07fd845149b999405056dd1cf268edcb1a/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b63e6e07fd845149b999405056dd1cf268edcb1a/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=b63e6e07fd845149b999405056dd1cf268edcb1a",
            "patch": "@@ -24,9 +24,10 @@\n     MergeModulelist,\n     PermuteForRope,\n     WeightConverter,\n-    build_glob_alt,\n+    WeightRenaming,\n+    build_glob_alternation,\n     convert_and_load_state_dict_in_model,\n-    match_glob,\n+    repl,\n )\n from transformers.utils.import_utils import is_triton_available\n \n@@ -38,52 +39,59 @@ def setUp(self):\n             \"model.layers.*.self_attn.q_proj.weight\",\n             \"embed_tokens.weight\",\n         ]\n-        self.alt_digits, self.map_digits = build_glob_alt(self.weight_globs_digits)\n+        self.alt_digits, self.map_digits, _ = build_glob_alternation(self.weight_globs_digits)\n \n         self.weight_globs_any = [\n             \"model.layers.*.mlp.gate_up_proj.weight\",\n             \"model.layers.*.self_attn.q_proj.weight\",\n             \"embed_tokens.weight\",\n         ]\n-        self.alt_any, self.map_any = build_glob_alt(self.weight_globs_any)\n+        self.alt_any, self.map_any, _ = build_glob_alternation(self.weight_globs_any)\n+\n+    @staticmethod\n+    def _match_glob(key, alt, mapping):\n+        matched = alt.search(key)\n+        return mapping.get(matched.lastgroup) if matched else None\n \n     def test_exact_match(self):\n-        self.assertEqual(match_glob(\"embed_tokens.weight\", self.alt_digits, self.map_digits), \"embed_tokens.weight\")\n+        self.assertEqual(\n+            self._match_glob(\"embed_tokens.weight\", self.alt_digits, self.map_digits), \"embed_tokens.weight\"\n+        )\n \n     def test_digits_only_star_accepts_digits(self):\n         self.assertEqual(\n-            match_glob(\"model.layers.0.mlp.gate_up_proj.weight\", self.alt_digits, self.map_digits),\n+            self._match_glob(\"model.layers.0.mlp.gate_up_proj.weight\", self.alt_digits, self.map_digits),\n             \"model.layers.*.mlp.gate_up_proj.weight\",\n         )\n         self.assertEqual(\n-            match_glob(\"model.layers.12.self_attn.q_proj.weight\", self.alt_digits, self.map_digits),\n+            self._match_glob(\"model.layers.12.self_attn.q_proj.weight\", self.alt_digits, self.map_digits),\n             \"model.layers.*.self_attn.q_proj.weight\",\n         )\n \n     def test_anychar_star_accepts_nondigits(self):\n         self.assertEqual(\n-            match_glob(\"model.layers.a.mlp.gate_up_proj.weight\", self.alt_any, self.map_any),\n+            self._match_glob(\"model.layers.a.mlp.gate_up_proj.weight\", self.alt_any, self.map_any),\n             \"model.layers.*.mlp.gate_up_proj.weight\",\n         )\n         self.assertEqual(\n-            match_glob(\"model.layers.00x.mlp.gate_up_proj.weight\", self.alt_any, self.map_any),\n+            self._match_glob(\"model.layers.00x.mlp.gate_up_proj.weight\", self.alt_any, self.map_any),\n             \"model.layers.*.mlp.gate_up_proj.weight\",\n         )\n \n     def test_no_match(self):\n-        self.assertIsNone(match_glob(\"model.layers.0.mlp.up_proj.weight\", self.alt_digits, self.map_digits))\n+        self.assertIsNone(self._match_glob(\"model.layers.0.mlp.up_proj.weight\", self.alt_digits, self.map_digits))\n \n     def test_leftmost_alternative_wins_for_overlapping_patterns(self):\n         # Overlapping patterns: both could match; ensure leftmost wins\n         globs = [\n             \"model.layers.*.mlp.*.weight\",  # broader (first)\n             \"model.layers.0.mlp.gate_up_proj.weight\",  # more specific (second)\n         ]\n-        alt, mapping = build_glob_alt(globs)\n+        alt, mapping, _ = build_glob_alternation(globs)\n \n         # Both branches match; Python's regex picks the leftmost alternative → index 0\n         self.assertEqual(\n-            match_glob(\"model.layers.0.mlp.gate_up_proj.weight\", alt, mapping), \"model.layers.*.mlp.*.weight\"\n+            self._match_glob(\"model.layers.0.mlp.gate_up_proj.weight\", alt, mapping), \"model.layers.*.mlp.*.weight\"\n         )\n \n     def test_multiple_patterns_same_prefix(self):\n@@ -92,34 +100,59 @@ def test_multiple_patterns_same_prefix(self):\n             \"model.layers.*.self_attn.k_proj.weight\",\n             \"model.layers.*.self_attn.v_proj.weight\",\n         ]\n-        alt, mapping = build_glob_alt(\n+        alt, mapping, _ = build_glob_alternation(\n             globs,\n         )\n \n         self.assertEqual(\n-            match_glob(\"model.layers.3.self_attn.q_proj.weight\", alt, mapping),\n+            self._match_glob(\"model.layers.3.self_attn.q_proj.weight\", alt, mapping),\n             \"model.layers.*.self_attn.q_proj.weight\",\n         )\n         self.assertEqual(\n-            match_glob(\"model.layers.3.self_attn.k_proj.weight\", alt, mapping),\n+            self._match_glob(\"model.layers.3.self_attn.k_proj.weight\", alt, mapping),\n             \"model.layers.*.self_attn.k_proj.weight\",\n         )\n         self.assertEqual(\n-            match_glob(\"model.layers.3.self_attn.v_proj.weight\", alt, mapping),\n+            self._match_glob(\"model.layers.3.self_attn.v_proj.weight\", alt, mapping),\n             \"model.layers.*.self_attn.v_proj.weight\",\n         )\n \n     def test_anchor_full_match_only(self):\n-        self.assertIsNotNone(match_glob(\"model.layers.0.mlp.gate_up_proj.weight.bar\", self.alt_any, self.map_any))\n+        self.assertIsNotNone(\n+            self._match_glob(\"model.layers.0.mlp.gate_up_proj.weight.bar\", self.alt_any, self.map_any)\n+        )\n \n     def test_large_batch_performance_smoke(self):\n         # Not a perf benchmark, but ensures building and matching a larger alternation is OK\n         globs = [f\"model.layers.*.mlp.block{i}.weight\" for i in range(200)]\n-        alt, mapping = build_glob_alt(\n-            globs,\n-        )\n+        alt, mapping, _ = build_glob_alternation(globs)\n         key = \"model.layers.123.mlp.block57.weight\"\n-        self.assertEqual(match_glob(key, alt, mapping), \"model.layers.*.mlp.block57.weight\")\n+        self.assertEqual(self._match_glob(key, alt, mapping), \"model.layers.*.mlp.block57.weight\")\n+\n+    def test_sub_key_rewrites_targets(self):\n+        renamings = [\n+            WeightRenaming(\"block_sparse_moe.experts.*.w1.weight\", \"mlp.experts.gate_up_proj\"),\n+            WeightRenaming(\"block_sparse_moe.experts.*.w2.weight\", \"mlp.experts.down_proj\"),\n+            WeightRenaming(\"model.language_model.*\", \"language_model\"),\n+        ]\n+        rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+\n+        def rename(original_key: str) -> str:\n+            return rename_alt.sub(lambda m: repl(m, rename_by_group), original_key).replace(\"\\\\\", \"\")\n+\n+        self.assertEqual(rename(\"foo.block_sparse_moe.experts.3.w1.weight\"), \"foo.mlp.experts.gate_up_proj\")\n+        self.assertEqual(rename(\"foo.block_sparse_moe.experts.3.w2.weight\"), \"foo.mlp.experts.down_proj\")\n+        self.assertEqual(rename(\"model.language_model.lm_head.weight\"), \"language_model\")\n+\n+    def test_sub_key_no_match_returns_original(self):\n+        renamings = [\n+            WeightRenaming(\"block_sparse_moe.experts.*.w1.weight\", \"*.mlp.experts.gate_up_proj\"),\n+        ]\n+        rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+\n+        key = \"unrelated.key\"\n+        renamed_key = rename_alt.sub(lambda m: repl(m, rename_by_group), key).replace(\"\\\\\", \"\")\n+        self.assertEqual(renamed_key, key)\n \n \n class DummyParamModule(nn.Module):\n@@ -215,7 +248,7 @@ def test_moe_and_qkv_conversion(self):\n                 ],\n                 operations=[Chunk(dim=0, chunks=3)],\n             ),\n-            WeightConverter(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n+            WeightRenaming(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n         ]\n         missing, unexpected, mismatch, misc = convert_and_load_state_dict_in_model(\n             model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=None"
        },
        {
            "sha": "936b41ea23d73f0cff9cc2fa957c58e29edff841",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b63e6e07fd845149b999405056dd1cf268edcb1a/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b63e6e07fd845149b999405056dd1cf268edcb1a/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=b63e6e07fd845149b999405056dd1cf268edcb1a",
            "patch": "@@ -628,7 +628,9 @@ def test_model_from_config_dtype_composite(self):\n         # but if the model has `_keep_in_fp32_modules` then those modules should be in fp32 no matter what\n         LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, dtype=\"auto\")\n-        self.assertEqual(model.model.language_model.dtype, torch.float32)\n+        self.assertEqual(\n+            model.model.language_model.dtype, torch.float32\n+        )  # remember config says float32 for text_config\n         self.assertEqual(model.model.vision_tower.dtype, torch.bfloat16)\n         self.assertEqual(model.model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n         self.assertIsInstance(model.config.dtype, torch.dtype)"
        }
    ],
    "stats": {
        "total": 854,
        "additions": 525,
        "deletions": 329
    }
}