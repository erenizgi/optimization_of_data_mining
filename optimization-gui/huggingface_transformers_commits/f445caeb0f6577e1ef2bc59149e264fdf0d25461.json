{
    "author": "yonigozlan",
    "message": "Fix hidden torchvision>=0.15 dependency issue (#39928)\n\n* use pil_torch_interpolation_mapping for NEAREST/NEAREST_EXACT\n\n* fix min torchvision version\n\n* use InterpolationMode directly\n\n* remove unused is_torchvision_greater_or_equal,\n\n* nit",
    "sha": "f445caeb0f6577e1ef2bc59149e264fdf0d25461",
    "files": [
        {
            "sha": "d58e589c4601dd78d0e7c4f636441b57c5213e61",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -31,6 +31,7 @@\n     is_torch_available,\n     is_torch_tensor,\n     is_torchvision_available,\n+    is_torchvision_v2_available,\n     is_vision_available,\n     logging,\n     requires_backends,\n@@ -59,7 +60,9 @@\n         from torchvision.transforms import InterpolationMode\n \n         pil_torch_interpolation_mapping = {\n-            PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT,\n+            PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else InterpolationMode.NEAREST,\n             PILImageResampling.BOX: InterpolationMode.BOX,\n             PILImageResampling.BILINEAR: InterpolationMode.BILINEAR,\n             PILImageResampling.HAMMING: InterpolationMode.HAMMING,"
        },
        {
            "sha": "f0c84da0ff13af338af73d8b87dcd8f83ed24406",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -57,6 +57,7 @@\n if is_torchvision_v2_available():\n     from torchvision.io import read_image\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.io import read_image\n     from torchvision.transforms import functional as F\n@@ -454,10 +455,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "0c04bfc089c11696ca2adacc3945d94a34f6eecc",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -48,6 +48,7 @@\n if is_torchvision_v2_available():\n     from torchvision.io import read_image\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.io import read_image\n     from torchvision.transforms import functional as F\n@@ -445,10 +446,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "37eef3717a9aecfbbdf3497495b294357ce0c6d7",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -70,6 +70,7 @@\n if is_torchvision_v2_available():\n     from torchvision.io import read_image\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.io import read_image\n     from torchvision.transforms import functional as F\n@@ -466,10 +467,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "58457064412dcf181c5af7a79130a7a91ee2bfaf",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -33,7 +33,6 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -210,7 +209,9 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                 }\n             )\n "
        },
        {
            "sha": "317d1e483342a26c740b9baf12b0090797aef10e",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -51,6 +51,7 @@\n if is_torchvision_v2_available():\n     from torchvision.io import read_image\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.io import read_image\n     from torchvision.transforms import functional as F\n@@ -476,10 +477,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "c853e7cc3a05e67795e978e2c8b3d6fb945f58ec",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -62,6 +62,7 @@\n \n if is_torchvision_v2_available():\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.transforms import functional as F\n \n@@ -369,7 +370,9 @@ def _preprocess(\n                         image=grouped_segmentation_maps[shape],\n                         size=size,\n                         size_divisor=size_divisor,\n-                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT\n+                        if is_torchvision_v2_available()\n+                        else F.InterpolationMode.NEAREST,\n                     )\n             resized_images_grouped[shape] = stacked_images\n             if segmentation_maps is not None:"
        },
        {
            "sha": "0ee6e049316a058e270dc28c9812abedd353fff6",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -66,6 +66,7 @@\n \n if is_torchvision_v2_available():\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.transforms import functional as F\n \n@@ -370,7 +371,9 @@ def _preprocess(\n                         image=grouped_segmentation_maps[shape],\n                         size=size,\n                         size_divisor=size_divisor,\n-                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT\n+                        if is_torchvision_v2_available()\n+                        else F.InterpolationMode.NEAREST,\n                     )\n             resized_images_grouped[shape] = stacked_images\n             if segmentation_maps is not None:"
        },
        {
            "sha": "e50d71025d545d59685910ee23a259f7138b8abd",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -31,7 +31,6 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -139,7 +138,9 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                 }\n             )\n "
        },
        {
            "sha": "442f88a3a8484a7cd41c790a20692dedd3013770",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -29,7 +29,6 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -140,7 +139,9 @@ def _preprocess_image_like_inputs(\n                     \"do_rescale\": False,\n                     \"do_flip_channel_order\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                 }\n             )\n "
        },
        {
            "sha": "ba3770bbcca695a69c3a233aeae0926539bbccb1",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -457,7 +457,11 @@ def _preprocess(\n             for shape, stacked_segmentation_maps in grouped_segmentation_maps.items():\n                 if do_resize:\n                     stacked_segmentation_maps = self.resize(\n-                        stacked_segmentation_maps, size=size, interpolation=F.InterpolationMode.NEAREST_EXACT\n+                        stacked_segmentation_maps,\n+                        size=size,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT\n+                        if is_torchvision_v2_available()\n+                        else F.InterpolationMode.NEAREST,\n                     )\n                 processed_segmentation_maps_grouped[shape] = stacked_segmentation_maps\n             processed_segmentation_maps = reorder_images("
        },
        {
            "sha": "73efff36fbccbc6fb564449c575b68a2d99dad31",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -264,10 +264,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "cea1ade1affa0733b22cd0e6d7c66b82693e0e2b",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -36,7 +36,6 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -244,7 +243,9 @@ def _preprocess_image_like_inputs(\n                 {\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                     \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n                     \"pad_size\": segmentation_maps_kwargs.pop(\"mask_pad_size\"),\n                 }"
        },
        {
            "sha": "77ac7281ef1bf6b8b48358fba69c85192498f4de",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -36,7 +36,6 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -143,7 +142,9 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                 }\n             )\n             processed_segmentation_maps = self._preprocess("
        },
        {
            "sha": "fbf35afd820e6de2f42919f55ae0c7189b77469f",
            "filename": "src/transformers/models/segformer/modular_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -30,7 +30,6 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -100,7 +99,9 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n+                    if is_torchvision_v2_available()\n+                    else F.InterpolationMode.NEAREST,\n                 }\n             )\n             processed_segmentation_maps = self._preprocess("
        },
        {
            "sha": "85aeba94bfcc5ab3f02ad39e83e89e91c0bfb0a7",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f445caeb0f6577e1ef2bc59149e264fdf0d25461/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=f445caeb0f6577e1ef2bc59149e264fdf0d25461",
            "patch": "@@ -47,6 +47,7 @@\n if is_torchvision_v2_available():\n     from torchvision.io import read_image\n     from torchvision.transforms.v2 import functional as F\n+\n elif is_torchvision_available():\n     from torchvision.io import read_image\n     from torchvision.transforms import functional as F\n@@ -493,10 +494,16 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n+            resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n+        interpolation = (\n+            interpolation\n+            if interpolation is not None\n+            else F.InterpolationMode.NEAREST_EXACT\n+            if is_torchvision_v2_available()\n+            else F.InterpolationMode.NEAREST\n+        )\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        }
    ],
    "stats": {
        "total": 116,
        "additions": 88,
        "deletions": 28
    }
}