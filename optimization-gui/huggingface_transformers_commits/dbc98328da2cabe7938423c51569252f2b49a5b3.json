{
    "author": "Cyrilvallez",
    "message": "Several fixes for Gemma3n (#39135)\n\n* remove the skips\n\n* fix the epsilon to a small value (does not make sense otherwise)\n\n* safeguard\n\n* overload test_eager_matches_sdpa\n\n* Update test_modeling_common.py\n\n* skip appropriate tests\n\n* correct no_split_layer\n\n* fix all devices issue\n\n* fix backward\n\n* fix",
    "sha": "dbc98328da2cabe7938423c51569252f2b49a5b3",
    "files": [
        {
            "sha": "3a4995610d4b4b7f9384c0239cea48c6043488cf",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 46,
            "deletions": 36,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=dbc98328da2cabe7938423c51569252f2b49a5b3",
            "patch": "@@ -1135,9 +1135,17 @@ def correct(self, predictions: torch.Tensor, activated: torch.Tensor) -> torch.T\n         corrected += predictions  # add the original input\n         return corrected.contiguous().type_as(activated)\n \n+    def forward(self, corrected: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        This is only defined as the `forward` so that accelerate hooks can move correctly `correct_output_scale`\n+        (which is a nn.Parameter, not a Module) between devices when offloading. It is otherwise only used in\n+        `scale_corrected_output`\n+        \"\"\"\n+        return (corrected.type_as(self.correct_output_scale) * self.correct_output_scale).type_as(corrected)\n+\n     def scale_corrected_output(self, corrected: torch.Tensor) -> torch.Tensor:\n         \"\"\"Scales the provided 3D tensor of shape [batch_size, num_tokens, hidden_size].\"\"\"\n-        return (corrected.type_as(self.correct_output_scale) * self.correct_output_scale).type_as(corrected)\n+        return self.forward(corrected)\n \n \n class Gemma3nTextRotaryEmbedding(nn.Module):\n@@ -1290,7 +1298,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.v_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps, with_scale=False)\n \n         first_kv_shared_layer_idx = self.config.num_hidden_layers - self.config.num_kv_shared_layers\n-        self.is_kv_shared_layer = layer_idx >= first_kv_shared_layer_idx\n+        self.is_kv_shared_layer = layer_idx >= first_kv_shared_layer_idx > 0\n         # Find the index of the last sliding or full layer before sharing starts (or None if no sharing)\n         layer_type = config.layer_types[layer_idx]\n         self.kv_shared_layer_index = (\n@@ -1319,21 +1327,22 @@ def forward(\n         query_states = query_states.transpose(1, 2)\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n-            # HybridCache has complex slicing when layer_type == \"sliding_attention\" that impact Shared KV Cache.\n+            # Device of past layer may be different from current one\n+            indices = cache_position.to(past_key_value.key_cache[self.kv_shared_layer_index].device)\n+            # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n             if isinstance(past_key_value, HybridCache) and self.is_sliding:\n                 max_length = past_key_value.sliding_window\n-                if cache_position.shape[0] > max_length:\n-                    # If in the prefill phase for a \"sliding_attention\" layer and the prefill is larger than the cache,\n-                    # slice into the entire cache.\n-                    indices = slice(0, max_length)\n-                else:\n-                    # If prefill fits or generating for a \"sliding_attention\" layer, clamp to max_cache_len - 1\n-                    indices = cache_position.clamp(min=0, max=max_length - 1)\n-            else:\n-                indices = cache_position\n+                indices = (\n+                    slice(0, max_length)\n+                    if cache_position.shape[0] > max_length\n+                    else cache_position.clamp(min=0, max=max_length - 1)\n+                )\n \n-            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices]\n-            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices]\n+            # Device of past layer may be different from current one\n+            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices].to(query_states.device)\n+            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices].to(\n+                query_states.device\n+            )\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)\n             key_states = self.k_norm(key_states)\n@@ -1447,10 +1456,9 @@ def forward(\n         attn_ffw_laurel_gated = attn_laurel + attn_ffw_norm\n         corrected_predictions = self.altup.correct(predictions, attn_ffw_laurel_gated)\n \n-        first_prediction = corrected_predictions[self.config.altup_active_idx]\n-        first_prediction_clone = first_prediction.clone()\n+        first_prediction = corrected_predictions[self.config.altup_active_idx].clone()\n         if self.config.altup_correct_scale:\n-            first_prediction = self.altup.scale_corrected_output(first_prediction_clone)\n+            first_prediction = self.altup.scale_corrected_output(first_prediction)\n \n         # per_layer_input_gate adapted from jax.numpy.einsum(\"btd,dp->btp\", ...)\n         first_prediction = self.per_layer_input_gate(first_prediction)\n@@ -1475,7 +1483,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     config_class = Gemma3nConfig\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Gemma3nDecoderLayer\"]\n+    _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_3 = True\n     _supports_flash_attn_2 = True\n@@ -1656,18 +1664,17 @@ def forward(\n         position_embeddings_local = self.rotary_emb_local(hidden_states_0, position_ids)\n \n         # Expand hidden_states to support per-layer inputs\n-        target_magnitude: torch.Tensor = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n-        epsilon_tensor = torch.tensor(torch.finfo().min)\n+        target_magnitude = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n+        epsilon_tensor = torch.tensor(1e-5)\n \n         temp_hidden_states = [hidden_states_0]\n         for i in range(1, self.config.altup_num_inputs):\n             # altup_proj adapted from jax.numpy.einsum(\"btp,pd->btd\", ...)\n-            altup_proj: torch.Tensor = self.altup_projections[i - 1](hidden_states_0)\n-            current_hidden_state = altup_proj.type(hidden_states_0.dtype)\n-            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5\n-            current_hidden_state = current_hidden_state * (\n-                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)\n-            )\n+            altup_proj = self.altup_projections[i - 1](hidden_states_0)\n+            current_hidden_state = altup_proj.to(dtype=hidden_states_0.dtype, device=target_magnitude.device)\n+            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True)\n+            new_magnitude = torch.sqrt(torch.maximum(new_magnitude, epsilon_tensor.to(target_magnitude.device)))\n+            current_hidden_state = current_hidden_state * target_magnitude / new_magnitude\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states, dim=0)  # [num_altup_inputs, batch, seq_len, hidden_size]\n@@ -1685,9 +1692,9 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global=position_embeddings_global,\n-                position_embeddings_local=position_embeddings_local,\n-                per_layer_input=per_layer_input,\n+                position_embeddings_global,\n+                position_embeddings_local,\n+                per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n@@ -1712,11 +1719,10 @@ def forward(\n         for i in range(1, self.config.altup_num_inputs):\n             # altup_unembed_projections adapted from jax.numpy.einsum(\"btp,pd->btd\", ...)\n             altup_unemb_proj: torch.Tensor = self.altup_unembed_projections[i - 1](hidden_states[i])\n-            current_hidden_state = altup_unemb_proj.type(hidden_states_0.dtype)\n-            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5\n-            current_hidden_state = current_hidden_state * (\n-                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)\n-            )\n+            current_hidden_state = altup_unemb_proj.to(dtype=hidden_states_0.dtype, device=target_magnitude.device)\n+            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True)\n+            new_magnitude = torch.sqrt(torch.maximum(new_magnitude, epsilon_tensor.to(target_magnitude.device)))\n+            current_hidden_state = current_hidden_state * target_magnitude / new_magnitude\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states)\n@@ -1743,7 +1749,9 @@ def project_per_layer_inputs(\n         per_layer_inputs: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         per_layer_projection: torch.Tensor = self.per_layer_model_projection(inputs_embeds)\n-        per_layer_projection *= self.per_layer_projection_scale.type(inputs_embeds.dtype)\n+        per_layer_projection *= self.per_layer_projection_scale.to(\n+            dtype=inputs_embeds.dtype, device=per_layer_projection.device\n+        )\n         per_layer_projection = per_layer_projection.reshape(\n             *inputs_embeds.shape[:-1],\n             self.config.num_hidden_layers,\n@@ -1758,7 +1766,9 @@ def project_per_layer_inputs(\n             # per-layer inputs are sometimes padded with zeros, slice the relevant embeddings.\n             per_layer_inputs = per_layer_inputs[..., : self.config.num_hidden_layers, :]\n \n-        return (per_layer_projection + per_layer_inputs) * self.per_layer_input_scale.type(inputs_embeds.dtype)\n+        return (per_layer_projection + per_layer_inputs) * self.per_layer_input_scale.to(\n+            dtype=inputs_embeds.dtype, device=per_layer_projection.device\n+        )\n \n \n @auto_docstring(custom_intro=\"The base Gemma 3n language model with a language modeling head.\")"
        },
        {
            "sha": "b0a5099ff56b5334aa75e65f600e32d4ecb988dd",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 46,
            "deletions": 36,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=dbc98328da2cabe7938423c51569252f2b49a5b3",
            "patch": "@@ -1685,9 +1685,17 @@ def correct(self, predictions: torch.Tensor, activated: torch.Tensor) -> torch.T\n         corrected += predictions  # add the original input\n         return corrected.contiguous().type_as(activated)\n \n+    def forward(self, corrected: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        This is only defined as the `forward` so that accelerate hooks can move correctly `correct_output_scale`\n+        (which is a nn.Parameter, not a Module) between devices when offloading. It is otherwise only used in\n+        `scale_corrected_output`\n+        \"\"\"\n+        return (corrected.type_as(self.correct_output_scale) * self.correct_output_scale).type_as(corrected)\n+\n     def scale_corrected_output(self, corrected: torch.Tensor) -> torch.Tensor:\n         \"\"\"Scales the provided 3D tensor of shape [batch_size, num_tokens, hidden_size].\"\"\"\n-        return (corrected.type_as(self.correct_output_scale) * self.correct_output_scale).type_as(corrected)\n+        return self.forward(corrected)\n \n \n class Gemma3nTextRotaryEmbedding(Gemma2RotaryEmbedding):\n@@ -1732,7 +1740,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.v_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps, with_scale=False)\n \n         first_kv_shared_layer_idx = self.config.num_hidden_layers - self.config.num_kv_shared_layers\n-        self.is_kv_shared_layer = layer_idx >= first_kv_shared_layer_idx\n+        self.is_kv_shared_layer = layer_idx >= first_kv_shared_layer_idx > 0\n         # Find the index of the last sliding or full layer before sharing starts (or None if no sharing)\n         layer_type = config.layer_types[layer_idx]\n         self.kv_shared_layer_index = (\n@@ -1761,21 +1769,22 @@ def forward(\n         query_states = query_states.transpose(1, 2)\n \n         if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n-            # HybridCache has complex slicing when layer_type == \"sliding_attention\" that impact Shared KV Cache.\n+            # Device of past layer may be different from current one\n+            indices = cache_position.to(past_key_value.key_cache[self.kv_shared_layer_index].device)\n+            # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n             if isinstance(past_key_value, HybridCache) and self.is_sliding:\n                 max_length = past_key_value.sliding_window\n-                if cache_position.shape[0] > max_length:\n-                    # If in the prefill phase for a \"sliding_attention\" layer and the prefill is larger than the cache,\n-                    # slice into the entire cache.\n-                    indices = slice(0, max_length)\n-                else:\n-                    # If prefill fits or generating for a \"sliding_attention\" layer, clamp to max_cache_len - 1\n-                    indices = cache_position.clamp(min=0, max=max_length - 1)\n-            else:\n-                indices = cache_position\n+                indices = (\n+                    slice(0, max_length)\n+                    if cache_position.shape[0] > max_length\n+                    else cache_position.clamp(min=0, max=max_length - 1)\n+                )\n \n-            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices]\n-            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices]\n+            # Device of past layer may be different from current one\n+            key_states = past_key_value.key_cache[self.kv_shared_layer_index][:, :, indices].to(query_states.device)\n+            value_states = past_key_value.value_cache[self.kv_shared_layer_index][:, :, indices].to(\n+                query_states.device\n+            )\n         else:\n             key_states = self.k_proj(hidden_states).view(hidden_shape)\n             key_states = self.k_norm(key_states)\n@@ -1880,10 +1889,9 @@ def forward(\n         attn_ffw_laurel_gated = attn_laurel + attn_ffw_norm\n         corrected_predictions = self.altup.correct(predictions, attn_ffw_laurel_gated)\n \n-        first_prediction = corrected_predictions[self.config.altup_active_idx]\n-        first_prediction_clone = first_prediction.clone()\n+        first_prediction = corrected_predictions[self.config.altup_active_idx].clone()\n         if self.config.altup_correct_scale:\n-            first_prediction = self.altup.scale_corrected_output(first_prediction_clone)\n+            first_prediction = self.altup.scale_corrected_output(first_prediction)\n \n         # per_layer_input_gate adapted from jax.numpy.einsum(\"btd,dp->btp\", ...)\n         first_prediction = self.per_layer_input_gate(first_prediction)\n@@ -1906,7 +1914,7 @@ def forward(\n class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     config_class = Gemma3nConfig\n     base_model_prefix = \"\"\n-    _no_split_modules = [\"Gemma3nDecoderLayer\"]\n+    _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     def _init_weights(self, module):\n         # important: this ported version of Gemma2 isn't meant for training from scratch - only\n@@ -1995,7 +2003,9 @@ def project_per_layer_inputs(\n         per_layer_inputs: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         per_layer_projection: torch.Tensor = self.per_layer_model_projection(inputs_embeds)\n-        per_layer_projection *= self.per_layer_projection_scale.type(inputs_embeds.dtype)\n+        per_layer_projection *= self.per_layer_projection_scale.to(\n+            dtype=inputs_embeds.dtype, device=per_layer_projection.device\n+        )\n         per_layer_projection = per_layer_projection.reshape(\n             *inputs_embeds.shape[:-1],\n             self.config.num_hidden_layers,\n@@ -2010,7 +2020,9 @@ def project_per_layer_inputs(\n             # per-layer inputs are sometimes padded with zeros, slice the relevant embeddings.\n             per_layer_inputs = per_layer_inputs[..., : self.config.num_hidden_layers, :]\n \n-        return (per_layer_projection + per_layer_inputs) * self.per_layer_input_scale.type(inputs_embeds.dtype)\n+        return (per_layer_projection + per_layer_inputs) * self.per_layer_input_scale.to(\n+            dtype=inputs_embeds.dtype, device=per_layer_projection.device\n+        )\n \n     @can_return_tuple\n     @auto_docstring\n@@ -2091,18 +2103,17 @@ def forward(\n         position_embeddings_local = self.rotary_emb_local(hidden_states_0, position_ids)\n \n         # Expand hidden_states to support per-layer inputs\n-        target_magnitude: torch.Tensor = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n-        epsilon_tensor = torch.tensor(torch.finfo().min)\n+        target_magnitude = torch.mean(hidden_states_0**2, dim=-1, keepdim=True) ** 0.5\n+        epsilon_tensor = torch.tensor(1e-5)\n \n         temp_hidden_states = [hidden_states_0]\n         for i in range(1, self.config.altup_num_inputs):\n             # altup_proj adapted from jax.numpy.einsum(\"btp,pd->btd\", ...)\n-            altup_proj: torch.Tensor = self.altup_projections[i - 1](hidden_states_0)\n-            current_hidden_state = altup_proj.type(hidden_states_0.dtype)\n-            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5\n-            current_hidden_state = current_hidden_state * (\n-                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)\n-            )\n+            altup_proj = self.altup_projections[i - 1](hidden_states_0)\n+            current_hidden_state = altup_proj.to(dtype=hidden_states_0.dtype, device=target_magnitude.device)\n+            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True)\n+            new_magnitude = torch.sqrt(torch.maximum(new_magnitude, epsilon_tensor.to(target_magnitude.device)))\n+            current_hidden_state = current_hidden_state * target_magnitude / new_magnitude\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states, dim=0)  # [num_altup_inputs, batch, seq_len, hidden_size]\n@@ -2120,9 +2131,9 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                position_embeddings_global=position_embeddings_global,\n-                position_embeddings_local=position_embeddings_local,\n-                per_layer_input=per_layer_input,\n+                position_embeddings_global,\n+                position_embeddings_local,\n+                per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n@@ -2147,11 +2158,10 @@ def forward(\n         for i in range(1, self.config.altup_num_inputs):\n             # altup_unembed_projections adapted from jax.numpy.einsum(\"btp,pd->btd\", ...)\n             altup_unemb_proj: torch.Tensor = self.altup_unembed_projections[i - 1](hidden_states[i])\n-            current_hidden_state = altup_unemb_proj.type(hidden_states_0.dtype)\n-            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5\n-            current_hidden_state = current_hidden_state * (\n-                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)\n-            )\n+            current_hidden_state = altup_unemb_proj.to(dtype=hidden_states_0.dtype, device=target_magnitude.device)\n+            new_magnitude = torch.mean(current_hidden_state**2, dim=-1, keepdim=True)\n+            new_magnitude = torch.sqrt(torch.maximum(new_magnitude, epsilon_tensor.to(target_magnitude.device)))\n+            current_hidden_state = current_hidden_state * target_magnitude / new_magnitude\n             temp_hidden_states.append(current_hidden_state)\n \n         hidden_states = torch.stack(temp_hidden_states)"
        },
        {
            "sha": "78349b8b906d6a4191da86e277e9d961157d669a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc98328da2cabe7938423c51569252f2b49a5b3/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=dbc98328da2cabe7938423c51569252f2b49a5b3",
            "patch": "@@ -1642,7 +1642,6 @@ def set_model_tester_for_less_flaky_test(test_case):\n         \"AriaVisionText2TextModelTester\",\n         \"GPTNeoModelTester\",\n         \"DPTModelTester\",\n-        \"Gemma3nTextModelTester\",  # cannot have a single layer combined with the cache sharing config attrs in the tester\n     ]\n     if test_case.model_tester.__class__.__name__ in exceptional_classes:\n         target_num_hidden_layers = None"
        },
        {
            "sha": "060bf15ea1e937f68c7ee13077a86ef1e74c18c2",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 68,
            "deletions": 2,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc98328da2cabe7938423c51569252f2b49a5b3/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc98328da2cabe7938423c51569252f2b49a5b3/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=dbc98328da2cabe7938423c51569252f2b49a5b3",
            "patch": "@@ -39,13 +39,20 @@\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n+    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    _test_eager_matches_sdpa_inference,\n+    floats_tensor,\n+    ids_tensor,\n+)\n from ..gemma.test_modeling_gemma import GemmaModelTester\n \n \n@@ -256,6 +263,7 @@ def __init__(\n         vocab_size=99,\n         vocab_size_per_layer_input=99,\n         hidden_size=16,\n+        hidden_size_per_layer_input=16,\n         num_hidden_layers=4,  # override to correctly test sharing cache pattern\n         num_kv_shared_layers=2,  # important to override\n         layer_types=[\n@@ -291,6 +299,7 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.vocab_size_per_layer_input = vocab_size_per_layer_input\n         self.hidden_size = hidden_size\n+        self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_hidden_layers = num_hidden_layers\n         self.num_kv_shared_layers = num_kv_shared_layers\n         self.layer_types = layer_types\n@@ -317,7 +326,6 @@ def __init__(\n         for_causal_lm_class = Gemma3nForCausalLM\n \n \n-@unittest.skip(\"Skipped for now!\")\n @require_torch\n class Gemma3nTextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Gemma3nTextModel, Gemma3nForCausalLM) if is_torch_available() else ()\n@@ -365,6 +373,64 @@ def _check_hidden_states_for_generate(\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(\n+        self,\n+        name,\n+        torch_dtype,\n+        padding_side,\n+        use_attention_mask,\n+        output_attentions,\n+        enable_kernels,\n+    ):\n+        \"We need to relax a bit the `atols` for fp32 here due to the altup projections\"\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-3,  # this was relaxed\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-3,  # this was relaxed\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-3,  # this was relaxed\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-3,  # this was relaxed\n+            (\"cuda\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+        _test_eager_matches_sdpa_inference(\n+            self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels, atols=atols\n+        )\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\n+        \"Gemma3n has a special shape for hidden states (due to per-layer projs) which is not compatible with contrastive decoding\"\n+    )\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\n+        \"Gemma3n has a special shape for hidden states (due to per-layer projs) which is not compatible with contrastive decoding\"\n+    )\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\n+        \"Gemma3n has a special shape for hidden states (due to per-layer projs) which is not compatible with contrastive decoding\"\n+    )\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(\n+        \"Gemma3n has a special shape for hidden states (due to per-layer projs) which is not compatible with dola decoding\"\n+    )\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n \n class Gemma3nVision2TextModelTester:\n     text_config = {\"activation_sparsity_pattern\": None}"
        },
        {
            "sha": "dd463932148fa310f4537bcf20f6b2d123c64e10",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 331,
            "deletions": 315,
            "changes": 646,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc98328da2cabe7938423c51569252f2b49a5b3/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc98328da2cabe7938423c51569252f2b49a5b3/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=dbc98328da2cabe7938423c51569252f2b49a5b3",
            "patch": "@@ -156,6 +156,334 @@\n ] + [(\"fp32_pad_left_output_attentions\", \"fp32\", \"left\", True, True, False)]\n \n \n+def _test_eager_matches_sdpa_inference(\n+    self,\n+    name,\n+    torch_dtype,\n+    padding_side,\n+    use_attention_mask,\n+    output_attentions,\n+    enable_kernels,\n+    atols=None,\n+    rtols=None,\n+):\n+    \"\"\"\n+    This test is written as a regular function to be able to overload it easily with different tolerances.\n+    Otherwise, `paramterezie.expand` prevents it as it removes the original function from the namespace.\n+    \"\"\"\n+    # TODO: we shouldn't need to do this skip, i.e. the test would be composable from the model tester. CLIP-like\n+    # models have a custom mixin, which we detect to skip this test.\n+    if any(\".CLIPModelTesterMixin\" in str(base) for base in self.__class__.__bases__):\n+        self.skipTest(reason=\"CLIP-like models have a different `test_eager_matches_sdpa_inference`\")\n+\n+    if not self.has_attentions:\n+        self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+    if not self.all_model_classes[0]._supports_sdpa:\n+        self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+    # convert shorthand name to torch.dtype\n+    if torch_dtype == \"fp16\":\n+        torch_dtype = torch.float16\n+    elif torch_dtype == \"bf16\":\n+        torch_dtype = torch.bfloat16\n+    elif torch_dtype == \"fp32\":\n+        torch_dtype = torch.float32\n+\n+    if not is_torch_fp16_available_on_device(torch_device) and torch_dtype == torch.float16:\n+        self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+    if not is_torch_bf16_available_on_device(torch_device) and torch_dtype == torch.bfloat16:\n+        self.skipTest(\n+            f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+        )\n+\n+    # Dictionary of tolerances for eager <> sdpa tests. Key = (device, sdpa_kernels_enabled, dtype)\n+    if atols is None:\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-6,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-6,\n+            (\"cuda\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+    if rtols is None:\n+        rtols = {\n+            (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-4,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-4,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,  # (different from others)\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+\n+    set_model_tester_for_less_flaky_test(self)\n+\n+    for model_class in self.all_model_classes:\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        set_config_for_less_flaky_test(config)\n+        model = model_class(config)\n+        # TODO: standardize the interfaces for musicgen models, see other todo in this test\n+        if model.__class__.__name__ == \"MusicgenMelodyForConditionalGeneration\":\n+            is_encoder_decoder = True\n+        else:\n+            is_encoder_decoder = model.config.is_encoder_decoder\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model_from_pretrained_kwargs = {\n+                \"pretrained_model_name_or_path\": tmpdirname,\n+                \"torch_dtype\": torch_dtype,\n+            }\n+\n+            if hasattr(config, \"use_mask_token\") or \"use_mask_token\" in inspect.signature(model.__init__).parameters:\n+                model_from_pretrained_kwargs[\"use_mask_token\"] = True\n+\n+            # TODO: remove this try/except, models should have a shared API\n+            try:\n+                model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"sdpa\")\n+            except ValueError:\n+                model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs)\n+            model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n+\n+            model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n+            model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n+\n+        set_model_for_less_flaky_test(model_eager)\n+        set_model_for_less_flaky_test(model_sdpa)\n+\n+        can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+        if not (self.has_attentions and can_output_attn) and output_attentions:\n+            self.skipTest(reason=\"Model does not support output_attentions\")\n+\n+        # TODO: if we can also check with `batch_size=1` without being flaky?\n+        for batch_size in [7]:\n+            # musicgen decoder models; TODO: find better abstraction\n+            if (\n+                model.__class__.__name__.startswith(\"Musicgen\")\n+                and hasattr(self.model_tester, \"num_codebooks\")\n+                and not hasattr(model_eager, \"text_encoder\")\n+            ):\n+                input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+            else:\n+                input_data_batch_size = batch_size\n+\n+            processed_inputs = {}\n+            processed_inputs[model.main_input_name] = inputs_dict[model.main_input_name]\n+\n+            for key in getattr(self, \"additional_model_inputs\", []):\n+                # Some models don't have all `additional_model_inputs`, especially when we\n+                # craft cases to test model in different settings\n+                if key in inputs_dict:\n+                    processed_inputs[key] = inputs_dict[key]\n+\n+            for key, value in processed_inputs.items():\n+                if torch.is_floating_point(value):\n+                    value = value.to(torch_dtype)\n+\n+                # extend value to have at least `input_data_batch_size` elements\n+                if value.shape[0] < input_data_batch_size:\n+                    size = (input_data_batch_size - value.shape[0], *value.shape[1:])\n+                    if torch.is_floating_point(value):\n+                        extension = torch.rand(size=size, dtype=value.dtype, device=torch_device)\n+                    else:\n+                        extension = torch.randint(high=5, size=size, dtype=value.dtype, device=torch_device)\n+                    value = torch.cat((value, extension), dim=0).to(torch_device)\n+\n+                processed_inputs[key] = value[:input_data_batch_size]\n+\n+            if not use_attention_mask:\n+                dummy_attention_mask = None\n+            else:\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+                if dummy_attention_mask is None:\n+                    if is_encoder_decoder:\n+                        seqlen = inputs_dict.get(\"decoder_input_ids\", processed_inputs[model.main_input_name]).shape[\n+                            -1\n+                        ]\n+                    else:\n+                        seqlen = processed_inputs[model.main_input_name].shape[-1]\n+                    dummy_attention_mask = torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n+\n+                # extend dummy_attention_mask to have at least `batch_size` elements\n+                if dummy_attention_mask.shape[0] < batch_size:\n+                    size = (batch_size - dummy_attention_mask.shape[0], *dummy_attention_mask.shape[1:])\n+                    extension = torch.ones(size=size, dtype=dummy_attention_mask.dtype, device=torch_device)\n+                    dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n+\n+                dummy_attention_mask = dummy_attention_mask[:batch_size].to(torch_device)\n+\n+                dummy_attention_mask[:] = 1\n+                if padding_side == \"left\":\n+                    dummy_attention_mask[-1, :2] = 0\n+                    dummy_attention_mask[-1, 2:] = 1\n+                elif padding_side == \"right\":\n+                    dummy_attention_mask[-1, -2:] = 0\n+                    dummy_attention_mask[-1, :-2] = 1\n+\n+            if is_encoder_decoder:\n+                # musicgen encoder-decoder models; TODO: find better abstraction\n+                if model.__class__.__name__.startswith(\"Musicgen\") and hasattr(self.model_tester, \"num_codebooks\"):\n+                    input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                else:\n+                    input_data_batch_size = batch_size\n+\n+                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", processed_inputs[model.main_input_name])\n+                decoder_input_ids = decoder_input_ids[:input_data_batch_size]\n+                if decoder_input_ids.shape[0] != input_data_batch_size:\n+                    extension = torch.ones(\n+                        input_data_batch_size - decoder_input_ids.shape[0],\n+                        *decoder_input_ids.shape[1:],\n+                        dtype=decoder_input_ids.dtype,\n+                        device=torch_device,\n+                    )\n+                    decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n+                    decoder_input_ids = decoder_input_ids.to(torch_device)\n+\n+                # TODO: never an `attention_mask` arg here?\n+                processed_inputs.update(\n+                    {\n+                        \"decoder_input_ids\": decoder_input_ids,\n+                        \"decoder_attention_mask\": dummy_attention_mask,\n+                        \"output_hidden_states\": True,\n+                    }\n+                )\n+            else:\n+                processed_inputs.update(\n+                    {\n+                        \"output_hidden_states\": True,\n+                    }\n+                )\n+\n+                # Otherwise fails for e.g. WhisperEncoderModel\n+                if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n+                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                if self.has_attentions and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters:\n+                    processed_inputs[\"output_attentions\"] = output_attentions\n+            if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n+                dummy_mask = torch.ones((self.model_tester.num_masks,))\n+\n+                # In case of additional token (like class) we define a custom `mask_length`\n+                if hasattr(self.model_tester, \"mask_length\"):\n+                    mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n+                else:\n+                    mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n+                dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n+                dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n+                processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n+\n+            if \"noise\" in inspect.signature(model_eager.forward).parameters:\n+                np.random.seed(2)\n+                num_patches = int((self.model_tester.image_size // self.model_tester.patch_size) ** 2)\n+                noise = np.random.uniform(size=(batch_size, num_patches))\n+                processed_inputs[\"noise\"] = torch.from_numpy(noise)\n+\n+            # TODO: test gradients as well (& for FA2 as well!)\n+            with torch.no_grad():\n+                with sdpa_kernel(\n+                    enable_flash=enable_kernels,\n+                    enable_math=True,\n+                    enable_mem_efficient=enable_kernels,\n+                ):\n+                    prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                    prepared_inputs = {\n+                        k: v.to(torch_device) if isinstance(v, torch.Tensor) else v for k, v in prepared_inputs.items()\n+                    }\n+                    outputs_eager = model_eager(**prepared_inputs)\n+                    outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+            if \"logits_per_text\" in outputs_eager:\n+                key = \"logits_per_text\"\n+            elif \"vision_hidden_states\" in outputs_eager:\n+                key = \"vision_hidden_states\"\n+            elif \"audio_values\" in outputs_eager:\n+                key = \"audio_values\"\n+            elif \"decoder_hidden_states\" in outputs_eager:\n+                key = \"decoder_hidden_states\"\n+            elif \"logits\" in outputs_eager and \"Classification\" in model_class.__name__:\n+                key = \"logits\"\n+            elif \"language_model_outputs\" in outputs_eager and \"blip\" in model_class.__name__.lower():\n+                outputs_eager = outputs_eager[\"language_model_outputs\"]\n+                outputs_sdpa = outputs_sdpa[\"language_model_outputs\"]\n+                key = \"hidden_states\" if \"hidden_states\" in outputs_eager else \"decoder_hidden_states\"\n+            else:\n+                key = \"hidden_states\"\n+\n+            # TODO: rename logits -> hidden_states\n+            logits_eager = outputs_eager[key]\n+            logits_sdpa = outputs_sdpa[key]\n+\n+            if key in [\"vision_hidden_states\", \"decoder_hidden_states\", \"hidden_states\"]:\n+                logits_eager = logits_eager[-1]\n+                logits_sdpa = logits_sdpa[-1]\n+\n+            if key == \"logits_per_text\":\n+                nan_mask = torch.isnan(logits_eager)\n+                logits_eager[nan_mask] = 0\n+                logits_sdpa[nan_mask] = 0\n+\n+            if torch_device in [\"cpu\", \"cuda\"]:\n+                atol = atols[torch_device, enable_kernels, torch_dtype]\n+                rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+            elif torch_device == \"hpu\":\n+                atol = atols[\"cuda\", enable_kernels, torch_dtype]\n+                rtol = rtols[\"cuda\", enable_kernels, torch_dtype]\n+            elif torch_device == \"xpu\":\n+                # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                # which is implemented on PyTorch level using aten operators and is\n+                # device agnostic with respect to implementation of each aten operator.\n+                atol = atols[\"cuda\", False, torch_dtype]\n+                rtol = rtols[\"cuda\", False, torch_dtype]\n+            else:\n+                atol = 1e-7\n+                rtol = 1e-4\n+\n+            # Masked tokens output slightly deviates - we don't mind that.\n+            if use_attention_mask:\n+                _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                _logits_eager[:-1] = logits_eager[:-1]\n+\n+                if padding_side == \"left\":\n+                    _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                    _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n+                elif padding_side == \"right\":\n+                    _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                    _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+\n+                logits_sdpa = _logits_sdpa\n+                logits_eager = _logits_eager\n+\n+            results = [\n+                torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+            ]\n+            # If 80% batch elements have matched results, it's fine\n+            if np.mean(results) < 0.8:\n+                mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n+                raise ValueError(\n+                    f\"mean relative difference for {key}: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n+                    f\"{rtol}\"\n+                )\n+\n+\n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)\n     for key in configs_no_init.__dict__.keys():\n@@ -3405,321 +3733,9 @@ def test_sdpa_can_dispatch_composite_models(self):\n     def test_eager_matches_sdpa_inference(\n         self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n     ):\n-        # TODO: we shouldn't need to do this skip, i.e. the test would be composable from the model tester. CLIP-like\n-        # models have a custom mixin, which we detect to skip this test.\n-        if any(\".CLIPModelTesterMixin\" in str(base) for base in self.__class__.__bases__):\n-            self.skipTest(reason=\"CLIP-like models have a different `test_eager_matches_sdpa_inference`\")\n-\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        # convert shorthand name to torch.dtype\n-        if torch_dtype == \"fp16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bf16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"fp32\":\n-            torch_dtype = torch.float32\n-\n-        if not is_torch_fp16_available_on_device(torch_device) and torch_dtype == torch.float16:\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if not is_torch_bf16_available_on_device(torch_device) and torch_dtype == torch.bfloat16:\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Dictionary of tolerances for eager <> sdpa tests. Key = (device, sdpa_kernels_enabled, dtype)\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,  # (different from others)\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        set_model_tester_for_less_flaky_test(self)\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            set_config_for_less_flaky_test(config)\n-            model = model_class(config)\n-            # TODO: standardize the interfaces for musicgen models, see other todo in this test\n-            if model.__class__.__name__ == \"MusicgenMelodyForConditionalGeneration\":\n-                is_encoder_decoder = True\n-            else:\n-                is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_from_pretrained_kwargs = {\n-                    \"pretrained_model_name_or_path\": tmpdirname,\n-                    \"torch_dtype\": torch_dtype,\n-                }\n-\n-                if (\n-                    hasattr(config, \"use_mask_token\")\n-                    or \"use_mask_token\" in inspect.signature(model.__init__).parameters\n-                ):\n-                    model_from_pretrained_kwargs[\"use_mask_token\"] = True\n-\n-                # TODO: remove this try/except, models should have a shared API\n-                try:\n-                    model_sdpa = model_class.from_pretrained(\n-                        **model_from_pretrained_kwargs, attn_implementation=\"sdpa\"\n-                    )\n-                except ValueError:\n-                    model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs)\n-                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n-\n-                model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n-                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n-\n-            set_model_for_less_flaky_test(model_eager)\n-            set_model_for_less_flaky_test(model_sdpa)\n-\n-            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                self.skipTest(reason=\"Model does not support output_attentions\")\n-\n-            # TODO: if we can also check with `batch_size=1` without being flaky?\n-            for batch_size in [7]:\n-                # musicgen decoder models; TODO: find better abstraction\n-                if (\n-                    model.__class__.__name__.startswith(\"Musicgen\")\n-                    and hasattr(self.model_tester, \"num_codebooks\")\n-                    and not hasattr(model_eager, \"text_encoder\")\n-                ):\n-                    input_data_batch_size = batch_size * self.model_tester.num_codebooks\n-                else:\n-                    input_data_batch_size = batch_size\n-\n-                processed_inputs = {}\n-                processed_inputs[model.main_input_name] = inputs_dict[model.main_input_name]\n-\n-                for key in getattr(self, \"additional_model_inputs\", []):\n-                    # Some models don't have all `additional_model_inputs`, especially when we\n-                    # craft cases to test model in different settings\n-                    if key in inputs_dict:\n-                        processed_inputs[key] = inputs_dict[key]\n-\n-                for key, value in processed_inputs.items():\n-                    if torch.is_floating_point(value):\n-                        value = value.to(torch_dtype)\n-\n-                    # extend value to have at least `input_data_batch_size` elements\n-                    if value.shape[0] < input_data_batch_size:\n-                        size = (input_data_batch_size - value.shape[0], *value.shape[1:])\n-                        if torch.is_floating_point(value):\n-                            extension = torch.rand(size=size, dtype=value.dtype, device=torch_device)\n-                        else:\n-                            extension = torch.randint(high=5, size=size, dtype=value.dtype, device=torch_device)\n-                        value = torch.cat((value, extension), dim=0).to(torch_device)\n-\n-                    processed_inputs[key] = value[:input_data_batch_size]\n-\n-                if not use_attention_mask:\n-                    dummy_attention_mask = None\n-                else:\n-                    dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                    if dummy_attention_mask is None:\n-                        if is_encoder_decoder:\n-                            seqlen = inputs_dict.get(\n-                                \"decoder_input_ids\", processed_inputs[model.main_input_name]\n-                            ).shape[-1]\n-                        else:\n-                            seqlen = processed_inputs[model.main_input_name].shape[-1]\n-                        dummy_attention_mask = torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-\n-                    # extend dummy_attention_mask to have at least `batch_size` elements\n-                    if dummy_attention_mask.shape[0] < batch_size:\n-                        size = (batch_size - dummy_attention_mask.shape[0], *dummy_attention_mask.shape[1:])\n-                        extension = torch.ones(size=size, dtype=dummy_attention_mask.dtype, device=torch_device)\n-                        dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-\n-                    dummy_attention_mask = dummy_attention_mask[:batch_size].to(torch_device)\n-\n-                    dummy_attention_mask[:] = 1\n-                    if padding_side == \"left\":\n-                        dummy_attention_mask[-1, :2] = 0\n-                        dummy_attention_mask[-1, 2:] = 1\n-                    elif padding_side == \"right\":\n-                        dummy_attention_mask[-1, -2:] = 0\n-                        dummy_attention_mask[-1, :-2] = 1\n-\n-                if is_encoder_decoder:\n-                    # musicgen encoder-decoder models; TODO: find better abstraction\n-                    if model.__class__.__name__.startswith(\"Musicgen\") and hasattr(self.model_tester, \"num_codebooks\"):\n-                        input_data_batch_size = batch_size * self.model_tester.num_codebooks\n-                    else:\n-                        input_data_batch_size = batch_size\n-\n-                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", processed_inputs[model.main_input_name])\n-                    decoder_input_ids = decoder_input_ids[:input_data_batch_size]\n-                    if decoder_input_ids.shape[0] != input_data_batch_size:\n-                        extension = torch.ones(\n-                            input_data_batch_size - decoder_input_ids.shape[0],\n-                            *decoder_input_ids.shape[1:],\n-                            dtype=decoder_input_ids.dtype,\n-                            device=torch_device,\n-                        )\n-                        decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                        decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                    # TODO: never an `attention_mask` arg here?\n-                    processed_inputs.update(\n-                        {\n-                            \"decoder_input_ids\": decoder_input_ids,\n-                            \"decoder_attention_mask\": dummy_attention_mask,\n-                            \"output_hidden_states\": True,\n-                        }\n-                    )\n-                else:\n-                    processed_inputs.update(\n-                        {\n-                            \"output_hidden_states\": True,\n-                        }\n-                    )\n-\n-                    # Otherwise fails for e.g. WhisperEncoderModel\n-                    if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                        processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                    if self.has_attentions and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters:\n-                        processed_inputs[\"output_attentions\"] = output_attentions\n-                if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n-                    dummy_mask = torch.ones((self.model_tester.num_masks,))\n-\n-                    # In case of additional token (like class) we define a custom `mask_length`\n-                    if hasattr(self.model_tester, \"mask_length\"):\n-                        mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n-                    else:\n-                        mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n-                    dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                    dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                    processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                if \"noise\" in inspect.signature(model_eager.forward).parameters:\n-                    np.random.seed(2)\n-                    num_patches = int((self.model_tester.image_size // self.model_tester.patch_size) ** 2)\n-                    noise = np.random.uniform(size=(batch_size, num_patches))\n-                    processed_inputs[\"noise\"] = torch.from_numpy(noise)\n-\n-                # TODO: test gradients as well (& for FA2 as well!)\n-                with torch.no_grad():\n-                    with sdpa_kernel(\n-                        enable_flash=enable_kernels,\n-                        enable_math=True,\n-                        enable_mem_efficient=enable_kernels,\n-                    ):\n-                        prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                        prepared_inputs = {\n-                            k: v.to(torch_device) if isinstance(v, torch.Tensor) else v\n-                            for k, v in prepared_inputs.items()\n-                        }\n-                        outputs_eager = model_eager(**prepared_inputs)\n-                        outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                if \"logits_per_text\" in outputs_eager:\n-                    key = \"logits_per_text\"\n-                elif \"vision_hidden_states\" in outputs_eager:\n-                    key = \"vision_hidden_states\"\n-                elif \"audio_values\" in outputs_eager:\n-                    key = \"audio_values\"\n-                elif \"decoder_hidden_states\" in outputs_eager:\n-                    key = \"decoder_hidden_states\"\n-                elif \"logits\" in outputs_eager and \"Classification\" in model_class.__name__:\n-                    key = \"logits\"\n-                elif \"language_model_outputs\" in outputs_eager and \"blip\" in model_class.__name__.lower():\n-                    outputs_eager = outputs_eager[\"language_model_outputs\"]\n-                    outputs_sdpa = outputs_sdpa[\"language_model_outputs\"]\n-                    key = \"hidden_states\" if \"hidden_states\" in outputs_eager else \"decoder_hidden_states\"\n-                else:\n-                    key = \"hidden_states\"\n-\n-                # TODO: rename logits -> hidden_states\n-                logits_eager = outputs_eager[key]\n-                logits_sdpa = outputs_sdpa[key]\n-\n-                if key in [\"vision_hidden_states\", \"decoder_hidden_states\", \"hidden_states\"]:\n-                    logits_eager = logits_eager[-1]\n-                    logits_sdpa = logits_sdpa[-1]\n-\n-                if key == \"logits_per_text\":\n-                    nan_mask = torch.isnan(logits_eager)\n-                    logits_eager[nan_mask] = 0\n-                    logits_sdpa[nan_mask] = 0\n-\n-                if torch_device in [\"cpu\", \"cuda\"]:\n-                    atol = atols[torch_device, enable_kernels, torch_dtype]\n-                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                elif torch_device == \"hpu\":\n-                    atol = atols[\"cuda\", enable_kernels, torch_dtype]\n-                    rtol = rtols[\"cuda\", enable_kernels, torch_dtype]\n-                elif torch_device == \"xpu\":\n-                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                    # which is implemented on PyTorch level using aten operators and is\n-                    # device agnostic with respect to implementation of each aten operator.\n-                    atol = atols[\"cuda\", False, torch_dtype]\n-                    rtol = rtols[\"cuda\", False, torch_dtype]\n-                else:\n-                    atol = 1e-7\n-                    rtol = 1e-4\n-\n-                # Masked tokens output slightly deviates - we don't mind that.\n-                if use_attention_mask:\n-                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                    _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                    _logits_eager[:-1] = logits_eager[:-1]\n-\n-                    if padding_side == \"left\":\n-                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                    elif padding_side == \"right\":\n-                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                    logits_sdpa = _logits_sdpa\n-                    logits_eager = _logits_eager\n-\n-                results = [\n-                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                ]\n-                # If 80% batch elements have matched results, it's fine\n-                if np.mean(results) < 0.8:\n-                    mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n-                    raise ValueError(\n-                        f\"mean relative difference for {key}: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n-                        f\"{rtol}\"\n-                    )\n+        _test_eager_matches_sdpa_inference(\n+            self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+        )\n \n     @require_torch_sdpa\n     @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 881,
        "additions": 491,
        "deletions": 390
    }
}