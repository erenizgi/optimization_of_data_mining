{
    "author": "IlyasMoutawwakil",
    "message": "Fix grad accum arbitrary value (#36691)",
    "sha": "6f3e0b68e0030051d9181fb1f494e539adeb069c",
    "files": [
        {
            "sha": "4842970c7772fa1454d471a9b78c40058ffaba36",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f3e0b68e0030051d9181fb1f494e539adeb069c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f3e0b68e0030051d9181fb1f494e539adeb069c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6f3e0b68e0030051d9181fb1f494e539adeb069c",
            "patch": "@@ -813,7 +813,7 @@ def is_torch_hpu_available():\n \n         def patched_masked_fill_(self, mask, value):\n             if self.dtype == torch.int64:\n-                logger.warning(\n+                logger.warning_once(\n                     \"In-place tensor.masked_fill_(mask, value) is not supported for int64 tensors on Gaudi1. \"\n                     \"This operation will be performed out-of-place using tensor[mask] = value.\"\n                 )"
        },
        {
            "sha": "6bbf43e8b92e928c28a673da8cb4b00622d9407a",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f3e0b68e0030051d9181fb1f494e539adeb069c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f3e0b68e0030051d9181fb1f494e539adeb069c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=6f3e0b68e0030051d9181fb1f494e539adeb069c",
            "patch": "@@ -896,9 +896,8 @@ def tokenize_function(examples):\n \n         # all diff truth should be quite close\n         self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n-\n-        # max diff broken should be very off\n-        self.assertGreater(max(diff_broken), 1.3, f\"Difference {max(diff_broken)} is not greater than 1.3\")\n+        # max diff broken should be very off (\"very off\" is arbitrary, but as long as it's bigger than 0.1, it's fine)\n+        self.assertGreater(max(diff_broken), 0.7, f\"Difference {max(diff_broken)} is not greater than 0.7\")\n \n         loss_base = sum(base_loss_callback.losses)\n         loss_broken = sum(broken_loss_callback.losses)"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 3,
        "deletions": 4
    }
}