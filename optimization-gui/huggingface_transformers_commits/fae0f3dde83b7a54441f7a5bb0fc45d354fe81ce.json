{
    "author": "faaany",
    "message": "[tests] fix `EsmModelIntegrationTest::test_inference_bitsandbytes`  (#36225)\n\nfix failed test",
    "sha": "fae0f3dde83b7a54441f7a5bb0fc45d354fe81ce",
    "files": [
        {
            "sha": "7be71c22c783620133afe33537c3a5a979620ce8",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fae0f3dde83b7a54441f7a5bb0fc45d354fe81ce/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fae0f3dde83b7a54441f7a5bb0fc45d354fe81ce/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=fae0f3dde83b7a54441f7a5bb0fc45d354fe81ce",
            "patch": "@@ -335,13 +335,13 @@ def test_inference_no_head(self):\n     def test_inference_bitsandbytes(self):\n         model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t36_3B_UR50D\", load_in_8bit=True)\n \n-        input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]])\n+        input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]]).to(model.device)\n         # Just test if inference works\n         with torch.no_grad():\n             _ = model(input_ids)[0]\n \n         model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t36_3B_UR50D\", load_in_4bit=True)\n \n-        input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]])\n+        input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]]).to(model.device)\n         # Just test if inference works\n         _ = model(input_ids)[0]"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}