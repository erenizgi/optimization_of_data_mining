{
    "author": "keetrap",
    "message": "Added Support for Custom Quantization (#35915)\n\n* Added Support for Custom Quantization\n\n* Update code\n\n* code reformatted\n\n* Updated Changes\n\n* Updated Changes\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "8eaae6bee973e2068b5ed5622a708d99ce3dd862",
    "files": [
        {
            "sha": "16b31cd8ebe4b68e68288dac2772d3c76c7866ff",
            "filename": "examples/quantization/custom_quantization.py",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/8eaae6bee973e2068b5ed5622a708d99ce3dd862/examples%2Fquantization%2Fcustom_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8eaae6bee973e2068b5ed5622a708d99ce3dd862/examples%2Fquantization%2Fcustom_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization.py?ref=8eaae6bee973e2068b5ed5622a708d99ce3dd862",
            "patch": "@@ -0,0 +1,78 @@\n+import json\n+from typing import Any, Dict\n+\n+import torch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.quantizers import HfQuantizer, register_quantization_config, register_quantizer\n+from transformers.utils.quantization_config import QuantizationConfigMixin\n+\n+\n+@register_quantization_config(\"custom\")\n+class CustomConfig(QuantizationConfigMixin):\n+    def __init__(self):\n+        self.quant_method = \"custom\"\n+        self.bits = 8\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        output = {\n+            \"num_bits\": self.bits,\n+        }\n+        return output\n+\n+    def __repr__(self):\n+        config_dict = self.to_dict()\n+        return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n+\n+    def to_diff_dict(self) -> Dict[str, Any]:\n+        config_dict = self.to_dict()\n+\n+        default_config_dict = CustomConfig().to_dict()\n+\n+        serializable_config_dict = {}\n+\n+        for key, value in config_dict.items():\n+            if value != default_config_dict[key]:\n+                serializable_config_dict[key] = value\n+\n+        return serializable_config_dict\n+\n+\n+@register_quantizer(\"custom\")\n+class CustomQuantizer(HfQuantizer):\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+        self.scale_map = {}\n+        self.device = kwargs.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n+        self.torch_dtype = kwargs.get(\"torch_dtype\", torch.float32)\n+\n+    def _process_model_before_weight_loading(self, model, **kwargs):\n+        return True\n+\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n+        return True\n+\n+    def is_serializable(self) -> bool:\n+        return True\n+\n+    def is_trainable(self) -> bool:\n+        return False\n+\n+\n+model_8bit = AutoModelForCausalLM.from_pretrained(\n+    \"facebook/opt-350m\", quantization_config=CustomConfig(), torch_dtype=\"auto\"\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n+input_text = \"once there is\"\n+inputs = tokenizer(input_text, return_tensors=\"pt\")\n+output = model_8bit.generate(\n+    **inputs,\n+    max_length=100,\n+    num_return_sequences=1,\n+    no_repeat_ngram_size=2,\n+)\n+generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n+\n+print(generated_text)"
        },
        {
            "sha": "5bc952e7bd8ac5d489455a0071acfebc155dba67",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8eaae6bee973e2068b5ed5622a708d99ce3dd862",
            "patch": "@@ -3706,8 +3706,10 @@ def from_pretrained(\n             device_map = hf_quantizer.update_device_map(device_map)\n \n             # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n-            user_agent[\"quant\"] = hf_quantizer.quantization_config.quant_method.value\n-\n+            if hasattr(hf_quantizer.quantization_config.quant_method, \"value\"):\n+                user_agent[\"quant\"] = hf_quantizer.quantization_config.quant_method.value\n+            else:\n+                user_agent[\"quant\"] = hf_quantizer.quantization_config.quant_method\n             # Force-set to `True` for more mem efficiency\n             if low_cpu_mem_usage is None:\n                 low_cpu_mem_usage = True"
        },
        {
            "sha": "96c8d4fa5043d59c3dc367a9782f425da2f29dc6",
            "filename": "src/transformers/quantizers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fquantizers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fquantizers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2F__init__.py?ref=8eaae6bee973e2068b5ed5622a708d99ce3dd862",
            "patch": "@@ -11,5 +11,5 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from .auto import AutoHfQuantizer, AutoQuantizationConfig\n+from .auto import AutoHfQuantizer, AutoQuantizationConfig, register_quantization_config, register_quantizer\n from .base import HfQuantizer"
        },
        {
            "sha": "64634f98a44aec4cce5e572982531fe1196c1912",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8eaae6bee973e2068b5ed5622a708d99ce3dd862/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=8eaae6bee973e2068b5ed5622a708d99ce3dd862",
            "patch": "@@ -35,6 +35,7 @@\n     TorchAoConfig,\n     VptqConfig,\n )\n+from .base import HfQuantizer\n from .quantizer_aqlm import AqlmHfQuantizer\n from .quantizer_awq import AwqQuantizer\n from .quantizer_bitnet import BitNetHfQuantizer\n@@ -226,3 +227,35 @@ def supports_quant_method(quantization_config_dict):\n             )\n             return False\n         return True\n+\n+\n+def register_quantization_config(method: str):\n+    \"\"\"Register a custom quantization configuration.\"\"\"\n+\n+    def register_config_fn(cls):\n+        if method in AUTO_QUANTIZATION_CONFIG_MAPPING:\n+            raise ValueError(f\"Config '{method}' already registered\")\n+\n+        if not issubclass(cls, QuantizationConfigMixin):\n+            raise ValueError(\"Config must extend QuantizationConfigMixin\")\n+\n+        AUTO_QUANTIZATION_CONFIG_MAPPING[method] = cls\n+        return cls\n+\n+    return register_config_fn\n+\n+\n+def register_quantizer(name: str):\n+    \"\"\"Register a custom quantizer.\"\"\"\n+\n+    def register_quantizer_fn(cls):\n+        if name in AUTO_QUANTIZER_MAPPING:\n+            raise ValueError(f\"Quantizer '{name}' already registered\")\n+\n+        if not issubclass(cls, HfQuantizer):\n+            raise ValueError(\"Quantizer must extend HfQuantizer\")\n+\n+        AUTO_QUANTIZER_MAPPING[name] = cls\n+        return cls\n+\n+    return register_quantizer_fn"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 116,
        "deletions": 3
    }
}