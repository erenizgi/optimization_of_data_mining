{
    "author": "winglian",
    "message": "prevent creating a view/leaf param for low rank optimizers w FSDP (#37379)\n\nprevent creating a view/leaf param for low rank optimizers:",
    "sha": "6a75528cbc0c3985a5ba0a436cf8974b1dac8c01",
    "files": [
        {
            "sha": "4c953ac12d6af7f87a400b3405a0f6be2c98e9ad",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a75528cbc0c3985a5ba0a436cf8974b1dac8c01/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a75528cbc0c3985a5ba0a436cf8974b1dac8c01/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6a75528cbc0c3985a5ba0a436cf8974b1dac8c01",
            "patch": "@@ -1363,7 +1363,6 @@ def setup_low_rank_optimizer(\n                 and args.optim_target_modules.replace(\"_\", \"-\") == \"all-linear\"\n             )\n \n-            target_params = []\n             target_params_names = []\n             for module_name, module in model.named_modules():\n                 target_module_exists, is_regex = check_target_module_exists(\n@@ -1380,12 +1379,12 @@ def setup_low_rank_optimizer(\n                 if not target_module_exists and not all_linear:\n                     continue\n \n-                target_params.append(module.weight)\n                 target_params_names.append(module_name + \".weight\")\n \n-            if len(target_params) == 0:\n+            if len(target_params_names) == 0:\n                 raise ValueError(f\"No target modules found for {optimizer_name} ({args.optim_target_modules}).\")\n \n+            target_params = [p for n, p in model.named_parameters() if n in target_params_names]\n             non_target_params = [p for n, p in model.named_parameters() if n not in target_params_names]\n             optim_kwargs.update(optim_args)\n "
        }
    ],
    "stats": {
        "total": 5,
        "additions": 2,
        "deletions": 3
    }
}