{
    "author": "NielsRogge",
    "message": "[Kosmos 2.5] Rename checkpoints (#40338)",
    "sha": "1499f9e3566cc402c64cc66a460799f3f01eaa07",
    "files": [
        {
            "sha": "530f1d459ae76bdc3734795488282a27dff276b3",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 50,
            "deletions": 10,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/1499f9e3566cc402c64cc66a460799f3f01eaa07/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1499f9e3566cc402c64cc66a460799f3f01eaa07/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=1499f9e3566cc402c64cc66a460799f3f01eaa07",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n -->\n-*This model was released on 2023-09-23 and added to Hugging Face Transformers on 2025-08-19.*\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-19.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n@@ -48,14 +48,14 @@ import requests\n from PIL import Image, ImageDraw\n from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n \n-repo = \"ydshieh/kosmos-2.5\"\n-device = f\"{infer_device()}:0\"\n+repo = \"microsoft/kosmos-2.5\"\n+device = \"cuda:0\"\n dtype = torch.bfloat16\n model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)\n \n # sample image\n-url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n image = Image.open(requests.get(url, stream=True).raw)\n \n prompt = \"<md>\"\n@@ -87,14 +87,14 @@ import requests\n from PIL import Image, ImageDraw\n from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n \n-repo = \"ydshieh/kosmos-2.5\"\n-device = f\"{infer_device()}:0\"\n+repo = \"microsoft/kosmos-2.5\"\n+device = \"cuda:0\"\n dtype = torch.bfloat16\n model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)\n \n # sample image\n-url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n image = Image.open(requests.get(url, stream=True).raw)\n \n # bs = 1\n@@ -160,12 +160,52 @@ image.save(\"output.png\")\n </hfoptions>\n \n \n-## Example\n-**Markdown Task:** For usage instructions, please refer to [md.py](https://huggingface.co/ydshieh/kosmos-2.5/blob/main/md.py).\n+## Chat version\n \n-**OCR Task:** For usage instructions, please refer to [ocr.py](https://huggingface.co/ydshieh/kosmos-2.5/blob/main/ocr.py).\n+The authors also released Kosmos-2.5 Chat, which is a chat version optimized for document understanding. You can use it like so:\n \n+```python\n+import re\n+import torch\n+import requests\n+from PIL import Image, ImageDraw\n+from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n+\n+repo = \"microsoft/kosmos-2.5-chat\"\n+device = \"cuda:0\"\n+dtype = torch.bfloat16\n+\n+model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo,\n+                                                          device_map=device,\n+                                                          torch_dtype=dtype,\n+                                                          attn_implementation=\"flash_attention_2\")\n+processor = AutoProcessor.from_pretrained(repo)\n \n+# sample image\n+url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n+\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+question = \"What is the sub total of the receipt?\"\n+template = \"<md>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n+prompt = template.format(question)\n+inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+\n+height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n+raw_width, raw_height = image.size\n+scale_height = raw_height / height\n+scale_width = raw_width / width\n+\n+inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\n+inputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\n+generated_ids = model.generate(\n+    **inputs,\n+    max_new_tokens=1024,\n+)\n+\n+generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+print(generated_text[0])\n+```\n \n ## Kosmos2_5Config\n "
        },
        {
            "sha": "28543f7c6355bbd78fcdd7fddf96993376dae0c9",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1499f9e3566cc402c64cc66a460799f3f01eaa07/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1499f9e3566cc402c64cc66a460799f3f01eaa07/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=1499f9e3566cc402c64cc66a460799f3f01eaa07",
            "patch": "@@ -1730,13 +1730,13 @@ def forward(\n         >>> import torch\n         >>> from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n \n-        >>> repo = \"ydshieh/kosmos-2.5\"\n+        >>> repo = \"microsoft/kosmos-2.5\"\n         >>> device = \"cuda:0\"\n         >>> dtype = torch.bfloat16 # torch.float16\n         >>> model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\n         >>> processor = AutoProcessor.from_pretrained(repo)\n \n-        >>> url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+        >>> url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n \n         >>> image = Image.open(requests.get(url, stream=True).raw)\n "
        },
        {
            "sha": "6efda1a6cba80224164b9539c1a2fca1893fd860",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=1499f9e3566cc402c64cc66a460799f3f01eaa07",
            "patch": "@@ -523,7 +523,7 @@ def check_same_values(layer_1, layer_2):\n \n     @slow\n     def test_model_from_pretrained(self):\n-        model_name = \"ydshieh/kosmos-2.5\"\n+        model_name = \"microsoft/kosmos-2.5\"\n         model = Kosmos2_5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n@@ -669,11 +669,11 @@ def run_example(self, prompt, image, model, processor):\n         return generated_ids, generated_text\n \n     def test_eager(self):\n-        url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+        url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         dtype = torch.bfloat16\n-        repo = \"ydshieh/kosmos-2.5\"\n+        repo = \"microsoft/kosmos-2.5\"\n         model = Kosmos2_5ForConditionalGeneration.from_pretrained(\n             repo, device_map=torch_device, dtype=dtype, attn_implementation=\"eager\"\n         )\n@@ -706,11 +706,11 @@ def test_eager(self):\n         self.assertListEqual(generated_text, EXPECTED_TEXT[self.cuda_compute_capability_major_version])\n \n     def test_sdpa(self):\n-        url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+        url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         dtype = torch.bfloat16\n-        repo = \"ydshieh/kosmos-2.5\"\n+        repo = \"microsoft/kosmos-2.5\"\n         model = Kosmos2_5ForConditionalGeneration.from_pretrained(\n             repo, device_map=torch_device, dtype=dtype, attn_implementation=\"sdpa\"\n         )\n@@ -747,11 +747,11 @@ def test_sdpa(self):\n     @pytest.mark.flash_attn_test\n     @slow\n     def test_FA2(self):\n-        url = \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/receipt_00008.png\"\n+        url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         dtype = torch.bfloat16\n-        repo = \"ydshieh/kosmos-2.5\"\n+        repo = \"microsoft/kosmos-2.5\"\n         model = Kosmos2_5ForConditionalGeneration.from_pretrained(\n             repo,\n             device_map=torch_device,"
        },
        {
            "sha": "fdafa5d48b320c591142227d13177775fef61997",
            "filename": "tests/models/kosmos2_5/test_processor_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py?ref=1499f9e3566cc402c64cc66a460799f3f01eaa07",
            "patch": "@@ -52,7 +52,7 @@ class Kosmos2_5ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n         image_processor = Kosmos2_5ImageProcessor()\n-        tokenizer = AutoTokenizer.from_pretrained(\"ydshieh/kosmos-2.5\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/kosmos-2.5\")\n         processor = Kosmos2_5Processor(image_processor, tokenizer)\n         processor.save_pretrained(self.tmpdirname)\n \n@@ -67,7 +67,7 @@ def tearDown(self):\n \n     def test_image_procesor_load_save_reload(self):\n         # make sure load from Hub repo. -> save -> reload locally work\n-        image_processor = Kosmos2_5ImageProcessor.from_pretrained(\"ydshieh/kosmos-2.5\")\n+        image_processor = Kosmos2_5ImageProcessor.from_pretrained(\"microsoft/kosmos-2.5\")\n         with TemporaryDirectory() as tmp_dir:\n             image_processor.save_pretrained(tmp_dir)\n             reloaded_image_processor = Kosmos2_5ImageProcessor.from_pretrained(tmp_dir)\n@@ -120,7 +120,7 @@ def test_tokenizer_decode(self):\n         self.assertListEqual(decoded_tok, decoded_processor)\n \n     def test_can_load_various_tokenizers(self):\n-        for checkpoint in [\"ydshieh/kosmos-2.5\"]:\n+        for checkpoint in [\"microsoft/kosmos-2.5\"]:\n             processor = AutoProcessor.from_pretrained(checkpoint)\n             tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n             self.assertEqual(processor.tokenizer.__class__, tokenizer.__class__)\n@@ -300,7 +300,7 @@ def test_structured_kwargs_nested_from_dict(self):\n     @require_torch\n     def test_full_processor(self):\n         url = \"https://huggingface.co/kirp/kosmos2_5/resolve/main/receipt_00008.png\"\n-        processor = AutoProcessor.from_pretrained(\"ydshieh/kosmos-2.5\")\n+        processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2.5\")\n         texts = [\"<md>\", \"<ocr>\"]\n         expected_input_ids = [\n             [100288],"
        },
        {
            "sha": "4c61415f5395a7b31b5acbcf0f59acdfbf0828e7",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1499f9e3566cc402c64cc66a460799f3f01eaa07/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=1499f9e3566cc402c64cc66a460799f3f01eaa07",
            "patch": "@@ -334,7 +334,7 @@ def test_mistral3_integration_batched_generate(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/view.jpg\"},\n+                        {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/view.jpg\"},\n                         {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n                     ],\n                 },\n@@ -402,7 +402,7 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/view.jpg\"},\n+                        {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/view.jpg\"},\n                         {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n                     ],\n                 },\n@@ -413,11 +413,11 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n                     \"content\": [\n                         {\n                             \"type\": \"image\",\n-                            \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                            \"url\": \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n                         },\n                         {\n                             \"type\": \"image\",\n-                            \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                            \"url\": \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n                         },\n                         {\n                             \"type\": \"text\","
        }
    ],
    "stats": {
        "total": 94,
        "additions": 67,
        "deletions": 27
    }
}