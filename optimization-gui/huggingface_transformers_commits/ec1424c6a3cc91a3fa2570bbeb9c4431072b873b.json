{
    "author": "klae01",
    "message": "Update modeling_mamba2.py, fix pad size (#32599)\n\n* Update modeling_mamba2.py\r\n\r\nFix pad_size calculation to ensure it's less than self.chunk_size\r\n\r\n* [run_slow] mamba2\r\n\r\n* [run-slow] mamba2\r\n\r\n* [run-slow] Add @require_read_token decorator to failing tests for token propagation\r\n\r\n* [run_slow] mamba2",
    "sha": "ec1424c6a3cc91a3fa2570bbeb9c4431072b873b",
    "files": [
        {
            "sha": "19d53437130e2466527ae502d3dfe499201aa724",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec1424c6a3cc91a3fa2570bbeb9c4431072b873b/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec1424c6a3cc91a3fa2570bbeb9c4431072b873b/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=ec1424c6a3cc91a3fa2570bbeb9c4431072b873b",
            "patch": "@@ -510,7 +510,7 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n             C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            pad_size = self.chunk_size - (seq_len % self.chunk_size)\n+            pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)\n "
        },
        {
            "sha": "f19358a22f4b319de0c79e87e6e0559028e95089",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec1424c6a3cc91a3fa2570bbeb9c4431072b873b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec1424c6a3cc91a3fa2570bbeb9c4431072b873b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=ec1424c6a3cc91a3fa2570bbeb9c4431072b873b",
            "patch": "@@ -291,6 +291,7 @@ def setUp(self):\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, from_slow=True, legacy=False)\n         self.prompt = (\"[INST]Write a hello world program in C++.\",)\n \n+    @require_read_token\n     @parameterized.expand(\n         [\n             (torch_device,),\n@@ -319,6 +320,7 @@ def test_simple_generate(self, device):\n         ground_truth_sentence = \"\"\"<s>[INST]Write a hello world program in C++.[/INST] Sure, here is a simple \"Hello, World!\" program in C++:\\n\\n```cpp\\n#include <iostream>\\n\\n\"\"\"\n         self.assertEqual(output_sentence, ground_truth_sentence)\n \n+    @require_read_token\n     @slow\n     @require_torch_gpu\n     def test_batched_equivalence_with_cache(self):\n@@ -349,6 +351,7 @@ def test_batched_equivalence_with_cache(self):\n             individual_output = tokenizer.batch_decode(individual_gen, skip_special_tokens=True)[0]\n             self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n \n+    @require_read_token\n     @slow\n     @require_torch_gpu\n     def test_batched_equivalence_without_cache(self):"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 4,
        "deletions": 1
    }
}