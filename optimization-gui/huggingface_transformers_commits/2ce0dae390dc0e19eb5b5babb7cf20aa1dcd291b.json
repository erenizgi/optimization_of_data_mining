{
    "author": "Cyrilvallez",
    "message": "Switch the order of args in StaticCache (for BC and future logic) (#40100)\n\n* switch order for BC and future logic\n\n* in generate as well",
    "sha": "2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b",
    "files": [
        {
            "sha": "8e3aff3c8bd3edc05454d0e1e395fc032ac572da",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b",
            "patch": "@@ -1148,15 +1148,15 @@ class StaticCache(Cache):\n     >>> # Prepare a cache class and pass it to model's forward\n     >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n     >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = StaticCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> past_key_values = StaticCache(config=model.config, max_cache_len=max_generated_length)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache filled with key/values from generation\n     StaticCache()\n     ```\n     \"\"\"\n \n-    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n         layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n         super().__init__(layers=layers)\n \n@@ -1183,15 +1183,15 @@ class OffloadedStaticCache(Cache):\n \n     >>> # Prepare a cache class with offloading\n     >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = OffloadedStaticCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> past_key_values = OffloadedStaticCache(config=model.config, max_cache_len=max_generated_length)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache with offloaded layers\n     OffloadedStaticCache()\n     ```\n     \"\"\"\n \n-    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n         layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n         super().__init__(layers=layers, offloading=True)\n \n@@ -1214,15 +1214,15 @@ class SlidingWindowCache(Cache):\n     >>> # Prepare a cache class and pass it to model's forward\n     >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n     >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = SlidingWindowCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> past_key_values = SlidingWindowCache(config=model.config, max_cache_len=max_generated_length)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache filled with key/values from generation\n     SlidingWindowCache()\n     ```\n     \"\"\"\n \n-    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n         layers = [SlidingWindowLayer(max_cache_len, config.sliding_window) for _ in range(config.num_hidden_layers)]\n         super().__init__(layers=layers)\n \n@@ -1249,15 +1249,15 @@ class HybridCache(Cache):\n     >>> # Prepare a cache class and pass it to model's forward\n     >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n     >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = HybridCache(max_cache_len=max_generated_length, config=model.config)\n+    >>> past_key_values = HybridCache(config=model.config, max_cache_len=max_generated_length)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache filled with key/values from generation\n     HybridCache()\n     ```\n     \"\"\"\n \n-    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n         if hasattr(config, \"layer_types\"):\n             layers = []\n             for layer_type in config.layer_types:\n@@ -1288,8 +1288,8 @@ class OffloadedHybridCache(Cache):\n     See `Cache` for details on common methods that are implemented by all cache classes.\n     \"\"\"\n \n-    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, max_cache_len: int, config: PretrainedConfig, **kwargs):\n+    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n         if hasattr(config, \"layer_types\"):\n             layers = []\n             for layer_type in config.layer_types:"
        },
        {
            "sha": "eabc6f2926d39edd247ba030b1af698a3fa82748",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b",
            "patch": "@@ -1859,10 +1859,7 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n             )\n \n         if need_new_cache:\n-            cache_kwargs = {\n-                \"max_cache_len\": max_cache_len,\n-                \"config\": self.config.get_text_config(),\n-            }\n+            cache_kwargs = {\"config\": self.config.get_text_config(), \"max_cache_len\": max_cache_len}\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:\n                 encoder_kwargs = cache_kwargs.copy()"
        },
        {
            "sha": "a56a48ab844d6ff0a17fb5d08a6fe8c9854d86fe",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b",
            "patch": "@@ -670,7 +670,7 @@ def __init__(\n             raise AssertionError(\"Model must have caching enabled.\")\n \n         # Initialize the HybridCache\n-        self.cache = HybridCache(max_cache_len=generation_config.cache_config.get(\"max_cache_len\"), config=config)\n+        self.cache = HybridCache(config=config, max_cache_len=generation_config.cache_config.get(\"max_cache_len\"))\n         head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n         max_batch_size = generation_config.cache_config.get(\"batch_size\")\n@@ -818,7 +818,7 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.config = model.config\n \n         # Initialize static cache for decoder and DynamicCache for encoder\n-        self.static_cache = StaticCache(max_cache_len=max_static_cache_length, config=self.config)\n+        self.static_cache = StaticCache(config=self.config, max_cache_len=max_static_cache_length)\n         head_dim = getattr(self.config, \"head_dim\", self.config.hidden_size // self.config.num_attention_heads)\n         num_heads = getattr(self.config, \"num_key_value_heads\", self.config.num_attention_heads)\n         self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, \"cpu\")"
        },
        {
            "sha": "26be82b9da8221ec2328d5c7214a6af18519f288",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=2ce0dae390dc0e19eb5b5babb7cf20aa1dcd291b",
            "patch": "@@ -504,7 +504,7 @@ def test_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(max_cache_len=max_cache_len, config=self.model.config)\n+        past_key_values = StaticCache(config=self.model.config, max_cache_len=max_cache_len)\n \n         padded_attention_mask = torch.nn.functional.pad(\n             input=mask_shared_prefix,\n@@ -546,7 +546,7 @@ def test_partial_stacked_causal_mask_static_cache(self):\n \n         # upgrade the model with StaticCache\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n-        past_key_values = StaticCache(max_cache_len=max_cache_len, config=self.model.config)\n+        past_key_values = StaticCache(config=self.model.config, max_cache_len=max_cache_len)\n \n         # forward run for the first part of input\n         part_a = 3  # split point"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 19,
        "deletions": 22
    }
}