{
    "author": "winglian",
    "message": "make LlamaModel._update_causal_mask torch compilable (#35187)\n\n* make LlamaModel._update_causal_mask torch compilable\n\n* chore: lint (make fix-copies)\n\n* fix-copies\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
    "files": [
        {
            "sha": "b96697bc0779e6fc85e11f69fb4ea8b3f235bd81",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1012,7 +1012,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "9d7325c502d6b7024b5cafc39697d83abdb5107d",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -740,7 +740,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "90a02dd5bb9fee19411c75ed5fe87f3c231ec97b",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1385,7 +1385,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "5c8f1b3957ab388f917e42f921652b5dd50027b9",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -583,7 +583,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "a65d3ee64a234a25163e40ab0fa0d7fe6c13ab9e",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -910,7 +910,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "3f2e7c384d7d6320a9c65af7a5d9494adb145cf5",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1111,7 +1111,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "71cd6b6158ca0bcc691f75e8036f9c5069120c87",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -633,7 +633,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "706847650b818e2c2cbc0d04400240e246f7863f",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -644,7 +644,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "4e41c80d69f22e60ee2b447e8e77c6d490913438",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -792,7 +792,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "f512938e75f9a724d8d0278380b61c9adebb549f",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -931,7 +931,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "fba67ae03a5979b2f15a2cd09ff0d41b958105e5",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -667,7 +667,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "00749b7eb07fbcbcbf67f5b0b6e3252333e2b537",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -891,7 +891,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "7e758947b6dd8a49201ffe648765e952a1169ffd",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -646,7 +646,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "e6b9682b5ae803e8c6de7b9ba781753c774dab1c",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1362,7 +1362,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "a2a86fd4c22f4a01af4fc5262edc81ef56b0d1de",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1126,7 +1126,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "df46e15bce0009f919595a2c1fb32575328d1e7d",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -632,7 +632,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "15958e772c90ebda215b2f254b59b2e869d0a44a",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1600,7 +1600,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "6523ab6812179c64ceadad1e40648fc02801de33",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1076,7 +1076,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "e4017536017f4318b41732087c3601aef7694578",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1192,7 +1192,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "75618f1c7e00c7f0f83fa5d1b9d2dc4dbd9323cc",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -878,7 +878,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "39bfa726deeedfaf277a9dc4dcae90dbee99cbf5",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -608,7 +608,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "89b5f4abe1c39c5c3a44e49e8c88698190449636",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -609,7 +609,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "27712741b7c28fa441c12137bae65831cda2d207",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -683,7 +683,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "5aa038d3ccfaa818c1a16924835767b788eb8d40",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -606,7 +606,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "41115a058d2e0abc91f2767c2746fa69ca2773a4",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1587,7 +1587,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "bb5366ef764feccea6d9f17a5d502e98bbb66169",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1000,7 +1000,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "5dba7594e7e9a1cf2d80e6c72bc5503dc5b8d80f",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -617,7 +617,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "7214a36e9a3921e04b7cb81e4f089c634dc1ae31",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -938,7 +938,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "daeae8f9dcc2b353dced79448eeca9dbda8420b6",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1136,7 +1136,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "fe6cfbc5c3fdf200a9b2db1403ca4b3028b196eb",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1205,7 +1205,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "af21f714eff2941cb603504c42d5353134911d51",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1538,7 +1538,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "2b007cb2c771572cb1f677c03930ee8121954c87",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -849,7 +849,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "21bb2c869b763360ff273762d00765f53aaa2701",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=5e7aedebebbdee0d7eb0b8b2d771e45783dbf8c7",
            "patch": "@@ -1375,7 +1375,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        }
    ],
    "stats": {
        "total": 66,
        "additions": 33,
        "deletions": 33
    }
}