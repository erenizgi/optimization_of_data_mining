{
    "author": "bvantuan",
    "message": "Fix missing initializations for models created in 2024 (#38987)\n\n* fix GroundingDino\n\n* fix SuperGlue\n\n* fix GroundingDino\n\n* fix MambaModel\n\n* fix OmDetTurbo\n\n* fix SegGpt\n\n* fix Qwen2Audio\n\n* fix Mamba2\n\n* fix DabDetr\n\n* fix Dac\n\n* fix FalconMamba\n\n* skip timm initialization\n\n* fix Encodec and MusicgenMelody\n\n* fix Musicgen\n\n* skip timm initialization test\n\n* fix OmDetTurbo\n\n* clean the code\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* add reviewed changes\n\n* add back timm\n\n* style\n\n* better check for parametrizations\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "e355c0a11c927d9e8f22409559c0fae76ccc598c",
    "files": [
        {
            "sha": "119a7a0b16284fe22514cf33ce2fea0afabd3dab",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -829,6 +829,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n@@ -841,6 +844,8 @@ def _init_weights(self, module):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias_value = -math.log((1 - prior_prob) / prior_prob)\n             module.class_embed.bias.data.fill_(bias_value)\n+        elif isinstance(module, nn.PReLU):\n+            module.reset_parameters()\n \n \n # Modified from transformers.models.detr.modeling_detr.DetrEncoder with Detr->DabDetr,DETR->ConditionalDETR"
        },
        {
            "sha": "398d258bef0890407a6d78e2707cb1e31cd3390f",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -480,6 +480,12 @@ def _init_weights(self, module):\n         if isinstance(module, nn.Conv1d):\n             nn.init.trunc_normal_(module.weight, std=0.02)\n             nn.init.constant_(module.bias, 0)\n+        elif isinstance(module, Snake1d):\n+            module.alpha.data.fill_(1.0)\n+        elif isinstance(module, nn.ConvTranspose1d):\n+            module.reset_parameters()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=0.02)\n \n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm"
        },
        {
            "sha": "6e610ba2953553f8af9c0649ac366b729b73be3d",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -235,7 +235,7 @@ class EncodecLSTM(nn.Module):\n     LSTM without worrying about the hidden state, nor the layout of the data. Expects input as convolutional layout.\n     \"\"\"\n \n-    def __init__(self, config, dimension):\n+    def __init__(self, config: EncodecConfig, dimension: int):\n         super().__init__()\n         self.lstm = nn.LSTM(dimension, dimension, config.num_lstm_layers)\n \n@@ -452,22 +452,16 @@ class EncodecPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+        if isinstance(module, nn.GroupNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.ConvTranspose1d):\n+            module.reset_parameters()\n         elif isinstance(module, nn.LSTM):\n             for name, param in module.named_parameters():\n                 if \"weight\" in name:\n@@ -659,7 +653,7 @@ def _decode_frame(self, codes: torch.Tensor, scale: Optional[torch.Tensor] = Non\n \n     def decode(\n         self,\n-        audio_codes: torch.Tensor,\n+        audio_codes: torch.LongTensor,\n         audio_scales: torch.Tensor,\n         padding_mask: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n@@ -708,10 +702,10 @@ def decode(\n     @auto_docstring\n     def forward(\n         self,\n-        input_values: torch.Tensor,\n-        padding_mask: Optional[torch.Tensor] = None,\n+        input_values: torch.FloatTensor,\n+        padding_mask: Optional[torch.BoolTensor] = None,\n         bandwidth: Optional[float] = None,\n-        audio_codes: Optional[torch.Tensor] = None,\n+        audio_codes: Optional[torch.LongTensor] = None,\n         audio_scales: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], EncodecOutput]:"
        },
        {
            "sha": "942053be3e70c0af1dfdfae3f94a2bdd347ec201",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 33,
            "deletions": 20,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -445,9 +445,16 @@ class FalconMambaPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n+        std = self.config.initializer_range\n         if isinstance(module, FalconMambaMixer):\n+            # S4D real initialization. These are not discretized!\n+            # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n+            A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n+            A = A.expand(module.intermediate_size, -1).contiguous()\n+            module.A_log.copy_(torch.log(A))\n             module.A_log._no_weight_decay = True\n             module.D._no_weight_decay = True\n+            module.D.data.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n@@ -462,33 +469,39 @@ def _init_weights(self, module):\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            with torch.no_grad():\n-                module.dt_proj.bias.copy_(inv_dt)\n+            module.dt_proj.bias.copy_(inv_dt)\n             module.dt_proj.bias._no_reinit = True\n \n+            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            if module.conv1d.bias is not None:\n+                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n+                    nn.init.zeros_(module.conv1d.bias)\n+            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+\n+            if self.config.rescale_prenorm_residual:\n+                # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n+                #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n+                #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n+                #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n+                #\n+                # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n+                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n+                # We need to reinit p since this code could be called multiple times\n+                # Having just p *= scale would repeatedly scale it down\n+                p = module.out_proj.weight\n+                p /= math.sqrt(self.config.num_hidden_layers)\n+\n         if isinstance(module, nn.Linear):\n+            if not getattr(module.weight, \"_no_reinit\", False):\n+                nn.init.normal_(module.weight, std=std)\n             if module.bias is not None:\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n+        elif isinstance(module, FalconMambaRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=self.config.initializer_range)\n-\n-        if self.config.rescale_prenorm_residual:\n-            # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n-            #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n-            #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n-            #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n-            #\n-            # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-            for name, p in module.named_parameters():\n-                if name in [\"out_proj.weight\"]:\n-                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n-                    # We need to reinit p since this code could be called multiple times\n-                    # Having just p *= scale would repeatedly scale it down\n-                    nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n-                    with torch.no_grad():\n-                        p /= math.sqrt(self.config.num_hidden_layers)\n+            nn.init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "743f74a1215bccc15335b0ea8fa4919d429d1ce3",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -1414,16 +1414,18 @@ def _init_weights(self, module):\n             module.out_vision_proj.bias.data.fill_(0)\n             nn.init.xavier_uniform_(module.out_text_proj.weight)\n             module.out_text_proj.bias.data.fill_(0)\n-        elif isinstance(module, (GroundingDinoEncoderLayer, GroundingDinoDecoderLayer)):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.normal_(p, mean=0.0, std=std)\n+        elif isinstance(module, GroundingDinoFusionLayer):\n+            module.vision_param.data.fill_(1e-4)\n+            module.text_param.data.fill_(1e-4)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "7da4ef57878698b949bdd5e2d9efb21634ee581e",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 33,
            "deletions": 20,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -382,9 +382,16 @@ class MambaPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n+        std = self.config.initializer_range\n         if isinstance(module, MambaMixer):\n+            # S4D real initialization. These are not discretized!\n+            # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n+            A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n+            A = A.expand(module.intermediate_size, -1).contiguous()\n+            module.A_log.copy_(torch.log(A))\n             module.A_log._no_weight_decay = True\n             module.D._no_weight_decay = True\n+            module.D.data.fill_(1.0)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n@@ -399,33 +406,39 @@ def _init_weights(self, module):\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            with torch.no_grad():\n-                module.dt_proj.bias.copy_(inv_dt)\n+            module.dt_proj.bias.copy_(inv_dt)\n             module.dt_proj.bias._no_reinit = True\n \n+            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            if module.conv1d.bias is not None:\n+                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n+                    nn.init.zeros_(module.conv1d.bias)\n+            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+\n+            if self.config.rescale_prenorm_residual:\n+                # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n+                #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n+                #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n+                #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n+                #\n+                # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n+                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n+                # We need to reinit p since this code could be called multiple times\n+                # Having just p *= scale would repeatedly scale it down\n+                p = module.out_proj.weight\n+                p /= math.sqrt(self.config.num_hidden_layers)\n+\n         if isinstance(module, nn.Linear):\n+            if not getattr(module.weight, \"_no_reinit\", False):\n+                nn.init.normal_(module.weight, std=std)\n             if module.bias is not None:\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n+        elif isinstance(module, MambaRMSNorm):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=self.config.initializer_range)\n-\n-        if self.config.rescale_prenorm_residual:\n-            # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n-            #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n-            #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n-            #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n-            #\n-            # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-            for name, p in module.named_parameters():\n-                if name in [\"out_proj.weight\"]:\n-                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n-                    # We need to reinit p since this code could be called multiple times\n-                    # Having just p *= scale would repeatedly scale it down\n-                    nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n-                    with torch.no_grad():\n-                        p /= math.sqrt(self.config.num_hidden_layers)\n+            nn.init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "e601b4d8a6972e0c0136a3d4daee744af73e8ac2",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 32,
            "deletions": 20,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -721,9 +721,15 @@ class Mamba2PreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n+        std = self.config.initializer_range\n         if isinstance(module, Mamba2Mixer):\n+            # S4D real initialization. These are not discretized!\n+            # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n+            A = torch.arange(1, self.config.num_heads + 1)\n+            module.A_log.copy_(torch.log(A))\n             module.A_log._no_weight_decay = True\n             module.D._no_weight_decay = True\n+            module.D.data.fill_(1.0)\n \n             dt = torch.exp(\n                 torch.rand(self.config.num_heads)\n@@ -733,33 +739,39 @@ def _init_weights(self, module):\n \n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            with torch.no_grad():\n-                module.dt_bias.copy_(inv_dt)\n+            module.dt_bias.copy_(inv_dt)\n             module.dt_bias._no_reinit = True\n \n+            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            if module.conv1d.bias is not None:\n+                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n+                    nn.init.zeros_(module.conv1d.bias)\n+            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+\n+            if self.config.rescale_prenorm_residual:\n+                # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n+                #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n+                #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n+                #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n+                #\n+                # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n+                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n+                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n+                # We need to reinit p since this code could be called multiple times\n+                # Having just p *= scale would repeatedly scale it down\n+                p = module.out_proj.weight\n+                p /= math.sqrt(self.config.num_hidden_layers)\n+\n         if isinstance(module, nn.Linear):\n+            if not getattr(module.weight, \"_no_reinit\", False):\n+                nn.init.normal_(module.weight, std=std)\n             if module.bias is not None:\n                 if not getattr(module.bias, \"_no_reinit\", False):\n                     nn.init.zeros_(module.bias)\n+        elif isinstance(module, (Mamba2RMSNorm, MambaRMSNormGated)):\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=self.config.initializer_range)\n-\n-        if self.config.rescale_prenorm_residual:\n-            # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n-            #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n-            #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n-            #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n-            #\n-            # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-            for name, p in module.named_parameters():\n-                if name in [\"out_proj.weight\"]:\n-                    # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n-                    # We need to reinit p since this code could be called multiple times\n-                    # Having just p *= scale would repeatedly scale it down\n-                    nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n-                    with torch.no_grad():\n-                        p /= math.sqrt(self.config.num_hidden_layers)\n+            nn.init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "139256c7c71c413ee6b7544a0e2af6f1211e1a38",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -440,10 +440,13 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "55e28ca58f71ffefa4883360b79bc7feb15b2089",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -406,10 +406,13 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n@@ -1286,7 +1289,7 @@ def __init__(\n             The text encoder model that encodes text into hidden states for conditioning.\n         audio_encoder (`PreTrainedModel`, *optional*):\n             The audio encoder model that encodes audio into hidden states for conditioning.\n-        decoder (`MusicgenForCausalLM`, *optional*):\n+        decoder (`MusicgenMelodyForCausalLM`, *optional*):\n             The decoder model that generates audio tokens based on conditioning signals.\n         \"\"\"\n         if config is None and None in (text_encoder, audio_encoder, decoder):"
        },
        {
            "sha": "9bac40553d9f5d039e58b3446a73ab8c9f1cdd6b",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -1006,10 +1006,15 @@ def linear_init_(module_to_init):\n             nn.init.xavier_uniform_(module.query_position_head.layers[1].weight)\n             for layer in module.channel_projection_layers:\n                 nn.init.xavier_uniform_(layer[0].weight)\n+        elif isinstance(module, OmDetTurboLanguageBackbone):\n+            nn.init.normal_(module.text_projection, std=self.config.text_projection_in_dim**-0.5)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             module.weight.data.normal_(mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, OmDetTurboDecoder):"
        },
        {
            "sha": "f90f7ff9cf9581541ed01ce17cdf87e09a0c23cf",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -283,6 +283,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "364483359ee9ddafc6598ee44595e813b7e330eb",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -604,7 +604,7 @@ class SegGptPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SegGptEmbeddings\", \"SegGptLayer\"]\n \n-    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+    def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n@@ -615,7 +615,7 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n             )\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, (nn.LayerNorm, SegGptLayerNorm)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, SegGptAttention):"
        },
        {
            "sha": "ce92e7b66bb71798ec26cf611faac2f9ef2ac33b",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -551,17 +551,18 @@ class SuperGluePreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d)):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, nn.BatchNorm1d):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        elif isinstance(module, SuperGlueMultiLayerPerceptron):\n-            nn.init.constant_(module.linear.bias, 0.0)\n+\n+        if hasattr(module, \"bin_score\"):\n+            module.bin_score.data.fill_(1.0)\n \n \n @auto_docstring("
        },
        {
            "sha": "a429561b7158a0844df7da8909a2aa69b8f3ba72",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -310,12 +310,13 @@ def test_attention_outputs(self):\n \n     def test_feed_forward_chunking(self):\n         (original_config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()\n+        # original_config.norm_type = \"time_group_norm\"\n         for model_class in self.all_model_classes:\n             torch.manual_seed(0)\n             config = copy.deepcopy(original_config)\n             config.chunk_length_s = None\n             config.overlap = None\n-            config.sampling_rate = 10\n+            config.sampling_rate = 20\n \n             model = model_class(config)\n             model.to(torch_device)\n@@ -326,9 +327,9 @@ def test_feed_forward_chunking(self):\n             hidden_states_no_chunk = model(**inputs)[1]\n \n             torch.manual_seed(0)\n-            config.chunk_length_s = 1\n+            config.chunk_length_s = 2\n             config.overlap = 0\n-            config.sampling_rate = 10\n+            config.sampling_rate = 20\n \n             model = model_class(config)\n             model.to(torch_device)"
        },
        {
            "sha": "cada419ea03add7b1469d474608782eb44bc2bc0",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -33,7 +33,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -359,9 +359,11 @@ def test_falcon_mamba_lm_head_forward_and_backwards(self):\n \n     def test_initialization(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.rescale_prenorm_residual = True\n \n+        configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n-            model = model_class(config=config)\n+            model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if \"dt_proj.bias\" in name:\n                     dt = torch.exp(\n@@ -380,6 +382,19 @@ def test_initialization(self):\n                     if param.requires_grad:\n                         # check if it's a ones like\n                         torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n+                else:\n+                    if param.requires_grad:\n+                        if (\n+                            \"mixer.conv1d.weight\" in name\n+                            or \"mixer.dt_proj.weight\" in name\n+                            or \"mixer.out_proj.weight\" in name\n+                        ):\n+                            continue\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n \n     @slow\n     # Ignore copy"
        },
        {
            "sha": "d632f99e2ca09b10f202185c4d34b1a6ef196718",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -586,6 +586,8 @@ def test_initialization(self):\n                         or \"value_proj\" in name\n                         or \"output_proj\" in name\n                         or \"reference_points\" in name\n+                        or \"vision_proj\" in name\n+                        or \"text_proj\" in name\n                     ):\n                         continue\n                     self.assertIn("
        },
        {
            "sha": "b570d1a130b63210878acf3d351182674d610b76",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -326,9 +326,11 @@ def test_mamba_lm_head_forward_and_backwards(self):\n \n     def test_initialization(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.rescale_prenorm_residual = True\n \n+        configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n-            model = model_class(config=config)\n+            model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if \"dt_proj.bias\" in name:\n                     dt = torch.exp(\n@@ -347,6 +349,19 @@ def test_initialization(self):\n                     if param.requires_grad:\n                         # check if it's a ones like\n                         torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n+                else:\n+                    if param.requires_grad:\n+                        if (\n+                            \"mixer.conv1d.weight\" in name\n+                            or \"mixer.dt_proj.weight\" in name\n+                            or \"mixer.out_proj.weight\" in name\n+                        ):\n+                            continue\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "c9cec231e64bc95fb1099b579b3d2662d6546a5e",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 3,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n \n+import math\n import unittest\n \n from transformers import AutoTokenizer, Mamba2Config, is_torch_available\n@@ -28,7 +29,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -276,14 +277,37 @@ def test_mamba2_slow_vs_fast_forward_grouped(self):\n \n     def test_initialization(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.rescale_prenorm_residual = True\n \n+        configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n-            model = model_class(config=config)\n+            model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n-                if \"D\" in name:\n+                if \"dt_proj.bias\" in name:\n+                    dt = torch.exp(\n+                        torch.tensor([0, 1]) * (math.log(config.time_step_max) - math.log(config.time_step_min))\n+                        + math.log(config.time_step_min)\n+                    ).clamp(min=config.time_step_floor)\n+                    inv_dt = dt + torch.log(-torch.expm1(-dt))\n+                    if param.requires_grad:\n+                        self.assertTrue(param.data.max().item() <= inv_dt[1])\n+                        self.assertTrue(param.data.min().item() >= inv_dt[0])\n+                elif \"A_log\" in name:\n+                    A = torch.arange(1, config.num_heads + 1)\n+                    torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n+                elif \"D\" in name:\n                     if param.requires_grad:\n                         # check if it's a ones like\n                         torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n+                else:\n+                    if param.requires_grad:\n+                        if \"mixer.conv1d.weight\" in name or \"mixer.dt_bias\" in name or \"mixer.out_proj.weight\" in name:\n+                            continue\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n \n     @unittest.skip(reason=\"Mamba 2 weights are not tied\")\n     def test_tied_weights_keys(self):"
        },
        {
            "sha": "9d76ad392cc93e5cebdf078bd7b340cb931a3117",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -629,6 +629,7 @@ def test_initialization(self):\n                         or \"decoder.channel_projection_layers\" in name\n                         or \"query_position_head\" in name\n                         or \"decoder.encoder_vision_features\" in name\n+                        or \"language_backbone.text_projection\" in name\n                     ):\n                         continue\n                     self.assertIn("
        },
        {
            "sha": "3f103309a04cf0c04b748a3da0d4d28af53fcf73",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -153,10 +153,18 @@ def test_torchscript_output_attentions(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(reason=\"TimmWrapper initialization is managed on the timm side\")\n+    def test_can_init_all_missing_weights(self):\n+        pass\n+\n     @unittest.skip(reason=\"TimmWrapper initialization is managed on the timm side\")\n     def test_initialization(self):\n         pass\n \n+    @unittest.skip(reason=\"TimmWrapper initialization is managed on the timm side\")\n+    def test_mismatched_shapes_have_properly_initialized_weights(self):\n+        pass\n+\n     @unittest.skip(reason=\"Need to use a timm model and there is no tiny model available.\")\n     def test_model_is_small(self):\n         pass"
        },
        {
            "sha": "da48081d6bf71e8349fcdcc004db464db12f11fa",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e355c0a11c927d9e8f22409559c0fae76ccc598c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e355c0a11c927d9e8f22409559c0fae76ccc598c",
            "patch": "@@ -855,7 +855,7 @@ def test_can_init_all_missing_weights(self):\n             # For now, skip everything older than 2025 and \"important models\" (too much models to patch otherwise)\n             # Use `supports_cache_class` as a proxy to judge \"important\" models in order to prioritize them\n             # TODO: relax this as we patch more and more models\n-            if addition_year < 2025 and not model_class._supports_cache_class:\n+            if addition_year < 2024 and not model_class._supports_cache_class:\n                 self.skipTest(reason=f\"{model_class} is not a priorited model for now.\")\n \n             # Monkey patch the method to add a seed (we do it on PreTrainedModel._initialize_weights, which wraps\n@@ -895,6 +895,11 @@ def seeded_initialize_weights(self, module):\n                 model_from_config.state_dict().items(), model_from_pretrained.state_dict().items()\n             ):\n                 self.assertEqual(k1, k2, \"The keys from each model should be the same\")\n+\n+                # In case using torch.nn.utils.parametrizations on a module, we should skip the resulting keys\n+                if re.search(r\"\\.parametrizations\\..*?\\.original[01]\", k1):\n+                    continue\n+\n                 # Since we added the seed, they should be exactly the same (i.e. using allclose maybe be wrong due\n                 # to very low std in init function)\n                 if not (v1 == v2).all():"
        }
    ],
    "stats": {
        "total": 327,
        "additions": 229,
        "deletions": 98
    }
}