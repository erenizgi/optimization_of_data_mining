{
    "author": "Cyrilvallez",
    "message": "Remove `set_model_tester_for_less_flaky_tests` (#40982)\n\nremove",
    "sha": "5f6e278a5177d8b85945a2cdb6b776dacee34914",
    "files": [
        {
            "sha": "b66d92f6902664f8c96885282a347c94e927377c",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f6e278a5177d8b85945a2cdb6b776dacee34914/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f6e278a5177d8b85945a2cdb6b776dacee34914/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=5f6e278a5177d8b85945a2cdb6b776dacee34914",
            "patch": "@@ -15,7 +15,6 @@\n import ast\n import collections\n import contextlib\n-import copy\n import doctest\n import functools\n import gc\n@@ -1598,58 +1597,6 @@ def assert_screenout(out, what):\n     assert match_str != -1, f\"expecting to find {what} in output: f{out_pr}\"\n \n \n-def set_model_tester_for_less_flaky_test(test_case):\n-    # NOTE: this function edits the config object, which may lead to hard-to-debug side-effects. Use with caution.\n-    # Do not use in tests/models where objects behave very differently based on the config's hidden layer settings\n-    # (e.g. KV caches, sliding window attention, ...)\n-\n-    # TODO (if possible): Avoid exceptional cases\n-    exceptional_classes = [\n-        \"ZambaModelTester\",\n-        \"Zamba2ModelTester\",\n-        \"RwkvModelTester\",\n-        \"AriaVisionText2TextModelTester\",\n-        \"GPTNeoModelTester\",\n-        \"DPTModelTester\",\n-        \"Qwen3NextModelTester\",\n-    ]\n-    if test_case.model_tester.__class__.__name__ in exceptional_classes:\n-        return\n-\n-    target_num_hidden_layers = 1\n-    if hasattr(test_case.model_tester, \"out_features\") or hasattr(test_case.model_tester, \"out_indices\"):\n-        target_num_hidden_layers = None\n-\n-    if hasattr(test_case.model_tester, \"num_hidden_layers\") and target_num_hidden_layers is not None:\n-        test_case.model_tester.num_hidden_layers = target_num_hidden_layers\n-    if (\n-        hasattr(test_case.model_tester, \"vision_config\")\n-        and \"num_hidden_layers\" in test_case.model_tester.vision_config\n-        and target_num_hidden_layers is not None\n-    ):\n-        test_case.model_tester.vision_config = copy.deepcopy(test_case.model_tester.vision_config)\n-        if isinstance(test_case.model_tester.vision_config, dict):\n-            test_case.model_tester.vision_config[\"num_hidden_layers\"] = 1\n-        else:\n-            test_case.model_tester.vision_config.num_hidden_layers = 1\n-    if (\n-        hasattr(test_case.model_tester, \"text_config\")\n-        and \"num_hidden_layers\" in test_case.model_tester.text_config\n-        and target_num_hidden_layers is not None\n-    ):\n-        test_case.model_tester.text_config = copy.deepcopy(test_case.model_tester.text_config)\n-        if isinstance(test_case.model_tester.text_config, dict):\n-            test_case.model_tester.text_config[\"num_hidden_layers\"] = 1\n-        else:\n-            test_case.model_tester.text_config.num_hidden_layers = 1\n-\n-    # A few model class specific handling\n-\n-    # For Albert\n-    if hasattr(test_case.model_tester, \"num_hidden_groups\"):\n-        test_case.model_tester.num_hidden_groups = test_case.model_tester.num_hidden_layers\n-\n-\n def set_config_for_less_flaky_test(config):\n     target_attrs = [\n         \"rms_norm_eps\","
        },
        {
            "sha": "be428c3b4ffaf83b8f901acc879cf4f219408203",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f6e278a5177d8b85945a2cdb6b776dacee34914/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f6e278a5177d8b85945a2cdb6b776dacee34914/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=5f6e278a5177d8b85945a2cdb6b776dacee34914",
            "patch": "@@ -23,7 +23,6 @@\n     require_vision,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n-    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -360,8 +359,6 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                         msg += str(e)\n                         raise AssertionError(msg)\n \n-        set_model_tester_for_less_flaky_test(self)\n-\n         config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n         set_config_for_less_flaky_test(config)\n "
        }
    ],
    "stats": {
        "total": 56,
        "additions": 0,
        "deletions": 56
    }
}