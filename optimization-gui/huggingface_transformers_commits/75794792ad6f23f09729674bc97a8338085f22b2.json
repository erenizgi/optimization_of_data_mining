{
    "author": "zucchini-nlp",
    "message": "BLIPs clean-up  (#35560)\n\n* blips clean up\n\n* update processor\n\n* readability\n\n* fix processor length\n\n* fix copies\n\n* tmp\n\n* update and fix copies\n\n* why keep these, delete?\n\n* fix test fetcher\n\n* irrelevant comment\n\n* fix tests\n\n* fix tests\n\n* fix copies",
    "sha": "75794792ad6f23f09729674bc97a8338085f22b2",
    "files": [
        {
            "sha": "66c7c95e6b8fcf4702479a2265e6c3e263694704",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 81,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -1539,16 +1539,25 @@ def forward(\n \n         # step 3: use the language model, conditioned on the query outputs and the prompt\n         language_model_inputs = self.language_projection(query_output)\n-        language_model_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n+\n         inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-        inputs_embeds = torch.cat([language_model_inputs, inputs_embeds], dim=1)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n-        expected_device = language_model_attention_mask.device\n-        attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n+\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n+            special_image_mask, language_model_inputs\n+        )\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -2026,44 +2035,26 @@ def forward(\n         )\n         vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n         query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n-        language_model_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = (\n-                special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n-            )\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n-                special_image_mask, language_model_inputs\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n         else:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_model_attention_mask, attention_mask.to(language_model_attention_mask.device)], dim=1\n-            )\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n+            special_image_mask, language_model_inputs\n+        )\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -2172,69 +2163,36 @@ def generate(\n             query_output = query_output.to(image_embeds.dtype)\n \n         language_model_inputs = self.language_projection(query_output)\n-        language_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             if input_ids is None:\n-                start_tokens = [self.config.text_config.bos_token_id]\n-                if getattr(self.config, \"image_token_id\", None) is not None:\n-                    start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n+                image_tokens = [self.config.image_token_index] * self.config.num_query_tokens\n+                start_tokens = image_tokens + [self.config.text_config.bos_token_id]\n                 input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n                 input_ids = input_ids.repeat(batch_size, 1)\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = (\n-                special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n-                special_image_mask, language_model_inputs\n-            )\n-\n-            attention_mask = attention_mask.to(language_attention_mask.device)\n+            special_image_mask = special_image_mask.all(-1)\n         else:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_attention_mask, attention_mask.to(language_attention_mask.device)], dim=1\n-            )\n+            special_image_mask = input_ids == self.config.image_token_id\n \n-            # add image_embeds length to max_length, so that the final max_length in counted only on token embeds\n-            # -1 is to account for the prepended BOS after `generate.`\n-            # TODO (joao, raushan): refactor `generate` to avoid these operations with VLMs\n-            if not self.language_model.config.is_encoder_decoder:\n-                generate_kwargs[\"max_length\"] = (\n-                    generate_kwargs.get(\"max_length\", 20) + language_model_inputs.shape[1] - 1\n-                )\n-                generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n-            if input_ids is not None:\n-                input_ids = input_ids.to(language_model_inputs.device)\n             inputs[\"input_ids\"] = input_ids\n \n         outputs = self.language_model.generate(**inputs, **generate_kwargs)\n+\n         return outputs\n \n \n@@ -2362,8 +2320,13 @@ def forward(\n \n         if use_image_text_matching_head:\n             query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-            query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=query_tokens.device)\n-            attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n+            if self.config.image_token_index is not None:\n+                input_ids = input_ids[:, self.config.num_query_tokens :]\n+            else:\n+                query_attention_mask = torch.ones(\n+                    query_tokens.size()[:-1], dtype=torch.long, device=query_tokens.device\n+                )\n+                attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n \n             query_embeds = self.embeddings(\n                 input_ids=input_ids,\n@@ -2395,6 +2358,10 @@ def forward(\n             image_embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n             image_embeds = image_embeds.to(dtype=self.vision_projection.weight.dtype)\n \n+            if self.config.image_token_index is not None:\n+                input_ids = input_ids[:, self.config.num_query_tokens :]\n+                attention_mask = attention_mask[:, self.config.num_query_tokens :]\n+\n             query_embeds = self.embeddings(\n                 input_ids=input_ids,\n             )"
        },
        {
            "sha": "71a1f5ef4db8f76487c6587f809fe706d0c7d34c",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 32,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -112,52 +112,42 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+\n         # BC for explicit return_tensors\n-        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n-            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n-        else:\n-            return_tensors = None\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        max_length = output_kwargs[\"text_kwargs\"].pop(\"max_length\", None)\n+        if max_length is not None:\n+            output_kwargs[\"text_kwargs\"][\"max_length\"] = max_length - self.num_query_tokens\n+\n         encoding = BatchFeature(tensor_type=return_tensors)\n         if text is not None:\n             if isinstance(text, str):\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-            text_encoding = {}\n+            # We need this hacky manipulation because BLIP expects image tokens to be at the beginning even before BOS token\n+            text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n-            return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n-            output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors\n-\n-            # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n-            # because BLIP expects image tokens to be at the beginning even before BOS token\n-            if self.num_query_tokens is not None:\n+            if images is not None and self.num_query_tokens is not None:\n+                # Image tokens should not be padded/truncated or prepended with special BOS token\n                 image_tokens = self.image_token.content * self.num_query_tokens\n-                image_token_encoding = self.tokenizer(\n-                    [image_tokens] * len(text), add_special_tokens=False, return_tensors=None\n-                )\n-                for k in _text_encoding:\n-                    text_encoding[k] = [\n-                        img_encoding + txt_encoding\n-                        for img_encoding, txt_encoding in zip(image_token_encoding[k], _text_encoding[k])\n-                    ]\n-            else:\n-                text_encoding = _text_encoding\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n-                    \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-                )\n-\n-            # cast to desired return tensors type\n-            encoding.update(BatchEncoding(text_encoding, tensor_type=return_tensors))\n-        # add pixel_values encoding. If we also have text_encoding, update image encoding and return it.\n+                output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = False\n+                output_kwargs[\"text_kwargs\"][\"padding\"] = False\n+                output_kwargs[\"text_kwargs\"][\"truncation\"] = False\n+                image_text_encoding = self.tokenizer(image_tokens, **output_kwargs[\"text_kwargs\"])\n+                for k in text_encoding:\n+                    text_encoding[k] = [image_text_encoding[k] + sample for sample in text_encoding[k]]\n+            encoding.update(text_encoding)\n+\n+        # Now add pixel_values encoding. If we also have text_encoding, update image encoding and return it.\n         # else, return the text encoding.\n-\n         if images is not None:\n             image_encoding = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             encoding.update(image_encoding)\n+\n+        # Cast to desired return tensors type\n+        encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer"
        },
        {
            "sha": "72482558d7dd0e4eee21a9c1aa9c09cf7281a77e",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 23,
            "deletions": 64,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -799,7 +799,7 @@ def forward(\n                     self.chunk_size_feed_forward,\n                     self.seq_len_dim,\n                     attention_output[:, query_length:, :],\n-                )\n+                ).to(layer_output.device)\n                 layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n         else:\n             layer_output = apply_chunking_to_forward(\n@@ -1560,40 +1560,24 @@ def forward(\n         )\n         vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n         query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n-        language_model_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_model_attention_mask, attention_mask.to(language_model_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -1682,54 +1666,29 @@ def generate(\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n         )\n-        language_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             if input_ids is None:\n-                start_tokens = [self.config.text_config.bos_token_id]\n-                if getattr(self.config, \"image_token_id\", None) is not None:\n-                    start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n-                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                image_tokens = [self.config.image_token_index] * self.config.num_query_tokens\n+                start_tokens = image_tokens + [self.config.text_config.bos_token_id]\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n                 input_ids = input_ids.repeat(batch_size, 1)\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_attention_mask, attention_mask.to(language_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n \n-            # add image_embeds length to max_length, so that the final max_length in counted only on token embeds\n-            # -1 is to account for the prepended BOS after `generate.`\n-            if not self.language_model.config.is_encoder_decoder:\n-                generate_kwargs[\"max_length\"] = (\n-                    generate_kwargs.get(\"max_length\", 20) + language_model_inputs.shape[1] - 1\n-                )\n-                generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:"
        },
        {
            "sha": "30a06da209be856ba5da20ce29cd75e84dcd898f",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 22,
            "deletions": 33,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import AddedToken, BatchEncoding, PreTokenizedInput, TextInput\n+from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import logging\n from ..auto import AutoTokenizer\n \n@@ -78,6 +78,7 @@ def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_toke\n         else:\n             self.image_token = tokenizer.image_token\n         self.num_query_tokens = num_query_tokens\n+\n         super().__init__(image_processor, tokenizer, qformer_tokenizer)\n \n     def __call__(\n@@ -111,52 +112,40 @@ def __call__(\n             **kwargs,\n         )\n \n-        encoding = BatchFeature()\n-\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        encoding = {}\n         if text is not None:\n             if isinstance(text, str):\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-            # we have to concatenate lists - so we keep track of return_tensors here\n-            return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n-            output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors\n-            # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n-            # because BLIP expects image tokens to be at the beginning even before BOS token\n-            if self.num_query_tokens is not None and images is not None:\n-                text_encoding = {}\n-                image_tokens = self.image_token.content * self.num_query_tokens\n-                image_token_encoding = self.tokenizer(\n-                    [image_tokens] * len(text), add_special_tokens=False, return_tensors=None\n-                )\n-                for k in _text_encoding:\n-                    text_encoding[k] = [\n-                        img_encoding + txt_encoding\n-                        for img_encoding, txt_encoding in zip(image_token_encoding[k], _text_encoding[k])\n-                    ]\n-            else:\n-                text_encoding = _text_encoding\n-                if images is not None:\n-                    logger.warning_once(\n-                        \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n-                        \"Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your InstructBLIP model. \"\n-                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\"\n-                    )\n-\n-            # cast to desired return tensors type after concatenating\n-            text_encoding = BatchEncoding(text_encoding, tensor_type=return_tensors)\n-\n-            encoding.update(text_encoding)\n             qformer_text_encoding = self.qformer_tokenizer(text, **output_kwargs[\"text_kwargs\"])\n             encoding[\"qformer_input_ids\"] = qformer_text_encoding.pop(\"input_ids\")\n             encoding[\"qformer_attention_mask\"] = qformer_text_encoding.pop(\"attention_mask\")\n \n+            # We need this hacky manipulation because BLIP expects image tokens to be at the beginning even before BOS token\n+            if output_kwargs[\"text_kwargs\"].get(\"max_length\") is not None:\n+                output_kwargs[\"text_kwargs\"][\"max_length\"] -= self.num_query_tokens\n+            text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+            if images is not None:\n+                # Image tokens should not be padded/truncated or prepended with special BOS token\n+                image_tokens = self.image_token.content * self.num_query_tokens\n+                output_kwargs[\"text_kwargs\"][\"add_special_tokens\"] = False\n+                output_kwargs[\"text_kwargs\"][\"padding\"] = False\n+                output_kwargs[\"text_kwargs\"][\"truncation\"] = False\n+                image_text_encoding = self.tokenizer(image_tokens, **output_kwargs[\"text_kwargs\"])\n+                for k in text_encoding:\n+                    text_encoding[k] = [image_text_encoding[k] + sample for sample in text_encoding[k]]\n+            encoding.update(text_encoding)\n+\n         if images is not None:\n             image_encoding = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             encoding.update(image_encoding)\n \n+        # Cast to desired return tensors type\n+        encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer"
        },
        {
            "sha": "5d3aeec452e43f46ecf3362390e41c01bded9be5",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 23,
            "deletions": 65,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -660,7 +660,7 @@ def forward(\n                     self.chunk_size_feed_forward,\n                     self.seq_len_dim,\n                     attention_output[:, query_length:, :],\n-                )\n+                ).to(layer_output.device)\n                 layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n         else:\n             layer_output = apply_chunking_to_forward(\n@@ -1527,40 +1527,24 @@ def forward(\n         )\n         vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n         query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n-        language_model_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_model_attention_mask, attention_mask.to(language_model_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -1650,54 +1634,28 @@ def generate(\n             return_dict=True,\n         )\n \n-        language_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n-\n         if inputs_embeds is None:\n             if input_ids is None:\n-                start_tokens = [self.config.text_config.bos_token_id]\n-                if getattr(self.config, \"video_token_id\", None) is not None:\n-                    start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                video_tokens = [self.config.video_token_index] * self.config.num_query_tokens * 4\n+                start_tokens = video_tokens + [self.config.text_config.bos_token_id]\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n                 input_ids = input_ids.repeat(batch_size, 1)\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_attention_mask, attention_mask.to(language_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n \n-            # add image_embeds length to max_length, so that the final max_length in counted only on token embeds\n-            # -1 is to account for the prepended BOS after `generate.`\n-            if not self.language_model.config.is_encoder_decoder:\n-                generate_kwargs[\"max_length\"] = (\n-                    generate_kwargs.get(\"max_length\", 20) + language_model_inputs.shape[1] - 1\n-                )\n-                generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:"
        },
        {
            "sha": "5c4404a6c4d36c3407eee7709a51b207a49bf236",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 22,
            "deletions": 64,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -464,40 +464,24 @@ def forward(\n         )\n         vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n         query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n-        language_model_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_model_attention_mask, attention_mask.to(language_model_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -587,54 +571,28 @@ def generate(\n             return_dict=True,\n         )\n \n-        language_attention_mask = torch.ones(\n-            language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n-        )\n-\n         if inputs_embeds is None:\n             if input_ids is None:\n-                start_tokens = [self.config.text_config.bos_token_id]\n-                if getattr(self.config, \"video_token_id\", None) is not None:\n-                    start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                video_tokens = [self.config.video_token_index] * self.config.num_query_tokens * 4\n+                start_tokens = video_tokens + [self.config.text_config.bos_token_id]\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n                 input_ids = input_ids.repeat(batch_size, 1)\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_id\", None) is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n-        else:\n-            logger.warning_once(\n-                \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n-                \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n-            )\n-            inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n-            attention_mask = torch.cat(\n-                [language_attention_mask, attention_mask.to(language_attention_mask.device)], dim=1\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n             )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n \n-            # add image_embeds length to max_length, so that the final max_length in counted only on token embeds\n-            # -1 is to account for the prepended BOS after `generate.`\n-            if not self.language_model.config.is_encoder_decoder:\n-                generate_kwargs[\"max_length\"] = (\n-                    generate_kwargs.get(\"max_length\", 20) + language_model_inputs.shape[1] - 1\n-                )\n-                generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:"
        },
        {
            "sha": "7ce638805b7a83a28f4de6f970598f6c0b39a55f",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 29,
            "deletions": 36,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -23,7 +23,6 @@\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import (\n     AddedToken,\n-    BatchEncoding,\n     PaddingStrategy,\n     PreTokenizedInput,\n     TextInput,\n@@ -99,15 +98,14 @@ def __call__(\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least one of images or text.\")\n \n-        encoding = BatchFeature()\n-\n+        encoding = {}\n         if text is not None:\n             if isinstance(text, str):\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-            _text_encoding = self.tokenizer(\n+            qformer_text_encoding = self.qformer_tokenizer(\n                 text=text,\n                 add_special_tokens=add_special_tokens,\n                 padding=padding,\n@@ -122,38 +120,17 @@ def __call__(\n                 return_token_type_ids=return_token_type_ids,\n                 return_length=return_length,\n                 verbose=verbose,\n-                return_tensors=None,  # required to concatenate below\n+                return_tensors=return_tensors,\n                 **kwargs,\n             )\n+            encoding[\"qformer_input_ids\"] = qformer_text_encoding.pop(\"input_ids\")\n+            encoding[\"qformer_attention_mask\"] = qformer_text_encoding.pop(\"attention_mask\")\n \n-            # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n-            # because BLIP expects image tokens to be at the beginning even before BOS token\n-            if self.num_query_tokens is not None and images is not None:\n-                text_encoding = {}\n-                video_tokens = (\n-                    self.video_token.content * self.num_query_tokens * 4\n-                )  # InstrucBLIP works with 4 frames only\n-                video_token_encoding = self.tokenizer(\n-                    [video_tokens] * len(text), add_special_tokens=False, return_tensors=None\n-                )\n-                for k in _text_encoding:\n-                    text_encoding[k] = [\n-                        img_encoding + txt_encoding\n-                        for img_encoding, txt_encoding in zip(video_token_encoding[k], _text_encoding[k])\n-                    ]\n-            else:\n-                text_encoding = _text_encoding\n-                if images is not None:\n-                    logger.warning_once(\n-                        \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n-                        \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n-                    )\n-\n-            # cast to desired return tensors type after concatenating\n-            text_encoding = BatchEncoding(text_encoding, tensor_type=return_tensors)\n-            encoding.update(text_encoding)\n-            qformer_text_encoding = self.qformer_tokenizer(\n+            # We need this hacky manipulation because BLIP expects image tokens to be at the beginning even before BOS token\n+            # InstrucBLIP works with 4 frames only\n+            if max_length is not None:\n+                max_length -= self.num_query_tokens\n+            text_encoding = self.tokenizer(\n                 text=text,\n                 add_special_tokens=add_special_tokens,\n                 padding=padding,\n@@ -168,16 +145,32 @@ def __call__(\n                 return_token_type_ids=return_token_type_ids,\n                 return_length=return_length,\n                 verbose=verbose,\n-                return_tensors=return_tensors,\n+                return_tensors=None,  # required to concatenate below\n                 **kwargs,\n             )\n-            encoding[\"qformer_input_ids\"] = qformer_text_encoding.pop(\"input_ids\")\n-            encoding[\"qformer_attention_mask\"] = qformer_text_encoding.pop(\"attention_mask\")\n+\n+            if images is not None:\n+                video_tokens = self.video_token.content * self.num_query_tokens * 4\n+                video_text_encoding = self.tokenizer(\n+                    video_tokens,\n+                    add_special_tokens=False,  # required to concatenate below\n+                    return_attention_mask=return_attention_mask,\n+                    return_overflowing_tokens=return_overflowing_tokens,\n+                    return_special_tokens_mask=return_special_tokens_mask,\n+                    return_offsets_mapping=return_offsets_mapping,\n+                    return_token_type_ids=return_token_type_ids,\n+                    return_length=return_length,\n+                    return_tensors=None,\n+                )\n+                for k in text_encoding:\n+                    text_encoding[k] = [video_text_encoding[k] + sample for sample in text_encoding[k]]\n+            encoding.update(text_encoding)\n \n         if images is not None:\n             image_encoding = self.video_processor(images, return_tensors=return_tensors)\n             encoding.update(image_encoding)\n \n+        encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer"
        },
        {
            "sha": "d5606ac705c9c505cab8d66bb6a516719505b1fd",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 35,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -29,6 +29,7 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n+    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_sdpa,\n     require_vision,\n@@ -777,7 +778,14 @@ def get_config(self):\n # this model tester uses an encoder-decoder language model (T5)\n class Blip2ModelTester:\n     def __init__(\n-        self, parent, vision_kwargs=None, qformer_kwargs=None, text_kwargs=None, is_training=True, num_query_tokens=10\n+        self,\n+        parent,\n+        vision_kwargs=None,\n+        qformer_kwargs=None,\n+        text_kwargs=None,\n+        is_training=True,\n+        num_query_tokens=10,\n+        image_token_index=4,\n     ):\n         if vision_kwargs is None:\n             vision_kwargs = {}\n@@ -792,11 +800,10 @@ def __init__(\n         self.text_model_tester = Blip2TextModelTester(parent, **text_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n         self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n-        self.encoder_seq_length = (\n-            self.text_model_tester.encoder_seq_length + num_query_tokens\n-        )  # need enc seq_length for gen tests\n+        self.encoder_seq_length = self.text_model_tester.encoder_seq_length\n         self.is_training = is_training\n         self.num_query_tokens = num_query_tokens\n+        self.image_token_index = image_token_index\n \n     def prepare_config_and_inputs(self):\n         _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n@@ -819,6 +826,7 @@ def get_config(self):\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n             num_query_tokens=self.num_query_tokens,\n+            image_token_index=self.image_token_index,\n         )\n \n     def create_and_check_for_conditional_generation(\n@@ -1872,37 +1880,7 @@ def test_inference_t5_multi_accelerator(self):\n         self.assertEqual(predictions[0].tolist(), expected_ids_and_text[0])\n         self.assertEqual(generated_text, expected_ids_and_text[1])\n \n-    def test_expansion_in_processing(self):\n-        processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n-        model = Blip2ForConditionalGeneration.from_pretrained(\n-            \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n-        ).to(torch_device)\n-\n-        image = prepare_img()\n-        prompt = \"Question: which city is this? Answer:\"\n-\n-        # Make sure we will go the legacy path by setting these args to None\n-        processor.num_query_tokens = None\n-        model.config.image_token_index = None\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-\n-        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n-\n-        # Add args to the config to trigger new logic when inputs are expanded in processing file\n-        processor.num_query_tokens = model.config.num_query_tokens\n-        processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n-        model.config.image_token_index = len(processor.tokenizer) - 1\n-        model.resize_token_embeddings(processor.tokenizer.vocab_size, pad_to_multiple_of=64)\n-\n-        # Generate again with new inputs\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text_expanded = processor.batch_decode(predictions_expanded, skip_special_tokens=True)[0].strip()\n-\n-        self.assertTrue(generated_text_expanded == generated_text)\n-\n-    @require_torch_accelerator\n+    @require_torch_gpu\n     def test_inference_itm(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n         processor = Blip2Processor.from_pretrained(model_name)"
        },
        {
            "sha": "d2b63f8e408e595e5b866aea5918541e140000f6",
            "filename": "tests/models/blip_2/test_processor_blip_2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -48,6 +48,9 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n+    def prepare_processor_dict(self):\n+        return {\"num_query_tokens\": 1}\n+\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n@@ -84,26 +87,12 @@ def test_image_processor(self):\n         for key in input_feat_extract.keys():\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n-    def test_tokenizer(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-\n-        encoded_processor = processor(text=input_str)\n-\n-        encoded_tok = tokenizer(input_str, return_token_type_ids=False)\n-\n-        for key in encoded_tok.keys():\n-            self.assertListEqual(encoded_tok[key], encoded_processor[key][0])\n-\n     def test_processor(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n \n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n@@ -119,8 +108,9 @@ def test_processor(self):\n     def test_tokenizer_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n \n         predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n \n@@ -132,8 +122,9 @@ def test_tokenizer_decode(self):\n     def test_model_input_names(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n-        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n \n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()"
        },
        {
            "sha": "fe309e700c8216bb171193c17817e4bf9ceefc75",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -809,34 +809,3 @@ def test_inference_interpolate_pos_encoding(self):\n             predictions[0].tolist(), [0, 37, 1023, 753, 3, 9, 2335, 3823, 30, 8, 2608, 28, 3, 9, 1782, 5, 1]\n         )\n         self.assertEqual(generated_text, \"The image features a woman sitting on the beach with a dog.\")\n-\n-    def test_expansion_in_processing(self):\n-        processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n-        model = InstructBlipForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-flan-t5-xl\",\n-            torch_dtype=torch.bfloat16,\n-        ).to(torch_device)\n-\n-        image = prepare_img()\n-        prompt = \"What's in the image?\"\n-\n-        # Make sure we will go the legacy path by setting these args to None\n-        processor.num_query_tokens = None\n-        model.config.image_token_index = None\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-\n-        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n-\n-        # Add args to the config to trigger new logic when inputs are expanded in processing file\n-        processor.num_query_tokens = model.config.num_query_tokens\n-        processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n-        model.config.image_token_index = len(processor.tokenizer) - 2\n-        model.resize_token_embeddings(processor.tokenizer.vocab_size, pad_to_multiple_of=64)\n-\n-        # Generate again with new inputs\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text_expanded = processor.batch_decode(predictions_expanded, skip_special_tokens=True)[0].strip()\n-\n-        self.assertTrue(generated_text_expanded == generated_text)"
        },
        {
            "sha": "984826aaa29655e534535e69f3c695cbc8bb4d03",
            "filename": "tests/models/instructblip/test_processor_instructblip.py",
            "status": "modified",
            "additions": 25,
            "deletions": 28,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -59,6 +59,9 @@ def get_image_processor(self, **kwargs):\n     def get_qformer_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).qformer_tokenizer\n \n+    def prepare_processor_dict(self):\n+        return {\"num_query_tokens\": 1}\n+\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n@@ -90,9 +93,13 @@ def test_image_processor(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         image_input = self.prepare_image_inputs()\n@@ -103,35 +110,17 @@ def test_image_processor(self):\n         for key in input_feat_extract.keys():\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n-    def test_tokenizer(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        qformer_tokenizer = self.get_qformer_tokenizer()\n-\n-        processor = InstructBlipProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n-        )\n-\n-        input_str = [\"lower newer\"]\n-\n-        encoded_processor = processor(text=input_str)\n-\n-        encoded_tokens = tokenizer(input_str, return_token_type_ids=False)\n-        encoded_tokens_qformer = qformer_tokenizer(input_str, return_token_type_ids=False)\n-\n-        for key in encoded_tokens.keys():\n-            self.assertListEqual(encoded_tokens[key], encoded_processor[key])\n-\n-        for key in encoded_tokens_qformer.keys():\n-            self.assertListEqual(encoded_tokens_qformer[key], encoded_processor[\"qformer_\" + key])\n-\n     def test_processor(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         input_str = \"lower newer\"\n@@ -141,7 +130,7 @@ def test_processor(self):\n \n         self.assertListEqual(\n             list(inputs.keys()),\n-            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n         )\n \n         # test if it raises when no input is passed\n@@ -152,9 +141,13 @@ def test_tokenizer_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n@@ -168,9 +161,13 @@ def test_model_input_names(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         input_str = \"lower newer\"\n@@ -180,5 +177,5 @@ def test_model_input_names(self):\n \n         self.assertListEqual(\n             list(inputs.keys()),\n-            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n         )"
        },
        {
            "sha": "5f056d446467d2b7ebf187f0ee73390c94a1e4e7",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -750,34 +750,3 @@ def test_inference_vicuna_7b(self):\n             generated_text,\n             \"Explain what is happening in this short video. a baby girl wearing glasses is reading a book on the bed 1080p\",\n         )\n-\n-    def test_expansion_in_processing(self):\n-        processor = InstructBlipVideoProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n-        model = InstructBlipVideoForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-vicuna-7b\",\n-            load_in_8bit=True,\n-        )\n-\n-        clip = prepare_video()\n-        prompt = \"Explain what is happening in this short video.\"\n-\n-        # Make sure we will go the legacy path by setting these args to None\n-        processor.num_query_tokens = None\n-        model.config.video_token_index = None\n-        inputs = processor(images=clip, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-\n-        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n-\n-        # Add args to the config to trigger new logic when inputs are expanded in processing file\n-        processor.num_query_tokens = model.config.num_query_tokens\n-        processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<video>\"]})\n-        model.config.video_token_index = len(processor.tokenizer) - 1\n-        model.resize_token_embeddings(len(processor.tokenizer), pad_to_multiple_of=64)\n-\n-        # Generate again with new inputs\n-        inputs = processor(images=clip, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n-        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n-        generated_text_expanded = processor.batch_decode(predictions_expanded, skip_special_tokens=True)[0].strip()\n-\n-        self.assertTrue(generated_text_expanded == generated_text)"
        },
        {
            "sha": "3d3633365a88ca638f1012ec751a3b600c914527",
            "filename": "tests/models/instructblipvideo/test_processor_instructblipvideo.py",
            "status": "modified",
            "additions": 30,
            "deletions": 9,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -59,6 +59,9 @@ def get_tokenizer(self, **kwargs):\n     def get_qformer_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).qformer_tokenizer\n \n+    def prepare_processor_dict(self):\n+        return {\"num_query_tokens\": 1}\n+\n     def get_video_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n \n@@ -93,9 +96,13 @@ def test_video_processor(self):\n         video_processor = self.get_video_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer, video_processor=video_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            video_processor=video_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         image_input = self.prepare_image_inputs()\n@@ -110,15 +117,17 @@ def test_tokenizer(self):\n         video_processor = self.get_video_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer, video_processor=video_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            video_processor=video_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         input_str = [\"lower newer\"]\n-\n         encoded_processor = processor(text=input_str)\n-\n         encoded_tokens = tokenizer(input_str, return_token_type_ids=False)\n         encoded_tokens_qformer = qformer_tokenizer(input_str, return_token_type_ids=False)\n \n@@ -132,9 +141,13 @@ def test_processor(self):\n         video_processor = self.get_video_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer, video_processor=video_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            video_processor=video_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         input_str = \"lower newer\"\n@@ -144,7 +157,7 @@ def test_processor(self):\n \n         self.assertListEqual(\n             list(inputs.keys()),\n-            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n         )\n \n         # test if it raises when no input is passed\n@@ -155,9 +168,13 @@ def test_tokenizer_decode(self):\n         video_processor = self.get_video_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer, video_processor=video_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            video_processor=video_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n@@ -171,9 +188,13 @@ def test_model_input_names(self):\n         video_processor = self.get_video_processor()\n         tokenizer = self.get_tokenizer()\n         qformer_tokenizer = self.get_qformer_tokenizer()\n+        processor_kwargs = self.prepare_processor_dict()\n \n         processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer, video_processor=video_processor, qformer_tokenizer=qformer_tokenizer\n+            tokenizer=tokenizer,\n+            video_processor=video_processor,\n+            qformer_tokenizer=qformer_tokenizer,\n+            **processor_kwargs,\n         )\n \n         input_str = \"lower newer\"\n@@ -183,5 +204,5 @@ def test_model_input_names(self):\n \n         self.assertListEqual(\n             list(inputs.keys()),\n-            [\"input_ids\", \"attention_mask\", \"qformer_input_ids\", \"qformer_attention_mask\", \"pixel_values\"],\n+            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n         )"
        },
        {
            "sha": "6c36448e5aba7e964fcffecbeba7464bd801c930",
            "filename": "tests/utils/tiny_model_summary.json",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Futils%2Ftiny_model_summary.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/tests%2Futils%2Ftiny_model_summary.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftiny_model_summary.json?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -626,7 +626,7 @@\n         \"model_classes\": [\n             \"Blip2ForConditionalGeneration\"\n         ],\n-        \"sha\": \"35e1ef43da3554af62eb29a7b3dbbef3f3bef48e\"\n+        \"sha\": \"d0de11fd1f8ca481231c07ee0934924be96cb281\"\n     },\n     \"Blip2Model\": {\n         \"tokenizer_classes\": ["
        },
        {
            "sha": "cafc7409bd51568ff7066d7fd086ab23cb250794",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 24,
            "deletions": 54,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/75794792ad6f23f09729674bc97a8338085f22b2/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75794792ad6f23f09729674bc97a8338085f22b2/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=75794792ad6f23f09729674bc97a8338085f22b2",
            "patch": "@@ -52,11 +52,9 @@\n import argparse\n import collections\n import glob\n-import importlib.util\n import json\n import os\n import re\n-import tempfile\n from contextlib import contextmanager\n from pathlib import Path\n from typing import Optional, Union\n@@ -323,58 +321,30 @@ def get_impacted_files_from_tiny_model_summary(diff_with_last_commit: bool = Fal\n             if key in new_keys:\n                 impacted_model_classes.extend(new_content[key][\"model_classes\"])\n \n-        # get the module where the model classes are defined. We want to use the main `__init__` file, but it requires\n-        # all the framework being installed, which is not ideal for a simple script like test fetcher.\n-        # So we create a temporary and modified main `__init__` and access its `_import_structure`.\n-        with open(folder / \"src/transformers/__init__.py\") as fp:\n-            lines = fp.readlines()\n-            new_lines = []\n-            # Get all the code related to `_import_structure`\n-            for line in lines:\n-                if line == \"_import_structure = {\\n\":\n-                    new_lines.append(line)\n-                elif line == \"# Direct imports for type-checking\\n\":\n-                    break\n-                elif len(new_lines) > 0:\n-                    # bypass the framework check so we can get all the information even if frameworks are not available\n-                    line = re.sub(r\"is_.+_available\\(\\)\", \"True\", line)\n-                    line = line.replace(\"OptionalDependencyNotAvailable\", \"Exception\")\n-                    line = line.replace(\"Exception()\", \"Exception\")\n-                    new_lines.append(line)\n-\n-        # create and load the temporary module\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            with open(os.path.join(tmpdirname, \"temp_init.py\"), \"w\") as fp:\n-                fp.write(\"\".join(new_lines))\n-\n-            spec = importlib.util.spec_from_file_location(\"temp_init\", os.path.join(tmpdirname, \"temp_init.py\"))\n-            module = importlib.util.module_from_spec(spec)\n-            spec.loader.exec_module(module)\n-            # Finally, get `_import_structure` that we need\n-            import_structure = module._import_structure\n-\n-            # map model classes to their defined module\n-            reversed_structure = {}\n-            for key, values in import_structure.items():\n-                for value in values:\n-                    reversed_structure[value] = key\n-\n-            # Get the corresponding modeling file path\n-            for model_class in impacted_model_classes:\n-                module = reversed_structure[model_class]\n-                framework = \"\"\n-                if model_class.startswith(\"TF\"):\n-                    framework = \"tf\"\n-                elif model_class.startswith(\"Flax\"):\n-                    framework = \"flax\"\n-                fn = (\n-                    f\"modeling_{module.split('.')[-1]}.py\"\n-                    if framework == \"\"\n-                    else f\"modeling_{framework}_{module.split('.')[-1]}.py\"\n-                )\n-                files.add(\n-                    f\"src.transformers.{module}.{fn}\".replace(\".\", os.path.sep).replace(f\"{os.path.sep}py\", \".py\")\n-                )\n+        # Add imports via `define_import_structure` after the #35167 as we remove explicit import in `__init__.py`\n+        from transformers.utils.import_utils import define_import_structure\n+\n+        reversed_structure = {}\n+        new_imported_modules_from_import_structure = define_import_structure(\"src/transformers/__init__.py\")\n+        for mapping in new_imported_modules_from_import_structure.values():\n+            for _module, _imports in mapping.items():\n+                for _import in _imports:\n+                    reversed_structure[_import] = _module\n+\n+        # Get the corresponding modeling file path\n+        for model_class in impacted_model_classes:\n+            module = reversed_structure[model_class]\n+            framework = \"\"\n+            if model_class.startswith(\"TF\"):\n+                framework = \"tf\"\n+            elif model_class.startswith(\"Flax\"):\n+                framework = \"flax\"\n+            fn = (\n+                f\"modeling_{module.split('.')[-1]}.py\"\n+                if framework == \"\"\n+                else f\"modeling_{framework}_{module.split('.')[-1]}.py\"\n+            )\n+            files.add(f\"src.transformers.{module}.{fn}\".replace(\".\", os.path.sep).replace(f\"{os.path.sep}py\", \".py\"))\n \n     return sorted(files)\n "
        }
    ],
    "stats": {
        "total": 873,
        "additions": 291,
        "deletions": 582
    }
}