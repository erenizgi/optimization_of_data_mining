{
    "author": "notkisk",
    "message": "Fix ESM token_dropout crash when using inputs_embeds instead of input_ids (#40181)\n\n* fix: Error after calling ESM model with input embeddings not input ids\n\n* propagate changes to other models",
    "sha": "e4bd2c858daa5815aa271fde3b63c4a85923599d",
    "files": [
        {
            "sha": "eeb1e3b6030f40f3b2c557522f21f35b9fbf248e",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4bd2c858daa5815aa271fde3b63c4a85923599d/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4bd2c858daa5815aa271fde3b63c4a85923599d/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=e4bd2c858daa5815aa271fde3b63c4a85923599d",
            "patch": "@@ -216,7 +216,7 @@ def forward(\n         # a factor of (fraction of unmasked tokens during training) / (fraction of unmasked tokens in sample).\n         # This is analogous to the way that dropout layers scale down outputs during evaluation when not\n         # actually dropping out values (or, equivalently, scale up their un-dropped outputs in training).\n-        if self.token_dropout:\n+        if self.token_dropout and input_ids is not None:\n             embeddings = embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n             mask_ratio_train = 0.15 * 0.8  # Hardcoded as the ratio used in all ESM model training runs\n             src_lengths = attention_mask.sum(-1)"
        },
        {
            "sha": "05c57a035427b3989ee45f1d0ee47039bab32334",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4bd2c858daa5815aa271fde3b63c4a85923599d/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4bd2c858daa5815aa271fde3b63c4a85923599d/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=e4bd2c858daa5815aa271fde3b63c4a85923599d",
            "patch": "@@ -138,7 +138,7 @@ def forward(\n         # a factor of (fraction of unmasked tokens during training) / (fraction of unmasked tokens in sample).\n         # This is analogous to the way that dropout layers scale down outputs during evaluation when not\n         # actually dropping out values (or, equivalently, scale up their un-dropped outputs in training).\n-        if self.token_dropout:\n+        if self.token_dropout and input_ids is not None:\n             embeddings = embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n             mask_ratio_train = 0.15 * 0.8  # Hardcoded as the ratio used in all EVOLLA_SA_PROT model training runs\n             src_lengths = attention_mask.sum(-1)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}