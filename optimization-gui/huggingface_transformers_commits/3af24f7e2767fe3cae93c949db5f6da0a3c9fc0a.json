{
    "author": "flashJd",
    "message": "Refine parameter type annotations (#37666)",
    "sha": "3af24f7e2767fe3cae93c949db5f6da0a3c9fc0a",
    "files": [
        {
            "sha": "1ab60a964f20f0bf1a111c04e14c37b223f02641",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3af24f7e2767fe3cae93c949db5f6da0a3c9fc0a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3af24f7e2767fe3cae93c949db5f6da0a3c9fc0a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=3af24f7e2767fe3cae93c949db5f6da0a3c9fc0a",
            "patch": "@@ -902,7 +902,9 @@ def sanitize_special_tokens(self) -> int:\n         return self.add_tokens(self.all_special_tokens_extended, special_tokens=True)\n \n     def add_special_tokens(\n-        self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True\n+        self,\n+        special_tokens_dict: Dict[str, Union[str, AddedToken, Sequence[Union[str, AddedToken]]]],\n+        replace_additional_special_tokens=True,\n     ) -> int:\n         \"\"\"\n         Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n@@ -926,7 +928,7 @@ def add_special_tokens(\n         `'</s>'`).\n \n         Args:\n-            special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n+            special_tokens_dict (dictionary *str* to *str*, `tokenizers.AddedToken`, or `Sequence[Union[str, AddedToken]]`):\n                 Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n                 `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n \n@@ -1004,7 +1006,7 @@ def add_special_tokens(\n         return added_tokens\n \n     def add_tokens(\n-        self, new_tokens: Union[str, AddedToken, List[Union[str, AddedToken]]], special_tokens: bool = False\n+        self, new_tokens: Union[str, AddedToken, Sequence[Union[str, AddedToken]]], special_tokens: bool = False\n     ) -> int:\n         \"\"\"\n         Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n@@ -1018,7 +1020,7 @@ def add_tokens(\n         In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n \n         Args:\n-            new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n+            new_tokens (`str`, `tokenizers.AddedToken` or a sequence of *str* or `tokenizers.AddedToken`):\n                 Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n                 token to let you personalize its behavior: whether this token should only match against a single word,\n                 whether this token should strip all potential whitespaces on the left side, whether this token should"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}