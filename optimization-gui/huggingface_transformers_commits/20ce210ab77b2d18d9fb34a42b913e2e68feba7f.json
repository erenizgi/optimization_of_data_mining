{
    "author": "ArthurZucker",
    "message": "Revert \"remove dtensors, not explicit (#39840)\" (#39912)\n\n* Revert \"remove dtensors, not explicit (#39840)\"\nThis did not work with generation (lm_head needs extra care!)\nThis reverts commit 6dfd561d9cd722dfc09f702355518c6d09b9b4e3.\n\n* update\n\n* style?",
    "sha": "20ce210ab77b2d18d9fb34a42b913e2e68feba7f",
    "files": [
        {
            "sha": "2134dceb84b9dca0d4cce1f97df058b52ca41a8c",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=20ce210ab77b2d18d9fb34a42b913e2e68feba7f",
            "patch": "@@ -130,6 +130,7 @@ def check_output(self, want, got, optionflags):\n \n if is_torch_available():\n     import torch\n+\n     # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n     # We set it to `False` for CI. See https://github.com/pytorch/pytorch/issues/157274#issuecomment-3090791615\n     torch.backends.cudnn.allow_tf32 = False"
        },
        {
            "sha": "353cc1d0817488507a24c8da7d4abb019452a36b",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 56,
            "deletions": 61,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=20ce210ab77b2d18d9fb34a42b913e2e68feba7f",
            "patch": "@@ -150,7 +150,6 @@ def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weig\n     \"F64\": torch.float64,\n     \"I64\": torch.int64,\n     \"F8_E4M3\": torch.float8_e4m3fn,\n-    \"F8_E5M2\": torch.float8_e5m2,\n }\n \n \n@@ -526,43 +525,6 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return param\n \n \n-class ReduceFromModelParallelRegion(torch.autograd.Function):\n-    \"\"\"\n-    All-reduce in forward pass, identity in backward pass.\n-    This is the `g` function in the paper: https://arxiv.org/abs/1909.08053\n-    \"\"\"\n-\n-    @staticmethod\n-    def forward(ctx, x, device_mesh):\n-        if device_mesh.size() == 1:\n-            return x\n-        dist.all_reduce(x, op=dist.ReduceOp.SUM, group=device_mesh.get_group())\n-        return x\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        return grad_output\n-\n-\n-class CopyToModelParallelRegion(torch.autograd.Function):\n-    \"\"\"\n-    Copy in forward pass, all-reduce in backward pass.\n-    This is the `f` function in the paper: https://arxiv.org/abs/1909.08053\n-    \"\"\"\n-\n-    @staticmethod\n-    def forward(ctx, x, device_mesh):\n-        ctx.device_mesh = device_mesh\n-        return x\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        if ctx.device_mesh.size() == 1:\n-            return grad_output\n-        dist.all_reduce(grad_output, op=dist.ReduceOp.SUM, group=ctx.device_mesh.get_group())\n-        return grad_output\n-\n-\n class ColwiseParallel(TensorParallelLayer):\n     \"\"\"\n     General tensor parallel layer for transformers.\n@@ -585,8 +547,15 @@ def __init__(\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        # TODO: figure out dynamo support for instance method and switch this to instance method\n         # annotate module input placements/sharding with input_layouts\n         input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        # transform the input layouts to the desired layouts of ColwiseParallel\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n         return input_tensor\n \n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n@@ -595,19 +564,41 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         # weight would become Shard(1)\n         if param_type == \"bias\":\n             parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n+            shard = [Shard(-1)]\n         else:\n+            shard = [Shard(-2)]\n             parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n \n         parameter = parameter.to(param_casting_dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n-\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(\n+                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+            )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-        outputs = CopyToModelParallelRegion.apply(outputs, device_mesh)\n-        return outputs\n+        # outputs is a shard on last dimension DTensor, i.e. Shard(-1)\n+        if outputs.placements != output_layouts:\n+            outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n+        # back to local tensor\n+        return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n+\n+\n+class PackedColwiseParallel(ColwiseParallel):\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -2)\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n+        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n class RowwiseParallel(TensorParallelLayer):\n@@ -644,15 +635,23 @@ def __init__(\n         self.use_dtensor = use_dtensor\n \n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        if param_type == \"bias\":\n-            parameter = param[:]\n-        else:\n+        # Rowwise shard weight to Shard(1), bias to Replicate(), weight be Shard(1)\n+        # means Rowwise as nn.Linear is input * weight^T + bias, where\n+        # weight would become Shard(0)\n+        if param_type != \"bias\":\n             parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n+            shard = [Shard(-1)]\n+        else:\n+            shard = [Replicate()]\n+            parameter = param[:]\n \n         parameter = parameter.to(param_casting_dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n-\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(\n+                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+            )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n     @staticmethod\n@@ -662,13 +661,24 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n             mod.bias = None\n \n         input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n         return input_tensor\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-        outputs = ReduceFromModelParallelRegion.apply(outputs, device_mesh)\n+        # Rowwise sharding produces partial output, depending on output layouts:\n+        # 1. to replicate -> allreduce\n+        # 2. to shard -> reduce_scatter\n+        if outputs.placements != output_layouts:\n+            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n+        outputs = outputs.to_local()  # otherwise the `+=` op will gather\n         if hasattr(mod, \"_bias\"):\n             outputs += mod._bias\n+        # back to local tensor if use_local_output is True\n         return outputs\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n@@ -694,21 +704,6 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n             )\n \n \n-class PackedColwiseParallel(ColwiseParallel):\n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        # NOTE(3outeille): need to be deprecated as no longer using dtensors\n-        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n-        # means Colwise as Linear is input * weight^T + bias, where\n-        # weight would become Shard(1)\n-        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -2)\n-        parameter = parameter.to(param_casting_dtype)\n-        if to_contiguous:\n-            parameter = parameter.contiguous()\n-        if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n-        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n-\n-\n class PackedRowwiseParallel(RowwiseParallel):\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)"
        },
        {
            "sha": "03e9cf531470fc6128fe5b3421c3baffeb7c056f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=20ce210ab77b2d18d9fb34a42b913e2e68feba7f",
            "patch": "@@ -4087,16 +4087,9 @@ def save_pretrained(\n         for shard_file, tensors in filename_to_tensors:\n             shard = {}\n             for tensor in tensors:\n-                if _is_dtensor_available and getattr(self, \"_device_mesh\", None) is not None:\n-                    plan = _get_parameter_tp_plan(tensor, self._tp_plan)\n-                    full_tensor = state_dict[tensor]\n-                    if isinstance(state_dict[tensor], DTensor):\n-                        full_tensor = full_tensor.full_tensor()\n-                    elif plan is not None:\n-                        shard_dim = -1 if \"rowwise\" in plan else 0\n-                        gather_list = [torch.empty_like(full_tensor) for _ in range(self._device_mesh.size())]\n-                        torch.distributed.all_gather(gather_list, full_tensor)\n-                        full_tensor = torch.cat(gather_list, dim=shard_dim)\n+                if _is_dtensor_available and isinstance(state_dict[tensor], DTensor):\n+                    full_tensor = state_dict[tensor].full_tensor()\n+                    # to get the correctly ordered tensor we need to repack if packed\n                     if _get_parameter_tp_plan(tensor, self._tp_plan) in (\"local_packed_rowwise\",):\n                         full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n                     shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly"
        },
        {
            "sha": "1904fc8bd1e73563f6c8fa804cbfb60213032347",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ce210ab77b2d18d9fb34a42b913e2e68feba7f/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=20ce210ab77b2d18d9fb34a42b913e2e68feba7f",
            "patch": "@@ -101,6 +101,14 @@ def test_model_forward(self):\n             model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", tp_plan=\"auto\")\n             torch.distributed.barrier()\n \n+            has_dtensor = 0\n+            for name, parameter in model.named_parameters():\n+                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+                    has_dtensor = 1\n+                    break\n+\n+            assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n             tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n             prompt = \"Can I help\"\n \n@@ -110,8 +118,7 @@ def test_model_forward(self):\n             next_token_logits = outputs[0][:, -1, :]\n             next_token = torch.argmax(next_token_logits, dim=-1)\n             response = tokenizer.decode(next_token)\n-            print(response)\n-            # assert response == \"with\"\n+            assert response == \"with\"\n \n             torch.distributed.barrier()\n             torch.distributed.destroy_process_group()\n@@ -136,6 +143,14 @@ def test_model_generate(self):\n \n             model.forward = torch.compile(model.forward)\n \n+            has_dtensor = 0\n+            for name, parameter in model.named_parameters():\n+                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+                    has_dtensor = 1\n+                    break\n+\n+            assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n             tokenizer = AutoTokenizer.from_pretrained(model_id)\n             prompt = \"Can I help\"\n "
        }
    ],
    "stats": {
        "total": 150,
        "additions": 77,
        "deletions": 73
    }
}