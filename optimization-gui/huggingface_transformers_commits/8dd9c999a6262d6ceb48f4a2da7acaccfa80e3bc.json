{
    "author": "Cyrilvallez",
    "message": "Use timm-side buffers initialization (#43124)\n\n* make the switch\n\n* pin version\n\n* don't protect import if it is used without protection....\n\n* add batchnorm2d\n\n* fix\n\n* fix\n\n* oupsi\n\n* use public API\n\n* fix typo",
    "sha": "8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
    "files": [
        {
            "sha": "bfb157e40334bb22f448cf9e796213af4f98c104",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -140,7 +140,7 @@\n     \"tensorboard\",\n     \"timeout-decorator\",\n     \"tiktoken\",\n-    \"timm>=1.0.20\",\n+    \"timm>=1.0.23\",\n     \"tokenizers>=0.22.0,<=0.23.0\",\n     \"torch>=2.2\",\n     \"torchaudio\","
        },
        {
            "sha": "569170f4904b122cfbed9c855564146c83be0509",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -75,7 +75,7 @@\n     \"tensorboard\": \"tensorboard\",\n     \"timeout-decorator\": \"timeout-decorator\",\n     \"tiktoken\": \"tiktoken\",\n-    \"timm\": \"timm>=1.0.20\",\n+    \"timm\": \"timm>=1.0.23\",\n     \"tokenizers\": \"tokenizers>=0.22.0,<=0.23.0\",\n     \"torch\": \"torch>=2.2\",\n     \"torchaudio\": \"torchaudio\","
        },
        {
            "sha": "dc51d76f225c5f3a579ea9f99e4c0ae519f1cc47",
            "filename": "src/transformers/integrations/timm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 216,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/4520b549f230db350523c4f63677fedd789510f8/src%2Ftransformers%2Fintegrations%2Ftimm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4520b549f230db350523c4f63677fedd789510f8/src%2Ftransformers%2Fintegrations%2Ftimm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftimm.py?ref=4520b549f230db350523c4f63677fedd789510f8",
            "patch": "@@ -1,216 +0,0 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-This integration has for unique goal to allow re-initialization of the non-persistent buffers of `timm` models.\n-Indeed, as we load models fully on meta by default, we need a way to get back the correct value of non-persistent buffers.\n-We assume that everything else, i.e. parameters and persistent buffers, will correctly reside in model checkpoints, so we\n-won't need to reinit them.\n-Do not rely on it, as we will work to integrate it directly in `timm`, to then remove this file without warning.\n-\"\"\"\n-\n-from math import comb\n-\n-import torch\n-\n-from .. import initialization as init\n-from ..utils import is_timm_available\n-\n-\n-if is_timm_available():\n-    from timm.layers import ndgrid\n-    from timm.layers.blur_pool import BlurPool2d\n-    from timm.layers.lambda_layer import LambdaLayer, rel_pos_indices\n-    from timm.layers.pos_embed_rel import (\n-        RelPosBias,\n-        RelPosBiasTf,\n-        RelPosMlp,\n-        gen_relative_log_coords,\n-        gen_relative_position_index,\n-        generate_lookup_tensor,\n-    )\n-    from timm.layers.pos_embed_sincos import (\n-        FourierEmbed,\n-        RotaryEmbedding,\n-        RotaryEmbeddingCat,\n-        RotaryEmbeddingDinoV3,\n-        RotaryEmbeddingMixed,\n-        freq_bands,\n-        pixel_freq_bands,\n-    )\n-    from timm.models.beit import Attention\n-    from timm.models.beit import gen_relative_position_index as beit_gen_relative_position_index\n-    from timm.models.efficientformer_v2 import Attention2d, Attention2dDownsample\n-    from timm.models.eva import EvaAttention\n-    from timm.models.levit import AttentionDownsample\n-    from timm.models.swin_transformer import SwinTransformerBlock, get_relative_position_index\n-    from timm.models.swin_transformer import WindowAttention as SwinWindowAttention\n-    from timm.models.swin_transformer_v2 import SwinTransformerV2Block\n-    from timm.models.swin_transformer_v2 import WindowAttention as Swin2WindowAttention\n-    from timm.models.swin_transformer_v2_cr import SwinTransformerV2CrBlock, WindowMultiHeadAttention\n-    from timm.models.vision_transformer import ParallelScalingBlock\n-\n-    # This one is very recent and is not necesarily in all versions we support (we require timm>=1.0.20)\n-    try:\n-        from timm.models.csatv2 import _DCT_MEAN, _DCT_VAR, LearnableDct2d\n-    except Exception:\n-        _DCT_MEAN, _DCT_VAR, LearnableDct2d = None, None, type(None)\n-\n-\n-def _maybe_reinit_non_persistent_buffer(module):\n-    \"\"\"Reinit the non-persistent buffers of `module` if it matches any timm Module which has any.\"\"\"\n-    # This is a loooong list of hardcoded combinations from timm, as the modules do not provide a nice way to do\n-    # it natively\n-    if isinstance(module, FourierEmbed):\n-        init.copy_(module.bands, pixel_freq_bands(module.max_res, module.num_bands))\n-    elif isinstance(module, RotaryEmbedding):\n-        if module.bands is not None:\n-            bands = (\n-                pixel_freq_bands(module.dim // 4, float(module.max_res), linear_bands=module.linear_bands)\n-                if module.in_pixels\n-                else freq_bands(module.dim // 4, temperature=module.temperature, step=1)\n-            )\n-            init.copy_(module.bands, bands)\n-        elif module.pos_embed_sin is not None:\n-            emb_sin, emb_cos = module._get_pos_embed_values(module.feat_shape)\n-            init.copy_(module.pos_embed_sin, emb_sin)\n-            init.copy_(module.pos_embed_cos, emb_cos)\n-    elif isinstance(module, RotaryEmbeddingCat):\n-        if module.bands is not None:\n-            bands = (\n-                pixel_freq_bands(module.dim // 4, float(module.max_res), linear_bands=module.linear_bands)\n-                if module.in_pixels\n-                else freq_bands(module.dim // 4, temperature=module.temperature, step=1)\n-            )\n-            init.copy_(module.bands, bands)\n-        elif module.pos_embed is not None:\n-            init.copy_(module.pos_embed, module._get_pos_embed_values(feat_shape=module.feat_shape))\n-    elif isinstance(module, RotaryEmbeddingMixed):\n-        if module.t_x is not None:\n-            t_x, t_y = module._get_grid_values(module.feat_shape)\n-            init.copy_(module.t_x, t_x)\n-            init.copy(module.t_y, t_y)\n-    elif isinstance(module, RotaryEmbeddingDinoV3):\n-        init.copy_(module.periods, module._compute_periods())\n-        if module.pos_embed_cached is not None:\n-            init.copy_(module.pos_embed_cached, module._create_embed(module.feat_shape, no_aug=True))\n-    elif isinstance(module, RelPosBias):\n-        has_class_token = module.relative_position_bias_table.shape[0] > (2 * module.window_size[0] - 1) * (\n-            2 * module.window_size[1] - 1\n-        )\n-        init.copy_(\n-            module.relative_position_index,\n-            gen_relative_position_index(module.window_size, class_token=has_class_token).view(-1),\n-        )\n-    elif isinstance(module, RelPosMlp):\n-        init.copy_(module.relative_position_index, gen_relative_position_index(module.window_size).view(-1))\n-        # This one is supposed to pass args `pretrained_window_size` as well to `gen_relative_log_coords`, but it's\n-        # not recorded as class attributes in `__init__` and we have no way to infer its value back as we do for `mode` here...\n-        # Let's hope it's always default value\n-        mode = \"cr\" if module.bias_gain is None else \"swin\"\n-        init.copy_(module.rel_coords_log, gen_relative_log_coords(module.window_size, mode=mode))\n-    elif isinstance(module, RelPosBiasTf):\n-        init.copy_(module.height_lookup, generate_lookup_tensor(module.window_size[0]))\n-        init.copy_(module.width_lookup, generate_lookup_tensor(module.window_size[1]))\n-    elif isinstance(module, LearnableDct2d):\n-        init.copy_(module.mean, torch.tensor(_DCT_MEAN))\n-        init.copy_(module.var, torch.tensor(_DCT_VAR))\n-        init.copy_(module.imagenet_mean, torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1))\n-        init.copy_(module.imagenet_std, torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1))\n-    elif isinstance(module, LambdaLayer):\n-        if module.rel_pos_indices is not None:\n-            rel_size = module.pos_enb.shape[:2]\n-            feat_size = [(s + 1) // 2 for s in rel_size]\n-            init.copy_(module.rel_pos_indices, rel_pos_indices(feat_size))\n-    elif isinstance(module, AttentionDownsample):\n-        k_pos = torch.stack(\n-            ndgrid(\n-                torch.arange(module.resolution[0], dtype=torch.long),\n-                torch.arange(module.resolution[1], dtype=torch.long),\n-            )\n-        ).flatten(1)\n-        q_pos = torch.stack(\n-            ndgrid(\n-                torch.arange(0, module.resolution[0], step=module.stride, dtype=torch.long),\n-                torch.arange(0, module.resolution[1], step=module.stride, dtype=torch.long),\n-            )\n-        ).flatten(1)\n-        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n-        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n-        init.copy_(module.attention_bias_idxs, rel_pos)\n-    elif isinstance(\n-        module,\n-        EvaAttention,\n-    ):\n-        if module.k_bias is not None:\n-            init.zeros_(module.k_bias)\n-    elif isinstance(module, ParallelScalingBlock):\n-        if module.qkv_bias is not None:\n-            init.zeros_(module.qkv_bias)\n-    elif isinstance(module, Attention):\n-        if module.k_bias is not None:\n-            init.zeros_(module.k_bias)\n-        if module.relative_position_index is not None:\n-            init.copy_(module.relative_position_index, beit_gen_relative_position_index(module.window_size))\n-    elif isinstance(module, SwinTransformerV2CrBlock):\n-        if module.attn_mask is not None:\n-            init.copy_(module.attn_mask, module.get_attn_mask())\n-    elif isinstance(module, WindowMultiHeadAttention):\n-        module._make_pair_wise_relative_positions()\n-    elif isinstance(module, BlurPool2d):\n-        coeffs = torch.tensor(\n-            [comb(module.filt_size - 1, k) for k in range(module.filt_size)], dtype=torch.float32\n-        ) / (2 ** (module.filt_size - 1))\n-        blur_filter = (coeffs[:, None] * coeffs[None, :])[None, None, :, :]\n-        if module.channels is not None:\n-            blur_filter = blur_filter.repeat(module.channels, 1, 1, 1)\n-        init.copy_(module.filt, blur_filter)\n-    elif isinstance(module, Swin2WindowAttention):\n-        module._make_pair_wise_relative_positions()\n-        if module.k_bias is not None:\n-            init.zeros_(module.k_bias)\n-    elif isinstance(module, SwinTransformerV2Block):\n-        if module.attn_mask is not None:\n-            init.copy_(module.attn_mask, module.get_attn_mask())\n-    elif isinstance(module, SwinWindowAttention):\n-        init.copy_(module.relative_position_index, get_relative_position_index(*module.window_size))\n-    elif isinstance(module, SwinTransformerBlock):\n-        if module.attn_mask is not None:\n-            init.copy_(module.attn_mask, module.get_attn_mask())\n-    elif isinstance(module, Attention2d):\n-        pos = torch.stack(\n-            ndgrid(\n-                torch.arange(module.resolution[0], dtype=torch.long),\n-                torch.arange(module.resolution[1], dtype=torch.long),\n-            )\n-        ).flatten(1)\n-        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()\n-        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n-        init.copy_(module.attention_bias_idxs, rel_pos)\n-    elif isinstance(module, Attention2dDownsample):\n-        k_pos = torch.stack(\n-            ndgrid(\n-                torch.arange(module.resolution[0], dtype=torch.long),\n-                torch.arange(module.resolution[1], dtype=torch.long),\n-            )\n-        ).flatten(1)\n-        q_pos = torch.stack(\n-            ndgrid(\n-                torch.arange(0, module.resolution[0], step=2, dtype=torch.long),\n-                torch.arange(0, module.resolution[1], step=2, dtype=torch.long),\n-            )\n-        ).flatten(1)\n-        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n-        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n-        init.copy_(module.attention_bias_idxs, rel_pos)"
        },
        {
            "sha": "bab7d1f8e04ea90e193e0711ecf0c81bfa6dc312",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -16,23 +16,19 @@\n from typing import Optional, Union\n \n import torch\n+from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import is_timm_available, is_torch_available, requires_backends\n+from ...utils import is_timm_available, requires_backends\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_timm_backbone import TimmBackboneConfig\n \n \n if is_timm_available():\n     import timm\n \n-    from ...integrations.timm import _maybe_reinit_non_persistent_buffer\n-\n-\n-if is_torch_available():\n-    from torch import Tensor\n-\n \n class TimmBackbone(PreTrainedModel, BackboneMixin):\n     \"\"\"\n@@ -121,7 +117,12 @@ def unfreeze_batch_norm_2d(self):\n     def _init_weights(self, module):\n         \"\"\"We need to at least re-init the non-persistent buffers if the model was initialized on meta device (we\n         assume weights and persistent buffers will be part of checkpoint as we have no way to control timm inits)\"\"\"\n-        _maybe_reinit_non_persistent_buffer(module)\n+        if hasattr(module, \"init_non_persistent_buffers\"):\n+            module.init_non_persistent_buffers()\n+        elif isinstance(module, nn.BatchNorm2d) and getattr(module, \"running_mean\", None) is not None:\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+            init.zeros_(module.num_batches_tracked)\n \n     def forward(\n         self,"
        },
        {
            "sha": "cf1b80f5758414866d68ab8ec107d49cad685814",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -29,8 +29,6 @@\n if is_timm_available():\n     import timm\n \n-    from ...integrations.timm import _maybe_reinit_non_persistent_buffer\n-\n \n @dataclass\n @auto_docstring(\n@@ -117,7 +115,12 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n         # Also, reinit all non-persistemt buffers if any!\n-        _maybe_reinit_non_persistent_buffer(module)\n+        if hasattr(module, \"init_non_persistent_buffers\"):\n+            module.init_non_persistent_buffers()\n+        elif isinstance(module, nn.BatchNorm2d) and getattr(module, \"running_mean\", None) is not None:\n+            init.zeros_(module.running_mean)\n+            init.ones_(module.running_var)\n+            init.zeros_(module.num_batches_tracked)\n \n     def _timm_model_supports_gradient_checkpointing(self):\n         \"\"\""
        },
        {
            "sha": "2072605b6b7fd9dc8b1ee42bfec81bcb1f2005c6",
            "filename": "tests/models/gemma3n/test_processing_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -14,17 +14,13 @@\n \n import unittest\n \n-from transformers import is_speech_available\n+from transformers.models.gemma3n import Gemma3nProcessor\n from transformers.testing_utils import require_sentencepiece, require_torch, require_torchaudio, require_vision\n \n from ...test_processing_common import ProcessorTesterMixin\n from .test_feature_extraction_gemma3n import floats_list\n \n \n-if is_speech_available():\n-    from transformers.models.gemma3n import Gemma3nProcessor\n-\n-\n # TODO: omni-modal processor can't run tests from `ProcessorTesterMixin`\n @require_torch\n @require_torchaudio"
        },
        {
            "sha": "1f0b729bbf541ef1be291ac5d09ed4ea9768b400",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -135,10 +135,14 @@ def test_feed_forward_chunking(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    @unittest.skip(reason=\"TimmBackbone initialization is managed on the timm side\")\n+    @unittest.skip(reason=\"TimmBackbone uses a pretrained model initialization in __init__, not random weights\")\n     def test_can_init_all_missing_weights(self):\n         pass\n \n+    @unittest.skip(reason=\"TimmBackbone uses a pretrained model initialization in __init__, not random weights\")\n+    def test_init_weights_can_init_buffers(self):\n+        pass\n+\n     @unittest.skip(reason=\"TimmBackbone models doesn't have inputs_embeds\")\n     def test_inputs_embeds(self):\n         pass"
        },
        {
            "sha": "b85a15180d931f8aaf1330ed4794d9bfb6458e2b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8dd9c999a6262d6ceb48f4a2da7acaccfa80e3bc",
            "patch": "@@ -1157,13 +1157,12 @@ def test_can_init_all_missing_weights(self):\n         addition_year = 0  # if we cannot find it, set it to 0 (i.e. oldest)\n         if match_object := re.search(r\"^# Copyright (\\d{4})\", source_code, re.MULTILINE | re.IGNORECASE):\n             addition_year = int(match_object.group(1))\n+        # For now, skip everything older than 2023 and \"important models\" (too many models to patch otherwise)\n+        # TODO: relax this as we patch more and more models\n+        if addition_year < 2023:\n+            self.skipTest(reason=\"Not a prioritized model for now.\")\n \n         for model_class in self.all_model_classes:\n-            # For now, skip everything older than 2023 and \"important models\" (too much models to patch otherwise)\n-            # TODO: relax this as we patch more and more models\n-            if addition_year < 2023:\n-                self.skipTest(reason=f\"{model_class} is not a prioritized model for now.\")\n-\n             # This context manager makes sure that we get the same results deterministically for random new weights\n             with seeded_weight_init():\n                 # First, initialize the model from __init__ -> this ensure everything is correctly initialized, even if\n@@ -1282,14 +1281,6 @@ def test_init_weights_can_init_buffers(self):\n                 buf_name, immediate_parent_class, pretrained_parent_class = find_parent_traceback(\n                     buffer, model_from_init\n                 )\n-\n-                # We cannot control timm model weights initialization, so skip in this case\n-                if (pretrained_parent_class == \"TimmWrapperPreTrainedModel\" and \"timm_model.\" in buffer) or (\n-                    pretrained_parent_class == \"TimmBackbone\" and \"_backbone.\" in buffer\n-                ):\n-                    different_buffers.discard(buffer)\n-                    continue\n-\n                 # Add it to the traceback\n                 traceback = (\n                     f\"`{buf_name}` in module `{immediate_parent_class}` called from `{pretrained_parent_class}`\\n\""
        }
    ],
    "stats": {
        "total": 275,
        "additions": 27,
        "deletions": 248
    }
}