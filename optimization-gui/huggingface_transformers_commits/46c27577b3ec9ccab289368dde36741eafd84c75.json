{
    "author": "ErezSC42",
    "message": "fix to jamba config, asserting attention and expert offset (#33316)\n\n* fix to jamba config, asserting attention and expert offset\r\n\r\n* fix foramtting\r\n\r\n* fix foramtting\r\n\r\n* fix foramtting\r\n\r\n* changed to error raise instead of assertion, added unittests\r\n\r\n* fix\r\n\r\n* changed t_ to property_\r\n\r\n* changed t_ to property_\r\n\r\n* quickfix\r\n\r\n* ran code styler",
    "sha": "46c27577b3ec9ccab289368dde36741eafd84c75",
    "files": [
        {
            "sha": "b493db7ed456b35e574c0c0f3f86a426db8fd654",
            "filename": "src/transformers/models/jamba/configuration_jamba.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/46c27577b3ec9ccab289368dde36741eafd84c75/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46c27577b3ec9ccab289368dde36741eafd84c75/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py?ref=46c27577b3ec9ccab289368dde36741eafd84c75",
            "patch": "@@ -193,6 +193,9 @@ def __init__(\n         self.attn_layer_period = attn_layer_period\n         self.attn_layer_offset = attn_layer_offset\n \n+        self._check_supported_offset(\"attention\", self.attn_layer_period, self.attn_layer_offset)\n+        self._check_supported_offset(\"expert\", self.expert_layer_period, self.expert_layer_offset)\n+\n         self.use_mamba_kernels = use_mamba_kernels\n         self.mamba_d_state = mamba_d_state\n         self.mamba_d_conv = mamba_d_conv\n@@ -222,3 +225,9 @@ def layers_num_experts(self):\n             self.num_experts if i % self.expert_layer_period == self.expert_layer_offset else 1\n             for i in range(self.num_hidden_layers)\n         ]\n+\n+    def _check_supported_offset(self, property_: str, period: int, offset: int):\n+        if offset >= period:\n+            raise ValueError(\n+                f\"{property_} layer offset ({offset}) must be smaller than {property_} layer period ({period})\"\n+            )"
        },
        {
            "sha": "6e1a2cf2cf9c44a5e2310c7786631a808da847f9",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 43,
            "deletions": 1,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/46c27577b3ec9ccab289368dde36741eafd84c75/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46c27577b3ec9ccab289368dde36741eafd84c75/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=46c27577b3ec9ccab289368dde36741eafd84c75",
            "patch": "@@ -50,6 +50,48 @@\n     )\n \n \n+class JambaConfigTester(ConfigTester):\n+    def _create_attn_config(self, attn_layer_offset: int, attn_layer_period: int):\n+        _input_dict = self.inputs_dict.copy()\n+        _input_dict[\"attn_layer_offset\"] = attn_layer_offset\n+        _input_dict[\"attn_layer_period\"] = attn_layer_period\n+        return self.config_class(**_input_dict)\n+\n+    def _create_expert_config(self, expert_layer_offset: int, expert_layer_period: int):\n+        _input_dict = self.inputs_dict.copy()\n+        _input_dict[\"expert_layer_offset\"] = expert_layer_offset\n+        _input_dict[\"expert_layer_period\"] = expert_layer_period\n+        return self.config_class(**_input_dict)\n+\n+    def test_attn_offsets(self):\n+        self._create_attn_config(attn_layer_offset=0, attn_layer_period=4)\n+        self._create_attn_config(attn_layer_offset=1, attn_layer_period=4)\n+        self._create_attn_config(attn_layer_offset=2, attn_layer_period=4)\n+        self._create_attn_config(attn_layer_offset=3, attn_layer_period=4)\n+        with self.parent.assertRaises(ValueError):\n+            self._create_attn_config(attn_layer_offset=4, attn_layer_period=4)\n+        with self.parent.assertRaises(ValueError):\n+            self._create_attn_config(attn_layer_offset=5, attn_layer_period=4)\n+\n+    def test_expert_offsets(self):\n+        self._create_expert_config(expert_layer_offset=0, expert_layer_period=4)\n+        self._create_expert_config(expert_layer_offset=1, expert_layer_period=4)\n+        self._create_expert_config(expert_layer_offset=2, expert_layer_period=4)\n+        self._create_expert_config(expert_layer_offset=3, expert_layer_period=4)\n+        with self.parent.assertRaises(ValueError):\n+            self._create_expert_config(expert_layer_offset=4, expert_layer_period=4)\n+        with self.parent.assertRaises(ValueError):\n+            self._create_expert_config(expert_layer_offset=5, expert_layer_period=4)\n+\n+    def test_jamba_offset_properties(self):\n+        self.test_attn_offsets()\n+        self.test_expert_offsets()\n+\n+    def run_common_tests(self):\n+        self.test_jamba_offset_properties()\n+        return super().run_common_tests()\n+\n+\n class JambaModelTester:\n     def __init__(\n         self,\n@@ -302,7 +344,7 @@ class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n \n     def setUp(self):\n         self.model_tester = JambaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=JambaConfig, hidden_size=37)\n+        self.config_tester = JambaConfigTester(self, config_class=JambaConfig, hidden_size=37)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 52,
        "deletions": 1
    }
}