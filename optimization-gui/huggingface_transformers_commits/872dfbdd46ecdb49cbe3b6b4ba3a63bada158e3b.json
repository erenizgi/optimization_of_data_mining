{
    "author": "NielsRogge",
    "message": "[ViTPose] Convert more checkpoints (#35638)\n\n* Convert more checkpoints\r\n\r\n* Update docs, convert huge variant\r\n\r\n* Update model name\r\n\r\n* Update src/transformers/models/vitpose/modeling_vitpose.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Remove print statements\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Link to collection\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
    "files": [
        {
            "sha": "1374b27ab45e5dbf25803c48cdcc6de4c70597b1",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -359,8 +359,8 @@ Flax), PyTorch, and/or TensorFlow.\n |                       [ViTMAE](model_doc/vit_mae)                        |       âœ…        |         âœ…         |      âŒ      |\n |                      [ViTMatte](model_doc/vitmatte)                      |       âœ…        |         âŒ         |      âŒ      |\n |                       [ViTMSN](model_doc/vit_msn)                        |       âœ…        |         âŒ         |      âŒ      |\n-|                       [VitPose](model_doc/vitpose)                       |       âœ…        |         âŒ         |      âŒ      |\n-|              [VitPoseBackbone](model_doc/vitpose_backbone)               |       âœ…        |         âŒ         |      âŒ      |\n+|                       [ViTPose](model_doc/vitpose)                       |       âœ…        |         âŒ         |      âŒ      |\n+|              [ViTPoseBackbone](model_doc/vitpose_backbone)               |       âœ…        |         âŒ         |      âŒ      |\n |                          [VITS](model_doc/vits)                          |       âœ…        |         âŒ         |      âŒ      |\n |                         [ViViT](model_doc/vivit)                         |       âœ…        |         âŒ         |      âŒ      |\n |                      [Wav2Vec2](model_doc/wav2vec2)                      |       âœ…        |         âœ…         |      âœ…      |"
        },
        {
            "sha": "4fbead04ea80a721109ab0b8c94b435e8879feac",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 61,
            "deletions": 27,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -10,24 +10,28 @@ an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express o\n specific language governing permissions and limitations under the License.\n -->\n \n-# VitPose\n+# ViTPose\n \n ## Overview\n \n-The VitPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. VitPose employs a standard, non-hierarchical [Vision Transformer](https://arxiv.org/pdf/2010.11929v2) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark.\n+The ViTPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. ViTPose employs a standard, non-hierarchical [Vision Transformer](vit) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark. The model was further improved in [ViTPose++: Vision Transformer for Generic Body Pose Estimation](https://arxiv.org/abs/2212.04246) where the authors employ\n+a mixture-of-experts (MoE) module in the ViT backbone along with pre-training on more data, which further enhances the performance.\n \n The abstract from the paper is the following:\n \n *Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art.*\n \n-![vitpose-architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png)\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> ViTPose architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.12484\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr) and [sangbumchoi](https://github.com/SangbumChoi).\n The original code can be found [here](https://github.com/ViTAE-Transformer/ViTPose).\n \n ## Usage Tips\n \n-ViTPose is a so-called top-down keypoint detection model. This means that one first uses an object detector, like [RT-DETR](rt_detr.md), to detect people (or other instances) in an image. Next, ViTPose takes the cropped images as input and predicts the keypoints.\n+ViTPose is a so-called top-down keypoint detection model. This means that one first uses an object detector, like [RT-DETR](rt_detr.md), to detect people (or other instances) in an image. Next, ViTPose takes the cropped images as input and predicts the keypoints for each of them.\n \n ```py\n import torch\n@@ -36,11 +40,7 @@ import numpy as np\n \n from PIL import Image\n \n-from transformers import (\n-    AutoProcessor,\n-    RTDetrForObjectDetection,\n-    VitPoseForPoseEstimation,\n-)\n+from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n \n device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n@@ -51,7 +51,7 @@ image = Image.open(requests.get(url, stream=True).raw)\n # Stage 1. Detect humans on the image\n # ------------------------------------------------------------------------\n \n-# You can choose detector by your choice\n+# You can choose any detector of your choice\n person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n \n@@ -89,9 +89,50 @@ pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[pers\n image_pose_result = pose_results[0]  # results for first image\n ```\n \n+### ViTPose++ models\n \n-### Visualization for supervision user\n-```py\n+The best [checkpoints](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335) are those of the [ViTPose++ paper](https://arxiv.org/abs/2212.04246). ViTPose++ models employ a so-called [Mixture-of-Experts (MoE)](https://huggingface.co/blog/moe) architecture for the ViT backbone, resulting in better performance.\n+\n+The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed. \n+An overview of the various dataset indices is provided below:\n+\n+- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n+- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n+- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n+- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n+- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n+- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n+\n+Pass the `dataset_index` argument in the forward of the model to indicate which experts to use for each example in the batch. Example usage is shown below:\n+\n+```python\n+image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\n+model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)\n+\n+inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n+\n+dataset_index = torch.tensor([0], device=device) # must be a tensor of shape (batch_size,)\n+\n+with torch.no_grad():\n+    outputs = model(**inputs, dataset_index=dataset_index)\n+```\n+\n+The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed. \n+An overview of the various dataset indices is provided below:\n+\n+- 0: [COCO validation 2017](https://cocodataset.org/#overview) dataset, using an object detector that gets 56 AP on the \"person\" class\n+- 1: [AiC](https://github.com/fabbrimatteo/AiC-Dataset) dataset\n+- 2: [MPII](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/software-and-datasets/mpii-human-pose-dataset) dataset\n+- 3: [AP-10K](https://github.com/AlexTheBad/AP-10K) dataset\n+- 4: [APT-36K](https://github.com/pandorgan/APT-36K) dataset\n+- 5: [COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody) dataset\n+\n+\n+### Visualization\n+\n+To visualize the various keypoints, one can either leverage the `supervision` [library](https://github.com/roboflow/supervision (requires `pip install supervision`):\n+\n+```python\n import supervision as sv\n \n xy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\n@@ -119,8 +160,9 @@ annotated_frame = vertex_annotator.annotate(\n )\n ```\n \n-### Visualization for advanced user\n-```py\n+Alternatively, one can also visualize the keypoints using [OpenCV](https://opencv.org/) (requires `pip install opencv-python`):\n+\n+```python\n import math\n import cv2\n \n@@ -223,26 +265,18 @@ pose_image\n ```\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-coco.jpg\" alt=\"drawing\" width=\"600\"/>\n \n-### MoE backbone\n-\n-To enable MoE (Mixture of Experts) function in the backbone, user has to give appropriate configuration such as `num_experts` and input value `dataset_index` to the backbone model.  However, it is not used in default parameters. Below is the code snippet for usage of MoE function.\n+## Resources\n \n-```py\n->>> from transformers import VitPoseBackboneConfig, VitPoseBackbone\n->>> import torch\n-\n->>> config = VitPoseBackboneConfig(num_experts=3, out_indices=[-1])\n->>> model = VitPoseBackbone(config)\n+A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViTPose. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n->>> pixel_values = torch.randn(3, 3, 256, 192)\n->>> dataset_index = torch.tensor([1, 2, 3])\n->>> outputs = model(pixel_values, dataset_index)\n-```\n+- A demo of ViTPose on images and video can be found [here](https://huggingface.co/spaces/hysts/ViTPose-transformers).\n+- A notebook illustrating inference and visualization can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTPose/Inference_with_ViTPose_for_human_pose_estimation.ipynb).\n \n ## VitPoseImageProcessor\n \n [[autodoc]] VitPoseImageProcessor\n     - preprocess\n+    - post_process_pose_estimation\n \n ## VitPoseConfig\n "
        },
        {
            "sha": "8ad7899dd04449f31945c624c9d004bac99bdd3c",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -650,8 +650,8 @@\n         (\"vit_msn\", \"ViTMSN\"),\n         (\"vitdet\", \"VitDet\"),\n         (\"vitmatte\", \"ViTMatte\"),\n-        (\"vitpose\", \"VitPose\"),\n-        (\"vitpose_backbone\", \"VitPoseBackbone\"),\n+        (\"vitpose\", \"ViTPose\"),\n+        (\"vitpose_backbone\", \"ViTPoseBackbone\"),\n         (\"vits\", \"VITS\"),\n         (\"vivit\", \"ViViT\"),\n         (\"wav2vec2\", \"Wav2Vec2\"),"
        },
        {
            "sha": "b1e55628a31b60c814a939e6574d05e4ec46f222",
            "filename": "src/transformers/models/vitpose/convert_vitpose_to_hf.py",
            "status": "modified",
            "additions": 104,
            "deletions": 31,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -15,6 +15,8 @@\n \"\"\"Convert VitPose checkpoints from the original repository.\n \n URL: https://github.com/vitae-transformer/vitpose\n+\n+Notebook to get the original logits: https://colab.research.google.com/drive/1QDX_2POTpl6JaZAV2WIFjuiqDsDwiqMZ?usp=sharing.\n \"\"\"\n \n import argparse\n@@ -43,34 +45,63 @@\n }\n \n MODEL_TO_FILE_NAME_MAPPING = {\n+    # VitPose models, simple decoder\n     \"vitpose-base-simple\": \"vitpose-b-simple.pth\",\n+    # VitPose models, classic decoder\n     \"vitpose-base\": \"vitpose-b.pth\",\n+    # VitPose models, COCO-AIC-MPII\n     \"vitpose-base-coco-aic-mpii\": \"vitpose_base_coco_aic_mpii.pth\",\n+    # VitPose+ models\n+    \"vitpose-plus-small\": \"vitpose+_small.pth\",\n     \"vitpose-plus-base\": \"vitpose+_base.pth\",\n+    \"vitpose-plus-large\": \"vitpose+_large.pth\",\n+    \"vitpose-plus-huge\": \"vitpose+_huge.pth\",\n }\n \n \n def get_config(model_name):\n-    num_experts = 6 if \"plus\" in model_name else 1\n-    part_features = 192 if \"plus\" in model_name else 0\n+    if \"plus\" in model_name:\n+        num_experts = 6\n+        if \"small\" in model_name:\n+            part_features = 96\n+            out_indices = [12]\n+        elif \"base\" in model_name:\n+            part_features = 192\n+            out_indices = [12]\n+        elif \"large\" in model_name:\n+            part_features = 256\n+            out_indices = [24]\n+        elif \"huge\" in model_name:\n+            part_features = 320\n+            out_indices = [32]\n+        else:\n+            raise ValueError(f\"Model {model_name} not supported\")\n+    else:\n+        num_experts = 1\n+        part_features = 0\n \n-    backbone_config = VitPoseBackboneConfig(out_indices=[12], num_experts=num_experts, part_features=part_features)\n     # size of the architecture\n     if \"small\" in model_name:\n-        backbone_config.hidden_size = 768\n-        backbone_config.intermediate_size = 2304\n-        backbone_config.num_hidden_layers = 8\n-        backbone_config.num_attention_heads = 8\n+        hidden_size = 384\n+        num_hidden_layers = 12\n+        num_attention_heads = 12\n     elif \"large\" in model_name:\n-        backbone_config.hidden_size = 1024\n-        backbone_config.intermediate_size = 4096\n-        backbone_config.num_hidden_layers = 24\n-        backbone_config.num_attention_heads = 16\n+        hidden_size = 1024\n+        num_hidden_layers = 24\n+        num_attention_heads = 16\n     elif \"huge\" in model_name:\n-        backbone_config.hidden_size = 1280\n-        backbone_config.intermediate_size = 5120\n-        backbone_config.num_hidden_layers = 32\n-        backbone_config.num_attention_heads = 16\n+        hidden_size = 1280\n+        num_hidden_layers = 32\n+        num_attention_heads = 16\n+\n+    backbone_config = VitPoseBackboneConfig(\n+        out_indices=out_indices,\n+        hidden_size=hidden_size,\n+        num_hidden_layers=num_hidden_layers,\n+        num_attention_heads=num_attention_heads,\n+        num_experts=num_experts,\n+        part_features=part_features,\n+    )\n \n     use_simple_decoder = \"simple\" in model_name\n \n@@ -155,9 +186,7 @@ def prepare_img():\n \n \n @torch.no_grad()\n-def write_model(model_path, model_name, push_to_hub, check_logits=True):\n-    os.makedirs(model_path, exist_ok=True)\n-\n+def write_model(model_name, model_path, push_to_hub, check_logits=True):\n     # ------------------------------------------------------------\n     # Vision model params and config\n     # ------------------------------------------------------------\n@@ -236,20 +265,27 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n \n     filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=\"vitpose_batch_data.pt\", repo_type=\"dataset\")\n     original_pixel_values = torch.load(filepath, map_location=\"cpu\")[\"img\"]\n+    # we allow for a small difference in the pixel values due to the original repository using cv2\n     assert torch.allclose(pixel_values, original_pixel_values, atol=1e-1)\n \n     dataset_index = torch.tensor([0])\n \n     with torch.no_grad():\n+        print(\"Shape of original_pixel_values: \", original_pixel_values.shape)\n+        print(\"First values of original_pixel_values: \", original_pixel_values[0, 0, :3, :3])\n+\n         # first forward pass\n-        outputs = model(pixel_values, dataset_index=dataset_index)\n+        outputs = model(original_pixel_values, dataset_index=dataset_index)\n         output_heatmap = outputs.heatmaps\n \n+        print(\"Shape of output_heatmap: \", output_heatmap.shape)\n+        print(\"First values: \", output_heatmap[0, 0, :3, :3])\n+\n         # second forward pass (flipped)\n         # this is done since the model uses `flip_test=True` in its test config\n-        pixel_values_flipped = torch.flip(pixel_values, [3])\n+        original_pixel_values_flipped = torch.flip(original_pixel_values, [3])\n         outputs_flipped = model(\n-            pixel_values_flipped,\n+            original_pixel_values_flipped,\n             dataset_index=dataset_index,\n             flip_pairs=torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]),\n         )\n@@ -261,6 +297,7 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n     pose_results = image_processor.post_process_pose_estimation(outputs, boxes=boxes)[0]\n \n     if check_logits:\n+        # Simple decoder checkpoints\n         if model_name == \"vitpose-base-simple\":\n             assert torch.allclose(\n                 pose_results[1][\"keypoints\"][0],\n@@ -272,6 +309,7 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n                 torch.tensor([8.66642594e-01]),\n                 atol=5e-2,\n             )\n+        # Classic decoder checkpoints\n         elif model_name == \"vitpose-base\":\n             assert torch.allclose(\n                 pose_results[1][\"keypoints\"][0],\n@@ -283,6 +321,7 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n                 torch.tensor([8.8235235e-01]),\n                 atol=5e-2,\n             )\n+        # COCO-AIC-MPII checkpoints\n         elif model_name == \"vitpose-base-coco-aic-mpii\":\n             assert torch.allclose(\n                 pose_results[1][\"keypoints\"][0],\n@@ -294,6 +333,18 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n                 torch.tensor([8.69966745e-01]),\n                 atol=5e-2,\n             )\n+        # VitPose+ models\n+        elif model_name == \"vitpose-plus-small\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([398.1597, 181.6902]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor(0.9051),\n+                atol=5e-2,\n+            )\n         elif model_name == \"vitpose-plus-base\":\n             assert torch.allclose(\n                 pose_results[1][\"keypoints\"][0],\n@@ -305,18 +356,43 @@ def write_model(model_path, model_name, push_to_hub, check_logits=True):\n                 torch.tensor([8.75046968e-01]),\n                 atol=5e-2,\n             )\n+        elif model_name == \"vitpose-plus-large\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([398.1409, 181.7412]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor(0.8746),\n+                atol=5e-2,\n+            )\n+        elif model_name == \"vitpose-plus-huge\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([398.2079, 181.8026]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor(0.8693),\n+                atol=5e-2,\n+            )\n         else:\n             raise ValueError(\"Model not supported\")\n     print(\"Conversion successfully done.\")\n \n-    # save the model to a local directory\n-    model.save_pretrained(model_path)\n-    image_processor.save_pretrained(model_path)\n+    if model_path is not None:\n+        os.makedirs(model_path, exist_ok=True)\n+        model.save_pretrained(model_path)\n+        image_processor.save_pretrained(model_path)\n \n     if push_to_hub:\n         print(f\"Pushing model and image processor for {model_name} to hub\")\n-        model.push_to_hub(f\"danelcsb/{model_name}\")\n-        image_processor.push_to_hub(f\"danelcsb/{model_name}\")\n+        # we created a community organization on the hub for this model\n+        # maintained by the Transformers team\n+        model.push_to_hub(f\"usyd-community/{model_name}\")\n+        image_processor.push_to_hub(f\"usyd-community/{model_name}\")\n \n \n def main():\n@@ -330,16 +406,13 @@ def main():\n         help=\"Name of the VitPose model you'd like to convert.\",\n     )\n     parser.add_argument(\n-        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to store the converted model.\"\n     )\n     parser.add_argument(\n         \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ðŸ¤— hub.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\",\n-        default=True,\n-        type=bool,\n-        help=\"Whether to check the logits of public converted model to the ðŸ¤— hub. You can disable when using custom model.\",\n+        \"--check_logits\", action=\"store_false\", help=\"Whether or not to verify the logits of the converted model.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "d89f95e26b54054bde2b7e15a32cd6fe783afe3a",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -393,7 +393,7 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n \n-    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm, VitPoseBackboneEmbeddings]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid"
        },
        {
            "sha": "73129956a3dcd850c41e1f5311eb4b71c21fbc68",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=872dfbdd46ecdb49cbe3b6b4ba3a63bada158e3b",
            "patch": "@@ -239,9 +239,7 @@ def default_image_processor(self):\n     @slow\n     def test_inference_pose_estimation(self):\n         image_processor = self.default_image_processor\n-        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\")\n-        model.to(torch_device)\n-        model.eval()\n+        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=torch_device)\n \n         image = prepare_img()\n         boxes = [[[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]]]\n@@ -284,9 +282,7 @@ def test_inference_pose_estimation(self):\n     @slow\n     def test_batched_inference(self):\n         image_processor = self.default_image_processor\n-        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\")\n-        model.to(torch_device)\n-        model.eval()\n+        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=torch_device)\n \n         image = prepare_img()\n         boxes = ["
        }
    ],
    "stats": {
        "total": 241,
        "additions": 172,
        "deletions": 69
    }
}