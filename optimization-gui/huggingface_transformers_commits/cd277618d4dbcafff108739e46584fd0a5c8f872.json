{
    "author": "guangy10",
    "message": "Roberta is ExecuTorch compatible (#34425)\n\n* Roberta is ExecuTorch compatible\r\n\r\n* [run_slow] roberta\r\n\r\n---------\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "cd277618d4dbcafff108739e46584fd0a5c8f872",
    "files": [
        {
            "sha": "1c128513b17d13f52167dfc3e247282c6fe1abe0",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd277618d4dbcafff108739e46584fd0a5c8f872/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd277618d4dbcafff108739e46584fd0a5c8f872/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=cd277618d4dbcafff108739e46584fd0a5c8f872",
            "patch": "@@ -16,7 +16,7 @@\n \n import unittest\n \n-from transformers import RobertaConfig, is_torch_available\n+from transformers import AutoTokenizer, RobertaConfig, is_torch_available\n from transformers.testing_utils import TestCasePlus, require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -41,6 +41,7 @@\n         RobertaEmbeddings,\n         create_position_ids_from_input_ids,\n     )\n+    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n \n ROBERTA_TINY = \"sshleifer/tiny-distilroberta-base\"\n \n@@ -576,3 +577,43 @@ def test_inference_classification_head(self):\n         # expected_tensor = roberta.predict(\"mnli\", input_ids, return_logits=True).detach()\n \n         self.assertTrue(torch.allclose(output, expected_tensor, atol=1e-4))\n+\n+    @slow\n+    def test_export(self):\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        roberta_model = \"FacebookAI/roberta-base\"\n+        device = \"cpu\"\n+        attn_implementation = \"sdpa\"\n+        max_length = 512\n+\n+        tokenizer = AutoTokenizer.from_pretrained(roberta_model)\n+        inputs = tokenizer(\n+            \"The goal of life is <mask>.\",\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=max_length,\n+        )\n+\n+        model = RobertaForMaskedLM.from_pretrained(\n+            roberta_model,\n+            device_map=device,\n+            attn_implementation=attn_implementation,\n+            use_cache=True,\n+        )\n+\n+        logits = model(**inputs).logits\n+        eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n+        self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+            strict=True,\n+        )\n+\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n+        self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 42,
        "deletions": 1
    }
}