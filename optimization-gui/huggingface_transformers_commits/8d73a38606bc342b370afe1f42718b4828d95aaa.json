{
    "author": "conditionedstimulus",
    "message": "Add DAB-DETR for object detection (#30803)\n\n* initial commit\r\n\r\n* encoder+decoder layer changes WIP\r\n\r\n* architecture checks\r\n\r\n* working version of detection + segmentation\r\n\r\n* fix modeling outputs\r\n\r\n* fix return dict + output att/hs\r\n\r\n* found the position embedding masking bug\r\n\r\n* pre-training version\r\n\r\n* added iamge processors\r\n\r\n* typo in init.py\r\n\r\n* iterupdate set to false\r\n\r\n* fixed num_labels in class_output linear layer bias init\r\n\r\n* multihead attention shape fixes\r\n\r\n* test improvements\r\n\r\n* test update\r\n\r\n* dab-detr model_doc update\r\n\r\n* dab-detr model_doc update2\r\n\r\n* test fix:test_retain_grad_hidden_states_attentions\r\n\r\n* config file clean and renaming variables\r\n\r\n* config file clean and renaming variables fix\r\n\r\n* updated convert_to_hf file\r\n\r\n* small fixes\r\n\r\n* style and qulity checks\r\n\r\n* return_dict fix\r\n\r\n* Merge branch main into add_dab_detr\r\n\r\n* small comment fix\r\n\r\n* skip test_inputs_embeds test\r\n\r\n* image processor updates + image processor test updates\r\n\r\n* check copies test fix update\r\n\r\n* updates for check_copies.py test\r\n\r\n* updates for check_copies.py test2\r\n\r\n* tied weights fix\r\n\r\n* fixed image processing tests and fixed shared weights issues\r\n\r\n* added numpy nd array option to get_Expected_values method in test_image_processing_dab_detr.py\r\n\r\n* delete prints from test file\r\n\r\n* SafeTensor modification to solve HF Trainer issue\r\n\r\n* removing the safetensor modifications\r\n\r\n* make fix copies and hf uplaod has been added.\r\n\r\n* fixed index.md\r\n\r\n* fixed repo consistency\r\n\r\n* styel fix and dabdetrimageprocessor docstring update\r\n\r\n* requested modifications after the first review\r\n\r\n* Update src/transformers/models/dab_detr/image_processing_dab_detr.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* repo consistency has been fixed\r\n\r\n* update copied NestedTensor function after main merge\r\n\r\n* Update src/transformers/models/dab_detr/modeling_dab_detr.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* temp commit\r\n\r\n* temp commit2\r\n\r\n* temp commit 3\r\n\r\n* unit tests are fixed\r\n\r\n* fixed repo consistency\r\n\r\n* updated expected_boxes varible values based on related notebook results in DABDETRIntegrationTests file.\r\n\r\n* temporarialy config modifications and repo consistency fixes\r\n\r\n* Put dilation parameter back to config\r\n\r\n* pattern embeddings have been added to the rename_keys method\r\n\r\n* add dilation comment to config + add as an exception in check_config_attributes SPECIAL CASES\r\n\r\n* delete FeatureExtractor part from docs.md\r\n\r\n* requested modifications in modeling_dab_detr.py\r\n\r\n* [run_slow] dab_detr\r\n\r\n* deleted last segmentation code part, updated conversion script and changed the hf path in test files\r\n\r\n* temp commit of requested modifications\r\n\r\n* temp commit of requested modifications 2\r\n\r\n* updated config file, resolved codepaths and refactored conversion script\r\n\r\n* updated decodelayer block types and refactored conversion script\r\n\r\n* style and quality update\r\n\r\n* small modifications based on the request\r\n\r\n* attentions are refactored\r\n\r\n* removed loss functions from modeling file, added loss function to lossutils, tried to move the MLP layer generation to config but it failed\r\n\r\n* deleted imageprocessor\r\n\r\n* fixed conversion script + quality and style\r\n\r\n* fixed config_att\r\n\r\n* [run_slow] dab_detr\r\n\r\n* changing model path in conversion file and in test file\r\n\r\n* fix Decoder variable naming\r\n\r\n* testing the old loss function\r\n\r\n* switched back to the new loss function and testing with the odl attention functions\r\n\r\n* switched back to the new last good result modeling file\r\n\r\n* moved back to the version when I asked the review\r\n\r\n* missing new line at the end of the file\r\n\r\n* old version test\r\n\r\n* turn back to newest mdoel versino but change image processor\r\n\r\n* style fix\r\n\r\n* style fix after merge main\r\n\r\n* [run_slow] dab_detr\r\n\r\n* [run_slow] dab_detr\r\n\r\n* added device and type for head bias data part\r\n\r\n* [run_slow] dab_detr\r\n\r\n* fixed model head bias data fill\r\n\r\n* changed test_inference_object_detection_head assertTrues to torch test assert_close\r\n\r\n* fixes part 1\r\n\r\n* quality update\r\n\r\n* self.bbox_embed in decoder has been restored\r\n\r\n* changed Assert true torch closeall methods to torch testing assertclose\r\n\r\n* modelcard markdown file has been updated\r\n\r\n* deleted intemediate list from decoder module\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "8d73a38606bc342b370afe1f42718b4828d95aaa",
    "files": [
        {
            "sha": "7557f295d96805ed3fe0acf1d27088d47e1994e0",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -643,6 +643,8 @@\n         title: ConvNeXTV2\n       - local: model_doc/cvt\n         title: CvT\n+      - local: model_doc/dab-detr\n+        title: DAB-DETR\n       - local: model_doc/deformable_detr\n         title: Deformable DETR\n       - local: model_doc/deit"
        },
        {
            "sha": "9c3c5c76954d1e1822697ae1395e2ab2c51a7ff5",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -110,6 +110,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                       [CPM-Ant](model_doc/cpmant)                        |       ✅        |         ❌         |      ❌      |\n |                          [CTRL](model_doc/ctrl)                          |       ✅        |         ✅         |      ❌      |\n |                           [CvT](model_doc/cvt)                           |       ✅        |         ✅         |      ❌      |\n+|                      [DAB-DETR](model_doc/dab-detr)                      |       ✅        |         ❌         |      ❌      |\n |                           [DAC](model_doc/dac)                           |       ✅        |         ❌         |      ❌      |\n |                   [Data2VecAudio](model_doc/data2vec)                    |       ✅        |         ❌         |      ❌      |\n |                    [Data2VecText](model_doc/data2vec)                    |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "6071ee6ca46022e23d32a4b1b9a7ebd37ba10a34",
            "filename": "docs/source/en/model_doc/dab-detr.md",
            "status": "added",
            "additions": 119,
            "deletions": 0,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,119 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# DAB-DETR\n+\n+## Overview\n+\n+The DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://arxiv.org/abs/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.\n+DAB-DETR is an enhanced variant of Conditional DETR. It utilizes dynamically updated anchor boxes to provide both a reference query point (x, y) and a reference anchor size (w, h), improving cross-attention computation. This new approach achieves 45.7% AP when trained for 50 epochs with a single ResNet-50 model as the backbone.\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dab_detr_convergence_plot.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+The abstract from the paper is the following:\n+\n+*We present in this paper a novel query formulation using dynamic anchor boxes\n+for DETR (DEtection TRansformer) and offer a deeper understanding of the role\n+of queries in DETR. This new formulation directly uses box coordinates as queries\n+in Transformer decoders and dynamically updates them layer-by-layer. Using box\n+coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR,\n+but also allows us to modulate the positional attention map using the box width\n+and height information. Such a design makes it clear that queries in DETR can be\n+implemented as performing soft ROI pooling layer-by-layer in a cascade manner.\n+As a result, it leads to the best performance on MS-COCO benchmark among\n+the DETR-like detection models under the same setting, e.g., AP 45.7% using\n+ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive\n+experiments to confirm our analysis and verify the effectiveness of our methods.*\n+\n+This model was contributed by [davidhajdu](https://huggingface.co/davidhajdu).\n+The original code can be found [here](https://github.com/IDEA-Research/DAB-DETR).\n+\n+## How to Get Started with the Model\n+\n+Use the code below to get started with the model.\n+\n+```python\n+import torch\n+import requests\n+\n+from PIL import Image\n+from transformers import AutoModelForObjectDetection, AutoImageProcessor\n+\n+url = 'http://images.cocodataset.org/val2017/000000039769.jpg' \n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n+model = AutoModelForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n+\n+inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\n+\n+for result in results:\n+    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n+        score, label = score.item(), label_id.item()\n+        box = [round(i, 2) for i in box.tolist()]\n+        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n+```\n+This should output\n+```\n+cat: 0.87 [14.7, 49.39, 320.52, 469.28]\n+remote: 0.86 [41.08, 72.37, 173.39, 117.2]\n+cat: 0.86 [344.45, 19.43, 639.85, 367.86]\n+remote: 0.61 [334.27, 75.93, 367.92, 188.81]\n+couch: 0.59 [-0.04, 1.34, 639.9, 477.09]\n+```\n+\n+There are three other ways to instantiate a DAB-DETR model (depending on what you prefer):\n+\n+Option 1: Instantiate DAB-DETR with pre-trained weights for entire model\n+```py\n+>>> from transformers import DabDetrForObjectDetection\n+\n+>>> model = DabDetrForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n+```\n+\n+Option 2: Instantiate DAB-DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone\n+```py\n+>>> from transformers import DabDetrConfig, DabDetrForObjectDetection\n+\n+>>> config = DabDetrConfig()\n+>>> model = DabDetrForObjectDetection(config)\n+```\n+Option 3: Instantiate DAB-DETR with randomly initialized weights for backbone + Transformer\n+```py\n+>>> config = DabDetrConfig(use_pretrained_backbone=False)\n+>>> model = DabDetrForObjectDetection(config)\n+```\n+\n+\n+## DabDetrConfig\n+\n+[[autodoc]] DabDetrConfig\n+\n+## DabDetrModel\n+\n+[[autodoc]] DabDetrModel\n+    - forward\n+\n+## DabDetrForObjectDetection\n+\n+[[autodoc]] DabDetrForObjectDetection\n+    - forward"
        },
        {
            "sha": "ea832eded4710db1fc4a02cf3829f7c357abe1ba",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -328,6 +328,7 @@\n         \"CTRLTokenizer\",\n     ],\n     \"models.cvt\": [\"CvtConfig\"],\n+    \"models.dab_detr\": [\"DabDetrConfig\"],\n     \"models.dac\": [\"DacConfig\", \"DacFeatureExtractor\"],\n     \"models.data2vec\": [\n         \"Data2VecAudioConfig\",\n@@ -1898,6 +1899,13 @@\n             \"CvtPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.dab_detr\"].extend(\n+        [\n+            \"DabDetrForObjectDetection\",\n+            \"DabDetrModel\",\n+            \"DabDetrPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.dac\"].extend(\n         [\n             \"DacModel\",\n@@ -5387,6 +5395,9 @@\n         CTRLTokenizer,\n     )\n     from .models.cvt import CvtConfig\n+    from .models.dab_detr import (\n+        DabDetrConfig,\n+    )\n     from .models.dac import (\n         DacConfig,\n         DacFeatureExtractor,\n@@ -6926,6 +6937,11 @@\n             CvtModel,\n             CvtPreTrainedModel,\n         )\n+        from .models.dab_detr import (\n+            DabDetrForObjectDetection,\n+            DabDetrModel,\n+            DabDetrPreTrainedModel,\n+        )\n         from .models.dac import (\n             DacModel,\n             DacPreTrainedModel,"
        },
        {
            "sha": "15f0397535e8c6df24cbd2148471ed71742d0636",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -217,6 +217,7 @@ def __getitem__(self, key):\n     \"silu\": nn.SiLU,\n     \"swish\": nn.SiLU,\n     \"tanh\": nn.Tanh,\n+    \"prelu\": nn.PReLU,\n }\n ACT2FN = ClassInstantier(ACT2CLS)\n "
        },
        {
            "sha": "86f8634a45e0847202663e6e43074df8d5dde837",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -128,6 +128,7 @@ def ForTokenClassification(logits, labels, config, **kwargs):\n     \"ForObjectDetection\": ForObjectDetectionLoss,\n     \"DeformableDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"DabDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"GroundingDinoForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,"
        },
        {
            "sha": "1667edbf37ab598c858353adcda72a1d1571b759",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -63,6 +63,7 @@\n     cpmant,\n     ctrl,\n     cvt,\n+    dab_detr,\n     dac,\n     data2vec,\n     dbrx,"
        },
        {
            "sha": "bd6fcb4a9d828e6c0192d91c34948f57fc184223",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -79,6 +79,7 @@\n         (\"cpmant\", \"CpmAntConfig\"),\n         (\"ctrl\", \"CTRLConfig\"),\n         (\"cvt\", \"CvtConfig\"),\n+        (\"dab-detr\", \"DabDetrConfig\"),\n         (\"dac\", \"DacConfig\"),\n         (\"data2vec-audio\", \"Data2VecAudioConfig\"),\n         (\"data2vec-text\", \"Data2VecTextConfig\"),\n@@ -399,6 +400,7 @@\n         (\"cpmant\", \"CPM-Ant\"),\n         (\"ctrl\", \"CTRL\"),\n         (\"cvt\", \"CvT\"),\n+        (\"dab-detr\", \"DAB-DETR\"),\n         (\"dac\", \"DAC\"),\n         (\"data2vec-audio\", \"Data2VecAudio\"),\n         (\"data2vec-text\", \"Data2VecText\"),"
        },
        {
            "sha": "87e2dab68708b9f66fed423f34cf1bbf82629254",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -78,6 +78,7 @@\n         (\"cpmant\", \"CpmAntModel\"),\n         (\"ctrl\", \"CTRLModel\"),\n         (\"cvt\", \"CvtModel\"),\n+        (\"dab-detr\", \"DabDetrModel\"),\n         (\"dac\", \"DacModel\"),\n         (\"data2vec-audio\", \"Data2VecAudioModel\"),\n         (\"data2vec-text\", \"Data2VecTextModel\"),\n@@ -592,6 +593,7 @@\n         (\"conditional_detr\", \"ConditionalDetrModel\"),\n         (\"convnext\", \"ConvNextModel\"),\n         (\"convnextv2\", \"ConvNextV2Model\"),\n+        (\"dab-detr\", \"DabDetrModel\"),\n         (\"data2vec-vision\", \"Data2VecVisionModel\"),\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n@@ -890,6 +892,7 @@\n     [\n         # Model for Object Detection mapping\n         (\"conditional_detr\", \"ConditionalDetrForObjectDetection\"),\n+        (\"dab-detr\", \"DabDetrForObjectDetection\"),\n         (\"deformable_detr\", \"DeformableDetrForObjectDetection\"),\n         (\"deta\", \"DetaForObjectDetection\"),\n         (\"detr\", \"DetrForObjectDetection\"),"
        },
        {
            "sha": "7eecc6eda0905e6395c49c18bf9cc5e28dbe88c2",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -52,7 +52,7 @@ class ConditionalDetrConfig(PretrainedConfig):\n             Number of object queries, i.e. detection slots. This is the maximal number of objects\n             [`ConditionalDetrModel`] can detect in a single image. For COCO, we recommend 100 queries.\n         d_model (`int`, *optional*, defaults to 256):\n-            Dimension of the layers.\n+            This parameter is a general dimension parameter, defining dimensions for components such as the encoder layer and projection parameters in the decoder layer, among others.\n         encoder_layers (`int`, *optional*, defaults to 6):\n             Number of encoder layers.\n         decoder_layers (`int`, *optional*, defaults to 6):"
        },
        {
            "sha": "d020b94cffde5ffad13f7886d348578786a3df13",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -74,6 +74,8 @@ class ConditionalDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n         intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n             Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n             layernorm.\n+        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+            Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n@@ -116,6 +118,8 @@ class ConditionalDetrModelOutput(Seq2SeqModelOutput):\n         intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n             Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n             layernorm.\n+        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+            Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "bfa364bd2152049fc92f575e102bf7b934ba4a6c",
            "filename": "src/transformers/models/dab_detr/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2F__init__.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_dab_detr import *\n+    from .modeling_dab_detr import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "398e6f26591f0498a6d15889d7ef486ea5d74e28",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "added",
            "additions": 260,
            "deletions": 0,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,260 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"DAB-DETR model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DabDetrConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DabDetrModel`]. It is used to instantiate\n+    a DAB-DETR model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the DAB-DETR\n+    [IDEA-Research/dab_detr-base](https://huggingface.co/IDEA-Research/dab_detr-base) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        use_timm_backbone (`bool`, *optional*, defaults to `True`):\n+            Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n+            API.\n+        backbone_config (`PretrainedConfig` or `dict`, *optional*):\n+            The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n+            case it will default to `ResNetConfig()`.\n+        backbone (`str`, *optional*, defaults to `\"resnet50\"`):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `True`):\n+            Whether to use pretrained weights for the backbone.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        num_queries (`int`, *optional*, defaults to 300):\n+            Number of object queries, i.e. detection slots. This is the maximal number of objects\n+            [`DabDetrModel`] can detect in a single image. For COCO, we recommend 100 queries.\n+        encoder_layers (`int`, *optional*, defaults to 6):\n+            Number of encoder layers.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in encoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Indicates whether the transformer model architecture is an encoder-decoder or not.\n+        activation_function (`str` or `function`, *optional*, defaults to `\"prelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            This parameter is a general dimension parameter, defining dimensions for components such as the encoder layer and projection parameters in the decoder layer, among others.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        init_std (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        init_xavier_std (`float`, *optional*, defaults to 1.0):\n+            The scaling factor used for the Xavier initialization gain in the HM Attention map module.\n+        auxiliary_loss (`bool`, *optional*, defaults to `False`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        dilation (`bool`, *optional*, defaults to `False`):\n+            Whether to replace stride with dilation in the last convolutional block (DC5). Only supported when `use_timm_backbone` = `True`.\n+        class_cost (`float`, *optional*, defaults to 2):\n+            Relative weight of the classification error in the Hungarian matching cost.\n+        bbox_cost (`float`, *optional*, defaults to 5):\n+            Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.\n+        giou_cost (`float`, *optional*, defaults to 2):\n+            Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.\n+        cls_loss_coefficient (`float`, *optional*, defaults to 2):\n+            Relative weight of the classification loss in the object detection loss function.\n+        bbox_loss_coefficient (`float`, *optional*, defaults to 5):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        giou_loss_coefficient (`float`, *optional*, defaults to 2):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        focal_alpha (`float`, *optional*, defaults to 0.25):\n+            Alpha parameter in the focal loss.\n+        temperature_height (`int`, *optional*, defaults to 20):\n+            Temperature parameter to tune the flatness of positional attention (HEIGHT)\n+        temperature_width (`int`, *optional*, defaults to 20):\n+            Temperature parameter to tune the flatness of positional attention (WIDTH)\n+        query_dim (`int`, *optional*, defaults to 4):\n+            Query dimension parameter represents the size of the output vector.\n+        random_refpoints_xy (`bool`, *optional*, defaults to `False`):\n+            Whether to fix the x and y coordinates of the anchor boxes with random initialization.\n+        keep_query_pos (`bool`, *optional*, defaults to `False`):\n+            Whether to concatenate the projected positional embedding from the object query into the original query (key) in every decoder layer.\n+        num_patterns (`int`, *optional*, defaults to 0):\n+            Number of pattern embeddings.\n+        normalize_before (`bool`, *optional*, defaults to `False`):\n+            Whether we use a normalization layer in the Encoder or not.\n+        sine_position_embedding_scale (`float`, *optional*, defaults to 'None'):\n+            Scaling factor applied to the normalized positional encodings.\n+        initializer_bias_prior_prob (`float`, *optional*):\n+            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.\n+            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.\n+\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import DabDetrConfig, DabDetrModel\n+\n+    >>> # Initializing a DAB-DETR IDEA-Research/dab_detr-base style configuration\n+    >>> configuration = DabDetrConfig()\n+\n+    >>> # Initializing a model (with random weights) from the IDEA-Research/dab_detr-base style configuration\n+    >>> model = DabDetrModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"dab-detr\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        use_timm_backbone=True,\n+        backbone_config=None,\n+        backbone=\"resnet50\",\n+        use_pretrained_backbone=True,\n+        backbone_kwargs=None,\n+        num_queries=300,\n+        encoder_layers=6,\n+        encoder_ffn_dim=2048,\n+        encoder_attention_heads=8,\n+        decoder_layers=6,\n+        decoder_ffn_dim=2048,\n+        decoder_attention_heads=8,\n+        is_encoder_decoder=True,\n+        activation_function=\"prelu\",\n+        hidden_size=256,\n+        dropout=0.1,\n+        attention_dropout=0.0,\n+        activation_dropout=0.0,\n+        init_std=0.02,\n+        init_xavier_std=1.0,\n+        auxiliary_loss=False,\n+        dilation=False,\n+        class_cost=2,\n+        bbox_cost=5,\n+        giou_cost=2,\n+        cls_loss_coefficient=2,\n+        bbox_loss_coefficient=5,\n+        giou_loss_coefficient=2,\n+        focal_alpha=0.25,\n+        temperature_height=20,\n+        temperature_width=20,\n+        query_dim=4,\n+        random_refpoints_xy=False,\n+        keep_query_pos=False,\n+        num_patterns=0,\n+        normalize_before=False,\n+        sine_position_embedding_scale=None,\n+        initializer_bias_prior_prob=None,\n+        **kwargs,\n+    ):\n+        if query_dim != 4:\n+            raise ValueError(\"The query dimensions has to be 4.\")\n+\n+        # We default to values which were previously hard-coded in the model. This enables configurability of the config\n+        # while keeping the default behavior the same.\n+        if use_timm_backbone and backbone_kwargs is None:\n+            backbone_kwargs = {}\n+            if dilation:\n+                backbone_kwargs[\"output_stride\"] = 16\n+            backbone_kwargs[\"out_indices\"] = [1, 2, 3, 4]\n+            backbone_kwargs[\"in_chans\"] = 3  # num_channels\n+        # Backwards compatibility\n+        elif not use_timm_backbone and backbone in (None, \"resnet50\"):\n+            if backbone_config is None:\n+                logger.info(\"`backbone_config` is `None`. Initializing the config with the default `ResNet` backbone.\")\n+                backbone_config = CONFIG_MAPPING[\"resnet\"](out_features=[\"stage4\"])\n+            elif isinstance(backbone_config, dict):\n+                backbone_model_type = backbone_config.get(\"model_type\")\n+                config_class = CONFIG_MAPPING[backbone_model_type]\n+                backbone_config = config_class.from_dict(backbone_config)\n+            backbone = None\n+            # set timm attributes to None\n+            dilation = None\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone_config = backbone_config\n+        self.num_queries = num_queries\n+        self.hidden_size = hidden_size\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.encoder_layers = encoder_layers\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.activation_dropout = activation_dropout\n+        self.activation_function = activation_function\n+        self.init_std = init_std\n+        self.init_xavier_std = init_xavier_std\n+        self.num_hidden_layers = encoder_layers\n+        self.auxiliary_loss = auxiliary_loss\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.backbone_kwargs = backbone_kwargs\n+        # Hungarian matcher\n+        self.class_cost = class_cost\n+        self.bbox_cost = bbox_cost\n+        self.giou_cost = giou_cost\n+        # Loss coefficients\n+        self.cls_loss_coefficient = cls_loss_coefficient\n+        self.bbox_loss_coefficient = bbox_loss_coefficient\n+        self.giou_loss_coefficient = giou_loss_coefficient\n+        self.focal_alpha = focal_alpha\n+        self.query_dim = query_dim\n+        self.random_refpoints_xy = random_refpoints_xy\n+        self.keep_query_pos = keep_query_pos\n+        self.num_patterns = num_patterns\n+        self.normalize_before = normalize_before\n+        self.temperature_width = temperature_width\n+        self.temperature_height = temperature_height\n+        self.sine_position_embedding_scale = sine_position_embedding_scale\n+        self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n+\n+__all__ = [\"DabDetrConfig\"]"
        },
        {
            "sha": "a6e5081b484c9b52ebce87c9244634c4c355721f",
            "filename": "src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "added",
            "additions": 233,
            "deletions": 0,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,233 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert DAB-DETR checkpoints.\"\"\"\n+\n+import argparse\n+import gc\n+import json\n+import re\n+from pathlib import Path\n+\n+import torch\n+from huggingface_hub import hf_hub_download\n+\n+from transformers import ConditionalDetrImageProcessor, DabDetrConfig, DabDetrForObjectDetection\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # convolutional projection + query embeddings + layernorm of decoder + class and bounding box heads\n+    # for dab-DETR, also convert reference point head and query scale MLP\n+    r\"input_proj\\.(bias|weight)\": r\"input_projection.\\1\",\n+    r\"refpoint_embed\\.weight\": r\"query_refpoint_embeddings.weight\",\n+    r\"class_embed\\.(bias|weight)\": r\"class_embed.\\1\",\n+    # negative lookbehind because of the overlap\n+    r\"(?<!transformer\\.decoder\\.)bbox_embed\\.layers\\.(\\d+)\\.(bias|weight)\": r\"bbox_predictor.layers.\\1.\\2\",\n+    r\"transformer\\.encoder\\.query_scale\\.layers\\.(\\d+)\\.(bias|weight)\": r\"encoder.query_scale.layers.\\1.\\2\",\n+    r\"transformer\\.decoder\\.bbox_embed\\.layers\\.(\\d+)\\.(bias|weight)\": r\"decoder.bbox_embed.layers.\\1.\\2\",\n+    r\"transformer\\.decoder\\.norm\\.(bias|weight)\": r\"decoder.layernorm.\\1\",\n+    r\"transformer\\.decoder\\.ref_point_head\\.layers\\.(\\d+)\\.(bias|weight)\": r\"decoder.ref_point_head.layers.\\1.\\2\",\n+    r\"transformer\\.decoder\\.ref_anchor_head\\.layers\\.(\\d+)\\.(bias|weight)\": r\"decoder.ref_anchor_head.layers.\\1.\\2\",\n+    r\"transformer\\.decoder\\.query_scale\\.layers\\.(\\d+)\\.(bias|weight)\": r\"decoder.query_scale.layers.\\1.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.0\\.ca_qpos_proj\\.(bias|weight)\": r\"decoder.layers.0.cross_attn.cross_attn_query_pos_proj.\\1\",\n+    # encoder layers: output projection, 2 feedforward neural networks and 2 layernorms + activation function\n+    # output projection\n+    r\"transformer\\.encoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.(bias|weight)\": r\"encoder.layers.\\1.self_attn.out_proj.\\2\",\n+    # FFN layers\n+    r\"transformer\\.encoder\\.layers\\.(\\d+)\\.linear(\\d)\\.(bias|weight)\": r\"encoder.layers.\\1.fc\\2.\\3\",\n+    # normalization layers\n+    # nm1\n+    r\"transformer\\.encoder\\.layers\\.(\\d+)\\.norm1\\.(bias|weight)\": r\"encoder.layers.\\1.self_attn_layer_norm.\\2\",\n+    # nm2\n+    r\"transformer\\.encoder\\.layers\\.(\\d+)\\.norm2\\.(bias|weight)\": r\"encoder.layers.\\1.final_layer_norm.\\2\",\n+    # activation function weight\n+    r\"transformer\\.encoder\\.layers\\.(\\d+)\\.activation\\.weight\": r\"encoder.layers.\\1.activation_fn.weight\",\n+    #########################################################################################################################################\n+    # decoder layers: 2 times output projection, 2 feedforward neural networks and 3 layernorms + activiation function weight\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn.output_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn.output_proj.\\2\",\n+    # FFNs\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.linear(\\d)\\.(bias|weight)\": r\"decoder.layers.\\1.mlp.fc\\2.\\3\",\n+    # nm1\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm1\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_layer_norm.\\2\",\n+    # nm2\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm2\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_layer_norm.\\2\",\n+    # nm3\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm3\\.(bias|weight)\": r\"decoder.layers.\\1.mlp.final_layer_norm.\\2\",\n+    # activation function weight\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.activation\\.weight\": r\"decoder.layers.\\1.mlp.activation_fn.weight\",\n+    # q, k, v projections and biases in self-attention in decoder\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.sa_qcontent_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_query_content_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.sa_kcontent_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_key_content_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.sa_qpos_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_query_pos_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.sa_kpos_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_key_pos_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.sa_v_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn_value_proj.\\2\",\n+    # q, k, v projections in cross-attention in decoder\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_qcontent_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_query_content_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_kcontent_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_key_content_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_kpos_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_key_pos_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_v_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_value_proj.\\2\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_qpos_sine_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn_query_pos_sine_proj.\\2\",\n+}\n+\n+\n+# Copied from transformers.models.mllama.convert_mllama_weights_to_hf.convert_old_keys_to_new_keys\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def write_image_processor(model_name, pytorch_dump_folder_path, push_to_hub):\n+    logger.info(\"Converting image processor...\")\n+    format = \"coco_detection\"\n+    image_processor = ConditionalDetrImageProcessor(format=format)\n+    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+    image_processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        image_processor.push_to_hub(repo_id=model_name, commit_message=\"Add new image processor\")\n+\n+\n+@torch.no_grad()\n+def write_model(model_name, pretrained_model_weights_path, pytorch_dump_folder_path, push_to_hub):\n+    # load modified config. Why? After loading the default config, the backbone kwargs are already set.\n+    if \"dc5\" in model_name:\n+        config = DabDetrConfig(dilation=True)\n+    else:\n+        # load default config\n+        config = DabDetrConfig()\n+    # set other attributes\n+    if \"dab-detr-resnet-50-dc5\" == model_name:\n+        config.temperature_height = 10\n+        config.temperature_width = 10\n+    if \"fixxy\" in model_name:\n+        config.random_refpoints_xy = True\n+    if \"pat3\" in model_name:\n+        config.num_patterns = 3\n+        # only when the number of patterns (num_patterns parameter in config) are more than 0 like r50-pat3 or r50dc5-pat3\n+        ORIGINAL_TO_CONVERTED_KEY_MAPPING.update({r\"transformer.patterns.weight\": r\"patterns.weight\"})\n+\n+    config.num_labels = 91\n+    repo_id = \"huggingface/label-files\"\n+    filename = \"coco-detection-id2label.json\"\n+    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+    id2label = {int(k): v for k, v in id2label.items()}\n+    config.id2label = id2label\n+    config.label2id = {v: k for k, v in id2label.items()}\n+    # load original model from local path\n+    loaded = torch.load(pretrained_model_weights_path, map_location=torch.device(\"cpu\"))[\"model\"]\n+    # Renaming the original model state dictionary to HF compatibile\n+    all_keys = list(loaded.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+    state_dict = {}\n+    for key in all_keys:\n+        if \"backbone.0.body\" in key:\n+            new_key = key.replace(\"backbone.0.body\", \"backbone.conv_encoder.model._backbone\")\n+            state_dict[new_key] = loaded[key]\n+        # Q, K, V encoder values mapping\n+        elif re.search(\"self_attn.in_proj_(weight|bias)\", key):\n+            # Dynamically find the layer number\n+            pattern = r\"layers\\.(\\d+)\\.self_attn\\.in_proj_(weight|bias)\"\n+            match = re.search(pattern, key)\n+            if match:\n+                layer_num = match.group(1)\n+            else:\n+                raise ValueError(f\"Pattern not found in key: {key}\")\n+\n+            in_proj_value = loaded.pop(key)\n+            if \"weight\" in key:\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.q_proj.weight\"] = in_proj_value[:256, :]\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.k_proj.weight\"] = in_proj_value[256:512, :]\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.v_proj.weight\"] = in_proj_value[-256:, :]\n+            elif \"bias\" in key:\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.q_proj.bias\"] = in_proj_value[:256]\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.k_proj.bias\"] = in_proj_value[256:512]\n+                state_dict[f\"encoder.layers.{layer_num}.self_attn.v_proj.bias\"] = in_proj_value[-256:]\n+        else:\n+            new_key = new_keys[key]\n+            state_dict[new_key] = loaded[key]\n+\n+    del loaded\n+    gc.collect()\n+    # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n+    prefix = \"model.\"\n+    for key in state_dict.copy().keys():\n+        if not key.startswith(\"class_embed\") and not key.startswith(\"bbox_predictor\"):\n+            val = state_dict.pop(key)\n+            state_dict[prefix + key] = val\n+    # finally, create HuggingFace model and load state dict\n+    model = DabDetrForObjectDetection(config)\n+    model.load_state_dict(state_dict)\n+    model.eval()\n+    logger.info(f\"Saving PyTorch model to {pytorch_dump_folder_path}...\")\n+    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+    model.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        model.push_to_hub(repo_id=model_name, commit_message=\"Add new model\")\n+\n+\n+def convert_dab_detr_checkpoint(model_name, pretrained_model_weights_path, pytorch_dump_folder_path, push_to_hub):\n+    logger.info(\"Converting image processor...\")\n+    write_image_processor(model_name, pytorch_dump_folder_path, push_to_hub)\n+\n+    logger.info(f\"Converting model {model_name}...\")\n+    write_model(model_name, pretrained_model_weights_path, pytorch_dump_folder_path, push_to_hub)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"dab-detr-resnet-50\",\n+        type=str,\n+        help=\"Name of the DAB_DETR model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pretrained_model_weights_path\",\n+        default=\"modelzoo/R50/checkpoint.pth\",\n+        type=str,\n+        help=\"The path of the original model weights like: modelzoo/checkpoint.pth\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=\"DAB_DETR\", type=str, help=\"Path to the folder to output PyTorch model.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        default=True,\n+        type=bool,\n+        help=\"Whether to upload the converted weights and image processor config to the HuggingFace model profile. Default is set to false.\",\n+    )\n+    args = parser.parse_args()\n+    convert_dab_detr_checkpoint(\n+        args.model_name, args.pretrained_model_weights_path, args.pytorch_dump_folder_path, args.push_to_hub\n+    )"
        },
        {
            "sha": "09c83147b910006ff0943e6944a08963aaa24ac6",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "added",
            "additions": 1716,
            "deletions": 0,
            "changes": 1716,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,1716 @@\n+# coding=utf-8\n+# Copyright 2024 IDEA Research and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch DAB-DETR model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.backbone_utils import load_backbone\n+from .configuration_dab_detr import DabDetrConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"DabDetrConfig\"\n+_CHECKPOINT_FOR_DOC = \"IDEA-Research/dab_detr-base\"\n+\n+\n+@dataclass\n+# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n+class DabDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    \"\"\"\n+    Base class for outputs of the Conditional DETR decoder. This class adds one attribute to\n+    BaseModelOutputWithCrossAttentions, namely an optional stack of intermediate decoder activations, i.e. the output\n+    of each decoder layer, each of them gone through a layernorm. This is useful when training the model with auxiliary\n+    decoding losses.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+            plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+            the self-attention heads.\n+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+            used to compute the weighted average in the cross-attention heads.\n+        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+            layernorm.\n+        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    reference_points: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n+class DabDetrModelOutput(Seq2SeqModelOutput):\n+    \"\"\"\n+    Base class for outputs of the Conditional DETR encoder-decoder model. This class adds one attribute to\n+    Seq2SeqModelOutput, namely an optional stack of intermediate decoder activations, i.e. the output of each decoder\n+    layer, each of them gone through a layernorm. This is useful when training the model with auxiliary decoding\n+    losses.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n+            layer plus the initial embedding outputs.\n+        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n+            weighted average in the self-attention heads.\n+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+            used to compute the weighted average in the cross-attention heads.\n+        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n+            layer plus the initial embedding outputs.\n+        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n+            weighted average in the self-attention heads.\n+        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+            layernorm.\n+        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    reference_points: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+# Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->DabDetr\n+class DabDetrObjectDetectionOutput(ModelOutput):\n+    \"\"\"\n+    Output type of [`DabDetrForObjectDetection`].\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+            scale-invariant IoU loss.\n+        loss_dict (`Dict`, *optional*):\n+            A dictionary containing the individual losses. Useful for logging.\n+        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+            Classification logits (including no-object) for all queries.\n+        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+            possible padding). You can use [`~DabDetrImageProcessor.post_process_object_detection`] to retrieve the\n+            unnormalized bounding boxes.\n+        auxiliary_outputs (`list[Dict]`, *optional*):\n+            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+            `pred_boxes`) for each decoder layer.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n+            layer plus the initial embedding outputs.\n+        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n+            weighted average in the self-attention heads.\n+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+            used to compute the weighted average in the cross-attention heads.\n+        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n+            layer plus the initial embedding outputs.\n+        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n+            weighted average in the self-attention heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    loss_dict: Optional[Dict] = None\n+    logits: torch.FloatTensor = None\n+    pred_boxes: torch.FloatTensor = None\n+    auxiliary_outputs: Optional[List[Dict]] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+# Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->DabDetr\n+class DabDetrFrozenBatchNorm2d(nn.Module):\n+    \"\"\"\n+    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n+\n+    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n+    torchvision.models.resnet[18,34,50,101] produce nans.\n+    \"\"\"\n+\n+    def __init__(self, n):\n+        super().__init__()\n+        self.register_buffer(\"weight\", torch.ones(n))\n+        self.register_buffer(\"bias\", torch.zeros(n))\n+        self.register_buffer(\"running_mean\", torch.zeros(n))\n+        self.register_buffer(\"running_var\", torch.ones(n))\n+\n+    def _load_from_state_dict(\n+        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+    ):\n+        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n+        if num_batches_tracked_key in state_dict:\n+            del state_dict[num_batches_tracked_key]\n+\n+        super()._load_from_state_dict(\n+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+        )\n+\n+    def forward(self, x):\n+        # move reshapes to the beginning\n+        # to make it user-friendly\n+        weight = self.weight.reshape(1, -1, 1, 1)\n+        bias = self.bias.reshape(1, -1, 1, 1)\n+        running_var = self.running_var.reshape(1, -1, 1, 1)\n+        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n+        epsilon = 1e-5\n+        scale = weight * (running_var + epsilon).rsqrt()\n+        bias = bias - running_mean * scale\n+        return x * scale + bias\n+\n+\n+# Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->DabDetr\n+def replace_batch_norm(model):\n+    r\"\"\"\n+    Recursively replace all `torch.nn.BatchNorm2d` with `DabDetrFrozenBatchNorm2d`.\n+\n+    Args:\n+        model (torch.nn.Module):\n+            input model\n+    \"\"\"\n+    for name, module in model.named_children():\n+        if isinstance(module, nn.BatchNorm2d):\n+            new_module = DabDetrFrozenBatchNorm2d(module.num_features)\n+\n+            if not module.weight.device == torch.device(\"meta\"):\n+                new_module.weight.data.copy_(module.weight)\n+                new_module.bias.data.copy_(module.bias)\n+                new_module.running_mean.data.copy_(module.running_mean)\n+                new_module.running_var.data.copy_(module.running_var)\n+\n+            model._modules[name] = new_module\n+\n+        if len(list(module.children())) > 0:\n+            replace_batch_norm(module)\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrConvEncoder with Detr->DabDetr\n+class DabDetrConvEncoder(nn.Module):\n+    \"\"\"\n+    Convolutional backbone, using either the AutoBackbone API or one from the timm library.\n+\n+    nn.BatchNorm2d layers are replaced by DabDetrFrozenBatchNorm2d as defined above.\n+\n+    \"\"\"\n+\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__()\n+\n+        self.config = config\n+        backbone = load_backbone(config)\n+\n+        # replace batch norm by frozen batch norm\n+        with torch.no_grad():\n+            replace_batch_norm(backbone)\n+        self.model = backbone\n+        self.intermediate_channel_sizes = self.model.channels\n+\n+    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n+        # send pixel_values through the model to get list of feature maps\n+        features = self.model(pixel_values).feature_maps\n+\n+        out = []\n+        for feature_map in features:\n+            # downsample pixel_mask to match shape of corresponding feature_map\n+            mask = nn.functional.interpolate(pixel_mask[None].float(), size=feature_map.shape[-2:]).to(torch.bool)[0]\n+            out.append((feature_map, mask))\n+        return out\n+\n+\n+# Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->DabDetr\n+class DabDetrConvModel(nn.Module):\n+    \"\"\"\n+    This module adds 2D position embeddings to all intermediate feature maps of the convolutional encoder.\n+    \"\"\"\n+\n+    def __init__(self, conv_encoder, position_embedding):\n+        super().__init__()\n+        self.conv_encoder = conv_encoder\n+        self.position_embedding = position_embedding\n+\n+    def forward(self, pixel_values, pixel_mask):\n+        # send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\n+        out = self.conv_encoder(pixel_values, pixel_mask)\n+        pos = []\n+        for feature_map, mask in out:\n+            # position encoding\n+            pos.append(self.position_embedding(feature_map, mask).to(feature_map.dtype))\n+\n+        return out, pos\n+\n+\n+# Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrSinePositionEmbedding with ConditionalDetr->DabDetr\n+class DabDetrSinePositionEmbedding(nn.Module):\n+    \"\"\"\n+    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n+    need paper, generalized to work on images.\n+    \"\"\"\n+\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embedding_dim = config.hidden_size / 2\n+        self.temperature_height = config.temperature_height\n+        self.temperature_width = config.temperature_width\n+        scale = config.sine_position_embedding_scale\n+        if scale is None:\n+            scale = 2 * math.pi\n+        self.scale = scale\n+\n+    def forward(self, pixel_values, pixel_mask):\n+        if pixel_mask is None:\n+            raise ValueError(\"No pixel mask provided\")\n+        y_embed = pixel_mask.cumsum(1, dtype=torch.float32)\n+        x_embed = pixel_mask.cumsum(2, dtype=torch.float32)\n+        y_embed = y_embed / (y_embed[:, -1:, :] + 1e-6) * self.scale\n+        x_embed = x_embed / (x_embed[:, :, -1:] + 1e-6) * self.scale\n+\n+        # We use float32 to ensure reproducibility of the original implementation\n+        dim_tx = torch.arange(self.embedding_dim, dtype=torch.float32, device=pixel_values.device)\n+        # Modifying dim_tx in place to avoid extra memory allocation -> dim_tx = self.temperature_width ** (2 * (dim_tx // 2) / self.embedding_dim)\n+        dim_tx //= 2\n+        dim_tx.mul_(2 / self.embedding_dim)\n+        dim_tx.copy_(self.temperature_width**dim_tx)\n+        pos_x = x_embed[:, :, :, None] / dim_tx\n+\n+        # We use float32 to ensure reproducibility of the original implementation\n+        dim_ty = torch.arange(self.embedding_dim, dtype=torch.float32, device=pixel_values.device)\n+        # Modifying dim_ty in place to avoid extra memory allocation -> dim_ty = self.temperature_height ** (2 * (dim_ty // 2) / self.embedding_dim)\n+        dim_ty //= 2\n+        dim_ty.mul_(2 / self.embedding_dim)\n+        dim_ty.copy_(self.temperature_height**dim_ty)\n+        pos_y = y_embed[:, :, :, None] / dim_ty\n+\n+        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n+        return pos\n+\n+\n+# function to generate sine positional embedding for 4d coordinates\n+def gen_sine_position_embeddings(pos_tensor, hidden_size=256):\n+    \"\"\"\n+    This function computes position embeddings using sine and cosine functions from the input positional tensor,\n+    which has a shape of (batch_size, num_queries, 4).\n+    The last dimension of `pos_tensor` represents the following coordinates:\n+    - 0: x-coord\n+    - 1: y-coord\n+    - 2: width\n+    - 3: height\n+\n+    The output shape is (batch_size, num_queries, 512), where final dim (hidden_size*2 = 512) is the total embedding dimension\n+    achieved by concatenating the sine and cosine values for each coordinate.\n+    \"\"\"\n+    scale = 2 * math.pi\n+    dim = hidden_size // 2\n+    dim_t = torch.arange(dim, dtype=torch.float32, device=pos_tensor.device)\n+    dim_t = 10000 ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / dim)\n+    x_embed = pos_tensor[:, :, 0] * scale\n+    y_embed = pos_tensor[:, :, 1] * scale\n+    pos_x = x_embed[:, :, None] / dim_t\n+    pos_y = y_embed[:, :, None] / dim_t\n+    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n+    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n+    if pos_tensor.size(-1) == 4:\n+        w_embed = pos_tensor[:, :, 2] * scale\n+        pos_w = w_embed[:, :, None] / dim_t\n+        pos_w = torch.stack((pos_w[:, :, 0::2].sin(), pos_w[:, :, 1::2].cos()), dim=3).flatten(2)\n+\n+        h_embed = pos_tensor[:, :, 3] * scale\n+        pos_h = h_embed[:, :, None] / dim_t\n+        pos_h = torch.stack((pos_h[:, :, 0::2].sin(), pos_h[:, :, 1::2].cos()), dim=3).flatten(2)\n+\n+        pos = torch.cat((pos_y, pos_x, pos_w, pos_h), dim=2)\n+    else:\n+        raise ValueError(\"Unknown pos_tensor shape(-1):{}\".format(pos_tensor.size(-1)))\n+    return pos\n+\n+\n+def inverse_sigmoid(x, eps=1e-5):\n+    x = x.clamp(min=0, max=1)\n+    x1 = x.clamp(min=eps)\n+    x2 = (1 - x).clamp(min=eps)\n+    return torch.log(x1 / x2)\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrAttention\n+class DetrAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper.\n+\n+    Here, we add position embeddings to the queries and keys (as explained in the DETR paper).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: DabDetrConfig,\n+        bias: bool = True,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.encoder_attention_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.head_dim = self.hidden_size // self.num_heads\n+        if self.head_dim * self.num_heads != self.hidden_size:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n+        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n+        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        object_queries: Optional[torch.Tensor] = None,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        batch_size, q_len, embed_dim = hidden_states.size()\n+        # add position embeddings to the hidden states before projecting to queries and keys\n+        if object_queries is not None:\n+            hidden_states_original = hidden_states\n+            hidden_states = hidden_states + object_queries\n+\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states_original)\n+\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n+\n+        if attention_mask is not None:\n+            attn_weights = attn_weights + attention_mask\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(batch_size, q_len, embed_dim)\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+# Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrAttention with ConditionalDetr->DABDETR,Conditional DETR->DabDetr\n+class DabDetrAttention(nn.Module):\n+    \"\"\"\n+    Cross-Attention used in DAB-DETR 'DAB-DETR for Fast Training Convergence' paper.\n+\n+    The key q_proj, k_proj, v_proj are defined outside the attention. This attention allows the dim of q, k to be\n+    different to v.\n+    \"\"\"\n+\n+    def __init__(self, config: DabDetrConfig, bias: bool = True, is_cross: bool = False):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size * 2 if is_cross else config.hidden_size\n+        self.output_dim = config.hidden_size\n+        self.attention_heads = config.decoder_attention_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.attention_head_dim = self.embed_dim // self.attention_heads\n+        if self.attention_head_dim * self.attention_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `attention_heads`:\"\n+                f\" {self.attention_heads}).\"\n+            )\n+        # head dimension of values\n+        self.values_head_dim = self.output_dim // self.attention_heads\n+        if self.values_head_dim * self.attention_heads != self.output_dim:\n+            raise ValueError(\n+                f\"output_dim must be divisible by attention_heads (got `output_dim`: {self.output_dim} and `attention_heads`: {self.attention_heads}).\"\n+            )\n+        self.scaling = self.attention_head_dim**-0.5\n+        self.output_proj = nn.Linear(self.output_dim, self.output_dim, bias=bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        key_states: Optional[torch.Tensor] = None,\n+        value_states: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        # scaling query and refactor key-, value states\n+        query_states = hidden_states * self.scaling\n+        query_states = query_states.view(batch_size, -1, self.attention_heads, self.attention_head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, -1, self.attention_heads, self.attention_head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, -1, self.attention_heads, self.values_head_dim).transpose(1, 2)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n+\n+        if attention_mask is not None:\n+            attn_weights = attn_weights + attention_mask\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_probs, value_states)\n+\n+        if attn_output.size() != (batch_size, self.attention_heads, q_len, self.values_head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(batch_size, self.attention_heads, q_len, self.values_head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(batch_size, q_len, self.output_dim)\n+        attn_output = self.output_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+class DabDetrDecoderLayerSelfAttention(nn.Module):\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__()\n+        self.dropout = config.dropout\n+        self.self_attn_query_content_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.self_attn_query_pos_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.self_attn_key_content_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.self_attn_key_pos_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.self_attn_value_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.self_attn = DabDetrAttention(config)\n+        self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        query_position_embeddings: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        residual = hidden_states\n+        query_content = self.self_attn_query_content_proj(hidden_states)\n+        query_pos = self.self_attn_query_pos_proj(query_position_embeddings)\n+        key_content = self.self_attn_key_content_proj(hidden_states)\n+        key_pos = self.self_attn_key_pos_proj(query_position_embeddings)\n+        value = self.self_attn_value_proj(hidden_states)\n+\n+        query = query_content + query_pos\n+        key = key_content + key_pos\n+\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=query,\n+            attention_mask=attention_mask,\n+            key_states=key,\n+            value_states=value,\n+            output_attentions=True,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        return hidden_states, attn_weights\n+\n+\n+class DabDetrDecoderLayerCrossAttention(nn.Module):\n+    def __init__(self, config: DabDetrConfig, is_first: bool = False):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.cross_attn_query_content_proj = nn.Linear(hidden_size, hidden_size)\n+        self.cross_attn_query_pos_proj = nn.Linear(hidden_size, hidden_size)\n+        self.cross_attn_key_content_proj = nn.Linear(hidden_size, hidden_size)\n+        self.cross_attn_key_pos_proj = nn.Linear(hidden_size, hidden_size)\n+        self.cross_attn_value_proj = nn.Linear(hidden_size, hidden_size)\n+        self.cross_attn_query_pos_sine_proj = nn.Linear(hidden_size, hidden_size)\n+        self.decoder_attention_heads = config.decoder_attention_heads\n+        self.cross_attn_layer_norm = nn.LayerNorm(hidden_size)\n+        self.cross_attn = DabDetrAttention(config, is_cross=True)\n+\n+        self.keep_query_pos = config.keep_query_pos\n+\n+        if not self.keep_query_pos and not is_first:\n+            self.cross_attn_query_pos_proj = None\n+\n+        self.is_first = is_first\n+        self.dropout = config.dropout\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        query_position_embeddings: Optional[torch.Tensor] = None,\n+        object_queries: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        query_sine_embed: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        query_content = self.cross_attn_query_content_proj(hidden_states)\n+        key_content = self.cross_attn_key_content_proj(encoder_hidden_states)\n+        value = self.cross_attn_value_proj(encoder_hidden_states)\n+\n+        batch_size, num_queries, n_model = query_content.shape\n+        _, height_width, _ = key_content.shape\n+\n+        key_pos = self.cross_attn_key_pos_proj(object_queries)\n+\n+        # For the first decoder layer, we add the positional embedding predicted from\n+        # the object query (the positional embedding) into the original query (key) in DETR.\n+        if self.is_first or self.keep_query_pos:\n+            query_pos = self.cross_attn_query_pos_proj(query_position_embeddings)\n+            query = query_content + query_pos\n+            key = key_content + key_pos\n+        else:\n+            query = query_content\n+            key = key_content\n+\n+        query = query.view(\n+            batch_size, num_queries, self.decoder_attention_heads, n_model // self.decoder_attention_heads\n+        )\n+        query_sine_embed = self.cross_attn_query_pos_sine_proj(query_sine_embed)\n+        query_sine_embed = query_sine_embed.view(\n+            batch_size, num_queries, self.decoder_attention_heads, n_model // self.decoder_attention_heads\n+        )\n+        query = torch.cat([query, query_sine_embed], dim=3).view(batch_size, num_queries, n_model * 2)\n+        key = key.view(batch_size, height_width, self.decoder_attention_heads, n_model // self.decoder_attention_heads)\n+        key_pos = key_pos.view(\n+            batch_size, height_width, self.decoder_attention_heads, n_model // self.decoder_attention_heads\n+        )\n+        key = torch.cat([key, key_pos], dim=3).view(batch_size, height_width, n_model * 2)\n+\n+        # Cross-Attention Block\n+        cross_attn_weights = None\n+        if encoder_hidden_states is not None:\n+            residual = hidden_states\n+\n+            hidden_states, cross_attn_weights = self.cross_attn(\n+                hidden_states=query,\n+                attention_mask=encoder_attention_mask,\n+                key_states=key,\n+                value_states=value,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+            hidden_states = residual + hidden_states\n+            hidden_states = self.cross_attn_layer_norm(hidden_states)\n+\n+        return hidden_states, cross_attn_weights\n+\n+\n+class DabDetrDecoderLayerFFN(nn.Module):\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.final_layer_norm = nn.LayerNorm(hidden_size)\n+        self.fc1 = nn.Linear(hidden_size, config.decoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.decoder_ffn_dim, hidden_size)\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.dropout = config.dropout\n+        self.activation_dropout = config.activation_dropout\n+        self.keep_query_pos = config.keep_query_pos\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        return hidden_states\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrEncoderLayer with DetrEncoderLayer->DabDetrEncoderLayer,DetrConfig->DabDetrConfig\n+class DabDetrEncoderLayer(nn.Module):\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = DetrAttention(config)\n+        self.self_attn_layer_norm = nn.LayerNorm(self.hidden_size)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.fc1 = nn.Linear(self.hidden_size, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.hidden_size)\n+        self.final_layer_norm = nn.LayerNorm(self.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        object_queries: torch.Tensor,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            object_queries (`torch.FloatTensor`, *optional*):\n+                Object queries (also called content embeddings), to be added to the hidden states.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            object_queries=object_queries,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+# Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderLayer with ConditionalDetr->DabDetr\n+class DabDetrDecoderLayer(nn.Module):\n+    def __init__(self, config: DabDetrConfig, is_first: bool = False):\n+        super().__init__()\n+        self.self_attn = DabDetrDecoderLayerSelfAttention(config)\n+        self.cross_attn = DabDetrDecoderLayerCrossAttention(config, is_first)\n+        self.mlp = DabDetrDecoderLayerFFN(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        object_queries: Optional[torch.Tensor] = None,\n+        query_position_embeddings: Optional[torch.Tensor] = None,\n+        query_sine_embed: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            object_queries (`torch.FloatTensor`, *optional*):\n+                object_queries that are added to the queries and keys\n+            in the cross-attention layer.\n+            query_position_embeddings (`torch.FloatTensor`, *optional*):\n+                object_queries that are added to the queries and keys\n+            in the self-attention layer.\n+            encoder_hidden_states (`torch.FloatTensor`):\n+                cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n+            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n+                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+\n+        \"\"\"\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            query_position_embeddings=query_position_embeddings,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states, cross_attn_weights = self.cross_attn(\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            query_position_embeddings=query_position_embeddings,\n+            object_queries=object_queries,\n+            encoder_attention_mask=encoder_attention_mask,\n+            query_sine_embed=query_sine_embed,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = self.mlp(hidden_states=hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with DetrMLPPredictionHead->DabDetrMLP\n+class DabDetrMLP(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+\n+    \"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [hidden_dim] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, input_tensor):\n+        for i, layer in enumerate(self.layers):\n+            input_tensor = nn.functional.relu(layer(input_tensor)) if i < self.num_layers - 1 else layer(input_tensor)\n+        return input_tensor\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->DabDetr\n+class DabDetrPreTrainedModel(PreTrainedModel):\n+    config_class = DabDetrConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [r\"DabDetrConvEncoder\", r\"DabDetrEncoderLayer\", r\"DabDetrDecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+        xavier_std = self.config.init_xavier_std\n+\n+        if isinstance(module, DabDetrMHAttentionMap):\n+            nn.init.zeros_(module.k_linear.bias)\n+            nn.init.zeros_(module.q_linear.bias)\n+            nn.init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n+            nn.init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DabDetrForObjectDetection):\n+            nn.init.constant_(module.bbox_predictor.layers[-1].weight.data, 0)\n+            nn.init.constant_(module.bbox_predictor.layers[-1].bias.data, 0)\n+\n+            # init prior_prob setting for focal loss\n+            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+            bias_value = -math.log((1 - prior_prob) / prior_prob)\n+            module.class_embed.bias.data.fill_(bias_value)\n+\n+\n+DAB_DETR_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`DabDetrConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+DAB_DETR_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it.\n+\n+            Pixel values can be obtained using [`AutoImageProcessor`]. See [`DetrImageProcessor.__call__`]\n+            for details.\n+\n+        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n+            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n+\n+            - 1 for pixels that are real (i.e. **not masked**),\n+            - 0 for pixels that are padding (i.e. **masked**).\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+# Modified from transformers.models.detr.modeling_detr.DetrEncoder with Detr->DabDetr,DETR->ConditionalDETR\n+class DabDetrEncoder(DabDetrPreTrainedModel):\n+    \"\"\"\n+    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n+    [`DabDetrEncoderLayer`].\n+\n+    The encoder updates the flattened feature map through multiple self-attention layers.\n+\n+    Small tweak for DAB-DETR:\n+\n+    - object_queries are added to the forward pass.\n+\n+    Args:\n+        config: DabDetrConfig\n+    \"\"\"\n+\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__(config)\n+\n+        self.dropout = config.dropout\n+        self.query_scale = DabDetrMLP(config.hidden_size, config.hidden_size, config.hidden_size, 2)\n+        self.layers = nn.ModuleList([DabDetrEncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.norm = nn.LayerNorm(config.hidden_size) if config.normalize_before else None\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask,\n+        object_queries,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, batch_size, hidden_size)`):\n+                Flattened feature map (output of the backbone + projection layer) that is passed to the encoder.\n+\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding pixel features. Mask values selected in `[0, 1]`:\n+\n+                - 1 for pixel features that are real (i.e. **not masked**),\n+                - 0 for pixel features that are padding (i.e. **masked**).\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+\n+            object_queries (`torch.FloatTensor` of shape `(sequence_length, batch_size, hidden_size)`):\n+                Object queries that are added to the queries in each self-attention layer.\n+\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = inputs_embeds\n+\n+        # expand attention_mask\n+        if attention_mask is not None:\n+            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            # pos scaler\n+            pos_scales = self.query_scale(hidden_states)\n+            # we add object_queries * pos_scaler as extra input to the encoder_layer\n+            scaled_object_queries = object_queries * pos_scales\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    scaled_object_queries,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    object_queries=scaled_object_queries,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if self.norm:\n+            hidden_states = self.norm(hidden_states)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+\n+# Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoder with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR\n+class DabDetrDecoder(DabDetrPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`DabDetrDecoderLayer`].\n+\n+    The decoder updates the query embeddings through multiple self-attention and cross-attention layers.\n+\n+    Some small tweaks for DAB-DETR:\n+\n+    - object_queries and query_position_embeddings are added to the forward pass.\n+    - if self.config.auxiliary_loss is set to True, also returns a stack of activations from all decoding layers.\n+\n+    Args:\n+        config: DabDetrConfig\n+    \"\"\"\n+\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.dropout = config.dropout\n+        self.num_layers = config.decoder_layers\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [DabDetrDecoderLayer(config, is_first=(layer_id == 0)) for layer_id in range(config.decoder_layers)]\n+        )\n+        # in DAB-DETR, the decoder uses layernorm after the last decoder layer output\n+        self.hidden_size = config.hidden_size\n+        self.layernorm = nn.LayerNorm(self.hidden_size)\n+\n+        # Default cond-elewise\n+        self.query_scale = DabDetrMLP(self.hidden_size, self.hidden_size, self.hidden_size, 2)\n+\n+        self.ref_point_head = DabDetrMLP(\n+            config.query_dim // 2 * self.hidden_size, self.hidden_size, self.hidden_size, 2\n+        )\n+\n+        self.bbox_embed = None\n+\n+        # Default decoder_modulate_hw_attn is True\n+        self.ref_anchor_head = DabDetrMLP(self.hidden_size, self.hidden_size, 2, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        encoder_hidden_states,\n+        memory_key_padding_mask,\n+        object_queries,\n+        query_position_embeddings,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(sequence_length, batch_size, hidden_size)`):\n+                The query embeddings that are passed into the decoder.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(encoder_sequence_length, batch_size, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+            memory_key_padding_mask (`torch.Tensor.bool` of shape `(batch_size, sequence_length)`):\n+                The memory_key_padding_mask indicates which positions in the memory (encoder outputs) should be ignored during the attention computation,\n+                ensuring padding tokens do not influence the attention mechanism.\n+            object_queries (`torch.FloatTensor` of shape `(sequence_length, batch_size, hidden_size)`, *optional*):\n+                Position embeddings that are added to the queries and keys in each cross-attention layer.\n+            query_position_embeddings (`torch.FloatTensor` of shape `(num_queries, batch_size, number_of_anchor_points)`):\n+                Position embeddings that are added to the queries and keys in each self-attention layer.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is not None:\n+            hidden_states = inputs_embeds\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+\n+        intermediate = []\n+        reference_points = query_position_embeddings.sigmoid()\n+        ref_points = [reference_points]\n+\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and memory_key_padding_mask is not None:\n+            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n+            memory_key_padding_mask = _prepare_4d_attention_mask(\n+                memory_key_padding_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+            )\n+\n+        for layer_id, decoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            obj_center = reference_points[..., : self.config.query_dim]\n+            query_sine_embed = gen_sine_position_embeddings(obj_center, self.hidden_size)\n+            query_pos = self.ref_point_head(query_sine_embed)\n+\n+            # For the first decoder layer, we do not apply transformation over p_s\n+            pos_transformation = 1 if layer_id == 0 else self.query_scale(hidden_states)\n+\n+            # apply transformation\n+            query_sine_embed = query_sine_embed[..., : self.hidden_size] * pos_transformation\n+\n+            # modulated Height Width attentions\n+            reference_anchor_size = self.ref_anchor_head(hidden_states).sigmoid()  # nq, bs, 2\n+            query_sine_embed[..., self.hidden_size // 2 :] *= (\n+                reference_anchor_size[..., 0] / obj_center[..., 2]\n+            ).unsqueeze(-1)\n+            query_sine_embed[..., : self.hidden_size // 2] *= (\n+                reference_anchor_size[..., 1] / obj_center[..., 3]\n+            ).unsqueeze(-1)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    None,\n+                    object_queries,\n+                    query_pos,\n+                    query_sine_embed,\n+                    encoder_hidden_states,\n+                    memory_key_padding_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=None,\n+                    object_queries=object_queries,\n+                    query_position_embeddings=query_pos,\n+                    query_sine_embed=query_sine_embed,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_attention_mask=memory_key_padding_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            # iter update\n+            hidden_states = layer_outputs[0]\n+\n+            if self.bbox_embed is not None:\n+                new_reference_points = self.bbox_embed(hidden_states)\n+\n+                new_reference_points[..., : self.config.query_dim] += inverse_sigmoid(reference_points)\n+                new_reference_points = new_reference_points[..., : self.config.query_dim].sigmoid()\n+                if layer_id != self.num_layers - 1:\n+                    ref_points.append(new_reference_points)\n+                reference_points = new_reference_points.detach()\n+\n+            intermediate.append(self.layernorm(hidden_states))\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (layer_outputs[2],)\n+\n+        # Layer normalization on hidden states\n+        hidden_states = self.layernorm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output_intermediate_hidden_states = torch.stack(intermediate)\n+        output_reference_points = torch.stack(ref_points)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                    output_intermediate_hidden_states,\n+                    output_reference_points,\n+                ]\n+                if v is not None\n+            )\n+        return DabDetrDecoderOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+            intermediate_hidden_states=output_intermediate_hidden_states,\n+            reference_points=output_reference_points,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The bare DAB-DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n+    hidden-states, intermediate hidden states, reference points, output coordinates without any specific head on top.\n+    \"\"\",\n+    DAB_DETR_START_DOCSTRING,\n+)\n+class DabDetrModel(DabDetrPreTrainedModel):\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__(config)\n+\n+        self.auxiliary_loss = config.auxiliary_loss\n+\n+        # Create backbone + positional encoding\n+        self.backbone = DabDetrConvEncoder(config)\n+        object_queries = DabDetrSinePositionEmbedding(config)\n+\n+        self.query_refpoint_embeddings = nn.Embedding(config.num_queries, config.query_dim)\n+        self.random_refpoints_xy = config.random_refpoints_xy\n+        if self.random_refpoints_xy:\n+            self.query_refpoint_embeddings.weight.data[:, :2].uniform_(0, 1)\n+            self.query_refpoint_embeddings.weight.data[:, :2] = inverse_sigmoid(\n+                self.query_refpoint_embeddings.weight.data[:, :2]\n+            )\n+            self.query_refpoint_embeddings.weight.data[:, :2].requires_grad = False\n+\n+        # Create projection layer\n+        self.input_projection = nn.Conv2d(\n+            self.backbone.intermediate_channel_sizes[-1], config.hidden_size, kernel_size=1\n+        )\n+        self.backbone = DabDetrConvModel(self.backbone, object_queries)\n+\n+        self.encoder = DabDetrEncoder(config)\n+        self.decoder = DabDetrDecoder(config)\n+\n+        # decoder related variables\n+        self.hidden_size = config.hidden_size\n+        self.num_queries = config.num_queries\n+\n+        self.num_patterns = config.num_patterns\n+        if not isinstance(self.num_patterns, int):\n+            logger.warning(\"num_patterns should be int but {}\".format(type(self.num_patterns)))\n+            self.num_patterns = 0\n+        if self.num_patterns > 0:\n+            self.patterns = nn.Embedding(self.num_patterns, self.hidden_size)\n+\n+        self.aux_loss = config.auxiliary_loss\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def freeze_backbone(self):\n+        for name, param in self.backbone.conv_encoder.model.named_parameters():\n+            param.requires_grad_(False)\n+\n+    def unfreeze_backbone(self):\n+        for name, param in self.backbone.conv_encoder.model.named_parameters():\n+            param.requires_grad_(True)\n+\n+    @add_start_docstrings_to_model_forward(DAB_DETR_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=DabDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.FloatTensor], DabDetrModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoModel\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"IDEA-Research/dab_detr-base\")\n+        >>> model = AutoModel.from_pretrained(\"IDEA-Research/dab_detr-base\")\n+\n+        >>> # prepare image for the model\n+        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        >>> # forward pass\n+        >>> outputs = model(**inputs)\n+\n+        >>> # the last hidden states are the final query embeddings of the Transformer decoder\n+        >>> # these are of shape (batch_size, num_queries, hidden_size)\n+        >>> last_hidden_states = outputs.last_hidden_state\n+        >>> list(last_hidden_states.shape)\n+        [1, 300, 256]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        batch_size, _, height, width = pixel_values.shape\n+        device = pixel_values.device\n+\n+        if pixel_mask is None:\n+            pixel_mask = torch.ones(((batch_size, height, width)), device=device)\n+\n+        # First, sent pixel_values + pixel_mask through Backbone to obtain the features\n+        # pixel_values should be of shape (batch_size, num_channels, height, width)\n+        # pixel_mask should be of shape (batch_size, height, width)\n+        features, object_queries_list = self.backbone(pixel_values, pixel_mask)\n+\n+        # get final feature map and downsampled mask\n+        feature_map, mask = features[-1]\n+\n+        if mask is None:\n+            raise ValueError(\"Backbone does not return downsampled pixel mask\")\n+\n+        flattened_mask = mask.flatten(1)\n+\n+        # Second, apply 1x1 convolution to reduce the channel dimension to hidden_size (256 by default)\n+        projected_feature_map = self.input_projection(feature_map)\n+\n+        # Third, flatten the feature map + object_queries of shape NxCxHxW to HWxNxC, and permute it to NxHWxC\n+        # In other words, turn their shape into ( sequence_length, batch_size, hidden_size)\n+        flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n+        object_queries = object_queries_list[-1].flatten(2).permute(0, 2, 1)\n+        reference_position_embeddings = self.query_refpoint_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n+\n+        # Fourth, sent flattened_features + flattened_mask + object_queries through encoder\n+        # flattened_features is a Tensor of shape (heigth*width, batch_size, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                inputs_embeds=flattened_features,\n+                attention_mask=flattened_mask,\n+                object_queries=object_queries,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n+        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # Fifth, sent query embeddings + object_queries through the decoder (which is conditioned on the encoder output)\n+        num_queries = reference_position_embeddings.shape[1]\n+        if self.num_patterns == 0:\n+            queries = torch.zeros(batch_size, num_queries, self.hidden_size, device=device)\n+        else:\n+            queries = (\n+                self.patterns.weight[:, None, None, :]\n+                .repeat(1, self.num_queries, batch_size, 1)\n+                .flatten(0, 1)\n+                .permute(1, 0, 2)\n+            )  # bs, n_q*n_pat, hidden_size\n+            reference_position_embeddings = reference_position_embeddings.repeat(\n+                1, self.num_patterns, 1\n+            )  # bs, n_q*n_pat,  hidden_size\n+\n+        # decoder outputs consists of (dec_features, dec_hidden, dec_attn)\n+        decoder_outputs = self.decoder(\n+            inputs_embeds=queries,\n+            query_position_embeddings=reference_position_embeddings,\n+            object_queries=object_queries,\n+            encoder_hidden_states=encoder_outputs[0],\n+            memory_key_padding_mask=flattened_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if not return_dict:\n+            # last_hidden_state\n+            output = (decoder_outputs[0],)\n+            reference_points = decoder_outputs[-1]\n+            intermediate_hidden_states = decoder_outputs[-2]\n+\n+            # it has to follow the order of DABDETRModelOutput that is based on ModelOutput\n+            # If we only use one of the variables then the indexing will change.\n+            # E.g: if we return everything then 'decoder_attentions' is decoder_outputs[2], if we only use output_attentions then its decoder_outputs[1]\n+            if output_hidden_states and output_attentions:\n+                output += (\n+                    decoder_outputs[1],\n+                    decoder_outputs[2],\n+                    decoder_outputs[3],\n+                    encoder_outputs[0],\n+                    encoder_outputs[1],\n+                    encoder_outputs[2],\n+                )\n+            elif output_hidden_states:\n+                # decoder_hidden_states, encoder_last_hidden_state, encoder_hidden_states\n+                output += (\n+                    decoder_outputs[1],\n+                    encoder_outputs[0],\n+                    encoder_outputs[1],\n+                )\n+            elif output_attentions:\n+                # decoder_self_attention, decoder_cross_attention, encoder_attentions\n+                output += (\n+                    decoder_outputs[1],\n+                    decoder_outputs[2],\n+                    encoder_outputs[1],\n+                )\n+\n+            output += (intermediate_hidden_states, reference_points)\n+\n+            return output\n+\n+        reference_points = decoder_outputs.reference_points\n+        intermediate_hidden_states = decoder_outputs.intermediate_hidden_states\n+\n+        return DabDetrModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            decoder_hidden_states=decoder_outputs.hidden_states if output_hidden_states else None,\n+            decoder_attentions=decoder_outputs.attentions if output_attentions else None,\n+            cross_attentions=decoder_outputs.cross_attentions if output_attentions else None,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state if output_hidden_states else None,\n+            encoder_hidden_states=encoder_outputs.hidden_states if output_hidden_states else None,\n+            encoder_attentions=encoder_outputs.attentions if output_attentions else None,\n+            intermediate_hidden_states=intermediate_hidden_states,\n+            reference_points=reference_points,\n+        )\n+\n+\n+# Copied from transformers.models.detr.modeling_detr.DetrMHAttentionMap with Detr->DabDetr\n+class DabDetrMHAttentionMap(nn.Module):\n+    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n+\n+    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True, std=None):\n+        super().__init__()\n+        self.num_heads = num_heads\n+        self.hidden_dim = hidden_dim\n+        self.dropout = nn.Dropout(dropout)\n+\n+        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n+        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n+\n+        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n+\n+    def forward(self, q, k, mask: Optional[Tensor] = None):\n+        q = self.q_linear(q)\n+        k = nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n+        queries_per_head = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n+        keys_per_head = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n+        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)\n+\n+        if mask is not None:\n+            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n+        weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n+        weights = self.dropout(weights)\n+        return weights\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    DAB_DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n+    top, for tasks such as COCO detection.\n+    \"\"\",\n+    DAB_DETR_START_DOCSTRING,\n+)\n+class DabDetrForObjectDetection(DabDetrPreTrainedModel):\n+    # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n+    _tied_weights_keys = [\n+        r\"bbox_predictor\\.layers\\.\\d+\\.(weight|bias)\",\n+        r\"model\\.decoder\\.bbox_embed\\.layers\\.\\d+\\.(weight|bias)\",\n+    ]\n+\n+    def __init__(self, config: DabDetrConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.auxiliary_loss = config.auxiliary_loss\n+        self.query_dim = config.query_dim\n+        # DAB-DETR encoder-decoder model\n+        self.model = DabDetrModel(config)\n+\n+        _bbox_embed = DabDetrMLP(config.hidden_size, config.hidden_size, 4, 3)\n+        # Object detection heads\n+        self.class_embed = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Default bbox_embed_diff_each_layer is False\n+        self.bbox_predictor = _bbox_embed\n+\n+        # Default iter_update is True\n+        self.model.decoder.bbox_embed = self.bbox_predictor\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    # taken from https://github.com/Atten4Vis/conditionalDETR/blob/master/models/dab_detr.py\n+    @torch.jit.unused\n+    def _set_aux_loss(self, outputs_class, outputs_coord):\n+        # this is a workaround to make torchscript happy, as torchscript\n+        # doesn't support dictionary with non-homogeneous values, such\n+        # as a dict having both a Tensor and a list.\n+        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n+\n+    @add_start_docstrings_to_model_forward(DAB_DETR_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=DabDetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[List[dict]] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.FloatTensor], DabDetrObjectDetectionOutput]:\n+        r\"\"\"\n+        labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n+            Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n+            following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n+            respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n+            in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n+\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoModelForObjectDetection\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n+        >>> model = AutoModelForObjectDetection.from_pretrained(\"IDEA-Research/dab-detr-resnet-50\")\n+\n+        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        >>>     outputs = model(**inputs)\n+\n+        >>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n+        >>> target_sizes = torch.tensor([(image.height, image.width)])\n+        >>> results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n+        >>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n+        ...     box = [round(i, 2) for i in box.tolist()]\n+        ...     print(\n+        ...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n+        ...         f\"{round(score.item(), 3)} at location {box}\"\n+        ...     )\n+        Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\n+        Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\n+        Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\n+        Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\n+        Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # First, sent images through DAB_DETR base model to obtain encoder + decoder outputs\n+        model_outputs = self.model(\n+            pixel_values,\n+            pixel_mask=pixel_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+            encoder_outputs=encoder_outputs,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        reference_points = model_outputs.reference_points if return_dict else model_outputs[-1]\n+        intermediate_hidden_states = model_outputs.intermediate_hidden_states if return_dict else model_outputs[-2]\n+\n+        # class logits + predicted bounding boxes\n+        logits = self.class_embed(intermediate_hidden_states[-1])\n+\n+        reference_before_sigmoid = inverse_sigmoid(reference_points)\n+        bbox_with_refinement = self.bbox_predictor(intermediate_hidden_states)\n+        bbox_with_refinement[..., : self.query_dim] += reference_before_sigmoid\n+        outputs_coord = bbox_with_refinement.sigmoid()\n+\n+        pred_boxes = outputs_coord[-1]\n+\n+        loss, loss_dict, auxiliary_outputs = None, None, None\n+        if labels is not None:\n+            outputs_class = None\n+            if self.config.auxiliary_loss:\n+                outputs_class = self.class_embed(intermediate_hidden_states)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+            )\n+\n+        if not return_dict:\n+            if auxiliary_outputs is not None:\n+                output = (logits, pred_boxes) + auxiliary_outputs + model_outputs\n+            else:\n+                output = (logits, pred_boxes) + model_outputs\n+            # Since DabDetrObjectDetectionOutput doesn't have reference points + intermedieate_hidden_states we cut down.\n+            return ((loss, loss_dict) + output) if loss is not None else output[:-2]\n+\n+        return DabDetrObjectDetectionOutput(\n+            loss=loss,\n+            loss_dict=loss_dict,\n+            logits=logits,\n+            pred_boxes=pred_boxes,\n+            auxiliary_outputs=auxiliary_outputs,\n+            last_hidden_state=model_outputs.last_hidden_state,\n+            decoder_hidden_states=model_outputs.decoder_hidden_states if output_hidden_states else None,\n+            decoder_attentions=model_outputs.decoder_attentions if output_attentions else None,\n+            cross_attentions=model_outputs.cross_attentions if output_attentions else None,\n+            encoder_last_hidden_state=model_outputs.encoder_last_hidden_state if output_hidden_states else None,\n+            encoder_hidden_states=model_outputs.encoder_hidden_states if output_hidden_states else None,\n+            encoder_attentions=model_outputs.encoder_attentions if output_attentions else None,\n+        )\n+\n+\n+__all__ = [\n+    \"DabDetrForObjectDetection\",\n+    \"DabDetrModel\",\n+    \"DabDetrPreTrainedModel\",\n+]"
        },
        {
            "sha": "3dd37c36a4ae843b93b5ae165c7d94937b5f3449",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -52,7 +52,7 @@ class DetrConfig(PretrainedConfig):\n             Number of object queries, i.e. detection slots. This is the maximal number of objects [`DetrModel`] can\n             detect in a single image. For COCO, we recommend 100 queries.\n         d_model (`int`, *optional*, defaults to 256):\n-            Dimension of the layers.\n+            This parameter is a general dimension parameter, defining dimensions for components such as the encoder layer and projection parameters in the decoder layer, among others.\n         encoder_layers (`int`, *optional*, defaults to 6):\n             Number of encoder layers.\n         decoder_layers (`int`, *optional*, defaults to 6):"
        },
        {
            "sha": "349ad988df40675f9ea84c048237c34616b3a9b3",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -2482,6 +2482,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class DabDetrForObjectDetection(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DabDetrModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DabDetrPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class DacModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/dab_detr/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/tests%2Fmodels%2Fdab_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/tests%2Fmodels%2Fdab_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2F__init__.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa"
        },
        {
            "sha": "d3d70d67d4c334224418a311b12e9d4b3663ac2b",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "added",
            "additions": 839,
            "deletions": 0,
            "changes": 839,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -0,0 +1,839 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DAB-DETR model.\"\"\"\n+\n+import inspect\n+import math\n+import unittest\n+from typing import Dict, List, Tuple\n+\n+from transformers import DabDetrConfig, ResNetConfig, is_torch_available, is_vision_available\n+from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn.functional as F\n+\n+    from transformers import (\n+        DabDetrForObjectDetection,\n+        DabDetrModel,\n+    )\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import ConditionalDetrImageProcessor\n+\n+\n+class DabDetrModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=8,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        intermediate_size=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        num_queries=12,\n+        num_channels=3,\n+        min_size=200,\n+        max_size=200,\n+        n_targets=8,\n+        num_labels=91,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.num_queries = num_queries\n+        self.num_channels = num_channels\n+        self.min_size = min_size\n+        self.max_size = max_size\n+        self.n_targets = n_targets\n+        self.num_labels = num_labels\n+\n+        # we also set the expected seq length for both encoder and decoder\n+        self.encoder_seq_length = math.ceil(self.min_size / 32) * math.ceil(self.max_size / 32)\n+        self.decoder_seq_length = self.num_queries\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.min_size, self.max_size])\n+\n+        pixel_mask = torch.ones([self.batch_size, self.min_size, self.max_size], device=torch_device)\n+\n+        labels = None\n+        if self.use_labels:\n+            # labels is a list of Dict (each Dict being the labels for a given example in the batch)\n+            labels = []\n+            for i in range(self.batch_size):\n+                target = {}\n+                target[\"class_labels\"] = torch.randint(\n+                    high=self.num_labels, size=(self.n_targets,), device=torch_device\n+                )\n+                target[\"boxes\"] = torch.rand(self.n_targets, 4, device=torch_device)\n+                target[\"masks\"] = torch.rand(self.n_targets, self.min_size, self.max_size, device=torch_device)\n+                labels.append(target)\n+\n+        config = self.get_config()\n+        return config, pixel_values, pixel_mask, labels\n+\n+    def get_config(self):\n+        resnet_config = ResNetConfig(\n+            num_channels=3,\n+            embeddings_size=10,\n+            hidden_sizes=[10, 20, 30, 40],\n+            depths=[1, 1, 2, 1],\n+            hidden_act=\"relu\",\n+            num_labels=3,\n+            out_features=[\"stage2\", \"stage3\", \"stage4\"],\n+            out_indices=[2, 3, 4],\n+        )\n+        return DabDetrConfig(\n+            hidden_size=self.hidden_size,\n+            encoder_layers=self.num_hidden_layers,\n+            decoder_layers=self.num_hidden_layers,\n+            encoder_attention_heads=self.num_attention_heads,\n+            decoder_attention_heads=self.num_attention_heads,\n+            encoder_ffn_dim=self.intermediate_size,\n+            decoder_ffn_dim=self.intermediate_size,\n+            dropout=self.hidden_dropout_prob,\n+            attention_dropout=self.attention_probs_dropout_prob,\n+            num_queries=self.num_queries,\n+            num_labels=self.num_labels,\n+            use_timm_backbone=False,\n+            backbone_config=resnet_config,\n+            backbone=None,\n+            use_pretrained_backbone=False,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_mask, labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask}\n+        return config, inputs_dict\n+\n+    def create_and_check_dab_detr_model(self, config, pixel_values, pixel_mask, labels):\n+        model = DabDetrModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.decoder_seq_length, self.hidden_size)\n+        )\n+\n+    def create_and_check_dab_detr_object_detection_head_model(self, config, pixel_values, pixel_mask, labels):\n+        model = DabDetrForObjectDetection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+        result = model(pixel_values=pixel_values, labels=labels)\n+\n+        self.parent.assertEqual(result.loss.shape, ())\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+\n+@require_torch\n+class DabDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            DabDetrModel,\n+            DabDetrForObjectDetection,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"image-feature-extraction\": DabDetrModel,\n+            \"object-detection\": DabDetrForObjectDetection,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    is_encoder_decoder = True\n+    test_torchscript = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+    zero_init_hidden_state = True\n+\n+    # special case for head models\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        if return_labels:\n+            if model_class.__name__ in [\"DabDetrForObjectDetection\"]:\n+                labels = []\n+                for i in range(self.model_tester.batch_size):\n+                    target = {}\n+                    target[\"class_labels\"] = torch.ones(\n+                        size=(self.model_tester.n_targets,), device=torch_device, dtype=torch.long\n+                    )\n+                    target[\"boxes\"] = torch.ones(\n+                        self.model_tester.n_targets, 4, device=torch_device, dtype=torch.float\n+                    )\n+                    target[\"masks\"] = torch.ones(\n+                        self.model_tester.n_targets,\n+                        self.model_tester.min_size,\n+                        self.model_tester.max_size,\n+                        device=torch_device,\n+                        dtype=torch.float,\n+                    )\n+                    labels.append(target)\n+                inputs_dict[\"labels\"] = labels\n+\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = DabDetrModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DabDetrConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_dab_detr_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_dab_detr_model(*config_and_inputs)\n+\n+    def test_dab_detr_object_detection_head_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_dab_detr_object_detection_head_model(*config_and_inputs)\n+\n+    # TODO: check if this works again for PyTorch 2.x.y\n+    @unittest.skip(reason=\"Got `CUDA error: misaligned address` with PyTorch 2.0.0.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR does not use inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR does not use inputs_embeds\")\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR does not have a get_input_embeddings method\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR is not a generative model\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DETR does not use token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @slow\n+    def test_model_outputs_equivalence(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def set_nan_tensor_to_zero(t):\n+            print(t)\n+            t[t != t] = 0\n+            return t\n+\n+        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n+            with torch.no_grad():\n+                tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)\n+                dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n+\n+                def recursive_check(tuple_object, dict_object):\n+                    if isinstance(tuple_object, (List, Tuple)):\n+                        for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif isinstance(tuple_object, Dict):\n+                        for tuple_iterable_value, dict_iterable_value in zip(\n+                            tuple_object.values(), dict_object.values()\n+                        ):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif tuple_object is None:\n+                        return\n+                    else:\n+                        torch.testing.assert_close(\n+                            set_nan_tensor_to_zero(tuple_object),\n+                            set_nan_tensor_to_zero(dict_object),\n+                            atol=1e-5,\n+                            rtol=1e-5,\n+                            msg=(\n+                                \"Tuple and dict output are not equal. Difference:\"\n+                                f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"\n+                                f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"\n+                                f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"\n+                            ),\n+                        )\n+\n+                recursive_check(tuple_output, dict_output)\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n+\n+            if self.has_attentions:\n+                tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                check_equivalence(model, tuple_inputs, dict_inputs, {\"output_attentions\": True})\n+\n+                tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                check_equivalence(model, tuple_inputs, dict_inputs, {\"output_attentions\": True})\n+\n+                tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                check_equivalence(\n+                    model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True, \"output_attentions\": True}\n+                )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            if hasattr(self.model_tester, \"encoder_seq_length\"):\n+                seq_length = self.model_tester.encoder_seq_length\n+                if hasattr(self.model_tester, \"chunk_length\") and self.model_tester.chunk_length > 1:\n+                    seq_length = seq_length * self.model_tester.chunk_length\n+            else:\n+                seq_length = self.model_tester.seq_length\n+\n+            self.assertListEqual(\n+                [hidden_states[0].shape[1], hidden_states[0].shape[2]],\n+                [seq_length, self.model_tester.hidden_size],\n+            )\n+\n+            if config.is_encoder_decoder:\n+                hidden_states = outputs.decoder_hidden_states\n+\n+                self.assertIsInstance(hidden_states, (list, tuple))\n+\n+                self.assertEqual(len(hidden_states), expected_num_layers)\n+                seq_len = getattr(self.model_tester, \"seq_length\", None)\n+                decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", seq_len)\n+\n+                self.assertListEqual(\n+                    [hidden_states[0].shape[1], hidden_states[0].shape[2]],\n+                    [decoder_seq_length, self.model_tester.hidden_size],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    # Had to modify the threshold to 2 decimals instead of 3 because sometimes it threw an error\n+    def test_batching_equivalence(self):\n+        \"\"\"\n+        Tests that the model supports batching and that the output is the nearly the same for the same input in\n+        different batch sizes.\n+        (Why \"nearly the same\" not \"exactly the same\"? Batching uses different matmul shapes, which often leads to\n+        different results: https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535)\n+        \"\"\"\n+\n+        def get_tensor_equivalence_function(batched_input):\n+            # models operating on continuous spaces have higher abs difference than LMs\n+            # instead, we can rely on cos distance for image/speech models, similar to `diffusers`\n+            if \"input_ids\" not in batched_input:\n+                return lambda tensor1, tensor2: (\n+                    1.0 - F.cosine_similarity(tensor1.float().flatten(), tensor2.float().flatten(), dim=0, eps=1e-38)\n+                )\n+            return lambda tensor1, tensor2: torch.max(torch.abs(tensor1 - tensor2))\n+\n+        def recursive_check(batched_object, single_row_object, model_name, key):\n+            if isinstance(batched_object, (list, tuple)):\n+                for batched_object_value, single_row_object_value in zip(batched_object, single_row_object):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            elif isinstance(batched_object, dict):\n+                for batched_object_value, single_row_object_value in zip(\n+                    batched_object.values(), single_row_object.values()\n+                ):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            # do not compare returned loss (0-dim tensor) / codebook ids (int) / caching objects\n+            elif batched_object is None or not isinstance(batched_object, torch.Tensor):\n+                return\n+            elif batched_object.dim() == 0:\n+                return\n+            else:\n+                # indexing the first element does not always work\n+                # e.g. models that output similarity scores of size (N, M) would need to index [0, 0]\n+                slice_ids = [slice(0, index) for index in single_row_object.shape]\n+                batched_row = batched_object[slice_ids]\n+                self.assertFalse(\n+                    torch.isnan(batched_row).any(), f\"Batched output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(batched_row).any(), f\"Batched output has `inf` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isnan(single_row_object).any(), f\"Single row output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(single_row_object).any(), f\"Single row output has `inf` in {model_name} for key={key}\"\n+                )\n+                self.assertTrue(\n+                    (equivalence(batched_row, single_row_object)) <= 1e-02,\n+                    msg=(\n+                        f\"Batched and Single row outputs are not equal in {model_name} for key={key}. \"\n+                        f\"Difference={equivalence(batched_row, single_row_object)}.\"\n+                    ),\n+                )\n+\n+        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n+        equivalence = get_tensor_equivalence_function(batched_input)\n+\n+        for model_class in self.all_model_classes:\n+            config.output_hidden_states = True\n+\n+            model_name = model_class.__name__\n+            if hasattr(self.model_tester, \"prepare_config_and_inputs_for_model_class\"):\n+                config, batched_input = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)\n+            batched_input_prepared = self._prepare_for_class(batched_input, model_class)\n+            model = model_class(config).to(torch_device).eval()\n+\n+            batch_size = self.model_tester.batch_size\n+            single_row_input = {}\n+            for key, value in batched_input_prepared.items():\n+                if isinstance(value, torch.Tensor) and value.shape[0] % batch_size == 0:\n+                    # e.g. musicgen has inputs of size (bs*codebooks). in most cases value.shape[0] == batch_size\n+                    single_batch_shape = value.shape[0] // batch_size\n+                    single_row_input[key] = value[:single_batch_shape]\n+                else:\n+                    single_row_input[key] = value\n+\n+            with torch.no_grad():\n+                model_batched_output = model(**batched_input_prepared)\n+                model_row_output = model(**single_row_input)\n+\n+            if isinstance(model_batched_output, torch.Tensor):\n+                model_batched_output = {\"model_output\": model_batched_output}\n+                model_row_output = {\"model_output\": model_row_output}\n+\n+            for key in model_batched_output:\n+                # DETR starts from zero-init queries to decoder, leading to cos_similarity = `nan`\n+                if hasattr(self, \"zero_init_hidden_state\") and \"decoder_hidden_states\" in key:\n+                    model_batched_output[key] = model_batched_output[key][1:]\n+                    model_row_output[key] = model_row_output[key][1:]\n+                recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        decoder_seq_length = self.model_tester.decoder_seq_length\n+        encoder_seq_length = self.model_tester.encoder_seq_length\n+        decoder_key_length = self.model_tester.decoder_seq_length\n+        encoder_key_length = self.model_tester.encoder_seq_length\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_attentions = True\n+            config.output_hidden_states = False\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+            out_len = len(outputs)\n+            if self.is_encoder_decoder:\n+                correct_outlen = 6\n+\n+                # loss is at first position\n+                if \"labels\" in inputs_dict:\n+                    correct_outlen += 1  # loss is added to beginning\n+                if \"past_key_values\" in outputs:\n+                    correct_outlen += 1  # past_key_values have been returned\n+\n+                self.assertEqual(out_len, correct_outlen)\n+\n+                # decoder attentions\n+                decoder_attentions = outputs.decoder_attentions\n+                self.assertIsInstance(decoder_attentions, (list, tuple))\n+                self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n+                self.assertListEqual(\n+                    list(decoder_attentions[0].shape[-3:]),\n+                    [self.model_tester.num_attention_heads, decoder_seq_length, decoder_key_length],\n+                )\n+\n+                # cross attentions\n+                cross_attentions = outputs.cross_attentions\n+                self.assertIsInstance(cross_attentions, (list, tuple))\n+                self.assertEqual(len(cross_attentions), self.model_tester.num_hidden_layers)\n+                self.assertListEqual(\n+                    list(cross_attentions[0].shape[-3:]),\n+                    [\n+                        self.model_tester.num_attention_heads,\n+                        decoder_seq_length,\n+                        encoder_key_length,\n+                    ],\n+                )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if hasattr(self.model_tester, \"num_hidden_states_types\"):\n+                added_hidden_states = self.model_tester.num_hidden_states_types\n+            elif self.is_encoder_decoder:\n+                # decoder_hidden_states, encoder_last_hidden_state, encoder_hidden_states\n+                added_hidden_states = 3\n+            else:\n+                added_hidden_states = 1\n+\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+\n+    def test_retain_grad_hidden_states_attentions(self):\n+        # removed retain_grad and grad on decoder_hidden_states, as queries don't require grad\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # no need to test all models as different heads yield the same functionality\n+        model_class = self.all_model_classes[0]\n+        model = model_class(config)\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n+\n+        # logits\n+        output = outputs[0]\n+\n+        encoder_hidden_states = outputs.encoder_hidden_states[0]\n+        encoder_hidden_states.retain_grad()\n+\n+        encoder_attentions = outputs.encoder_attentions[0]\n+        encoder_attentions.retain_grad()\n+\n+        decoder_attentions = outputs.decoder_attentions[0]\n+        decoder_attentions.retain_grad()\n+\n+        cross_attentions = outputs.cross_attentions[0]\n+        cross_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(encoder_hidden_states.grad)\n+        self.assertIsNotNone(encoder_attentions.grad)\n+        self.assertIsNotNone(decoder_attentions.grad)\n+        self.assertIsNotNone(cross_attentions.grad)\n+\n+    def test_forward_auxiliary_loss(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.auxiliary_loss = True\n+\n+        # only test for object detection and segmentation model\n+        for model_class in self.all_model_classes[1:]:\n+            model = model_class(config)\n+            model.to(torch_device)\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+            outputs = model(**inputs)\n+\n+            self.assertIsNotNone(outputs.auxiliary_outputs)\n+            self.assertEqual(len(outputs.auxiliary_outputs), self.model_tester.num_hidden_layers - 1)\n+\n+    def test_training(self):\n+        if not self.model_tester.is_training:\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+\n+        # We only have loss with ObjectDetection\n+        model_class = self.all_model_classes[-1]\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        model = model_class(config)\n+        model.to(torch_device)\n+        model.train()\n+        inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+        loss = model(**inputs).loss\n+        loss.backward()\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            if model.config.is_encoder_decoder:\n+                expected_arg_names = [\"pixel_values\", \"pixel_mask\"]\n+                expected_arg_names.extend(\n+                    [\"head_mask\", \"decoder_head_mask\", \"encoder_outputs\"]\n+                    if \"head_mask\" and \"decoder_head_mask\" in arg_names\n+                    else []\n+                )\n+                self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+            else:\n+                expected_arg_names = [\"pixel_values\", \"pixel_mask\"]\n+                self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_different_timm_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # let's pick a random timm backbone\n+        config.backbone = \"tf_mobilenetv3_small_075\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"DabDetrForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_labels,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n+            else:\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.backbone.conv_encoder.intermediate_channel_sizes), 3)\n+\n+            self.assertTrue(outputs)\n+\n+    def test_initialization(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        configs_no_init.init_xavier_std = 1e9\n+        # Copied from RT-DETR\n+        configs_no_init.initializer_bias_prior_prob = 0.2\n+        bias_value = -1.3863  # log_e ((1 - 0.2) / 0.2)\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if \"bbox_attention\" in name and \"bias\" not in name:\n+                        self.assertLess(\n+                            100000,\n+                            abs(param.data.max().item()),\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    # Modifed from RT-DETR\n+                    elif \"class_embed\" in name and \"bias\" in name:\n+                        bias_tensor = torch.full_like(param.data, bias_value)\n+                        torch.testing.assert_close(\n+                            param.data,\n+                            bias_tensor,\n+                            atol=1e-4,\n+                            rtol=1e-4,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    elif \"activation_fn\" in name and config.activation_function == \"prelu\":\n+                        self.assertTrue(\n+                            param.data.mean() == 0.25,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    elif \"backbone.conv_encoder.model\" in name:\n+                        continue\n+                    elif \"self_attn.in_proj_weight\" in name:\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e2).round() / 1e2).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+\n+TOLERANCE = 1e-4\n+CHECKPOINT = \"IDEA-Research/dab-detr-resnet-50\"\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_timm\n+@require_vision\n+@slow\n+class DabDetrModelIntegrationTests(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return ConditionalDetrImageProcessor.from_pretrained(CHECKPOINT) if is_vision_available() else None\n+\n+    def test_inference_no_head(self):\n+        model = DabDetrModel.from_pretrained(CHECKPOINT).to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        encoding = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(pixel_values=encoding.pixel_values)\n+\n+        expected_shape = torch.Size((1, 300, 256))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+        expected_slice = torch.tensor(\n+            [[-0.4879, -0.2594, 0.4524], [-0.4997, -0.4258, 0.4329], [-0.8220, -0.4996, 0.0577]]\n+        ).to(torch_device)\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=2e-4, rtol=2e-4)\n+\n+    def test_inference_object_detection_head(self):\n+        model = DabDetrForObjectDetection.from_pretrained(CHECKPOINT).to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        encoding = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+        pixel_values = encoding[\"pixel_values\"].to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(pixel_values)\n+\n+        # verify logits + box predictions\n+        expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.num_labels))\n+        self.assertEqual(outputs.logits.shape, expected_shape_logits)\n+        expected_slice_logits = torch.tensor(\n+            [[-10.1765, -5.5243, -8.9324], [-9.8138, -5.6721, -7.5161], [-10.3054, -5.6081, -8.5931]]\n+        ).to(torch_device)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, atol=3e-4, rtol=3e-4)\n+\n+        expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n+        self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n+        expected_slice_boxes = torch.tensor(\n+            [[0.3708, 0.3000, 0.2753], [0.5211, 0.6125, 0.9495], [0.2897, 0.6730, 0.5459]]\n+        ).to(torch_device)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4, rtol=1e-4)\n+\n+        # verify postprocessing\n+        results = image_processor.post_process_object_detection(\n+            outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.8732, 0.8563, 0.8554, 0.6079, 0.5896]).to(torch_device)\n+        expected_labels = [17, 75, 17, 75, 63]\n+        expected_boxes = torch.tensor([14.6970, 49.3892, 320.5165, 469.2765]).to(torch_device)\n+\n+        self.assertEqual(len(results[\"scores\"]), 5)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, atol=1e-4, rtol=1e-4)\n+        self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_boxes, atol=1e-4, rtol=1e-4)"
        },
        {
            "sha": "cfc0de5dc0caede3861b783f1bbb55f17bdbeca0",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d73a38606bc342b370afe1f42718b4828d95aaa/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d73a38606bc342b370afe1f42718b4828d95aaa/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=8d73a38606bc342b370afe1f42718b4828d95aaa",
            "patch": "@@ -161,6 +161,16 @@\n         \"giou_loss_coefficient\",\n         \"mask_loss_coefficient\",\n     ],\n+    \"DabDetrConfig\": [\n+        \"dilation\",\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"cls_loss_coefficient\",\n+        \"focal_alpha\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+    ],\n     \"DetrConfig\": [\n         \"bbox_cost\",\n         \"bbox_loss_coefficient\","
        }
    ],
    "stats": {
        "total": 3261,
        "additions": 3259,
        "deletions": 2
    }
}