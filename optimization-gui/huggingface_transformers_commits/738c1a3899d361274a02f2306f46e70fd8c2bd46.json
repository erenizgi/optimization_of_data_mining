{
    "author": "pyapyapya",
    "message": "ğŸŒ [i18n-KO] Translated `cache_explanation.md` to Korean (#39535)\n\n* update: _toctree.yml\n\n* docs: ko: cache_explanation.md\n\n* feat: nmt draft\n\n* fix: apply yijun-lee's comments\n\n* fix: apply 4N3MONE's comments\n\n* docs: update cache_position\n\n* docs: update cache-storage-implementation\n\n* update: add h2 tag in cache-position\n\n---------\n\nCo-authored-by: taehyeonjeon <xogus294@gmail.com>",
    "sha": "738c1a3899d361274a02f2306f46e70fd8c2bd46",
    "files": [
        {
            "sha": "7b2982db8bdf4d179bad9b0a04e6f48df3a5608c",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/738c1a3899d361274a02f2306f46e70fd8c2bd46/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/738c1a3899d361274a02f2306f46e70fd8c2bd46/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=738c1a3899d361274a02f2306f46e70fd8c2bd46",
            "patch": "@@ -295,6 +295,94 @@\n     title: ì»¤ë®¤ë‹ˆí‹° ë¦¬ì†ŒìŠ¤\n   - local: troubleshooting\n     title: ë¬¸ì œ í•´ê²°\n+  - local: gguf\n+    title: GGUF íŒŒì¼ë“¤ê³¼ì˜ ìƒí˜¸ ìš´ìš©ì„±\n+  - local: modular_transformers\n+    title: transformersì—ì„œì˜ ëª¨ë“ˆì„±\n+  title: (ë²ˆì—­ì¤‘) ê°œë°œì ê°€ì´ë“œ\n+- sections:\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) Getting started\n+  - local: quantization/bitsandbytes\n+    title: bitsandbytes\n+  - local: quantization/gptq\n+    title: GPTQ\n+  - local: quantization/awq\n+    title: AWQ\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) AQLM\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) VPTQ\n+  - local: quantization/quanto\n+    title: Quanto\n+  - local: quantization/quark\n+    title: Quark\n+  - local: quantization/eetq\n+    title: EETQ\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) HQQ\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) Optimum\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) Contribute new quantization method\n+  title: (ë²ˆì—­ì¤‘) ê²½ëŸ‰í™” ë©”ì†Œë“œ\n+- sections:\n+  - local: performance\n+    title: ì„±ëŠ¥ ë° í™•ì¥ì„±\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) Quantization\n+  - local: llm_optims\n+    title: LLM ì¶”ë¡  ìµœì í™”\n+  - local: cache_explanation\n+    title: ì–´í…ì…˜ í–‰ë ¬ ìºì‹±\n+  - sections:\n+    - local: in_translation\n+      title: (ë²ˆì—­ì¤‘) Methods and tools for efficient training on a single GPU\n+    - local: perf_train_gpu_many\n+      title: ë‹¤ì¤‘ GPUì—ì„œ í›ˆë ¨ ì§„í–‰í•˜ê¸°\n+    - local: deepspeed\n+      title: DeepSpeed\n+    - local: fsdp\n+      title: ì™„ì „ ë¶„í•  ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬\n+    - local: perf_train_cpu\n+      title: CPUì—ì„œ í›ˆë ¨\n+    - local: perf_train_cpu_many\n+      title: ë‹¤ì¤‘ CPUì—ì„œ í›ˆë ¨í•˜ê¸°\n+    - local: perf_train_tpu_tf\n+      title: TensorFlowë¡œ TPUì—ì„œ í›ˆë ¨í•˜ê¸°\n+    - local: perf_train_special\n+      title: Apple ì‹¤ë¦¬ì½˜ì—ì„œ PyTorch í•™ìŠµ\n+    - local: perf_hardware\n+      title: í›ˆë ¨ìš© ì‚¬ìš©ì ë§ì¶¤í˜• í•˜ë“œì›¨ì–´\n+    - local: hpo_train\n+      title: Trainer APIë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n+    title: (ë²ˆì—­ì¤‘) íš¨ìœ¨ì ì¸ í•™ìŠµ ê¸°ìˆ ë“¤\n+  - sections:\n+    - local: perf_infer_cpu\n+      title: CPUë¡œ ì¶”ë¡ í•˜ê¸°\n+    - local: perf_infer_gpu_one\n+      title: í•˜ë‚˜ì˜ GPUë¥¼ í™œìš©í•œ ì¶”ë¡ \n+    title: ì¶”ë¡  ìµœì í™”í•˜ê¸°\n+  - local: big_models\n+    title: ëŒ€í˜• ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”\n+  - local: debugging\n+    title: ë””ë²„ê¹…\n+  - local: tf_xla\n+    title: TensorFlow ëª¨ë¸ì„ ìœ„í•œ XLA í†µí•©\n+  - local: in_translation\n+    title: (ë²ˆì—­ì¤‘) Optimize inference using `torch.compile()`\n+  title: (ë²ˆì—­ì¤‘) ì„±ëŠ¥ ë° í™•ì¥ì„±\n+- sections:\n+    - local: contributing\n+      title: ğŸ¤— Transformersì— ê¸°ì—¬í•˜ëŠ” ë°©ë²•\n+    - local: add_new_model\n+      title: ğŸ¤— Transformersì— ìƒˆë¡œìš´ ëª¨ë¸ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•\n+    - local: add_new_pipeline\n+      title: ì–´ë–»ê²Œ ğŸ¤— Transformersì— íŒŒì´í”„ë¼ì¸ì„ ì¶”ê°€í•˜ë‚˜ìš”?\n+    - local: testing\n+      title: í…ŒìŠ¤íŠ¸\n+    - local: pr_checks\n+      title: Pull Requestì— ëŒ€í•œ ê²€ì‚¬\n   title: ë¦¬ì†ŒìŠ¤\n - isExpanded: false\n   sections:"
        },
        {
            "sha": "43767b4712967d296844af016bc0147c2a27be53",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "added",
            "additions": 184,
            "deletions": 0,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/738c1a3899d361274a02f2306f46e70fd8c2bd46/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/738c1a3899d361274a02f2306f46e70fd8c2bd46/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=738c1a3899d361274a02f2306f46e70fd8c2bd46",
            "patch": "@@ -0,0 +1,184 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ìºì‹±[[caching]]\n+ëˆ„êµ°ê°€ì™€ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ìˆëŠ”ë°, ìƒëŒ€ë°©ì´ ì´ì „ì— í–ˆë˜ ë§ì„ ê¸°ì–µí•˜ì§€ ëª»í•˜ê³  ë‹¹ì‹ ì´ ëŒ€ë‹µí•  ë•Œë§ˆë‹¤ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•œë‹¤ê³  ìƒìƒí•´ ë³´ì„¸ìš”. ì´ëŠ” ëŠë¦¬ê³  ë¹„íš¨ìœ¨ì ì´ê² ì£ ?\n+\n+ì´ ë¹„ìœ ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìê¸°íšŒê·€ ëª¨ë¸ì˜ ìƒì„±ì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì”© ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ê°ì˜ ìƒˆë¡œìš´ ì˜ˆì¸¡ì€ ì´ì „ì˜ ëª¨ë“  ë¬¸ë§¥ì— ì˜ì¡´í•©ë‹ˆë‹¤.\n+\n+1000ë²ˆì§¸ í† í°ì„ ì˜ˆì¸¡í•˜ë ¤ë©´, ëª¨ë¸ì€ ì´ì „ 999ê°œ í† í°ì˜ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì •ë³´ëŠ” ê° í† í° í‘œí˜„ë“¤ ì‚¬ì´ì˜ í–‰ë ¬ ê³±ì„ í†µí•´ í‘œí˜„ë©ë‹ˆë‹¤.\n+\n+1001ë²ˆì§¸ í† í°ì„ ì˜ˆì¸¡í•˜ë ¤ë©´, ì´ì „ 999ê°œ í† í°ì˜ ë™ì¼í•œ ì •ë³´ì— ë”í•˜ì—¬ 1000ë²ˆì§¸ í† í°ì˜ ì •ë³´ë„ í•„ìš”í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ í† í°ë§ˆë‹¤ ëª¨ë¸ì€ ë°˜ë³µì ìœ¼ë¡œ ë§ì€ í–‰ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤!\n+\n+ì´ëŸ¬í•œ ë¹„íš¨ìœ¨ì„±ì„ ì œê±°í•˜ê¸° ìœ„í•´ KV ìºì‹œ(Key-Value Cache)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œ ì´ì „ì— ì²˜ë¦¬í•œ í† í°ìœ¼ë¡œë¶€í„° ì–»ì€ í‚¤ì™€ ê°’ ìŒì„ ì €ì¥í•´ë‘ê³ , ì´í›„ í† í° ì˜ˆì¸¡ ì‹œ ì´ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì—°ì‚°ì„ ì¤„ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n+\n+> [!WARNING]\n+> ìºì‹±ì€ **ì¶”ë¡ **ì—ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. í•™ìŠµ ì¤‘ì— í™œì„±í™”ë˜ë©´ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ìºì‹±ì´ ì–´ë–»ê²Œ ê·¸ë¦¬ê³  ì™œ ì‘ë™í•˜ëŠ”ì§€ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´, ì–´í…ì…˜ í–‰ë ¬ì˜ êµ¬ì¡°ë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n+\n+## ì–´í…ì…˜ í–‰ë ¬[[attention-matrices]]\n+\n+**ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸ ì–´í…ì…˜**ì€ ë°°ì¹˜ í¬ê¸° `b`, ì–´í…ì…˜ í—¤ë“œ ìˆ˜ `h`, í˜„ì¬ê¹Œì§€ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ `T`, ì–´í…ì…˜ í—¤ë“œë‹¹ ì°¨ì› `d_head`ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.\n+\n+$$\n+\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_{\\text{head}}}} \\times \\text{mask} \\right) V\n+$$\n+\n+ì¿¼ë¦¬(`Q`), í‚¤(`K`), ê°’(`V`) í–‰ë ¬ì€ `(b, h, T, d_head)` í˜•íƒœì˜ ì…ë ¥ ì„ë² ë”©ì—ì„œì˜ íˆ¬ì˜ì…ë‹ˆë‹¤.\n+\n+ì¸ê³¼ì  ì–´í…ì…˜ì˜ ê²½ìš°, ë§ˆìŠ¤í¬ëŠ” ëª¨ë¸ì´ ë¯¸ë˜ í† í°ì— ì–´í…ì…˜ í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. í† í°ì´ í•œ ë²ˆ ì²˜ë¦¬ë˜ë©´, ê·¸ í‘œí˜„ì€ ë¯¸ë˜ í† í°ê³¼ ê´€ë ¨í•˜ì—¬ ì ˆëŒ€ ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” \\\\( K_{\\text{past}} \\\\)ì™€ \\\\( V_{\\text{past}} \\\\)ë¥¼ ìºì‹œí•˜ì—¬ ë§ˆì§€ë§‰ í† í°ì˜ í‘œí˜„ì„ ê³„ì‚°í•˜ëŠ” ë° ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n+\n+$$\n+\\text{Attention}(q_t, [\\underbrace{k_1, k_2, \\dots, k_{t-1}}_{\\text{cached}}, k_{t}], [\\underbrace{v_1, v_2, \\dots, v_{t-1}}_{\\text{cached}}, v_{t}])\n+$$\n+\n+ì¶”ë¡  ì‹œì—ëŠ” ë‹¤ìŒ í† í° \\\\( t+1 \\\\)ì„ ì˜ˆì¸¡í•˜ëŠ” í‘œí˜„ \\\\( x_t \\\\)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ í† í°ì˜ ì¿¼ë¦¬ë§Œ í•„ìš”í•©ë‹ˆë‹¤. ë‹¨ê³„ì—ì„œ ìƒˆë¡œìš´ í‚¤ì™€ ê°’ ë²¡í„°ê°€ ìºì‹œì— **ì €ì¥**ë˜ê³  ê³¼ê±° í‚¤ì™€ ê°’ì— **ì¶”ê°€**ë©ë‹ˆë‹¤.\n+\n+$$\n+K_{\\text{cache}} \\leftarrow \\text{concat}(K_{\\text{past}}, k_t), \\quad V_{\\text{cache}} \\leftarrow \\text{concat}(V_{\\text{past}}, v_t)\n+$$\n+\n+ì–´í…ì…˜ì€ ëª¨ë¸ì˜ ê° ë ˆì´ì–´ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ìºì‹±ì€ ë ˆì´ì–´ë³„ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n+\n+ìºì‹±ì´ íš¨ìœ¨ì„±ì„ ì–´ë–»ê²Œ ê°œì„ í•˜ëŠ”ì§€ ë¹„êµí•œ ì•„ë˜ í‘œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+| ìºì‹± ì—†ìŒ | ìºì‹± ì‚¬ìš© |\n+|---|---|\n+| ë‹¨ê³„ë§ˆë‹¤ ì´ì „ì˜ ëª¨ë“  `K`ì™€ `V`ë¥¼ ì¬ê³„ì‚°  | ë‹¨ê³„ë§ˆë‹¤ í˜„ì¬ì˜ `K`ì™€ `V`ë§Œ ê³„ì‚° |\n+| ë‹¨ê³„ë‹¹ ì–´í…ì…˜ ë¹„ìš©ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•´ **ì œê³±** | ë‹¨ê³„ë‹¹ ì–´í…ì…˜ ë¹„ìš©ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•´ **ì„ í˜•** (ë©”ëª¨ë¦¬ëŠ” ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ì§€ë§Œ, í† í°ë‹¹ ê³„ì‚°ì€ ë‚®ê²Œ ìœ ì§€ë¨) |\n+\n+\n+\n+## ìºì‹œ í´ë˜ìŠ¤[[cache-class]]\n+\n+ê¸°ë³¸ KV ìºì‹œ ì¸í„°í˜ì´ìŠ¤ëŠ” í˜„ì¬ í† í°ì˜ í‚¤ì™€ ê°’ í…ì„œë¥¼ ë°›ì•„ì„œ ì—…ë°ì´íŠ¸ëœ `K`ì™€ `V` í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ `forward` ë©”ì†Œë“œì— ì˜í•´ ë‚´ë¶€ì ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.\n+\n+```py\n+new_K, new_V = cache.update(k_t, v_t, layer_idx)\n+attn_output = attn_layer_idx_fn(q_t, new_K, new_V)\n+```\n+\n+Transformersì˜ [`Cache`] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ë•Œ, ì…€í”„ ì–´í…ì…˜ ëª¨ë“ˆì€ ê³¼ê±°ì™€ í˜„ì¬ ì •ë³´ë¥¼ í†µí•©í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+\n+1. ì–´í…ì…˜ ëª¨ë“ˆì€ í˜„ì¬ kv ìŒì„ ìºì‹œì— ì €ì¥ëœ ê³¼ê±° kv ìŒê³¼ ì—°ê²°í•©ë‹ˆë‹¤. ì´ëŠ” `(new_tokens_length, past_kv_length + new_tokens_length)` í˜•íƒœì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í˜„ì¬ì™€ ê³¼ê±° kv ìŒì´ ë³¸ì§ˆì ìœ¼ë¡œ ê²°í•©í•´ ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ë©°, ëª¨ë¸ì´ ì´ì „ ë¬¸ë§¥ê³¼ í˜„ì¬ ì…ë ¥ì„ ì¸ì‹í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n+\n+2. `forward` ë©”ì†Œë“œê°€ ë°˜ë³µì ìœ¼ë¡œ í˜¸ì¶œë  ë•Œ, ì–´í…ì…˜ ë§ˆìŠ¤í¬ í˜•íƒœê°€ ê³¼ê±°ì™€ í˜„ì¬ kv ìŒì˜ ê²°í•©ëœ ê¸¸ì´ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” `(batch_size, past_kv_length + new_tokens_length)` í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ [`~GenerationMixin.generate`]ì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ, [`Cache`]ë¡œ ìì²´ ìƒì„± ë£¨í”„ë¥¼ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´ ì´ë¥¼ ì—¼ë‘ì— ë‘ì„¸ìš”! ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” ê³¼ê±°ì™€ í˜„ì¬ í† í°ê°’ì„ ë³´ìœ í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+3. `cache_position`ì„ ì¸ì‹í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ìœ íš¨í•œ `cache_position` ê°’ì„ ì „ë‹¬í•´ì•¼ í•˜ë¯€ë¡œ `forward` ë©”ì†Œë“œë¡œ ë¯¸ë¦¬ ì±„ì›Œì§„ [`Cache`]ë¥¼ ì¬ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ì‹œí€€ìŠ¤ì—ì„œì˜ ì…ë ¥ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. `cache_position`ì€ íŒ¨ë”©ì— ì˜í–¥ë°›ì§€ ì•Šìœ¼ë©°, ê° í† í°ì— ëŒ€í•´ í•­ìƒ í•˜ë‚˜ì”© ë” ë§ì€ ìœ„ì¹˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, kv ìºì‹œê°€ 10ê°œì˜ í† í°ì„ í¬í•¨í•˜ë©´ - íŒ¨ë“œ í† í°ê³¼ ê´€ê³„ì—†ì´ - ë‹¤ìŒ í† í°ì˜ ìºì‹œ ìœ„ì¹˜ëŠ” `torch.tensor([10])`ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+## ìºì‹œ ì €ì¥ì†Œ êµ¬í˜„[[cache-storage-implementation]]\n+\n+ìºì‹œëŠ” ê° ë ˆì´ì–´ê°€ keyì™€ value ìºì‹œë¥¼ í¬í•¨í•˜ëŠ” ë ˆì´ì–´ ëª©ë¡ í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. key ë° value ìºì‹œëŠ” `[batch_size, num_heads, seq_len, head_dim]` í˜•íƒœì˜ í…ì„œì…ë‹ˆë‹¤.\n+\n+ë ˆì´ì–´ëŠ” ì„œë¡œ ë‹¤ë¥¸ íƒ€ì…ì¼ ìˆ˜ ìˆìœ¼ë©°(ì˜ˆ: `DynamicLayer`, `StaticLayer`, `SlidingWindowLayer`), ì´ëŠ” ì£¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ìºì‹œë¥¼ ì–´ë–»ê²Œ ê°±ì‹ í•˜ëŠ”ì§€ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n+\n+ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœëŠ” `DynamicLayer`ë¡œ, ë” ë§ì€ í† í°ì´ ì²˜ë¦¬ë¨ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ í™•ì¥ë©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¨ì›(`seq_len`)ì€ ìƒˆë¡œìš´ í† í°ì´ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ì¦ê°€í•©ë‹ˆë‹¤:\n+\n+```py\n+cache.layers[idx].keys = torch.cat([cache.layers[idx].keys, key_states], dim=-2)\n+cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], dim=-2)\n+```\n+\n+`StaticLayer`ë‚˜ `SlidingWindowLayer`ì™€ ê°™ì€ ë‹¤ë¥¸ ë ˆì´ì–´ íƒ€ì…ì€ ìºì‹œê°€ ìƒì„±ë  ë•Œ ê³ ì •ëœ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°€ì§€ë©°, ì´ëŠ” `torch.compile`ê³¼ í˜¸í™˜ë˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤. `SlidingWindowLayer`ì˜ ê²½ìš°, ìƒˆë¡œìš´ í† í°ì´ ì¶”ê°€ë˜ë©´ ê¸°ì¡´ í† í°ì€ ìºì‹œì—ì„œ ì œê±°ë©ë‹ˆë‹¤.\n+\n+ì•„ë˜ ì˜ˆì œëŠ” [`DynamicCache`]ë¡œ ìƒì„± ë£¨í”„ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë…¼ì˜ëœ ë°”ì™€ ê°™ì´, ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” ê³¼ê±°ì™€ í˜„ì¬ í† í°ê°’ì˜ ì—°ê²°ì´ë©° ë‹¤ìŒ í† í°ì„ ìœ„í•´ ìºì‹œ ìœ„ì¹˜ì— `1`ì´ ì¶”ê°€ë©ë‹ˆë‹¤.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+past_key_values = DynamicCache()\n+messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+\n+generated_ids = inputs.input_ids\n+cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n+max_new_tokens = 10\n+\n+for _ in range(max_new_tokens):\n+    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n+    # íƒìš•ì  ê¸°ë²•ìœ¼ë¡œ ë‹¤ìŒ í† í° í•˜ë‚˜ë¥¼ ìƒ˜í”Œë§\n+    next_token_ids = outputs.logits[:, -1:].argmax(-1)\n+    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n+    # ì²˜ë¦¬ë˜ì§€ ì•Šì€ í† í°ì„ ë‚¨ê²¨ë‘ì–´ ë‹¤ìŒ ìƒì„± ë‹¨ê³„ë¥¼ ìœ„í•œ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²½ìš° ìƒˆë¡œìš´ í† í° í•˜ë‚˜ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.\n+    # ìœ„ì—ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ìƒˆë¡œìš´ í† í°ì„ ìœ„í•´ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤\n+    attention_mask = inputs[\"attention_mask\"]\n+    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n+    inputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\n+    cache_position = cache_position[-1:] + 1 # ë‹¤ìŒ í† í°ì„ ìœ„í•´ í•˜ë‚˜ ë” ìœ„ì¹˜ ì¶”ê°€\n+\n+print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n+\"[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,\"\n+```\n+\n+## ìºì‹œ ìœ„ì¹˜[[cache-position]]\n+\n+ìºì‹œ ìœ„ì¹˜ëŠ” ì–´í…ì…˜ ìºì‹œì—ì„œ ìƒˆë¡œìš´ í† í°ì„ ì‚½ì…í•  ìœ„ì¹˜ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. ì´ëŠ” íŒ¨ë”©ì´ë‚˜ ë°°ì¹˜ êµ¬ì¡°ì™€ ë¬´ê´€í•˜ê²Œ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ê° í† í°ì˜ ì ˆëŒ€ì  ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ë¯¸ `N`ê°œì˜ í† í°ì„ ìºì‹œí–ˆê³  í˜„ì¬ `K`ê°œì˜ ìƒˆë¡œìš´ í† í°ì„ ì²˜ë¦¬í•˜ê³  ìˆë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í† í°ì— ëŒ€í•œ ìºì‹œ ìœ„ì¹˜ëŠ” `N`ë¶€í„° `N + K - 1`ê¹Œì§€ì˜ ë²”ìœ„ê°€ ë©ë‹ˆë‹¤. ì¦‰, `[N, N + 1, N + 2, ..., N + K - 1]` ìœ„ì¹˜ì˜ í† í°ë“¤ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n+\n+ìºì‹œ ìœ„ì¹˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë‘ ê°€ì§€ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:\n+\n+1. ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ì²˜ë¦¬í•  ìƒˆë¡œìš´ í† í°ì„ ì„ íƒí•˜ê³ , ì•„ì§ ìºì‹œë˜ì§€ ì•Šì€ í† í°ë§Œ ëª¨ë¸ì˜ `forward`ì— ì „ë‹¬ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n+2. í‚¤/ê°’ ìŒì„ ìºì‹œì˜ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ì €ì¥í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • ìºì‹œ ê¸¸ì´ë¥¼ ë¯¸ë¦¬ í• ë‹¹í•˜ëŠ” [`StaticCache`]ì™€ ê°™ì€ ê³ ì • í¬ê¸° ìºì‹œì—ì„œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.\n+\n+ìƒì„± ë£¨í”„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìºì‹œ ìœ„ì¹˜ë¥¼ ê´€ë¦¬í•˜ì§€ë§Œ, ì‚¬ìš©ì ì •ì˜ ìƒì„± ë©”ì†Œë“œë¥¼ ì‘ì„±í•  ë•ŒëŠ” ìºì‹œ ìœ„ì¹˜ê°€ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤. ìºì‹œ ìœ„ì¹˜ëŠ” ê³ ì •ëœ ìŠ¬ë¡¯ì— í‚¤/ê°’ ìƒíƒœë¥¼ ì½ê³  ì“°ëŠ” ë° ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n+\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n+\n+```\n+\n+\n+## ë ˆê±°ì‹œ ìºì‹œ í˜•ì‹[[legacy-cache-format]]\n+\n+[`Cache`] í´ë˜ìŠ¤ ì´ì „ì—ëŠ” ìºì‹œê°€ í…ì„œì˜ íŠœí”Œì˜ íŠœí”Œë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í˜•ì‹ì€ í…ìŠ¤íŠ¸ê°€ ìƒì„±ë¨ì— ë”°ë¼ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ë™ì ì´ë©°, [`DynamicCache`]ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.\n+\n+ë ˆê±°ì‹œ í˜•ì‹ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•œ ë°ì´í„° êµ¬ì¡°ì´ì§€ë§Œ ë‹¤ë¥´ê²Œ ì¡°ì§í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n+- ê° ë‚´ë¶€ íŠœí”Œì€ ë ˆì´ì–´ì˜ í‚¤ì™€ ê°’ í…ì„œë¥¼ í¬í•¨í•˜ëŠ” íŠœí”Œì˜ íŠœí”Œì…ë‹ˆë‹¤.\n+- í…ì„œëŠ” ë™ì¼í•œ í˜•íƒœ `[batch_size, num_heads, seq_len, head_dim]`ë¥¼ ê°–ìŠµë‹ˆë‹¤.\n+- ì´ í˜•ì‹ì€ ëœ ìœ ì—°í•˜ë©° ì–‘ìí™”ë‚˜ ì˜¤í”„ë¡œë”©ê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n+\n+í”„ë¡œì íŠ¸ê°€ ì´ ë ˆê±°ì‹œ í˜•ì‹ì— ì˜ì¡´í•œë‹¤ë©´, [`~DynamicCache.from_legacy_cache`]ë¥¼ ì‚¬ìš©í•˜ì—¬ [`DynamicCache`]ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë ˆê±°ì‹œ ìºì‹œ í˜•ì‹ì€ ì‚¬ìš©ì´ ì¤‘ë‹¨ë˜ì—ˆìœ¼ë©° `Transformers`ì—ì„œ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¹ì • í˜•ì‹ì—ì„œ ìºì‹œë¥¼ ì¡°ì‘í•˜ëŠ” ì»¤ìŠ¤í…€ ë¡œì§ì´ ìˆëŠ” ê²½ìš° ë„ì›€ì´ ë˜ëŠ” [`DynamicCache.to_legacy_cache`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠœí”Œ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+\n+# ìºì‹œë¥¼ ë°˜í™˜í•˜ë ¤ë©´ `return_dict_in_generate=True`ê°€ í•„ìš”í•˜ê³  `return_legacy_cache`ëŠ” ë°˜í™˜ëœ ìºì‹œë¥¼\n+# ë ˆê±°ì‹œ í˜•ì‹ìœ¼ë¡œ ê°•ì œí•©ë‹ˆë‹¤\n+generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n+\n+cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n+legacy_format_cache = cache.to_legacy_cache()\n+```"
        }
    ],
    "stats": {
        "total": 272,
        "additions": 272,
        "deletions": 0
    }
}