{
    "author": "gante",
    "message": "[pipeline] ASR pipeline kwargs are forwared to `generate` (#40375)\n\n* tmp commit\n\n* add test\n\n* PR suggestion",
    "sha": "37c14430c99edca79dfcdcb76f1209f291b12fab",
    "files": [
        {
            "sha": "9c4f0f6e1d631b81c22161973599e5cdd8bf36bf",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c14430c99edca79dfcdcb76f1209f291b12fab/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c14430c99edca79dfcdcb76f1209f291b12fab/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=37c14430c99edca79dfcdcb76f1209f291b12fab",
            "patch": "@@ -1594,7 +1594,7 @@ def language_to_id(language: str) -> int:\n                     # if task is defined it'll overwrite task ids that might have already been defined via the generation_config\n                     replace_or_add(init_tokens[i], task_id, generation_config.task_to_id.values())\n                 else:\n-                    raise ValueError(f\"The `{task}`task is not supported. The task should be one of `{TASK_IDS}`\")\n+                    raise ValueError(f\"The `{task}` task is not supported. The task should be one of `{TASK_IDS}`\")\n             elif language is not None and hasattr(generation_config, \"task_to_id\"):\n                 # if language is defined, but no task id is in `init_tokens`, default to transcribe\n                 if not any(ti in init_tokens[i] for ti in generation_config.task_to_id.values()):"
        },
        {
            "sha": "b4d1b96ea87f004abdfeae15b56e1b6523fefcba",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 23,
            "deletions": 16,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c14430c99edca79dfcdcb76f1209f291b12fab/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c14430c99edca79dfcdcb76f1209f291b12fab/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=37c14430c99edca79dfcdcb76f1209f291b12fab",
            "patch": "@@ -282,10 +282,13 @@ def _sanitize_parameters(\n         decoder_kwargs=None,\n         return_timestamps=None,\n         return_language=None,\n-        generate_kwargs=None,\n+        **generate_kwargs,\n     ):\n-        # No parameters on this pipeline right now\n         preprocess_params = {}\n+        forward_params = {}\n+        postprocess_params = {}\n+\n+        # Preprocess params\n         if chunk_length_s is not None:\n             if self.type in [\"seq2seq\", \"seq2seq_whisper\"] and not ignore_warning:\n                 type_warning = (\n@@ -305,14 +308,28 @@ def _sanitize_parameters(\n         if stride_length_s is not None:\n             preprocess_params[\"stride_length_s\"] = stride_length_s\n \n-        forward_params = defaultdict(dict)\n-        if generate_kwargs is not None:\n-            forward_params.update(generate_kwargs)\n+        # Forward params\n+        # BC: accept a dictionary of generation kwargs (as opposed to **generate_kwargs)\n+        if \"generate_kwargs\" in generate_kwargs:\n+            forward_params.update(generate_kwargs.pop(\"generate_kwargs\"))\n+        # Default use for kwargs: they are generation-time kwargs\n+        forward_params.update(generate_kwargs)\n \n-        postprocess_params = {}\n+        if getattr(self, \"assistant_model\", None) is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if getattr(self, \"assistant_tokenizer\", None) is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n+        # Postprocess params\n         if decoder_kwargs is not None:\n             postprocess_params[\"decoder_kwargs\"] = decoder_kwargs\n+        if return_language is not None:\n+            if self.type != \"seq2seq_whisper\":\n+                raise ValueError(\"Only Whisper can return language for now.\")\n+            postprocess_params[\"return_language\"] = return_language\n \n+        # Parameter used in more than one place\n         # in some models like whisper, the generation config has a `return_timestamps` key\n         if hasattr(self, \"generation_config\") and hasattr(self.generation_config, \"return_timestamps\"):\n             return_timestamps = return_timestamps or self.generation_config.return_timestamps\n@@ -335,16 +352,6 @@ def _sanitize_parameters(\n                 )\n             forward_params[\"return_timestamps\"] = return_timestamps\n             postprocess_params[\"return_timestamps\"] = return_timestamps\n-        if return_language is not None:\n-            if self.type != \"seq2seq_whisper\":\n-                raise ValueError(\"Only Whisper can return language for now.\")\n-            postprocess_params[\"return_language\"] = return_language\n-\n-        if getattr(self, \"assistant_model\", None) is not None:\n-            forward_params[\"assistant_model\"] = self.assistant_model\n-        if getattr(self, \"assistant_tokenizer\", None) is not None:\n-            forward_params[\"tokenizer\"] = self.tokenizer\n-            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n \n         return preprocess_params, forward_params, postprocess_params\n "
        },
        {
            "sha": "c7aa7b686b1fa34673b57af4ad583deabd4d4f42",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c14430c99edca79dfcdcb76f1209f291b12fab/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c14430c99edca79dfcdcb76f1209f291b12fab/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=37c14430c99edca79dfcdcb76f1209f291b12fab",
            "patch": "@@ -1791,6 +1791,32 @@ def test_pipeline_assisted_generation(self):\n         with self.assertRaises(ValueError):\n             _ = pipe(prompt, generate_kwargs={\"num_beams\": 2})\n \n+    @require_torch\n+    def test_pipeline_generation_kwargs(self):\n+        \"\"\"Tests that we can pass kwargs to `generate`, as in the text generation pipelines\"\"\"\n+        model = \"openai/whisper-tiny\"\n+        asr = pipeline(\"automatic-speech-recognition\", model=model)\n+        dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")\n+\n+        # BC: with `generate_kwargs` as a dictionary\n+        res = asr(\n+            dataset[0][\"audio\"],\n+            generate_kwargs={\"task\": \"transcribe\", \"max_new_tokens\": 256},\n+        )\n+        self.assertEqual(\n+            res[\"text\"], \" Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.\"\n+        )\n+\n+        # New: kwargs forwarded to `generate`\n+        res = asr(\n+            dataset[0][\"audio\"],\n+            max_new_tokens=256,\n+            task=\"transcribe\",\n+        )\n+        self.assertEqual(\n+            res[\"text\"], \" Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.\"\n+        )\n+\n \n def require_ffmpeg(test_case):\n     \"\"\""
        }
    ],
    "stats": {
        "total": 67,
        "additions": 50,
        "deletions": 17
    }
}