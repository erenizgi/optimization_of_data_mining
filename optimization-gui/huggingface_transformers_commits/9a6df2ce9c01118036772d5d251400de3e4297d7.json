{
    "author": "Sai-Suraj-27",
    "message": "Fix failing `CodeGenModelTests` (#42730)\n\n* Fixed failing codegen model tests.\n\n* Fix failing test_codegen_sample test\n\n* Remove breakpoint.",
    "sha": "9a6df2ce9c01118036772d5d251400de3e4297d7",
    "files": [
        {
            "sha": "021357a138a8e67934c30d7c23ce3281f8d1282f",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6df2ce9c01118036772d5d251400de3e4297d7/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6df2ce9c01118036772d5d251400de3e4297d7/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=9a6df2ce9c01118036772d5d251400de3e4297d7",
            "patch": "@@ -365,7 +365,7 @@ def test_batch_generation(self):\n         model.config.pad_token_id = model.config.eos_token_id\n \n         # use different length sentences to test batching\n-        sentences = [\"def hello_world():\", \"def greet(name):\"]\n+        sentences = [\"def greetings(name):\", \"def hello_world():\"]\n \n         inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n         input_ids = inputs[\"input_ids\"].to(torch_device)\n@@ -380,29 +380,30 @@ def test_batch_generation(self):\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_new_tokens=13,\n         )\n \n         outputs_tt = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n             token_type_ids=token_type_ids,\n+            max_new_tokens=13,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_new_tokens=13)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        output_padded = model.generate(input_ids=inputs_padded, max_new_tokens=13)\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n         non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n         padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n \n         expected_output_sentence = [\n-            'def hello_world():\\n    print(\"Hello World\")\\n\\nhellow_world()',\n-            'def greet(name):\\n    print(f\"Hello {name}\")\\n\\ng',\n+            'def greetings(name):\\n    print(f\"Hello {name}\")\\n\\n',\n+            'def hello_world():\\n    print(\"Hello World\")\\n\\nhello_world()',\n         ]\n         self.assertListEqual(expected_output_sentence, batch_out_sentence)\n         self.assertTrue(batch_out_sentence_tt != batch_out_sentence)  # token_type_ids should change output\n@@ -440,7 +441,7 @@ def test_lm_generate_codegen(self):\n             inputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\").to(torch_device)\n             expected_output = 'def hello_world():\\n    print(\"Hello World\")\\n\\nhello_world()\\n\\n'\n \n-            output_ids = model.generate(**inputs, do_sample=False)\n+            output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n             output_str = tokenizer.batch_decode(output_ids)[0]\n \n             self.assertEqual(output_str, expected_output)\n@@ -456,7 +457,7 @@ def test_codegen_sample(self):\n \n         tokenized = tokenizer(\"def hello_world():\", return_tensors=\"pt\", return_token_type_ids=True)\n         input_ids = tokenized.input_ids.to(torch_device)\n-        output_ids = model.generate(input_ids, do_sample=True)\n+        output_ids = model.generate(input_ids, do_sample=True, max_new_tokens=15)\n         output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n \n         token_type_ids = tokenized.token_type_ids.to(torch_device)"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 9,
        "deletions": 8
    }
}