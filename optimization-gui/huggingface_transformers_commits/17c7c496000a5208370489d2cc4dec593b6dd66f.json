{
    "author": "Xiao-Chenguang",
    "message": "Fix mixed torch.Tensor and DTensor in generate when use FSDP2 + LoRA (#42436)\n\n* Fix mixed torch.Tensor and DTensor error by registering fsdp forward for trainer.mode.generate\n\n* Apply fsdp forward register when fsdp is enabled\n\n---------\n\nCo-authored-by: yiminzme <yiminzme@gmail.com>\nCo-authored-by: Ferdinand Mom <47445085+3outeille@users.noreply.github.com>",
    "sha": "17c7c496000a5208370489d2cc4dec593b6dd66f",
    "files": [
        {
            "sha": "f460b45f396eaf59a5172746fb4d8cbf408f2a49",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17c7c496000a5208370489d2cc4dec593b6dd66f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17c7c496000a5208370489d2cc4dec593b6dd66f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=17c7c496000a5208370489d2cc4dec593b6dd66f",
            "patch": "@@ -2338,6 +2338,8 @@ def _inner_training_loop(\n \n         if self.is_fsdp_enabled:\n             self.model = self.model_wrapped = model\n+            # Fix `got mixed torch.Tensor and DTensor` error in model.generate() for FSDP2 with LoRA\n+            dist.fsdp.register_fsdp_forward_method(self.model, \"generate\")\n \n         # for the rest of this function `model` is the outside model, whether it was wrapped or not\n         if model is not self.model:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}