{
    "author": "gante",
    "message": "Docs: add more cross-references to the KV cache docs (#33323)\n\n* add more cross-references\r\n\r\n* nit\r\n\r\n* import guard\r\n\r\n* more import guards\r\n\r\n* nit\r\n\r\n* Update src/transformers/generation/configuration_utils.py",
    "sha": "2b789f27f383435b8db2fee3d10b0a1358c0c234",
    "files": [
        {
            "sha": "1a9ea1ac00190757eecec15a15630b90f1e57258",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -51,11 +51,11 @@ More concretely, key-value cache acts as a memory bank for these generative mode\n \n \n   See an example below for how to implement your own generation loop.\n-    \n+\n   ```python\n   >>> import torch\n   >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n- \n+\n   >>> model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n   >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n   >>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -69,10 +69,10 @@ More concretely, key-value cache acts as a memory bank for these generative mode\n   >>> max_new_tokens = 10\n \n   >>> for _ in range(max_new_tokens):\n-  ...     outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)     \n+  ...     outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n   ...     # Greedily sample one next token\n   ...     next_token_ids = outputs.logits[:, -1:].argmax(-1)\n-  ...     generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)   \n+  ...     generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n   ...\n   ...     # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n   ...     # and expanding attn mask for the new token, as explained above\n@@ -222,7 +222,7 @@ before successfully generating 40 beams.\n \n ### Static Cache\n \n-Since the \"DynamicCache\" dynamically grows with each generation step, it prevents you from taking advantage of JIT optimizations. The [`~StaticCache`] pre-allocates \n+Since the \"DynamicCache\" dynamically grows with each generation step, it prevents you from taking advantage of JIT optimizations. The [`~StaticCache`] pre-allocates\n a specific maximum size for the keys and values, allowing you to generate up to the maximum length without having to modify cache size. Check the below usage example.\n \n For more examples with Static Cache and JIT compilation, take a look at [StaticCache & torchcompile](./llm_optims#static-kv-cache-and-torchcompile)\n@@ -267,7 +267,7 @@ This will use the [`~OffloadedStaticCache`] implementation instead.\n \n As the name suggests, this cache type implements a sliding window over previous keys and values, retaining only the last `sliding_window` tokens. It should be used with models like Mistral that support sliding window attention. Additionally, similar to Static Cache, this one is JIT-friendly and can be used with the same compile tecniques as Static Cache.\n \n-Note that you can use this cache only for models that support sliding window, e.g. Mistral models. \n+Note that you can use this cache only for models that support sliding window, e.g. Mistral models.\n \n \n ```python\n@@ -324,7 +324,7 @@ We have seen how to use each of the cache types when generating. What if you wan\n \n The general format when doing iterative generation is as below. First you have to initialize an empty cache of the type you want, and you can start feeding in new prompts iteratively. Keeping track of dialogues history and formatting can be done with chat templates, read more on that in [chat_templating](./chat_templating)\n \n-In case you are using Sink Cache, you have to crop your inputs to that maximum length because Sink Cache can generate text longer than its maximum window size, but it expects the first input to not exceed the maximum cache length.  \n+In case you are using Sink Cache, you have to crop your inputs to that maximum length because Sink Cache can generate text longer than its maximum window size, but it expects the first input to not exceed the maximum cache length.\n \n \n ```python\n@@ -354,9 +354,9 @@ In case you are using Sink Cache, you have to crop your inputs to that maximum l\n ...     inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n ...     if isinstance(past_key_values, SinkCache):\n ...         inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n-... \n+...\n ...     input_length = inputs[\"input_ids\"].shape[1]\n-...     \n+...\n ...     outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n ...     completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n ...     messages.append({\"role\": \"assistant\", \"content\": completion})\n@@ -400,4 +400,4 @@ Sometimes you would want to first fill-in cache object with key/values for certa\n \n >>> print(responses)\n ['<s> You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTitle: The Ultimate Guide to Travelling: Tips, Tricks, and', '<s> You are a helpful assistant. What is the capital of France?\\n\\nYes, the capital of France is Paris.</s>']\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "16be638498dfd4cc602a6366b6958b2606648ade",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -24,7 +24,7 @@ This guide will show you how to use the optimization techniques available in Tra\n \n During decoding, a LLM computes the key-value (kv) values for each input token and since it is autoregressive, it computes the same kv values each time because the generated output becomes part of the input now. This is not very efficient because you're recomputing the same kv values each time.\n \n-To optimize this, you can use a kv-cache to store the past keys and values instead of recomputing them each time. However, since the kv-cache grows with each generation step and is dynamic, it prevents you from taking advantage of [`torch.compile`](./perf_torch_compile), a powerful optimization tool that fuses PyTorch code into fast and optimized kernels.\n+To optimize this, you can use a kv-cache to store the past keys and values instead of recomputing them each time. However, since the kv-cache grows with each generation step and is dynamic, it prevents you from taking advantage of [`torch.compile`](./perf_torch_compile), a powerful optimization tool that fuses PyTorch code into fast and optimized kernels. We have an entire guide dedicated to kv-caches [here](./kv_cache).\n \n The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value which allows you to combine it with `torch.compile` for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware.\n "
        },
        {
            "sha": "a675a6de39a2fcbf6d1082559beb4a4460c5d994",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -662,7 +662,7 @@ Using the key-value cache has two advantages:\n -   Significant increase in computational efficiency as less computations are performed compared to computing the full \\\\( \\mathbf{QK}^T \\\\) matrix. This leads to an increase in inference speed\n -   The maximum required memory is not increased quadratically with the number of generated tokens, but only increases linearly.\n \n-> One should *always* make use of the key-value cache as it leads to identical results and a significant speed-up for longer input sequences. Transformers has the key-value cache enabled by default when making use of the text pipeline or the [`generate` method](https://huggingface.co/docs/transformers/main_classes/text_generation).\n+> One should *always* make use of the key-value cache as it leads to identical results and a significant speed-up for longer input sequences. Transformers has the key-value cache enabled by default when making use of the text pipeline or the [`generate` method](https://huggingface.co/docs/transformers/main_classes/text_generation). We have an entire guide dedicated to caches [here](./kv_cache).\n \n <Tip warning={true}>\n "
        },
        {
            "sha": "773ef0ccfe55ebd45adaabc095f11712a885236c",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 3,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -43,11 +43,34 @@\n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n NEEDS_CACHE_CONFIG = {}\n+NEED_SETUP_CACHE_CLASSES_MAPPING = {}\n+QUANT_BACKEND_CLASSES_MAPPING = {}\n+ALL_CACHE_IMPLEMENTATIONS = []\n \n if is_torch_available():\n-    from ..cache_utils import QuantizedCacheConfig\n+    from ..cache_utils import (\n+        HQQQuantizedCache,\n+        HybridCache,\n+        MambaCache,\n+        OffloadedStaticCache,\n+        QuantizedCacheConfig,\n+        QuantoQuantizedCache,\n+        SlidingWindowCache,\n+        StaticCache,\n+    )\n \n     NEEDS_CACHE_CONFIG[\"quantized\"] = QuantizedCacheConfig\n+    NEED_SETUP_CACHE_CLASSES_MAPPING = {\n+        \"static\": StaticCache,\n+        \"offloaded_static\": OffloadedStaticCache,\n+        \"sliding_window\": SlidingWindowCache,\n+        \"hybrid\": HybridCache,\n+        \"mamba\": MambaCache,\n+    }\n+    QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n+    ALL_CACHE_IMPLEMENTATIONS = list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(\n+        QUANT_BACKEND_CLASSES_MAPPING.keys()\n+    )\n \n \n class GenerationMode(ExplicitEnum):\n@@ -70,7 +93,7 @@ class GenerationMode(ExplicitEnum):\n \n class GenerationConfig(PushToHubMixin):\n     # no-format\n-    r\"\"\"\n+    rf\"\"\"\n     Class that holds a configuration for a generation task. A `generate` call supports the following generation methods\n     for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n \n@@ -146,7 +169,10 @@ class GenerationConfig(PushToHubMixin):\n             Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n             speed up decoding.\n         cache_implementation (`str`, *optional*, default to `None`):\n-            Cache class that should be used when generating.\n+            Name of the cache class that will be instantiated in `generate`, for faster decoding. Possible values are:\n+            {ALL_CACHE_IMPLEMENTATIONS}. We support other cache types, but they must be manually instantiated and\n+            passed to `generate` through the `past_key_values` argument. See our\n+            [cache documentation](https://huggingface.co/docs/transformers/en/kv_cache) for further information.\n         cache_config (`CacheConfig` or `dict`, *optional*, default to `None`):\n             Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n             it will be converted to its repsective `CacheConfig` internally.\n@@ -699,6 +725,11 @@ def validate(self, is_init=False):\n                 )\n \n         # 5. check cache-related arguments\n+        if self.cache_implementation is not None and self.cache_implementation not in ALL_CACHE_IMPLEMENTATIONS:\n+            raise ValueError(\n+                f\"Invalid `cache_implementation` ({self.cache_implementation}). Choose one of: \"\n+                f\"{ALL_CACHE_IMPLEMENTATIONS}\"\n+            )\n         if self.cache_config is not None:\n             cache_class = NEEDS_CACHE_CONFIG.get(self.cache_implementation)\n             if cache_class is None:"
        },
        {
            "sha": "9981824b9611fdab3d13b56819525029795ba7ae",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 17,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -29,15 +29,8 @@\n     Cache,\n     DynamicCache,\n     EncoderDecoderCache,\n-    HQQQuantizedCache,\n-    HybridCache,\n-    MambaCache,\n     OffloadedCache,\n-    OffloadedStaticCache,\n     QuantizedCacheConfig,\n-    QuantoQuantizedCache,\n-    SlidingWindowCache,\n-    StaticCache,\n )\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n@@ -68,7 +61,12 @@\n     _prepare_attention_mask,\n     _prepare_token_type_ids,\n )\n-from .configuration_utils import GenerationConfig, GenerationMode\n+from .configuration_utils import (\n+    NEED_SETUP_CACHE_CLASSES_MAPPING,\n+    QUANT_BACKEND_CLASSES_MAPPING,\n+    GenerationConfig,\n+    GenerationMode,\n+)\n from .logits_process import (\n     EncoderNoRepeatNGramLogitsProcessor,\n     EncoderRepetitionPenaltyLogitsProcessor,\n@@ -118,15 +116,6 @@\n if is_accelerate_available():\n     from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n \n-NEED_SETUP_CACHE_CLASSES_MAPPING = {\n-    \"static\": StaticCache,\n-    \"offloaded_static\": OffloadedStaticCache,\n-    \"sliding_window\": SlidingWindowCache,\n-    \"hybrid\": HybridCache,\n-    \"mamba\": MambaCache,\n-}\n-QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n-\n \n @dataclass\n class GenerateDecoderOnlyOutput(ModelOutput):"
        },
        {
            "sha": "c4ae776959c7270d00468b2a22348bbbadc39992",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -551,7 +551,8 @@ def _init_weights(self, module: nn.Module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "46eea43e1285f8abb92144a7ae003da84446f3a1",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -429,7 +429,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "4010d9ec3a43274372d8a4f6b8466d260c9d59eb",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -721,7 +721,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "8db9f6e8b7d09f6889016fe5918eb3ff22a4262a",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -948,7 +948,8 @@ def _init_weights(self, module: nn.Module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "a340689a7c3f608ba2ba189427c48b38ec68f5fa",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -836,7 +836,8 @@ def forward(\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "085751cd9bc039ffbacaba06be8b7fec2a6fc7e7",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -726,7 +726,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "1909ef785015598b65cf613182a4805e6a4bdb2a",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -716,7 +716,8 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "43333e3d3338d16b69ab54ba19f382f9bcf8cb6b",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -578,7 +578,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "72590862b749f080dd6cdbedd29b53c23efcbba4",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -610,7 +610,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "259f01fd3cb1311e9a7ececa4dce1915dd5235e5",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -820,7 +820,8 @@ def forward(\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "bd7ce5696fa077c7f7625a968ef06624c0345c4a",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -621,7 +621,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "ff10b6e6d875f9f97eb89473fc8619747bd17ca1",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -720,7 +720,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "5e39c4ebbf2111ff350011b1f3ee507385d31370",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -841,7 +841,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "411dc478a1d58699f8db289c84dee43e3eb5544d",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -661,7 +661,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "5c21dd3c3f533437d78ccf7a83acf21b646ff650",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -728,8 +728,7 @@ class TFMistralPreTrainedModel(TFPreTrainedModel):\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            One formats is allowed:\n             - Tuple of `tuple(tf.Tensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "b79ff4e004fc83fd2d92c2612ecbee3126b644be",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -735,7 +735,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "007e69570e7821712f47d3d7f81dec431ab53c21",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -762,7 +762,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "df16557423d7a94d42cfb47d0f850bb44ce2a5a8",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -576,7 +576,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "a3039a5aa14ade5959c2ebc2c731146905ad4032",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -857,7 +857,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "f021c6ce2d339d74cf580fe36458a75f6ec2daf4",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -900,7 +900,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "0b6c28350b6a0302662764e7e31b26d611ff9e04",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -765,7 +765,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "03ac51a0f94f02a31ecf26ed0b50c53029913ab7",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -920,7 +920,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "00b73af8948eb12fc6e484f02779126b32f94b17",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -852,7 +852,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        },
        {
            "sha": "c9a81a36f749ba1ace2ea174b557dc0ecc4e7f31",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b789f27f383435b8db2fee3d10b0a1358c0c234/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=2b789f27f383435b8db2fee3d10b0a1358c0c234",
            "patch": "@@ -738,7 +738,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format."
        }
    ],
    "stats": {
        "total": 156,
        "additions": 99,
        "deletions": 57
    }
}