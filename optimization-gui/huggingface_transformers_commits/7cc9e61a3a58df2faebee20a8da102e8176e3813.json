{
    "author": "rootonchair",
    "message": "Add Fast Image Processor for Donut (#37081)\n\n* add donut fast image processor support\n\n* run make style\n\n* Update src/transformers/models/donut/image_processing_donut_fast.py\n\nCo-authored-by: Parteek <parteekkamboj112@gmail.com>\n\n* update test, remove none default values\n\n* add do_align_axis = True test, fix bug in slow image processor\n\n* run make style\n\n* remove np usage\n\n* make style\n\n* Apply suggestions from code review\n\n* Update src/transformers/models/donut/image_processing_donut_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* add size revert in preprocess\n\n* make style\n\n* fix copies\n\n* add test for preprocess with kwargs\n\n* make style\n\n* handle None input_data_format in align_long_axis\n\n---------\n\nCo-authored-by: Parteek <parteekkamboj112@gmail.com>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "7cc9e61a3a58df2faebee20a8da102e8176e3813",
    "files": [
        {
            "sha": "fe2d2d4fe00bdff7677c32c4a8720841b4cb51ab",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -208,6 +208,11 @@ print(answer)\n [[autodoc]] DonutImageProcessor\n     - preprocess\n \n+## DonutImageProcessorFast\n+\n+[[autodoc]] DonutImageProcessorFast\n+    - preprocess\n+\n ## DonutFeatureExtractor\n \n [[autodoc]] DonutFeatureExtractor"
        },
        {
            "sha": "4439d756382063757d257b64b26b949c18e4fa95",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -80,7 +80,7 @@\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\",)),\n-            (\"donut-swin\", (\"DonutImageProcessor\",)),\n+            (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\",)),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\",)),"
        },
        {
            "sha": "834c451f78fa0d4c5fe91f59719b6505c4c4e4e5",
            "filename": "src/transformers/models/donut/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_donut_swin import *\n     from .feature_extraction_donut import *\n     from .image_processing_donut import *\n+    from .image_processing_donut_fast import *\n     from .modeling_donut_swin import *\n     from .processing_donut import *\n else:"
        },
        {
            "sha": "72d051859a70d2cd83c1fc5c538361534aa82e7d",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -20,6 +20,7 @@\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n+    convert_to_rgb,\n     get_resize_output_image_size,\n     pad,\n     resize,\n@@ -151,10 +152,21 @@ def align_long_axis(\n         input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n         output_height, output_width = size[\"height\"], size[\"width\"]\n \n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        if input_data_format == ChannelDimension.LAST:\n+            rot_axes = (0, 1)\n+        elif input_data_format == ChannelDimension.FIRST:\n+            rot_axes = (1, 2)\n+        else:\n+            raise ValueError(f\"Unsupported data format: {input_data_format}\")\n+\n         if (output_width < output_height and input_width > input_height) or (\n             output_width > output_height and input_width < input_height\n         ):\n-            image = np.rot90(image, 3)\n+            image = np.rot90(image, 3, axes=rot_axes)\n \n         if data_format is not None:\n             image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n@@ -407,6 +419,8 @@ def preprocess(\n             resample=resample,\n         )\n \n+        images = [convert_to_rgb(image) for image in images]\n+\n         # All transformations expect numpy arrays.\n         images = [to_numpy_array(image) for image in images]\n "
        },
        {
            "sha": "be83b5cc5c6bd1b83d23ff1df9a853fbe023a74e",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "added",
            "additions": 289,
            "deletions": 0,
            "changes": 289,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -0,0 +1,289 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Donut.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class DonutFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    do_thumbnail: Optional[bool]\n+    do_align_long_axis: Optional[bool]\n+    do_pad: Optional[bool]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Donut image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        do_thumbnail (`bool`, *optional*, defaults to `self.do_thumbnail`):\n+            Whether to resize the image using thumbnail method.\n+        do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n+            Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n+        do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+            Whether to pad the image. If `random_padding` is set to `True`, each image is padded with a random\n+            amount of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n+            padded to the largest image size in the batch.\n+    \"\"\",\n+)\n+class DonutImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 2560, \"width\": 1920}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_thumbnail = True\n+    do_align_long_axis = False\n+    do_pad = True\n+    valid_kwargs = DonutFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[DonutFastImageProcessorKwargs]):\n+        size = kwargs.pop(\"size\", None)\n+        if isinstance(size, (tuple, list)):\n+            size = size[::-1]\n+        kwargs[\"size\"] = size\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            do_thumbnail (`bool`, *optional*, defaults to `self.do_thumbnail`):\n+                Whether to resize the image using thumbnail method.\n+            do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n+                Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image. If `random_padding` is set to `True`, each image is padded with a random\n+                amount of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n+                padded to the largest image size in the batch.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[DonutFastImageProcessorKwargs]) -> BatchFeature:\n+        if \"size\" in kwargs:\n+            size = kwargs.pop(\"size\")\n+            if isinstance(size, (tuple, list)):\n+                size = size[::-1]\n+            kwargs[\"size\"] = size\n+        return super().preprocess(images, **kwargs)\n+\n+    def align_long_axis(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Align the long axis of the image to the longest axis of the specified size.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                The image to be aligned.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to align the long axis to.\n+\n+        Returns:\n+            `torch.Tensor`: The aligned image.\n+        \"\"\"\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = size.height, size.width\n+\n+        if (output_width < output_height and input_width > input_height) or (\n+            output_width > output_height and input_width < input_height\n+        ):\n+            height_dim, width_dim = image.dim() - 2, image.dim() - 1\n+            image = torch.rot90(image, 3, dims=[height_dim, width_dim])\n+\n+        return image\n+\n+    def pad_image(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        random_padding: bool = False,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pad the image to the specified size.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                The image to be padded.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to pad the image to.\n+            random_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to use random padding or not.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The data format of the output image. If unset, the same format as the input image is used.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+        \"\"\"\n+        output_height, output_width = size.height, size.width\n+        input_height, input_width = image.shape[-2:]\n+\n+        delta_width = output_width - input_width\n+        delta_height = output_height - input_height\n+\n+        if random_padding:\n+            pad_top = torch.random.randint(low=0, high=delta_height + 1)\n+            pad_left = torch.random.randint(low=0, high=delta_width + 1)\n+        else:\n+            pad_top = delta_height // 2\n+            pad_left = delta_width // 2\n+\n+        pad_bottom = delta_height - pad_top\n+        pad_right = delta_width - pad_left\n+\n+        padding = (pad_left, pad_top, pad_right, pad_bottom)\n+        return F.pad(image, padding)\n+\n+    def pad(self, *args, **kwargs):\n+        logger.info(\"pad is deprecated and will be removed in version 4.27. Please use pad_image instead.\")\n+        return self.pad_image(*args, **kwargs)\n+\n+    def thumbnail(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any\n+        corresponding dimension of the specified size.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                The image to be resized.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to resize the image to.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                The resampling filter to use.\n+            data_format (`Optional[Union[str, ChannelDimension]]`, *optional*):\n+                The data format of the output image. If unset, the same format as the input image is used.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+        \"\"\"\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = size.height, size.width\n+\n+        # We always resize to the smallest of either the input or output size.\n+        height = min(input_height, output_height)\n+        width = min(input_width, output_width)\n+\n+        if height == input_height and width == input_width:\n+            return image\n+\n+        if input_height > input_width:\n+            width = int(input_width * height / input_height)\n+        elif input_width > input_height:\n+            height = int(input_height * width / input_width)\n+\n+        return self.resize(\n+            image,\n+            size=SizeDict(width=width, height=height),\n+            interpolation=F.InterpolationMode.BICUBIC,\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        do_thumbnail: bool,\n+        do_align_long_axis: bool,\n+        do_pad: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_align_long_axis:\n+                stacked_images = self.align_long_axis(image=stacked_images, size=size)\n+            if do_resize:\n+                shortest_edge = min(size.height, size.width)\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=SizeDict(shortest_edge=shortest_edge), interpolation=interpolation\n+                )\n+            if do_thumbnail:\n+                stacked_images = self.thumbnail(image=stacked_images, size=size)\n+            if do_pad:\n+                stacked_images = self.pad_image(image=stacked_images, size=size, random_padding=False)\n+\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"DonutImageProcessorFast\"]"
        },
        {
            "sha": "25b5c5e7bc858ad2437418c9925a398bf6b17dea",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -220,10 +220,21 @@ def align_long_axis(\n         input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n         output_height, output_width = size[\"height\"], size[\"width\"]\n \n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        if input_data_format == ChannelDimension.LAST:\n+            rot_axes = (0, 1)\n+        elif input_data_format == ChannelDimension.FIRST:\n+            rot_axes = (1, 2)\n+        else:\n+            raise ValueError(f\"Unsupported data format: {input_data_format}\")\n+\n         if (output_width < output_height and input_width > input_height) or (\n             output_width > output_height and input_width < input_height\n         ):\n-            image = np.rot90(image, 3)\n+            image = np.rot90(image, 3, axes=rot_axes)\n \n         if data_format is not None:\n             image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)"
        },
        {
            "sha": "29c3bff2a14b56b7186ce8c00d2f125051102f36",
            "filename": "tests/models/donut/test_image_processing_donut.py",
            "status": "modified",
            "additions": 142,
            "deletions": 104,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/7cc9e61a3a58df2faebee20a8da102e8176e3813/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7cc9e61a3a58df2faebee20a8da102e8176e3813/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py?ref=7cc9e61a3a58df2faebee20a8da102e8176e3813",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import is_flaky, require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import DonutImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import DonutImageProcessorFast\n+\n \n class DonutImageProcessingTester:\n     def __init__(\n@@ -96,6 +99,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class DonutImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = DonutImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DonutImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -106,122 +110,156 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_thumbnail\"))\n-        self.assertTrue(hasattr(image_processing, \"do_align_long_axis\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_thumbnail\"))\n+            self.assertTrue(hasattr(image_processing, \"do_align_long_axis\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 20})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 20})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+            # Previous config had dimensions in (width, height) order\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=(42, 84))\n+            self.assertEqual(image_processor.size, {\"height\": 84, \"width\": 42})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+    def test_image_processor_preprocess_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n-        # Previous config had dimensions in (width, height) order\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=(42, 84))\n-        self.assertEqual(image_processor.size, {\"height\": 84, \"width\": 42})\n+            height = 84\n+            width = 42\n+            # Previous config had dimensions in (width, height) order\n+            encoded_images = image_processing(image_inputs[0], size=(width, height), return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    height,\n+                    width,\n+                ),\n+            )\n \n     @is_flaky()\n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n \n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n \n     @is_flaky()\n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n \n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n \n     @is_flaky()\n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n \n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        self.assertEqual(\n-            encoded_images.shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                encoded_images.shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+\n+\n+@require_torch\n+@require_vision\n+class DonutImageProcessingAlignAxisTest(DonutImageProcessingTest):\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = DonutImageProcessingTester(self, do_align_axis=True)"
        }
    ],
    "stats": {
        "total": 572,
        "additions": 465,
        "deletions": 107
    }
}