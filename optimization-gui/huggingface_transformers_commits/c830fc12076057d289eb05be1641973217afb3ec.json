{
    "author": "MekkCyber",
    "message": "Adding activation kernels (#40890)\n\n* first commit\n\n* add mode\n\n* revert modeling\n\n* add compile\n\n* rm print",
    "sha": "c830fc12076057d289eb05be1641973217afb3ec",
    "files": [
        {
            "sha": "8bfd517add9f19fa48dc9a8ef58fb80cf07a2435",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c830fc12076057d289eb05be1641973217afb3ec/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c830fc12076057d289eb05be1641973217afb3ec/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=c830fc12076057d289eb05be1641973217afb3ec",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n from torch import Tensor, nn\n \n+from .integrations.hub_kernels import use_kernel_forward_from_hub\n from .utils import logging\n from .utils.import_utils import is_torchdynamo_compiling\n \n@@ -38,6 +39,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return nn.functional.gelu(input, approximate=\"tanh\")\n \n \n+@use_kernel_forward_from_hub(\"NewGELU\")\n class NewGELUActivation(nn.Module):\n     \"\"\"\n     Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n@@ -70,6 +72,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return self.act(input)\n \n \n+@use_kernel_forward_from_hub(\"FastGELU\")\n class FastGELUActivation(nn.Module):\n     \"\"\"\n     Applies GELU approximation that is slower than QuickGELU but more accurate. See: https://github.com/hendrycks/GELUs\n@@ -79,6 +82,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))\n \n \n+@use_kernel_forward_from_hub(\"QuickGELU\")\n class QuickGELUActivation(nn.Module):\n     \"\"\"\n     Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs"
        },
        {
            "sha": "5be21e2f9a51a632be02bb0e53cadafc9ed48d7d",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c830fc12076057d289eb05be1641973217afb3ec/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c830fc12076057d289eb05be1641973217afb3ec/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=c830fc12076057d289eb05be1641973217afb3ec",
            "patch": "@@ -84,6 +84,33 @@\n                 )\n             },\n         },\n+        \"FastGELU\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\",\n+                    layer_name=\"FastGELU\",\n+                    version=\">=0.0.4,<0.1.0\",\n+                )\n+            }\n+        },\n+        \"QuickGELU\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\",\n+                    layer_name=\"QuickGELU\",\n+                    version=\">=0.0.4,<0.1.0\",\n+                )\n+            }\n+        },\n+        \"NewGELU\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\",\n+                    layer_name=\"NewGELU\",\n+                    version=\">=0.0.4,<0.1.0\",\n+                )\n+            }\n+        },\n     }\n \n     register_kernel_mapping(_KERNEL_MAPPING)"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 31,
        "deletions": 0
    }
}