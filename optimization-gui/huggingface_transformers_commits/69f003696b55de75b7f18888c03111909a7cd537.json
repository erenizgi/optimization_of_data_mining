{
    "author": "molbap",
    "message": "Tiny doc fix (#42296)\n\nfix link",
    "sha": "69f003696b55de75b7f18888c03111909a7cd537",
    "files": [
        {
            "sha": "d5b2b350a9a2ff2fda0e56ffa44c96b636ba4281",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69f003696b55de75b7f18888c03111909a7cd537/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69f003696b55de75b7f18888c03111909a7cd537/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=69f003696b55de75b7f18888c03111909a7cd537",
            "patch": "@@ -125,9 +125,9 @@ If you're contributing a **vision-language model** (or any multimodal model that\n All new models should use the modular architecture pattern. Create a `modular_<model_name>.py` file using the modular model converter:\n \n - Use the CLI, [`transformers add-new-model-like`](https://github.com/huggingface/transformers/blob/main/src/transformers/cli/add_new_model_like.py) to generate a modular skeleton and get started\n-- All code should be in the modular file if possible. Modeling must be in it, it's better if configuration is in it as well. [Modular guide](./modular_transformers#implementing-a-modular-file) shows a quick way to set up a modular file.\n+- All code should be in the modular file if possible. Modeling must be in it, it's better if configuration is in it as well. [Modular guide](./docs/source/en/modular_transformers.md#implementing-a-modular-file) shows a quick way to set up a modular file.\n - Reuse existing patterns from similar models as much as possible\n-- You can make the model compatible with inference engines such as vLLM or SGLang, and enable zero-effort integration. See specific requirements for model implementation in [\"Transformers modeling backend\"](./transformers_as_backend#multimodal-models)\n+- You can make the model compatible with inference engines such as vLLM or SGLang, and enable zero-effort integration. See specific requirements for model implementation in [\"Transformers modeling backend\"](./docs/source/en/transformers_as_backend.md#multimodal-models)\n \n To verify your modular file is correct, run:\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}