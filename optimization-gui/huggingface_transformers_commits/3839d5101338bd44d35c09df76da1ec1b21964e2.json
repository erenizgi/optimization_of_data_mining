{
    "author": "SunMarc",
    "message": "`report_to` default changed to \"none\" + cleaning deprecated env var (#41375)\n\n* reporting\n\n* fix\n\n* fix",
    "sha": "3839d5101338bd44d35c09df76da1ec1b21964e2",
    "files": [
        {
            "sha": "28eb87fa0ada3677259a8a5b6615dffa6a3005b8",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=3839d5101338bd44d35c09df76da1ec1b21964e2",
            "patch": "@@ -109,7 +109,6 @@ training_args = TrainingArguments(\n     output_dir=\"my-awesome-model\",\n     num_train_epochs=30,\n     fp16=True,\n-    logging_dir=f\"{repo_name}/logs\",\n     logging_strategy=\"epoch\",\n     eval_strategy=\"epoch\",\n     save_strategy=\"epoch\","
        },
        {
            "sha": "f597e15ddd2f6971774b9c31c0c7cc3ee69727b6",
            "filename": "docs/source/ja/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=3839d5101338bd44d35c09df76da1ec1b21964e2",
            "patch": "@@ -110,7 +110,6 @@ training_args = TrainingArguments(\n     output_dir=\"my-awesome-model\",\n     num_train_epochs=30,\n     fp16=True,\n-    logging_dir=f\"{repo_name}/logs\",\n     logging_strategy=\"epoch\",\n     eval_strategy=\"epoch\",\n     save_strategy=\"epoch\","
        },
        {
            "sha": "863c27890733294f99c24be02135e4e553c7fb50",
            "filename": "docs/source/ko/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fko%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3839d5101338bd44d35c09df76da1ec1b21964e2/docs%2Fsource%2Fko%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=3839d5101338bd44d35c09df76da1ec1b21964e2",
            "patch": "@@ -115,7 +115,6 @@ training_args = TrainingArguments(\n     output_dir=\"my-awesome-model\",\n     num_train_epochs=30,\n     fp16=True,\n-    logging_dir=f\"{repo_name}/logs\",\n     logging_strategy=\"epoch\",\n     eval_strategy=\"epoch\",\n     save_strategy=\"epoch\","
        },
        {
            "sha": "73d3a1ab3b72691b85f426913eee1eb8fd8f3fac",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 83,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/3839d5101338bd44d35c09df76da1ec1b21964e2/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3839d5101338bd44d35c09df76da1ec1b21964e2/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=3839d5101338bd44d35c09df76da1ec1b21964e2",
            "patch": "@@ -103,13 +103,6 @@\n \n # Integration functions:\n def is_wandb_available():\n-    # any value of WANDB_DISABLED disables wandb\n-    if os.getenv(\"WANDB_DISABLED\", \"\").upper() in ENV_VARS_TRUE_VALUES:\n-        logger.warning(\n-            \"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the \"\n-            \"--report_to flag to control the integrations used for logging result (for instance --report_to none).\"\n-        )\n-        return False\n     if importlib.util.find_spec(\"wandb\") is not None:\n         import wandb\n \n@@ -129,13 +122,6 @@ def is_clearml_available():\n \n \n def is_comet_available():\n-    if os.getenv(\"COMET_MODE\", \"\").upper() == \"DISABLED\":\n-        logger.warning(\n-            \"Using the `COMET_MODE=DISABLED` environment variable is deprecated and will be removed in v5. Use the \"\n-            \"--report_to flag to control the integrations used for logging result (for instance --report_to none).\"\n-        )\n-        return False\n-\n     if _is_comet_installed is False:\n         return False\n \n@@ -557,56 +543,65 @@ def rewrite_logs(d):\n     return new_d\n \n \n+def default_logdir() -> str:\n+    \"\"\"\n+    Same default as PyTorch\n+    \"\"\"\n+    import socket\n+    from datetime import datetime\n+\n+    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n+    return os.path.join(\"runs\", current_time + \"_\" + socket.gethostname())\n+\n+\n class TensorBoardCallback(TrainerCallback):\n     \"\"\"\n     A [`TrainerCallback`] that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).\n \n     Args:\n         tb_writer (`SummaryWriter`, *optional*):\n             The writer to use. Will instantiate one if not set.\n+    Environment:\n+        - **TENSORBOARD_LOGGING_DIR** (`str`, *optional*, defaults to `None`):\n+            The logging dir to log the results. Default value is os.path.join(args.output_dir, default_logdir())\n     \"\"\"\n \n     def __init__(self, tb_writer=None):\n-        has_tensorboard = is_tensorboard_available()\n-        if not has_tensorboard:\n+        if not is_tensorboard_available():\n             raise RuntimeError(\n                 \"TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\"\n                 \" install tensorboardX.\"\n             )\n-        if has_tensorboard:\n-            try:\n-                from torch.utils.tensorboard import SummaryWriter\n-\n-                self._SummaryWriter = SummaryWriter\n-            except ImportError:\n-                try:\n-                    from tensorboardX import SummaryWriter\n+        try:\n+            from torch.utils.tensorboard import SummaryWriter\n+        except ImportError:\n+            from tensorboardX import SummaryWriter\n \n-                    self._SummaryWriter = SummaryWriter\n-                except ImportError:\n-                    self._SummaryWriter = None\n-        else:\n-            self._SummaryWriter = None\n+        self._SummaryWriter = SummaryWriter\n         self.tb_writer = tb_writer\n+        self.logging_dir = os.getenv(\"TENSORBOARD_LOGGING_DIR\", None)\n+        if self.logging_dir is not None:\n+            self.logging_dir = os.path.expanduser(self.logging_dir)\n \n-    def _init_summary_writer(self, args, log_dir=None):\n-        log_dir = log_dir or args.logging_dir\n+    def _init_summary_writer(self, args):\n         if self._SummaryWriter is not None:\n-            self.tb_writer = self._SummaryWriter(log_dir=log_dir)\n+            self.tb_writer = self._SummaryWriter(log_dir=self.logging_dir)\n \n     def on_train_begin(self, args, state, control, **kwargs):\n         if not state.is_world_process_zero:\n             return\n \n-        log_dir = None\n-\n         if state.is_hyper_param_search:\n             trial_name = state.trial_name\n             if trial_name is not None:\n-                log_dir = os.path.join(args.logging_dir, trial_name)\n+                # overwrite logging dir for trials\n+                self.logging_dir = os.path.join(args.output_dir, default_logdir(), trial_name)\n+\n+        if self.logging_dir is None:\n+            self.logging_dir = os.path.join(args.output_dir, default_logdir())\n \n         if self.tb_writer is None:\n-            self._init_summary_writer(args, log_dir)\n+            self._init_summary_writer(args)\n \n         if self.tb_writer is not None:\n             self.tb_writer.add_text(\"args\", args.to_json_string())\n@@ -671,13 +666,6 @@ def is_enabled(self) -> bool:\n     def _missing_(cls, value: Any) -> \"WandbLogModel\":\n         if not isinstance(value, str):\n             raise TypeError(f\"Expecting to have a string `WANDB_LOG_MODEL` setting, but got {type(value)}\")\n-        if value.upper() in ENV_VARS_TRUE_VALUES:\n-            raise DeprecationWarning(\n-                f\"Setting `WANDB_LOG_MODEL` as {os.getenv('WANDB_LOG_MODEL')} is deprecated and will be removed in \"\n-                \"version 5 of transformers. Use one of `'end'` or `'checkpoint'` instead.\"\n-            )\n-            logger.info(f\"Setting `WANDB_LOG_MODEL` from {os.getenv('WANDB_LOG_MODEL')} to `end` instead\")\n-            return WandbLogModel.END\n         logger.warning(\n             f\"Received unrecognized `WANDB_LOG_MODEL` setting value={value}; so disabling `WANDB_LOG_MODEL`\"\n         )\n@@ -692,24 +680,11 @@ class WandbCallback(TrainerCallback):\n     def __init__(self):\n         has_wandb = is_wandb_available()\n         if not has_wandb:\n-            # Check if wandb is actually installed but disabled via WANDB_DISABLED\n-            if importlib.util.find_spec(\"wandb\") is not None:\n-                # wandb is installed but disabled\n-                wandb_disabled = os.getenv(\"WANDB_DISABLED\", \"\").upper() in ENV_VARS_TRUE_VALUES\n-                if wandb_disabled:\n-                    raise RuntimeError(\n-                        \"You specified `report_to='wandb'` but also set the `WANDB_DISABLED` environment variable.\\n\"\n-                        \"This disables wandb logging, even though it was explicitly requested.\\n\\n\"\n-                        \"- To enable wandb logging: unset `WANDB_DISABLED`.\\n\"\n-                        \"- To disable logging: use `report_to='none'`.\\n\\n\"\n-                        \"Note: WANDB_DISABLED is deprecated and will be removed in v5.\"\n-                    )\n-            # If wandb is not installed at all, use the original error message\n             raise RuntimeError(\"WandbCallback requires wandb to be installed. Run `pip install wandb`.\")\n-        if has_wandb:\n-            import wandb\n \n-            self._wandb = wandb\n+        import wandb\n+\n+        self._wandb = wandb\n         self._initialized = False\n         self._log_model = WandbLogModel(os.getenv(\"WANDB_LOG_MODEL\", \"false\"))\n \n@@ -727,19 +702,11 @@ def setup(self, args, state, model, **kwargs):\n             to `\"end\"`, the model will be uploaded at the end of training. If set to `\"checkpoint\"`, the checkpoint\n             will be uploaded every `args.save_steps` . If set to `\"false\"`, the model will not be uploaded. Use along\n             with [`~transformers.TrainingArguments.load_best_model_at_end`] to upload best model.\n-\n-            <Deprecated version=\"5.0\">\n-\n-            Setting `WANDB_LOG_MODEL` as `bool` will be deprecated in version 5 of ðŸ¤— Transformers.\n-\n-            </Deprecated>\n         - **WANDB_WATCH** (`str`, *optional* defaults to `\"false\"`):\n             Can be `\"gradients\"`, `\"all\"`, `\"parameters\"`, or `\"false\"`. Set to `\"all\"` to log gradients and\n             parameters.\n         - **WANDB_PROJECT** (`str`, *optional*, defaults to `\"huggingface\"`):\n             Set this to a custom string to store results in a different project.\n-        - **WANDB_DISABLED** (`bool`, *optional*, defaults to `False`):\n-            Whether to disable wandb entirely. Set `WANDB_DISABLED=true` to disable.\n         \"\"\"\n         if self._wandb is None:\n             return\n@@ -749,9 +716,6 @@ def setup(self, args, state, model, **kwargs):\n         from wandb.sdk.lib.config_util import ConfigError as WandbConfigError\n \n         if state.is_world_process_zero:\n-            logger.info(\n-                'Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"'\n-            )\n             combined_dict = {**args.to_dict()}\n \n             if hasattr(model, \"config\") and model.config is not None:\n@@ -1108,13 +1072,8 @@ def setup(self, args, state, model):\n                 * `create`: Always create a new Comet Experiment.\n                 * `get`: Always try to append to an Existing Comet Experiment.\n                   Requires `COMET_EXPERIMENT_KEY` to be set.\n-                * `ONLINE`: **deprecated**, used to create an online\n-                  Experiment. Use `COMET_START_ONLINE=1` instead.\n-                * `OFFLINE`: **deprecated**, used to created an offline\n-                  Experiment. Use `COMET_START_ONLINE=0` instead.\n-                * `DISABLED`: **deprecated**, used to disable Comet logging.\n-                  Use the `--report_to` flag to control the integrations used\n-                  for logging result instead.\n+        - **COMET_START_ONLINE** (`bool`, *optional*):\n+            Whether to create an online or offline Experiment.\n         - **COMET_PROJECT_NAME** (`str`, *optional*):\n             Comet project name for experiments.\n         - **COMET_LOG_ASSETS** (`str`, *optional*, defaults to `TRUE`):\n@@ -1136,12 +1095,7 @@ def setup(self, args, state, model):\n \n             if comet_old_mode is not None:\n                 comet_old_mode = comet_old_mode.lower()\n-\n-                if comet_old_mode == \"online\":\n-                    online = True\n-                elif comet_old_mode == \"offline\":\n-                    online = False\n-                elif comet_old_mode in (\"get\", \"get_or_create\", \"create\"):\n+                if comet_old_mode in (\"get\", \"get_or_create\", \"create\"):\n                     mode = comet_old_mode\n                 elif comet_old_mode:\n                     logger.warning(\"Invalid COMET_MODE env value %r, Comet logging is disabled\", comet_old_mode)"
        },
        {
            "sha": "0d77e0ee441c2dbcc8243c89510ec27095bb2736",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 3,
            "deletions": 29,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/3839d5101338bd44d35c09df76da1ec1b21964e2/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3839d5101338bd44d35c09df76da1ec1b21964e2/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=3839d5101338bd44d35c09df76da1ec1b21964e2",
            "patch": "@@ -107,17 +107,6 @@\n     smp.init()\n \n \n-def default_logdir() -> str:\n-    \"\"\"\n-    Same default as PyTorch\n-    \"\"\"\n-    import socket\n-    from datetime import datetime\n-\n-    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n-    return os.path.join(\"runs\", current_time + \"_\" + socket.gethostname())\n-\n-\n def get_int_from_env(env_keys, default):\n     \"\"\"Returns the first positive env value found in the `env_keys` list or the default.\"\"\"\n     for e in env_keys:\n@@ -312,9 +301,6 @@ class TrainingArguments:\n         log_on_each_node (`bool`, *optional*, defaults to `True`):\n             In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n             node.\n-        logging_dir (`str`, *optional*):\n-            [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n-            *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n         logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n             The logging strategy to adopt during training. Possible values are:\n \n@@ -605,7 +591,7 @@ class TrainingArguments:\n             Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n             than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n             instance of `Dataset`.\n-        report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n+        report_to (`str` or `list[str]`, *optional*, defaults to `\"none\"`):\n             The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n             `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n             `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all integrations\n@@ -914,7 +900,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    logging_dir: Optional[str] = field(default=None, metadata={\"help\": \"Tensorboard log dir.\"})\n     logging_strategy: Union[IntervalStrategy, str] = field(\n         default=\"steps\",\n         metadata={\"help\": \"The logging strategy to use.\"},\n@@ -1199,7 +1184,7 @@ class TrainingArguments:\n         metadata={\"help\": \"Column name with precomputed lengths to use when grouping by length.\"},\n     )\n     report_to: Union[None, str, list[str]] = field(\n-        default=None, metadata={\"help\": \"The list of integrations to report the results and logs to.\"}\n+        default=\"none\", metadata={\"help\": \"The list of integrations to report the results and logs to.\"}\n     )\n     project: str = field(\n         default=\"huggingface\",\n@@ -1466,10 +1451,6 @@ def __post_init__(self):\n         # see https://github.com/huggingface/transformers/issues/10628\n         if self.output_dir is not None:\n             self.output_dir = os.path.expanduser(self.output_dir)\n-        if self.logging_dir is None and self.output_dir is not None:\n-            self.logging_dir = os.path.join(self.output_dir, default_logdir())\n-        if self.logging_dir is not None:\n-            self.logging_dir = os.path.expanduser(self.logging_dir)\n \n         if self.disable_tqdm is None:\n             self.disable_tqdm = logger.getEffectiveLevel() > logging.WARN\n@@ -1672,13 +1653,6 @@ def __post_init__(self):\n             mixed_precision_dtype = \"bf16\"\n         os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n \n-        if self.report_to is None:\n-            logger.info(\n-                \"The default value for the training argument `--report_to` will change in v5 (from all installed \"\n-                \"integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as \"\n-                \"now. You should start updating your code and make this info disappear :-).\"\n-            )\n-            self.report_to = \"all\"\n         if self.report_to == \"all\" or self.report_to == [\"all\"]:\n             # Import at runtime to avoid a circular import.\n             from .integrations import get_available_reporting_integrations\n@@ -2548,7 +2522,7 @@ def set_logging(\n                 Logger log level to use on the main process. Possible choices are the log levels as strings: `\"debug\"`,\n                 `\"info\"`, `\"warning\"`, `\"error\"` and `\"critical\"`, plus a `\"passive\"` level which doesn't set anything\n                 and lets the application set the level.\n-            report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n+            report_to (`str` or `list[str]`, *optional*, defaults to `\"none\"`):\n                 The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n                 `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`,\n                 `\"neptune\"`, `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 40,
        "deletions": 115
    }
}