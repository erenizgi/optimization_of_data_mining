{
    "author": "Cyrilvallez",
    "message": "A bit of cleaning ðŸ§¹ðŸ§¹ (#37215)\n\n* cleaning\n\n* CIs",
    "sha": "cdfb018d0300fef3b07d9220f3efe9c2a9974662",
    "files": [
        {
            "sha": "cac9ba1eea68a0040be2b6169d34a3a68f2e6d42",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdfb018d0300fef3b07d9220f3efe9c2a9974662/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdfb018d0300fef3b07d9220f3efe9c2a9974662/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=cdfb018d0300fef3b07d9220f3efe9c2a9974662",
            "patch": "@@ -298,24 +298,6 @@ def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n         return first_tuple[1].device\n \n \n-def get_first_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n-    \"\"\"\n-    Returns the first parameter dtype (can be non-floating) or asserts if none were found.\n-    \"\"\"\n-    try:\n-        return next(parameter.parameters()).dtype\n-    except StopIteration:\n-        # For nn.DataParallel compatibility in PyTorch > 1.5\n-\n-        def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n-            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n-            return tuples\n-\n-        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n-        first_tuple = next(gen)\n-        return first_tuple[1].dtype\n-\n-\n def get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     \"\"\"\n     Returns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\n@@ -365,17 +347,6 @@ def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n     return last_dtype\n \n \n-def get_state_dict_float_dtype(state_dict):\n-    \"\"\"\n-    Returns the first found floating dtype in `state_dict` or asserts if none were found.\n-    \"\"\"\n-    for t in state_dict.values():\n-        if t.is_floating_point():\n-            return t.dtype\n-\n-    raise ValueError(\"couldn't find any floating point dtypes in state_dict\")\n-\n-\n def get_state_dict_dtype(state_dict):\n     \"\"\"\n     Returns the first found floating dtype in `state_dict` if there is one, otherwise returns the first dtype."
        },
        {
            "sha": "c68f19a5cb9dd46b8a68b71172f0d6b6c25cda9d",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 0,
            "deletions": 72,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdfb018d0300fef3b07d9220f3efe9c2a9974662/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdfb018d0300fef3b07d9220f3efe9c2a9974662/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=cdfb018d0300fef3b07d9220f3efe9c2a9974662",
            "patch": "@@ -527,78 +527,6 @@ def cached_files(\n     return resolved_files\n \n \n-# TODO cyril: Deprecated and should be removed in 4.51\n-def get_file_from_repo(\n-    *args,\n-    **kwargs,\n-):\n-    \"\"\"\n-    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\n-\n-    Args:\n-        path_or_repo (`str` or `os.PathLike`):\n-            This can be either:\n-\n-            - a string, the *model id* of a model repo on huggingface.co.\n-            - a path to a *directory* potentially containing the file.\n-        filename (`str`):\n-            The name of the file to locate in `path_or_repo`.\n-        cache_dir (`str` or `os.PathLike`, *optional*):\n-            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n-            cache should not be used.\n-        force_download (`bool`, *optional*, defaults to `False`):\n-            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n-            exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n-            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n-            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n-        token (`str` or *bool*, *optional*):\n-            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n-            when running `huggingface-cli login` (stored in `~/.huggingface`).\n-        revision (`str`, *optional*, defaults to `\"main\"`):\n-            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n-            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n-            identifier allowed by git.\n-        local_files_only (`bool`, *optional*, defaults to `False`):\n-            If `True`, will only try to load the tokenizer configuration from local files.\n-        subfolder (`str`, *optional*, defaults to `\"\"`):\n-            In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n-            specify the folder name here.\n-\n-    <Tip>\n-\n-    Passing `token=True` is required when you want to use a private model.\n-\n-    </Tip>\n-\n-    Returns:\n-        `Optional[str]`: Returns the resolved file (to the cache folder if downloaded from a repo) or `None` if the\n-        file does not exist.\n-\n-    Examples:\n-\n-    ```python\n-    # Download a tokenizer configuration from huggingface.co and cache.\n-    tokenizer_config = get_file_from_repo(\"google-bert/bert-base-uncased\", \"tokenizer_config.json\")\n-    # This model does not have a tokenizer config so the result will be None.\n-    tokenizer_config = get_file_from_repo(\"FacebookAI/xlm-roberta-base\", \"tokenizer_config.json\")\n-    ```\n-    \"\"\"\n-    logger.warning(\n-        \"`get_file_from_repo` is deprecated and will be removed in version 4.51. Use `cached_file` instead.\"\n-    )\n-    return cached_file(\n-        *args,\n-        _raise_exceptions_for_gated_repo=False,\n-        _raise_exceptions_for_missing_entries=False,\n-        _raise_exceptions_for_connection_errors=False,\n-        **kwargs,\n-    )\n-\n-\n def download_url(url, proxies=None):\n     \"\"\"\n     Downloads a given url in a temporary file. This function is not safe to use in multiple processes. Its only use is"
        }
    ],
    "stats": {
        "total": 101,
        "additions": 0,
        "deletions": 101
    }
}