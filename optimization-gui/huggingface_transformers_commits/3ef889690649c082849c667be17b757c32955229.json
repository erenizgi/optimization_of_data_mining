{
    "author": "bzhangGo",
    "message": "Encoder-Decoder Gemma (#38332)\n\n* Initial submit\n\n* Fix bugs:\n1. add __init__ file\n2. tied word embedding\n3. support flash/flex attention\n4. model saving and loading\n\n* Code refactor:\n* Rename encdecgemma to t5gemma.\n* Split attention into self- and cross-attention\n* Split stack into encoder and decoder\n* Add test cases\n* Add auto configuration\n\n* Update configurations.\n\n* Fix bugs related to copy and attribute checks\n\n* Fix type union\n\n* Fix merge errors\n\n* run ruff format\n\n* Run make style and update tests.\n\n* Add t5gemma model doc.\n\n* ruff and style formatting.\n\n* Add missed module config.\n\n* Add dummy checkpoint link to pass tests (need updated when real checkpoints are uplioaded.).\n\n* Update model doc.\n\n* Minor updates following Arthur's comments:\n* replace docstrings with auto_docstrings\n* remove checkpoint layers\n* remove deprecate_kwargs\n\n* fix rebase errors\n\n* Fix docstring issues.\n\n* fix t5gemma doc issue.\n\n* run ruff format\n\n* Updates:\n* split encoder-only model out\n* make t5gemmamodel encoder-decoder only\n* update token and sequence classification\n* update tests",
    "sha": "3ef889690649c082849c667be17b757c32955229",
    "files": [
        {
            "sha": "bf089a0f6a6e2fcfb3533c9dea0d17c1c07e2ae6",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -655,6 +655,8 @@\n         title: SwitchTransformers\n       - local: model_doc/t5\n         title: T5\n+      - local: model_doc/t5gemma\n+        title: T5Gemma\n       - local: model_doc/t5v1.1\n         title: T5v1.1\n       - local: model_doc/tapex"
        },
        {
            "sha": "d8615a9add12c18cada00e2cc1cd2cec9b0814db",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "added",
            "additions": 107,
            "deletions": 0,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,107 @@\n+\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# T5Gemma\n+\n+T5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://arxiv.org/abs/2504.06225) by Google. It is a family of encoder-decoder large langauge models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\n+\n+T5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the offical Gemma 2 models (2B and 9B); and 2) [T5](https://arxiv.org/abs/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\n+\n+The pretrained varaints are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned varaints was post-trained with supervised fine-tuning and reinforcement learning.\n+\n+The example below demonstrates how to chat with the model with [`Pipeline`] or the [`AutoModel`] class, and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text2text-generation\",\n+    model=\"google/t5gemma-placeholder\",\n+    torch_dtype=torch.bfloat16,\n+    device=\"cuda\",\n+)\n+\n+pipe(\"Question: Why is the sky blue?\\nAnswer:\", max_new_tokens=50)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```python\n+import torch\n+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"google/t5gemma-placeholder\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"google/t5gemma-placeholder\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+\n+input_text = \"Question: Why is the sky blue?\\nAnswer:\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+outputs = model.generate(**input_ids, max_new_tokens=32)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n+\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```\n+echo -e \"Question: Why is the sky blue? Answer:\" | transformers run --task text2text-generation --model google/t5gemma-placeholder --device 0\n+```\n+\n+## T5GemmaConfig\n+\n+[[autodoc]] T5GemmaConfig\n+\n+## T5GemmaModuleConfig\n+\n+[[autodoc]] T5GemmaModuleConfig\n+\n+## T5GemmaModel\n+\n+[[autodoc]] T5GemmaModel\n+    - forward\n+\n+## T5GemmaEncoderModel\n+\n+[[autodoc]] T5GemmaEncoderModel\n+    - forward\n+\n+## T5GemmaForConditionalGeneration\n+\n+[[autodoc]] T5GemmaForConditionalGeneration\n+    - forward\n+\n+## T5GemmaForSequenceClassification\n+\n+[[autodoc]] T5GemmaForSequenceClassification\n+    - forward\n+\n+## T5GemmaForTokenClassification\n+\n+[[autodoc]] T5GemmaForTokenClassification\n+    - forward"
        },
        {
            "sha": "c53fdfc7a3864ef66bd1e0ebf308f499dab28fb1",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -294,6 +294,7 @@\n     from .swinv2 import *\n     from .switch_transformers import *\n     from .t5 import *\n+    from .t5gemma import *\n     from .table_transformer import *\n     from .tapas import *\n     from .textnet import *"
        },
        {
            "sha": "3758e237e26d8f77a604d336688b6860d821ee3d",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -333,6 +333,7 @@\n         (\"swinv2\", \"Swinv2Config\"),\n         (\"switch_transformers\", \"SwitchTransformersConfig\"),\n         (\"t5\", \"T5Config\"),\n+        (\"t5gemma\", \"T5GemmaConfig\"),\n         (\"table-transformer\", \"TableTransformerConfig\"),\n         (\"tapas\", \"TapasConfig\"),\n         (\"textnet\", \"TextNetConfig\"),\n@@ -721,6 +722,7 @@\n         (\"swinv2\", \"Swin Transformer V2\"),\n         (\"switch_transformers\", \"SwitchTransformers\"),\n         (\"t5\", \"T5\"),\n+        (\"t5gemma\", \"T5Gemma\"),\n         (\"t5v1.1\", \"T5v1.1\"),\n         (\"table-transformer\", \"Table Transformer\"),\n         (\"tapas\", \"TAPAS\"),"
        },
        {
            "sha": "935eb8fe8a3da5a094bb7bd405fab877b7cb93cf",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -310,6 +310,7 @@\n         (\"swinv2\", \"Swinv2Model\"),\n         (\"switch_transformers\", \"SwitchTransformersModel\"),\n         (\"t5\", \"T5Model\"),\n+        (\"t5gemma\", \"T5GemmaModel\"),\n         (\"table-transformer\", \"TableTransformerModel\"),\n         (\"tapas\", \"TapasModel\"),\n         (\"textnet\", \"TextNetModel\"),\n@@ -430,6 +431,7 @@\n         (\"squeezebert\", \"SqueezeBertForMaskedLM\"),\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n+        (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n         (\"transfo-xl\", \"TransfoXLLMHeadModel\"),\n         (\"tvlt\", \"TvltForPreTraining\"),\n@@ -524,6 +526,7 @@\n         (\"squeezebert\", \"SqueezeBertForMaskedLM\"),\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n+        (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"tapas\", \"TapasForMaskedLM\"),\n         (\"transfo-xl\", \"TransfoXLLMHeadModel\"),\n         (\"wav2vec2\", \"Wav2Vec2ForMaskedLM\"),\n@@ -1044,6 +1047,7 @@\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2ForTextToText\"),\n         (\"switch_transformers\", \"SwitchTransformersForConditionalGeneration\"),\n         (\"t5\", \"T5ForConditionalGeneration\"),\n+        (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"umt5\", \"UMT5ForConditionalGeneration\"),\n         (\"xlm-prophetnet\", \"XLMProphetNetForConditionalGeneration\"),\n     ]\n@@ -1156,6 +1160,7 @@\n         (\"stablelm\", \"StableLmForSequenceClassification\"),\n         (\"starcoder2\", \"Starcoder2ForSequenceClassification\"),\n         (\"t5\", \"T5ForSequenceClassification\"),\n+        (\"t5gemma\", \"T5GemmaForSequenceClassification\"),\n         (\"tapas\", \"TapasForSequenceClassification\"),\n         (\"transfo-xl\", \"TransfoXLForSequenceClassification\"),\n         (\"umt5\", \"UMT5ForSequenceClassification\"),\n@@ -1349,6 +1354,7 @@\n         (\"stablelm\", \"StableLmForTokenClassification\"),\n         (\"starcoder2\", \"Starcoder2ForTokenClassification\"),\n         (\"t5\", \"T5ForTokenClassification\"),\n+        (\"t5gemma\", \"T5GemmaForTokenClassification\"),\n         (\"umt5\", \"UMT5ForTokenClassification\"),\n         (\"xlm\", \"XLMForTokenClassification\"),\n         (\"xlm-roberta\", \"XLMRobertaForTokenClassification\"),\n@@ -1582,6 +1588,7 @@\n         (\"roformer\", \"RoFormerModel\"),\n         (\"squeezebert\", \"SqueezeBertModel\"),\n         (\"t5\", \"T5EncoderModel\"),\n+        (\"t5gemma\", \"T5GemmaEncoderModel\"),\n         (\"umt5\", \"UMT5EncoderModel\"),\n         (\"xlm\", \"XLMModel\"),\n         (\"xlm-roberta\", \"XLMRobertaModel\"),"
        },
        {
            "sha": "50a1a2732c31b25e170f8550d6dae49335206316",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -582,6 +582,13 @@\n                 \"T5TokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\n+            \"t5gemma\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"tapas\", (\"TapasTokenizer\", None)),\n         (\"tapex\", (\"TapexTokenizer\", None)),\n         (\"transfo-xl\", (\"TransfoXLTokenizer\", None)),"
        },
        {
            "sha": "aa8099e267826725cd4e6abcbb5098e4f37cc96e",
            "filename": "src/transformers/models/t5gemma/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2F__init__.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_encdecgemma2 import *\n+    from .modeling_encdecgemma2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "b3aa23d0becb8faac51fc9271f172648ce6a99bb",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "added",
            "additions": 333,
            "deletions": 0,
            "changes": 333,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,333 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/t5gemma/modular_t5gemma.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_t5gemma.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Optional, Union\n+\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+\n+\n+class T5GemmaModuleConfig(PretrainedConfig):\n+    r\"\"\"\n+        This is the configuration class to store the configuration of a [`T5GemmaModuleModel`]. It is used to instantiate an T5GemmaModule\n+        model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+        defaults will yield a similar configuration to that of the T5GemmaModule-7B.\n+        e.g. [google/t5_gemma_module-7b](https://huggingface.co/google/t5_gemma_module-7b)\n+        Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+        documentation from [`PretrainedConfig`] for more information.\n+        Args:\n+            vocab_size (`int`, *optional*, defaults to 256000):\n+                Vocabulary size of the T5GemmaModule model. Defines the number of different tokens that can be represented by the\n+                `inputs_ids` passed when calling [`T5GemmaModuleModel`]\n+            hidden_size (`int`, *optional*, defaults to 2304):\n+                Dimension of the hidden representations.\n+            intermediate_size (`int`, *optional*, defaults to 9216):\n+                Dimension of the MLP representations.\n+            num_hidden_layers (`int`, *optional*, defaults to 26):\n+                Number of hidden layers in the Transformer decoder.\n+            num_attention_heads (`int`, *optional*, defaults to 8):\n+                Number of attention heads for each attention layer in the Transformer decoder.\n+            num_key_value_heads (`int`, *optional*, defaults to 4):\n+                This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+                `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+                `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+                converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+                by meanpooling all the original heads within that group. For more details, check out [this\n+                paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+                `num_attention_heads`.\n+            head_dim (`int`, *optional*, defaults to 256):\n+                The attention head dimension.\n+            hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+                The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+                if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+            max_position_embeddings (`int`, *optional*, defaults to 8192):\n+                The maximum sequence length that this model might ever be used with.\n+            initializer_range (`float`, *optional*, defaults to 0.02):\n+                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+            rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+                The epsilon used by the rms normalization layers.\n+            use_cache (`bool`, *optional*, defaults to `True`):\n+                Whether or not the model should return the last key/values attentions (not used by all models). Only\n+                relevant if `config.is_decoder=True`.\n+            pad_token_id (`int`, *optional*, defaults to 0):\n+                Padding token id.\n+            eos_token_id (`int`, *optional*, defaults to 1):\n+                End of stream token id.\n+            bos_token_id (`int`, *optional*, defaults to 2):\n+                Beginning of stream token id.\n+            tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+                Whether to tie weight embeddings\n+            rope_theta (`float`, *optional*, defaults to 10000.0):\n+                The base period of the RoPE embeddings.\n+            attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+                Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+            attention_dropout (`float`, *optional*, defaults to 0.0):\n+                The dropout ratio for the attention probabilities.\n+            query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+                scaling factor used on the attention scores\n+            sliding_window (`int`, *optional*, defaults to 4096):\n+                in T5GemmaModule, every other layer uses sliding window attention. This is the size of the sliding window.\n+            layer_types (`list`, *optional*):\n+                Attention pattern for each layer.\n+            final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+                scaling factor when applying tanh softcapping on the logits.\n+            attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+                scaling factor when applying tanh softcapping on the attention scores.\n+\n+        ```python\n+        >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig\n+        >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration\n+        >>> configuration = T5GemmaModuleConfig()\n+        >>> # Initializing a model from the t5_gemma_module-7b style configuration\n+        >>> model = T5GemmaModuleModel(configuration)\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    Module config (encoder or decoder): the same as Gemma2Config.\"\"\"\n+\n+    model_type = \"t5_gemma_module\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=256000,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n+        head_dim=256,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        query_pre_attn_scalar=256,\n+        sliding_window=4096,\n+        layer_types=None,\n+        final_logit_softcapping=30.0,\n+        attn_logit_softcapping=50.0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n+\n+class T5GemmaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5GemmaModel`]. It is used to instantiate an T5Gemma\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to a hypothetical balanced Gemma2 encoder-decoder model.\n+    e.g. [google/t5gemma-placeholder](https://huggingface.co/google/t5gemma-placeholder)\n+    ```python\n+    >>> from transformers import T5GemmaConfig, T5GemmaModel\n+    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-placeholder\")\n+    >>> model = T5GemmaModel(t5gemma_config)\n+    ```\n+    Configuration objects inherit from [PretrainedConfig] and can be used to control the model outputs. Read the\n+    documentation from [PretrainedConfig] for more information.\n+    Args:\n+        encoder (`Union[T5GemmaModuleConfig, dict]`, optional, *optional*):\n+            Configuration for the encoder.\n+        decoder (`Union[T5GemmaModuleConfig, dict]`, optional, *optional*):\n+            Configuration for the decoder.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers (following T5).\n+        classifier_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for classifier (following T5).\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for attention.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether tie input and output embeddings.\n+        kwargs (additional keyword arguments, optional, *optional*):\n+            Will be passed to the PretrainedConfig base class.\n+    \"\"\"\n+\n+    model_type = \"t5gemma\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        # encoder\n+        \"encoder.layers.*.self_attn.q_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.k_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.v_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"encoder.layers.*.mlp.gate_proj\": \"colwise\",\n+        \"encoder.layers.*.mlp.up_proj\": \"colwise\",\n+        \"encoder.layers.*.mlp.down_proj\": \"rowwise\",\n+        # decoder\n+        \"decoder.layers.*.self_attn.q_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.k_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.v_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"decoder.layers.*.cross_attn.q_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.k_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.v_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.o_proj\": \"rowwise\",\n+        \"decoder.layers.*.mlp.gate_proj\": \"colwise\",\n+        \"decoder.layers.*.mlp.up_proj\": \"colwise\",\n+        \"decoder.layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        # encoder\n+        \"encoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"encoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"encoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+        # decoder\n+        \"decoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"decoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"decoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        encoder: Optional[Union[T5GemmaModuleConfig, dict[Any, Any]]] = None,\n+        decoder: Optional[Union[T5GemmaModuleConfig, dict[Any, Any]]] = None,\n+        is_encoder_decoder: bool = True,\n+        dropout_rate: float = 0.0,\n+        classifier_dropout_rate: float = 0.0,\n+        attention_dropout: float = 0.0,\n+        tie_word_embeddings: bool = True,\n+        **kwargs,\n+    ):\n+        # Encoder.\n+        if isinstance(encoder, dict):\n+            # From preset configuration\n+            encoder = T5GemmaModuleConfig(**encoder)\n+        elif encoder is None:\n+            # From scratch\n+            encoder = T5GemmaModuleConfig()\n+        else:\n+            assert isinstance(encoder, T5GemmaModuleConfig), f\"{type(encoder)} is not supported.\"\n+\n+        # Decoder.\n+        if isinstance(decoder, dict):\n+            # From preset configuration\n+            decoder = T5GemmaModuleConfig(**decoder)\n+        elif decoder is None:\n+            # From scratch\n+            decoder = encoder\n+        else:\n+            assert isinstance(decoder, T5GemmaModuleConfig), f\"{type(decoder)} is not supported.\"\n+\n+        # Decouple encoder and decoder config in any case\n+        encoder = T5GemmaModuleConfig(**encoder.to_dict())\n+        decoder = T5GemmaModuleConfig(**decoder.to_dict())\n+\n+        encoder.is_decoder = False\n+        encoder.dropout_rate = dropout_rate\n+        encoder.attention_dropout = attention_dropout\n+        self.encoder = encoder\n+\n+        decoder.is_decoder = True\n+        decoder.use_cache = True\n+        decoder.dropout_rate = dropout_rate\n+        decoder.attention_dropout = attention_dropout\n+        decoder.cross_attention_hidden_size = encoder.hidden_size\n+        self.decoder = decoder\n+\n+        for special_token_key in [\"bos_token_id\", \"pad_token_id\", \"eos_token_id\"]:\n+            if special_token_key not in kwargs:\n+                kwargs[special_token_key] = getattr(decoder, special_token_key)\n+\n+        super().__init__(**kwargs)\n+\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.use_cache = kwargs.get(\"use_cache\", decoder.use_cache)\n+        self.initializer_range = kwargs.get(\"initializer_range\", decoder.initializer_range)\n+        self.dropout_rate = dropout_rate\n+        self.attention_dropout = attention_dropout\n+        self.classifier_dropout_rate = classifier_dropout_rate\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+    def __setattr__(self, key, value):\n+        shared_attr_with_submodules = [\n+            \"output_hidden_states\",\n+            \"output_attentions\",\n+            \"_attn_implementation\",\n+            \"dropout_rate\",\n+            \"attention_dropout\",\n+        ]\n+\n+        if key in shared_attr_with_submodules:\n+            setattr(self.encoder, key, value)\n+            setattr(self.decoder, key, value)\n+        super().__setattr__(key, value)\n+\n+    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+        # Always return self, regardless of the decoder option.\n+        del decoder\n+        return self\n+\n+\n+__all__ = [\"T5GemmaConfig\", \"T5GemmaModuleConfig\"]"
        },
        {
            "sha": "7f3ce0927a50b250f8509916f89d186f2413c53c",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "added",
            "additions": 1506,
            "deletions": 0,
            "changes": 1506,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,1506 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/t5gemma/modular_t5gemma.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_t5gemma.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, can_return_tuple, logging\n+from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class T5GemmaRMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.zeros(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        # Llama does x.to(float16) * w whilst T5Gemma is (x * w).to(float16)\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        output = output * (1.0 + self.weight.float())\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+class T5GemmaMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_activation]\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, x):\n+        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n+        hidden_states = self.dropout(hidden_states)\n+        down_proj = self.down_proj(hidden_states)\n+        return down_proj\n+\n+\n+class T5GemmaRotaryEmbedding(nn.Module):\n+    def __init__(self, config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    if scaling is None:\n+        scaling = module.head_dim**-0.5\n+\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+\n+    if softcap is not None:\n+        attn_weights = attn_weights / softcap\n+        attn_weights = torch.tanh(attn_weights)\n+        attn_weights = attn_weights * softcap\n+    if attention_mask is not None:  # no matter the length, we just slice it\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+class T5GemmaSelfAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.attention_dropout = self.config.attention_dropout\n+        # Requied by flash attention: encoder selfattention is non-causal\n+        self.is_causal = config.is_decoder\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class T5GemmaCrossAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.query_pre_attn_scalar**-0.5\n+        self.attention_dropout = self.config.attention_dropout\n+\n+        # Requied by flash attention\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+\n+        self.k_proj = nn.Linear(\n+            config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+\n+        if config.cross_attention_hidden_size is None:\n+            raise ValueError(\"Cross-attention needs cross_attention_hidden_size to be specified.\")\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n+        encoder_hidden_states: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"Encoder hidden state is required for cross attention.\")\n+\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        # [batch, q_len, -1, head_dim] => [batch, -1, q_len, head_dim]\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            # after the first generated id, we can subsequently re-use all key/value_states from cache\n+            curr_past_key_value = past_key_value.cross_attention_cache\n+\n+        # conditions for calculating key and value states\n+        if (\n+            # no cache\n+            past_key_value is None\n+            # cross-attention but not cached yet\n+            or not is_updated\n+        ):\n+            encoder_input_shape = encoder_hidden_states.shape[:-1]\n+            encoder_hidden_shape = (*encoder_input_shape, -1, self.head_dim)\n+            # [batch, kv_len, -1, head_dim] => [batch, -1, kv_len, head_dim]\n+            key_states = self.k_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n+            value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n+\n+            # update cache\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n+        # cross-attention: reuse cached states\n+        else:\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=None,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class T5GemmaEncoderLayer(GradientCheckpointingLayer):\n+    \"\"\"Encoder sub-layer.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        # self attention\n+        self.self_attn = T5GemmaSelfAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # mlp\n+        self.mlp = T5GemmaMLP(config)\n+        self.pre_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # dropout\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = False,\n+        **kwargs,\n+    ) -> tuple[\n+        torch.FloatTensor,\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+    ]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            # Remove all caches for encoders.\n+            use_cache=False,\n+            past_key_value=None,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Mlp\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class T5GemmaDecoderLayer(T5GemmaEncoderLayer):\n+    \"\"\"Decoder sub-layer: an extra cross-attention layer.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # cross attention\n+        self.cross_attn = T5GemmaCrossAttention(config=config, layer_idx=layer_idx)\n+        self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[\n+        torch.FloatTensor,\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+    ]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value.self_attention_cache if past_key_value is not None else None,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Cross Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_cross_attn_layernorm(hidden_states)\n+        hidden_states, cross_attn_weights = self.cross_attn(\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_cross_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Mlp\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+class T5GemmaClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, num_labels: int, classifier_dropout_rate: float = 0.0):\n+        super().__init__()\n+        self.dropout = nn.Dropout(p=classifier_dropout_rate)\n+        self.out_proj = nn.Linear(hidden_size, num_labels)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.out_proj(hidden_states)\n+        return hidden_states\n+\n+\n+class T5GemmaLMHead(nn.Module):\n+    \"\"\"Head for language modeling (generation) tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, vocab_size: int, bias: bool = False):\n+        super().__init__()\n+        self.out_proj = nn.Linear(hidden_size, vocab_size, bias=bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        logits = self.out_proj(hidden_states)\n+        return logits\n+\n+\n+@auto_docstring\n+class T5GemmaPreTrainedModel(PreTrainedModel):\n+    config_class = T5GemmaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"T5GemmaBlock\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        # TODO: support intialization for encoders and decoders separately(?)\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, T5GemmaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, T5GemmaClassificationHead):\n+            scale = module.out_proj.weight.shape[0] ** -0.5\n+            module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+            if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:\n+                module.out_proj.bias.data.zero_()\n+        elif isinstance(module, T5GemmaLMHead):\n+            if not self.config.tie_word_embeddings:\n+                scale = module.out_proj.weight.shape[0] ** -0.5\n+                module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+\n+    def _shift_right(self, input_ids):\n+        \"\"\"\n+        Shifts input_ids to the right, prepends the decoder_start_token_id, and handles\n+        pad_token_id replacement for labels that were -100.\n+        This is a common preparation step for decoder inputs in sequence-to-sequence models.\n+        \"\"\"\n+        decoder_start_token_id = self.config.decoder.bos_token_id\n+        pad_token_id = self.config.decoder.pad_token_id\n+\n+        if decoder_start_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.bos_token_id has to be defined. \")\n+\n+        # shift inputs to the right\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n+\n+        if pad_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.pad_token_id has to be defined.\")\n+\n+        # Is this T5 specific?\n+        # replace possible -100 values in labels by `pad_token_id`\n+        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+        return shifted_input_ids\n+\n+\n+def bidirectional_mask_function(attention_mask: Optional[torch.Tensor]) -> Callable:\n+    \"\"\"\n+    This creates bidirectional attention mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # if attention mask is not given, all attention positions are considered valid.\n+        if attention_mask is None:\n+            return torch.ones((), dtype=torch.bool)\n+        # attention_mask: [batch_size, kv_len]\n+        return attention_mask[batch_idx, kv_idx].to(torch.bool)\n+\n+    return inner_mask\n+\n+\n+def sliding_window_bidirectional_mask_function(sliding_window: int) -> Callable:\n+    \"\"\"\n+    This creates bidirectional attention mask with sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return (q_idx - sliding_window < kv_idx) & (kv_idx < q_idx + sliding_window)\n+\n+    return inner_mask\n+\n+\n+def make_default_2d_attention_mask(\n+    token_ids: Optional[torch.LongTensor],\n+    hidden_states: torch.Tensor,\n+    pad_token_id: Optional[int],\n+) -> torch.Tensor:\n+    \"\"\"Construct the default attention mask.\"\"\"\n+    if token_ids is not None:\n+        if pad_token_id is None:\n+            raise ValueError(\"`pad_token_id` is required for padding information.\")\n+        attention_mask = (token_ids != pad_token_id).to(hidden_states.device, torch.long)\n+    else:\n+        attention_mask = torch.ones(\n+            (hidden_states.shape[0], hidden_states.shape[1]), device=hidden_states.device, dtype=torch.long\n+        )\n+    return attention_mask\n+\n+\n+class T5GemmaEncoder(T5GemmaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.norm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = T5GemmaRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5GemmaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutput:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # Input embeddings\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # Cache position: only used for mask construction.\n+        cache_position = torch.arange(0, inputs_embeds.shape[1], device=inputs_embeds.device)\n+\n+        # Postional ids.\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # Regular Attention mask.\n+        if attention_mask is None:\n+            attention_mask = make_default_2d_attention_mask(input_ids, inputs_embeds, self.config.pad_token_id)\n+\n+        # Attention masks\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+            }\n+            # Create the masks\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(attention_mask),\n+                ),\n+                \"sliding_attention\": create_sliding_window_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=sliding_window_bidirectional_mask_function(self.config.sliding_window),\n+                    and_mask_function=bidirectional_mask_function(attention_mask),\n+                ),\n+            }\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # normalized\n+        # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n+        hidden_states = hidden_states * normalizer\n+\n+        # transformer layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                position_embeddings,\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                output_attentions,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class T5GemmaDecoder(T5GemmaEncoder):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.layers = nn.ModuleList(\n+            [T5GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPastAndCrossAttentions:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"`encoder_hidden_states` must be given in decoder\")\n+\n+        # Input embeddings\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # Caching\n+        if not self.training and use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(\n+                self_attention_cache=DynamicCache(),\n+                cross_attention_cache=DynamicCache(),\n+            )\n+\n+        # Cache positions.\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # Position ids.\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # Regular Attention mask.\n+        if attention_mask is None and past_key_values is None:\n+            attention_mask = make_default_2d_attention_mask(input_ids, inputs_embeds, self.config.pad_token_id)\n+\n+        # Attention masks: Self attention\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+            }\n+            # Create the masks\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        # Attention masks: Cross attention\n+        if not isinstance(cross_attn_mask_mapping := encoder_attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": encoder_hidden_states,\n+                \"attention_mask\": encoder_attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+            }\n+            cross_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(encoder_attention_mask),\n+                ),\n+            }\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # normalized\n+        # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n+        hidden_states = hidden_states * normalizer\n+\n+        # transformer layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attns = () if output_attentions else None\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                position_embeddings,\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                past_key_values,\n+                output_attentions,\n+                use_cache,\n+                cache_position,\n+                encoder_hidden_states,\n+                cross_attn_mask_mapping[\"full_attention\"],\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+                all_cross_attns += (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attns,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaModel(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig):\n+        super().__init__(config)\n+\n+        if not config.is_encoder_decoder:\n+            raise ValueError(\"T5GemmaModel only support encoder-decoder modeling. Use `T5GemmaEncoderModel` instead.\")\n+\n+        self.encoder = T5GemmaEncoder(config.encoder)\n+        self.decoder = T5GemmaDecoder(config.decoder)\n+\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Seq2SeqModelOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        **flash_attn_kwargs: flash attention related parameters.\n+        \"\"\"\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        # Encode if needed (training, first prediction pass)\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                **flash_attn_kwargs,\n+            )\n+\n+        encoder_hidden_states = encoder_outputs.last_hidden_state\n+\n+        # Decode\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            position_ids=decoder_position_ids,\n+            inputs_embeds=decoder_inputs_embeds,\n+            past_key_values=past_key_values,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=attention_mask,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **flash_attn_kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaEncoderModel(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig):\n+        super().__init__(config)\n+\n+        if config.is_encoder_decoder:\n+            raise ValueError(\"T5GemmaEncoderModel only supports encoder-only model. Use `T5GemmaModel` instead.\")\n+\n+        self.encoder = T5GemmaEncoder(config.encoder)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        **flash_attn_kwargs: flash attention related parameters.\n+        \"\"\"\n+\n+        encoder_outputs = self.encoder(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            **flash_attn_kwargs,\n+        )\n+        return encoder_outputs\n+\n+\n+class T5GemmaForConditionalGeneration(T5GemmaPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.decoder.embed_tokens.weight\", \"lm_head.out_proj.weight\"]\n+    _tp_plan = {\"lm_head.out_proj\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head.out_proj\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config: T5GemmaConfig):\n+        config.is_encoder_decoder = True\n+        super().__init__(config)\n+\n+        self.model = T5GemmaModel(config)\n+        self.vocab_size = config.decoder.vocab_size\n+        self.lm_head = T5GemmaLMHead(config.decoder.hidden_size, self.vocab_size)\n+        self.loss_type = \"ForMaskedLMLoss\"\n+\n+        self.post_init()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.out_proj = new_embeddings\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.out_proj\n+\n+    def _tie_weights(self):\n+        # Decoder input and output embeddings are tied.\n+        if self.config.tie_word_embeddings:\n+            self._tie_or_clone_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n+\n+    def get_encoder(self):\n+        return self.model.encoder\n+\n+    def get_decoder(self):\n+        return self.model.decoder\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **loss_kwargs,\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        if self.training and self.config._attn_implementation != \"eager\":\n+            logger.warning_once(\n+                \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n+                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n+            )\n+\n+        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n+            # get decoder inputs from shifting lm labels to the right\n+            decoder_input_ids = self._shift_right(labels)\n+\n+        decoder_outputs: Seq2SeqModelOutput = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **loss_kwargs,\n+        )\n+\n+        hidden_states = decoder_outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+        decoder_config = self.get_decoder().config\n+        if decoder_config.final_logit_softcapping is not None:\n+            logits = logits / decoder_config.final_logit_softcapping\n+            logits = torch.tanh(logits)\n+            logits = logits * decoder_config.final_logit_softcapping\n+\n+        loss = None\n+        if labels is not None:\n+            # Input has right-shifted so we directly perform masked lm loss\n+            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.decoder_hidden_states,\n+            decoder_attentions=decoder_outputs.decoder_attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=decoder_outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=decoder_outputs.encoder_hidden_states,\n+            encoder_attentions=decoder_outputs.encoder_attentions,\n+        )\n+\n+    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n+        return self._shift_right(labels)\n+\n+\n+@auto_docstring\n+class T5GemmaForSequenceClassification(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n+        \"\"\"\n+        is_encoder_decoder (`Optional`, *optional*):\n+            Whether use encoder_decoder for sequence classification. When set to False, only encoder is used.\n+        \"\"\"\n+        if is_encoder_decoder is not None:\n+            config.is_encoder_decoder = is_encoder_decoder\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        if config.is_encoder_decoder:\n+            self.model = T5GemmaModel(config)\n+        else:\n+            self.model = T5GemmaEncoderModel(config)\n+\n+        hidden_size = config.encoder.hidden_size\n+        if config.is_encoder_decoder:\n+            hidden_size = config.decoder.hidden_size\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5GemmaClassificationHead(hidden_size, self.num_labels, classifier_dropout)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> SequenceClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if self.config.is_encoder_decoder and (input_ids is None and inputs_embeds is not None):\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__} in encoder-decoder mode.\"\n+            )\n+\n+        # Following T5, we automatically creates decoder_input_ids from input_ids if no decoder_input_ids are provided\n+        if self.config.is_encoder_decoder and (decoder_input_ids is None and decoder_inputs_embeds is None):\n+            if input_ids is None:\n+                raise ValueError(\n+                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n+                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n+                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n+                )\n+            decoder_input_ids = self._shift_right(input_ids)\n+\n+        if self.config.is_encoder_decoder:\n+            outputs: Seq2SeqModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                decoder_position_ids=decoder_position_ids,\n+                encoder_outputs=encoder_outputs,\n+                inputs_embeds=inputs_embeds,\n+                decoder_inputs_embeds=decoder_inputs_embeds,\n+                use_cache=False,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.decoder_hidden_states\n+            attentions = outputs.decoder_attentions\n+        else:\n+            outputs: BaseModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.hidden_states\n+            attentions = outputs.attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+\n+            if self.config.is_encoder_decoder:\n+                last_non_pad_token += 1  # due to the right shift.\n+                last_non_pad_token = torch.clamp(last_non_pad_token, max=decoder_input_ids.shape[-1] - 1)\n+        else:\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=pooled_logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaForTokenClassification(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n+        \"\"\"\n+        is_encoder_decoder (`Optional`, *optional*):\n+            Whether use encoder_decoder for token classification. When set to False, only encoder is used.\n+        \"\"\"\n+        if is_encoder_decoder is not None:\n+            config.is_encoder_decoder = is_encoder_decoder\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        if config.is_encoder_decoder:\n+            self.model = T5GemmaModel(config)\n+        else:\n+            self.model = T5GemmaEncoderModel(config)\n+\n+        hidden_size = config.encoder.hidden_size\n+        if config.is_encoder_decoder:\n+            hidden_size = config.decoder.hidden_size\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5GemmaClassificationHead(hidden_size, self.num_labels, classifier_dropout)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> TokenClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        if self.config.is_encoder_decoder and (input_ids is None and inputs_embeds is not None):\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__} in encoder-decoder mode.\"\n+            )\n+\n+        if self.config.is_encoder_decoder and (decoder_input_ids is None and decoder_inputs_embeds is None):\n+            if input_ids is None:\n+                raise ValueError(\n+                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n+                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n+                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n+                )\n+            decoder_input_ids = self._shift_right(input_ids)\n+\n+        if self.config.is_encoder_decoder:\n+            outputs: Seq2SeqModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                decoder_position_ids=decoder_position_ids,\n+                encoder_outputs=encoder_outputs,\n+                inputs_embeds=inputs_embeds,\n+                decoder_inputs_embeds=decoder_inputs_embeds,\n+                use_cache=False,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.decoder_hidden_states\n+            attentions = outputs.decoder_attentions\n+        else:\n+            outputs: BaseModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.hidden_states\n+            attentions = outputs.attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"T5GemmaForConditionalGeneration\",\n+    \"T5GemmaModel\",\n+    \"T5GemmaEncoderModel\",\n+    \"T5GemmaPreTrainedModel\",\n+    \"T5GemmaForSequenceClassification\",\n+    \"T5GemmaForTokenClassification\",\n+]"
        },
        {
            "sha": "aea5f3f74929e9049152d47d0a78f4d024ecc74c",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "added",
            "additions": 1455,
            "deletions": 0,
            "changes": 1455,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,1455 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n+from ..gemma2.configuration_gemma2 import Gemma2Config\n+from ..gemma2.modeling_gemma2 import (\n+    Gemma2Attention,\n+    Gemma2MLP,\n+    Gemma2PreTrainedModel,\n+    Gemma2RMSNorm,\n+    Gemma2RotaryEmbedding,\n+    create_causal_mask,\n+    create_sliding_window_causal_mask,\n+    eager_attention_forward,\n+)\n+\n+\n+# TODO(bzhanggo): figure out these documentations\n+_CHECKPOINT_FOR_DOC = \"google/t5gemma-placeholder\"\n+\n+\n+if is_torch_flex_attn_available():\n+    pass\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class T5GemmaModuleConfig(Gemma2Config):\n+    \"\"\"Module config (encoder or decoder): the same as Gemma2Config.\"\"\"\n+\n+    def __init__(self, **super_kwargs):\n+        super().__init__(**super_kwargs)\n+\n+\n+class T5GemmaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5GemmaModel`]. It is used to instantiate an T5Gemma\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to a hypothetical balanced Gemma2 encoder-decoder model.\n+    e.g. [google/t5gemma-placeholder](https://huggingface.co/google/t5gemma-placeholder)\n+    ```python\n+    >>> from transformers import T5GemmaConfig, T5GemmaModel\n+    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-placeholder\")\n+    >>> model = T5GemmaModel(t5gemma_config)\n+    ```\n+    Configuration objects inherit from [PretrainedConfig] and can be used to control the model outputs. Read the\n+    documentation from [PretrainedConfig] for more information.\n+    Args:\n+        encoder (`Union[T5GemmaModuleConfig, dict]`, optional, *optional*):\n+            Configuration for the encoder.\n+        decoder (`Union[T5GemmaModuleConfig, dict]`, optional, *optional*):\n+            Configuration for the decoder.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers (following T5).\n+        classifier_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for classifier (following T5).\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for attention.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether tie input and output embeddings.\n+        kwargs (additional keyword arguments, optional, *optional*):\n+            Will be passed to the PretrainedConfig base class.\n+    \"\"\"\n+\n+    model_type = \"t5gemma\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        # encoder\n+        \"encoder.layers.*.self_attn.q_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.k_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.v_proj\": \"colwise\",\n+        \"encoder.layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"encoder.layers.*.mlp.gate_proj\": \"colwise\",\n+        \"encoder.layers.*.mlp.up_proj\": \"colwise\",\n+        \"encoder.layers.*.mlp.down_proj\": \"rowwise\",\n+        # decoder\n+        \"decoder.layers.*.self_attn.q_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.k_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.v_proj\": \"colwise\",\n+        \"decoder.layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"decoder.layers.*.cross_attn.q_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.k_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.v_proj\": \"colwise\",\n+        \"decoder.layers.*.cross_attn.o_proj\": \"rowwise\",\n+        \"decoder.layers.*.mlp.gate_proj\": \"colwise\",\n+        \"decoder.layers.*.mlp.up_proj\": \"colwise\",\n+        \"decoder.layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        # encoder\n+        \"encoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"encoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"encoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+        # decoder\n+        \"decoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"decoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"decoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        encoder: Optional[Union[T5GemmaModuleConfig, dict[Any, Any]]] = None,\n+        decoder: Optional[Union[T5GemmaModuleConfig, dict[Any, Any]]] = None,\n+        is_encoder_decoder: bool = True,\n+        dropout_rate: float = 0.0,\n+        classifier_dropout_rate: float = 0.0,\n+        attention_dropout: float = 0.0,\n+        tie_word_embeddings: bool = True,\n+        **kwargs,\n+    ):\n+        # Encoder.\n+        if isinstance(encoder, dict):\n+            # From preset configuration\n+            encoder = T5GemmaModuleConfig(**encoder)\n+        elif encoder is None:\n+            # From scratch\n+            encoder = T5GemmaModuleConfig()\n+        else:\n+            assert isinstance(encoder, T5GemmaModuleConfig), f\"{type(encoder)} is not supported.\"\n+\n+        # Decoder.\n+        if isinstance(decoder, dict):\n+            # From preset configuration\n+            decoder = T5GemmaModuleConfig(**decoder)\n+        elif decoder is None:\n+            # From scratch\n+            decoder = encoder\n+        else:\n+            assert isinstance(decoder, T5GemmaModuleConfig), f\"{type(decoder)} is not supported.\"\n+\n+        # Decouple encoder and decoder config in any case\n+        encoder = T5GemmaModuleConfig(**encoder.to_dict())\n+        decoder = T5GemmaModuleConfig(**decoder.to_dict())\n+\n+        encoder.is_decoder = False\n+        encoder.dropout_rate = dropout_rate\n+        encoder.attention_dropout = attention_dropout\n+        self.encoder = encoder\n+\n+        decoder.is_decoder = True\n+        decoder.use_cache = True\n+        decoder.dropout_rate = dropout_rate\n+        decoder.attention_dropout = attention_dropout\n+        decoder.cross_attention_hidden_size = encoder.hidden_size\n+        self.decoder = decoder\n+\n+        for special_token_key in [\"bos_token_id\", \"pad_token_id\", \"eos_token_id\"]:\n+            if special_token_key not in kwargs:\n+                kwargs[special_token_key] = getattr(decoder, special_token_key)\n+\n+        super().__init__(**kwargs)\n+\n+        self.is_encoder_decoder = is_encoder_decoder\n+        self.use_cache = kwargs.get(\"use_cache\", decoder.use_cache)\n+        self.initializer_range = kwargs.get(\"initializer_range\", decoder.initializer_range)\n+        self.dropout_rate = dropout_rate\n+        self.attention_dropout = attention_dropout\n+        self.classifier_dropout_rate = classifier_dropout_rate\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+    def __setattr__(self, key, value):\n+        shared_attr_with_submodules = [\n+            \"output_hidden_states\",\n+            \"output_attentions\",\n+            \"_attn_implementation\",\n+            \"dropout_rate\",\n+            \"attention_dropout\",\n+        ]\n+\n+        if key in shared_attr_with_submodules:\n+            setattr(self.encoder, key, value)\n+            setattr(self.decoder, key, value)\n+        super().__setattr__(key, value)\n+\n+    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+        # Always return self, regardless of the decoder option.\n+        del decoder\n+        return self\n+\n+\n+class T5GemmaRMSNorm(Gemma2RMSNorm):\n+    pass\n+\n+\n+class T5GemmaMLP(Gemma2MLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, x):\n+        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n+        hidden_states = self.dropout(hidden_states)\n+        down_proj = self.down_proj(hidden_states)\n+        return down_proj\n+\n+\n+class T5GemmaRotaryEmbedding(Gemma2RotaryEmbedding):\n+    def __init__(self, config, device=None):\n+        super().__init__(config, device)\n+\n+\n+class T5GemmaSelfAttention(Gemma2Attention):\n+    def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # Requied by flash attention: encoder selfattention is non-causal\n+        self.is_causal = config.is_decoder\n+\n+\n+class T5GemmaCrossAttention(Gemma2Attention):\n+    def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # Cross-attention only supports global attention\n+        del self.sliding_window\n+\n+        # Requied by flash attention\n+        self.is_causal = False\n+\n+        if config.cross_attention_hidden_size is None:\n+            raise ValueError(\"Cross-attention needs cross_attention_hidden_size to be specified.\")\n+\n+        self.k_proj = nn.Linear(\n+            config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n+        encoder_hidden_states: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"Encoder hidden state is required for cross attention.\")\n+\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        # [batch, q_len, -1, head_dim] => [batch, -1, q_len, head_dim]\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            # after the first generated id, we can subsequently re-use all key/value_states from cache\n+            curr_past_key_value = past_key_value.cross_attention_cache\n+\n+        # conditions for calculating key and value states\n+        if (\n+            # no cache\n+            past_key_value is None\n+            # cross-attention but not cached yet\n+            or not is_updated\n+        ):\n+            encoder_input_shape = encoder_hidden_states.shape[:-1]\n+            encoder_hidden_shape = (*encoder_input_shape, -1, self.head_dim)\n+            # [batch, kv_len, -1, head_dim] => [batch, -1, kv_len, head_dim]\n+            key_states = self.k_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n+            value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n+\n+            # update cache\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n+        # cross-attention: reuse cached states\n+        else:\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=None,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+def bidirectional_mask_function(attention_mask: Optional[torch.Tensor]) -> Callable:\n+    \"\"\"\n+    This creates bidirectional attention mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # if attention mask is not given, all attention positions are considered valid.\n+        if attention_mask is None:\n+            return torch.ones((), dtype=torch.bool)\n+        # attention_mask: [batch_size, kv_len]\n+        return attention_mask[batch_idx, kv_idx].to(torch.bool)\n+\n+    return inner_mask\n+\n+\n+def sliding_window_bidirectional_mask_function(sliding_window: int) -> Callable:\n+    \"\"\"\n+    This creates bidirectional attention mask with sliding window.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return (q_idx - sliding_window < kv_idx) & (kv_idx < q_idx + sliding_window)\n+\n+    return inner_mask\n+\n+\n+class T5GemmaEncoderLayer(GradientCheckpointingLayer):\n+    \"\"\"Encoder sub-layer.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        # self attention\n+        self.self_attn = T5GemmaSelfAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # mlp\n+        self.mlp = T5GemmaMLP(config)\n+        self.pre_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # dropout\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = False,\n+        **kwargs,\n+    ) -> tuple[\n+        torch.FloatTensor,\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+    ]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            # Remove all caches for encoders.\n+            use_cache=False,\n+            past_key_value=None,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Mlp\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class T5GemmaDecoderLayer(T5GemmaEncoderLayer):\n+    \"\"\"Decoder sub-layer: an extra cross-attention layer.\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # cross attention\n+        self.cross_attn = T5GemmaCrossAttention(config=config, layer_idx=layer_idx)\n+        self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[\n+        torch.FloatTensor,\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+        Optional[tuple[torch.FloatTensor, torch.FloatTensor]],\n+    ]:\n+        # Self Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_self_attn_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value.self_attention_cache if past_key_value is not None else None,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_self_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Cross Attention\n+        residual = hidden_states\n+        hidden_states = self.pre_cross_attn_layernorm(hidden_states)\n+        hidden_states, cross_attn_weights = self.cross_attn(\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_cross_attn_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        # Mlp\n+        residual = hidden_states\n+        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + self.dropout(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+class T5GemmaClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, num_labels: int, classifier_dropout_rate: float = 0.0):\n+        super().__init__()\n+        self.dropout = nn.Dropout(p=classifier_dropout_rate)\n+        self.out_proj = nn.Linear(hidden_size, num_labels)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.out_proj(hidden_states)\n+        return hidden_states\n+\n+\n+class T5GemmaLMHead(nn.Module):\n+    \"\"\"Head for language modeling (generation) tasks.\"\"\"\n+\n+    def __init__(self, hidden_size: int, vocab_size: int, bias: bool = False):\n+        super().__init__()\n+        self.out_proj = nn.Linear(hidden_size, vocab_size, bias=bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        logits = self.out_proj(hidden_states)\n+        return logits\n+\n+\n+@auto_docstring\n+class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n+    config_class = T5GemmaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"T5GemmaBlock\"]\n+\n+    def _init_weights(self, module):\n+        # TODO: support intialization for encoders and decoders separately(?)\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, T5GemmaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, T5GemmaClassificationHead):\n+            scale = module.out_proj.weight.shape[0] ** -0.5\n+            module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+            if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:\n+                module.out_proj.bias.data.zero_()\n+        elif isinstance(module, T5GemmaLMHead):\n+            if not self.config.tie_word_embeddings:\n+                scale = module.out_proj.weight.shape[0] ** -0.5\n+                module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+\n+    def _shift_right(self, input_ids):\n+        \"\"\"\n+        Shifts input_ids to the right, prepends the decoder_start_token_id, and handles\n+        pad_token_id replacement for labels that were -100.\n+        This is a common preparation step for decoder inputs in sequence-to-sequence models.\n+        \"\"\"\n+        decoder_start_token_id = self.config.decoder.bos_token_id\n+        pad_token_id = self.config.decoder.pad_token_id\n+\n+        if decoder_start_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.bos_token_id has to be defined. \")\n+\n+        # shift inputs to the right\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n+\n+        if pad_token_id is None:\n+            raise ValueError(\"self.model.config.decoder.pad_token_id has to be defined.\")\n+\n+        # Is this T5 specific?\n+        # replace possible -100 values in labels by `pad_token_id`\n+        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+        return shifted_input_ids\n+\n+\n+def make_default_2d_attention_mask(\n+    token_ids: Optional[torch.LongTensor],\n+    hidden_states: torch.Tensor,\n+    pad_token_id: Optional[int],\n+) -> torch.Tensor:\n+    \"\"\"Construct the default attention mask.\"\"\"\n+    if token_ids is not None:\n+        if pad_token_id is None:\n+            raise ValueError(\"`pad_token_id` is required for padding information.\")\n+        attention_mask = (token_ids != pad_token_id).to(hidden_states.device, torch.long)\n+    else:\n+        attention_mask = torch.ones(\n+            (hidden_states.shape[0], hidden_states.shape[1]), device=hidden_states.device, dtype=torch.long\n+        )\n+    return attention_mask\n+\n+\n+class T5GemmaEncoder(T5GemmaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.norm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = T5GemmaRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        self.layers = nn.ModuleList(\n+            [T5GemmaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutput:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # Input embeddings\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # Cache position: only used for mask construction.\n+        cache_position = torch.arange(0, inputs_embeds.shape[1], device=inputs_embeds.device)\n+\n+        # Postional ids.\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # Regular Attention mask.\n+        if attention_mask is None:\n+            attention_mask = make_default_2d_attention_mask(input_ids, inputs_embeds, self.config.pad_token_id)\n+\n+        # Attention masks\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+            }\n+            # Create the masks\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(attention_mask),\n+                ),\n+                \"sliding_attention\": create_sliding_window_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=sliding_window_bidirectional_mask_function(self.config.sliding_window),\n+                    and_mask_function=bidirectional_mask_function(attention_mask),\n+                ),\n+            }\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # normalized\n+        # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n+        hidden_states = hidden_states * normalizer\n+\n+        # transformer layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                position_embeddings,\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                output_attentions,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class T5GemmaDecoder(T5GemmaEncoder):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.layers = nn.ModuleList(\n+            [T5GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPastAndCrossAttentions:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"`encoder_hidden_states` must be given in decoder\")\n+\n+        # Input embeddings\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # Caching\n+        if not self.training and use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(\n+                self_attention_cache=DynamicCache(),\n+                cross_attention_cache=DynamicCache(),\n+            )\n+\n+        # Cache positions.\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # Position ids.\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # Regular Attention mask.\n+        if attention_mask is None and past_key_values is None:\n+            attention_mask = make_default_2d_attention_mask(input_ids, inputs_embeds, self.config.pad_token_id)\n+\n+        # Attention masks: Self attention\n+        if not isinstance(self_attn_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values.self_attention_cache if past_key_values is not None else None,\n+            }\n+            # Create the masks\n+            self_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        # Attention masks: Cross attention\n+        if not isinstance(cross_attn_mask_mapping := encoder_attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": encoder_hidden_states,\n+                \"attention_mask\": encoder_attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": None,\n+            }\n+            cross_attn_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    **mask_kwargs,\n+                    or_mask_function=bidirectional_mask_function(encoder_attention_mask),\n+                ),\n+            }\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # normalized\n+        # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n+        hidden_states = hidden_states * normalizer\n+\n+        # transformer layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attns = () if output_attentions else None\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        for layer_module in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                position_embeddings,\n+                self_attn_mask_mapping[layer_module.attention_type],\n+                position_ids,\n+                past_key_values,\n+                output_attentions,\n+                use_cache,\n+                cache_position,\n+                encoder_hidden_states,\n+                cross_attn_mask_mapping[\"full_attention\"],\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+                all_cross_attns += (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attns,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaModel(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig):\n+        super().__init__(config)\n+\n+        if not config.is_encoder_decoder:\n+            raise ValueError(\"T5GemmaModel only support encoder-decoder modeling. Use `T5GemmaEncoderModel` instead.\")\n+\n+        self.encoder = T5GemmaEncoder(config.encoder)\n+        self.decoder = T5GemmaDecoder(config.decoder)\n+\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Seq2SeqModelOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        **flash_attn_kwargs: flash attention related parameters.\n+        \"\"\"\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        # Encode if needed (training, first prediction pass)\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                **flash_attn_kwargs,\n+            )\n+\n+        encoder_hidden_states = encoder_outputs.last_hidden_state\n+\n+        # Decode\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            position_ids=decoder_position_ids,\n+            inputs_embeds=decoder_inputs_embeds,\n+            past_key_values=past_key_values,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=attention_mask,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **flash_attn_kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaEncoderModel(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig):\n+        super().__init__(config)\n+\n+        if config.is_encoder_decoder:\n+            raise ValueError(\"T5GemmaEncoderModel only supports encoder-only model. Use `T5GemmaModel` instead.\")\n+\n+        self.encoder = T5GemmaEncoder(config.encoder)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.encoder.set_input_embeddings(new_embeddings)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        **flash_attn_kwargs: flash attention related parameters.\n+        \"\"\"\n+\n+        encoder_outputs = self.encoder(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            **flash_attn_kwargs,\n+        )\n+        return encoder_outputs\n+\n+\n+class T5GemmaForConditionalGeneration(T5GemmaPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.decoder.embed_tokens.weight\", \"lm_head.out_proj.weight\"]\n+    _tp_plan = {\"lm_head.out_proj\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head.out_proj\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config: T5GemmaConfig):\n+        config.is_encoder_decoder = True\n+        super().__init__(config)\n+\n+        self.model = T5GemmaModel(config)\n+        self.vocab_size = config.decoder.vocab_size\n+        self.lm_head = T5GemmaLMHead(config.decoder.hidden_size, self.vocab_size)\n+        self.loss_type = \"ForMaskedLMLoss\"\n+\n+        self.post_init()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.out_proj = new_embeddings\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.out_proj\n+\n+    def _tie_weights(self):\n+        # Decoder input and output embeddings are tied.\n+        if self.config.tie_word_embeddings:\n+            self._tie_or_clone_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n+\n+    def get_encoder(self):\n+        return self.model.encoder\n+\n+    def get_decoder(self):\n+        return self.model.decoder\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **loss_kwargs,\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        if self.training and self.config._attn_implementation != \"eager\":\n+            logger.warning_once(\n+                \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n+                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n+            )\n+\n+        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n+            # get decoder inputs from shifting lm labels to the right\n+            decoder_input_ids = self._shift_right(labels)\n+\n+        decoder_outputs: Seq2SeqModelOutput = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            decoder_position_ids=decoder_position_ids,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **loss_kwargs,\n+        )\n+\n+        hidden_states = decoder_outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+        decoder_config = self.get_decoder().config\n+        if decoder_config.final_logit_softcapping is not None:\n+            logits = logits / decoder_config.final_logit_softcapping\n+            logits = torch.tanh(logits)\n+            logits = logits * decoder_config.final_logit_softcapping\n+\n+        loss = None\n+        if labels is not None:\n+            # Input has right-shifted so we directly perform masked lm loss\n+            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.decoder_hidden_states,\n+            decoder_attentions=decoder_outputs.decoder_attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=decoder_outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=decoder_outputs.encoder_hidden_states,\n+            encoder_attentions=decoder_outputs.encoder_attentions,\n+        )\n+\n+    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n+        return self._shift_right(labels)\n+\n+\n+@auto_docstring\n+class T5GemmaForSequenceClassification(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n+        \"\"\"\n+        is_encoder_decoder (`Optional`, *optional*):\n+            Whether use encoder_decoder for sequence classification. When set to False, only encoder is used.\n+        \"\"\"\n+        if is_encoder_decoder is not None:\n+            config.is_encoder_decoder = is_encoder_decoder\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        if config.is_encoder_decoder:\n+            self.model = T5GemmaModel(config)\n+        else:\n+            self.model = T5GemmaEncoderModel(config)\n+\n+        hidden_size = config.encoder.hidden_size\n+        if config.is_encoder_decoder:\n+            hidden_size = config.decoder.hidden_size\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5GemmaClassificationHead(hidden_size, self.num_labels, classifier_dropout)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> SequenceClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        if self.config.is_encoder_decoder and (input_ids is None and inputs_embeds is not None):\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__} in encoder-decoder mode.\"\n+            )\n+\n+        # Following T5, we automatically creates decoder_input_ids from input_ids if no decoder_input_ids are provided\n+        if self.config.is_encoder_decoder and (decoder_input_ids is None and decoder_inputs_embeds is None):\n+            if input_ids is None:\n+                raise ValueError(\n+                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n+                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n+                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n+                )\n+            decoder_input_ids = self._shift_right(input_ids)\n+\n+        if self.config.is_encoder_decoder:\n+            outputs: Seq2SeqModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                decoder_position_ids=decoder_position_ids,\n+                encoder_outputs=encoder_outputs,\n+                inputs_embeds=inputs_embeds,\n+                decoder_inputs_embeds=decoder_inputs_embeds,\n+                use_cache=False,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.decoder_hidden_states\n+            attentions = outputs.decoder_attentions\n+        else:\n+            outputs: BaseModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.hidden_states\n+            attentions = outputs.attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+\n+            if self.config.is_encoder_decoder:\n+                last_non_pad_token += 1  # due to the right shift.\n+                last_non_pad_token = torch.clamp(last_non_pad_token, max=decoder_input_ids.shape[-1] - 1)\n+        else:\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=pooled_logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+@auto_docstring\n+class T5GemmaForTokenClassification(T5GemmaPreTrainedModel):\n+    def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n+        \"\"\"\n+        is_encoder_decoder (`Optional`, *optional*):\n+            Whether use encoder_decoder for token classification. When set to False, only encoder is used.\n+        \"\"\"\n+        if is_encoder_decoder is not None:\n+            config.is_encoder_decoder = is_encoder_decoder\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        if config.is_encoder_decoder:\n+            self.model = T5GemmaModel(config)\n+        else:\n+            self.model = T5GemmaEncoderModel(config)\n+\n+        hidden_size = config.encoder.hidden_size\n+        if config.is_encoder_decoder:\n+            hidden_size = config.decoder.hidden_size\n+\n+        classifier_dropout = getattr(config, \"classifier_dropout_rate\", 0.1)\n+        self.score = T5GemmaClassificationHead(hidden_size, self.num_labels, classifier_dropout)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        # encoder\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        # decoder\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.Tensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        # others\n+        encoder_outputs: Optional[BaseModelOutput] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> TokenClassifierOutput:\n+        r\"\"\"\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n+            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        if self.config.is_encoder_decoder and (input_ids is None and inputs_embeds is not None):\n+            raise NotImplementedError(\n+                f\"Passing input embeddings is currently not supported for {self.__class__.__name__} in encoder-decoder mode.\"\n+            )\n+\n+        if self.config.is_encoder_decoder and (decoder_input_ids is None and decoder_inputs_embeds is None):\n+            if input_ids is None:\n+                raise ValueError(\n+                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n+                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n+                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n+                )\n+            decoder_input_ids = self._shift_right(input_ids)\n+\n+        if self.config.is_encoder_decoder:\n+            outputs: Seq2SeqModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                decoder_position_ids=decoder_position_ids,\n+                encoder_outputs=encoder_outputs,\n+                inputs_embeds=inputs_embeds,\n+                decoder_inputs_embeds=decoder_inputs_embeds,\n+                use_cache=False,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.decoder_hidden_states\n+            attentions = outputs.decoder_attentions\n+        else:\n+            outputs: BaseModelOutput = self.model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+            last_hidden_state = outputs.last_hidden_state\n+            hidden_states = outputs.hidden_states\n+            attentions = outputs.attentions\n+\n+        logits = self.score(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"T5GemmaConfig\",\n+    \"T5GemmaModuleConfig\",\n+    \"T5GemmaForConditionalGeneration\",\n+    \"T5GemmaModel\",\n+    \"T5GemmaEncoderModel\",\n+    \"T5GemmaPreTrainedModel\",  # noqa: F822\n+    \"T5GemmaForSequenceClassification\",\n+    \"T5GemmaForTokenClassification\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/t5gemma/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/tests%2Fmodels%2Ft5gemma%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/tests%2Fmodels%2Ft5gemma%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2F__init__.py?ref=3ef889690649c082849c667be17b757c32955229"
        },
        {
            "sha": "ba49e913307f33ea5cf6c6b468523d3012b3eeef",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "added",
            "additions": 1701,
            "deletions": 0,
            "changes": 1701,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef889690649c082849c667be17b757c32955229/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef889690649c082849c667be17b757c32955229/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=3ef889690649c082849c667be17b757c32955229",
            "patch": "@@ -0,0 +1,1701 @@\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch T5Gemma model.\"\"\"\n+\n+import copy\n+import inspect\n+import unittest\n+\n+import pytest\n+from parameterized import parameterized\n+\n+from transformers import T5GemmaConfig, T5GemmaModuleConfig, is_torch_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_gpu,\n+    require_torch_sdpa,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn.functional as F\n+\n+    from transformers import (\n+        T5GemmaEncoderModel,\n+        T5GemmaForConditionalGeneration,\n+        T5GemmaForSequenceClassification,\n+        T5GemmaForTokenClassification,\n+        T5GemmaModel,\n+    )\n+    from transformers.cache_utils import Cache\n+\n+\n+class T5GemmaModelTester:\n+    config_class = T5GemmaConfig\n+    module_config_class = T5GemmaModuleConfig\n+\n+    if is_torch_available():\n+        model_class = T5GemmaModel\n+        for_causal_lm_class = T5GemmaForConditionalGeneration\n+        for_sequence_class = T5GemmaForSequenceClassification\n+        for_token_class = T5GemmaForTokenClassification\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        is_training=True,\n+        use_attention_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        # decoder-specific\n+        seq_length=7,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        # encoder-specific\n+        encoder_seq_length=7,\n+        encoder_hidden_size=32,\n+        encoder_num_hidden_layers=2,\n+        encoder_num_attention_heads=4,\n+        encoder_num_key_value_heads=2,\n+        encoder_intermediate_size=37,\n+        # common\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        scope=None,\n+        # special ids\n+        eos_token_id=1,\n+        pad_token_id=0,\n+        bos_token_id=2,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.use_attention_mask = use_attention_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        # decoder\n+        self.seq_length = seq_length\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        # encoder\n+        self.encoder_seq_length = encoder_seq_length\n+        self.encoder_hidden_size = encoder_hidden_size\n+        self.encoder_num_hidden_layers = encoder_num_hidden_layers\n+        self.encoder_num_attention_heads = encoder_num_attention_heads\n+        self.encoder_num_key_value_heads = encoder_num_key_value_heads\n+        self.encoder_intermediate_size = encoder_intermediate_size\n+        # common\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.scope = scope\n+        self.head_dim = self.hidden_size // self.num_attention_heads\n+        # assume encoder and decoder have the same head dimension.\n+        assert self.head_dim == self.encoder_hidden_size // self.encoder_num_attention_heads\n+        # special ids\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        # assume the number of attention heads are the same across encoder and decoder\n+        # only used for generation testing purpose.\n+        assert self.num_attention_heads == self.encoder_num_attention_heads\n+\n+    def get_encoder_config(self):\n+        return self.module_config_class(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.encoder_hidden_size,\n+            num_hidden_layers=self.encoder_num_hidden_layers,\n+            num_attention_heads=self.encoder_num_attention_heads,\n+            num_key_value_heads=self.encoder_num_key_value_heads,\n+            intermediate_size=self.encoder_intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            head_dim=self.head_dim,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def get_decoder_config(self):\n+        return self.module_config_class(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            intermediate_size=self.intermediate_size,\n+            cross_attention_hidden_size=self.encoder_hidden_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=True,\n+            initializer_range=self.initializer_range,\n+            head_dim=self.head_dim,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def get_config(self, is_encoder_decoder=True):\n+        return self.config_class(\n+            encoder=self.get_encoder_config(),\n+            decoder=self.get_decoder_config(),\n+            is_encoder_decoder=is_encoder_decoder,\n+            # Used for generation test.\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n+        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        # Remove BOS symbols from inputs.\n+        input_ids = torch.where(input_ids == self.bos_token_id, 42, input_ids)\n+        decoder_input_ids = torch.where(decoder_input_ids == self.bos_token_id, 42, decoder_input_ids)\n+\n+        attention_mask = None\n+        decoder_attention_mask = None\n+        if self.use_attention_mask:\n+            attention_mask = ids_tensor([self.batch_size, self.encoder_seq_length], vocab_size=2)\n+            decoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n+\n+        lm_labels = None\n+        if self.use_labels:\n+            lm_labels = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        config = self.get_config()\n+\n+        return (\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+        )\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTester.prepare_config_and_inputs_for_common\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+        ) = config_and_inputs\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).eval()\n+\n+        result = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+        )\n+\n+        decoder_output = result.last_hidden_state\n+        decoder_past = result.past_key_values\n+        encoder_output = result.encoder_last_hidden_state\n+\n+        self.parent.assertEqual(\n+            encoder_output.size(), (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size)\n+        )\n+        self.parent.assertEqual(decoder_output.size(), (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertIsNotNone(decoder_past)\n+        self.parent.assertEqual(len(decoder_past.self_attention_cache), config.decoder.num_hidden_layers)\n+        self.parent.assertEqual(len(decoder_past.cross_attention_cache.key_cache), config.decoder.num_hidden_layers)\n+\n+    def check_prepare_lm_labels_via_shift_left(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).eval()\n+\n+        # _shift_right should be called on labels\n+        shifted_labels = model._shift_right(lm_labels)\n+\n+        # first token should be decoder_start_token_id\n+        self.parent.assertTrue(torch.all(shifted_labels[:, 0] == config.decoder.bos_token_id))\n+\n+        # the rest should be the labels shifted by one, with -100 replaced by pad_token_id\n+        labels_without_ignore_index = lm_labels.masked_fill(lm_labels == -100, config.decoder.pad_token_id)\n+        self.parent.assertTrue(torch.all(shifted_labels[:, 1:] == labels_without_ignore_index[:, :-1]))\n+\n+    def create_and_check_with_lm_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.for_causal_lm_class(config=config).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            attention_mask=attention_mask,\n+            decoder_attention_mask=decoder_attention_mask,\n+            labels=lm_labels,\n+        )\n+        self.parent.assertEqual(len(outputs), 4)\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, self.vocab_size))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_with_sequence_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        labels = torch.tensor([1] * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = self.for_sequence_class(config=config).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=input_ids,\n+            labels=labels,\n+        )\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_encoderonly_for_sequence_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        is_encoder_decoder,\n+    ):\n+        labels = torch.tensor([1] * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = self.for_sequence_class(config=config, is_encoder_decoder=is_encoder_decoder)\n+        model = model.to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=input_ids,\n+            labels=labels,\n+        )\n+\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_encoderonly_for_token_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+        is_encoder_decoder,\n+    ):\n+        labels = torch.tensor([1] * self.seq_length * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = self.for_token_class(config=config, is_encoder_decoder=is_encoder_decoder)\n+        model = model.to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=input_ids,\n+            labels=labels,\n+        )\n+\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def create_and_check_decoder_model_past(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # first forward pass\n+        outputs = model(input_ids, encoder_hidden_states=encoder_hidden_states, use_cache=True)\n+        outputs_use_cache_conf = model(input_ids, encoder_hidden_states=encoder_hidden_states)\n+        outputs_no_past = model(input_ids, encoder_hidden_states=encoder_hidden_states, use_cache=False)\n+\n+        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n+        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n+\n+        output, past_key_values = outputs.to_tuple()\n+\n+        # create hypothetical next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n+\n+        output_from_no_past = model(next_input_ids, encoder_hidden_states=encoder_hidden_states)[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_decoder_model_attention_mask_past(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # create attention mask\n+        attn_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        half_seq_length = input_ids.shape[-1] // 2\n+        attn_mask[:, half_seq_length:] = 0\n+\n+        # first forward pass\n+        output, past_key_values = model(\n+            input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=attn_mask, use_cache=True\n+        ).to_tuple()\n+\n+        # create hypothetical next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n+\n+        # change a random masked slice from input_ids\n+        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).item() + 1\n+        random_other_next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size).squeeze(-1)\n+        input_ids[:, -random_seq_idx_to_change] = random_other_next_tokens\n+\n+        # append to next input_ids and attn_mask\n+        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n+        attn_mask = torch.cat(\n+            [attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)],\n+            dim=1,\n+        )\n+\n+        # get two different outputs\n+        output_from_no_past = model(\n+            next_input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=attn_mask\n+        )[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_values=past_key_values,\n+            attention_mask=attn_mask,\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_decoder_model_past_large_inputs(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).get_decoder().to(torch_device).eval()\n+        encoder_hidden_states = torch.ones(\n+            (self.batch_size, self.encoder_seq_length, self.encoder_hidden_size), dtype=torch.float32\n+        ).to(torch_device)\n+\n+        # first forward pass\n+        outputs = model(\n+            input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask, use_cache=True\n+        )\n+\n+        output, past_key_values = outputs.to_tuple()\n+\n+        # create hypothetical multiple next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n+        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n+        next_attention_mask = torch.cat([attention_mask, next_mask], dim=-1)\n+\n+        output_from_no_past = model(\n+            next_input_ids, encoder_hidden_states=encoder_hidden_states, attention_mask=next_attention_mask\n+        )[\"last_hidden_state\"]\n+        output_from_past = model(\n+            next_tokens,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=next_attention_mask,\n+            past_key_values=past_key_values,\n+        )[\"last_hidden_state\"]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n+\n+        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def create_and_check_generate_with_past_key_values(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.for_causal_lm_class(config=config).to(torch_device).eval()\n+        torch.manual_seed(0)\n+        output_without_past_cache = model.generate(\n+            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n+        )\n+        torch.manual_seed(0)\n+        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n+        self.parent.assertTrue(torch.all(output_with_past_cache == output_without_past_cache))\n+\n+    def create_and_check_model_fp16_forward(\n+        self,\n+        config,\n+        input_ids,\n+        decoder_input_ids,\n+        attention_mask,\n+        decoder_attention_mask,\n+        lm_labels,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).half().eval()\n+        output = model(input_ids, decoder_input_ids=input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n+        self.parent.assertFalse(torch.isnan(output).any().item())\n+\n+\n+@require_torch\n+class T5GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            T5GemmaModel,\n+            T5GemmaForConditionalGeneration,\n+            T5GemmaForSequenceClassification,\n+            T5GemmaForTokenClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": T5GemmaModel,\n+            \"summarization\": T5GemmaForConditionalGeneration,\n+            \"text-classification\": T5GemmaForSequenceClassification,\n+            \"text2text-generation\": T5GemmaForConditionalGeneration,\n+            \"translation\": T5GemmaForConditionalGeneration,\n+            \"zero-shot\": T5GemmaForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_headmasking = False\n+    test_pruning = False\n+    _is_stateful = True\n+    is_encoder_decoder = True\n+    model_split_percents = [0.5, 0.6]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = T5GemmaForConditionalGeneration if is_torch_available() else None\n+\n+    def setUp(self):\n+        self.model_tester = T5GemmaModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=T5GemmaConfig,\n+            # For faking the testing.\n+            hidden_size=37,\n+            vocab_size=self.model_tester.vocab_size,\n+            num_attention_heads=self.model_tester.num_attention_heads,\n+            num_hidden_layers=self.model_tester.num_hidden_layers,\n+        )\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.is_pipeline_test_to_skip\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        if tokenizer_name is None:\n+            return True\n+        if pipeline_test_case_name == \"QAPipelineTests\" and not tokenizer_name.endswith(\"Fast\"):\n+            return True\n+\n+        return False\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_config\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_shift_right\n+    def test_shift_right(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.check_prepare_lm_labels_via_shift_left(*config_and_inputs)\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_model\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Based on tests.models.t5.test_modeling_t5.T5ModelTest.test_inputs_embeds\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in (T5GemmaModel, T5GemmaForConditionalGeneration):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+\n+            if not self.is_encoder_decoder:\n+                input_ids = inputs[\"input_ids\"]\n+                del inputs[\"input_ids\"]\n+            else:\n+                encoder_input_ids = inputs[\"input_ids\"]\n+                decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n+                del inputs[\"input_ids\"]\n+                inputs.pop(\"decoder_input_ids\", None)\n+\n+            wte = model.get_input_embeddings()\n+            if not self.is_encoder_decoder:\n+                inputs[\"inputs_embeds\"] = wte(input_ids)\n+            else:\n+                inputs[\"inputs_embeds\"] = wte(encoder_input_ids)\n+                inputs[\"decoder_inputs_embeds\"] = wte(decoder_input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)[0]\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_config_and_model_silu_gated\n+    def test_config_and_model_silu_gated(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        config = config_and_inputs[0]\n+        config.feed_forward_proj = \"gated-silu\"\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_with_lm_head\n+    def test_with_lm_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_lm_head(*config_and_inputs)\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_with_sequence_classification_head\n+    def test_with_sequence_classification_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_sequence_classification_head(*config_and_inputs)\n+\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encoderonly_sequence_classification_head(self, is_encoder_decoder):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_encoderonly_for_sequence_classification_head(\n+            *config_and_inputs, is_encoder_decoder\n+        )\n+\n+    @parameterized.expand([(True,), (False,)])\n+    def test_encoderonly_token_classification_head(self, is_encoder_decoder):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_encoderonly_for_token_classification_head(\n+            *config_and_inputs, is_encoder_decoder\n+        )\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_decoder_model_past\n+    def test_decoder_model_past(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_past(*config_and_inputs)\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_decoder_model_past_with_attn_mask\n+    def test_decoder_model_past_with_attn_mask(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_attention_mask_past(*config_and_inputs)\n+\n+    # Based on tests.models.t5.test_modeling_t5.T5ModelTest.test_decoder_model_past_with_3d_attn_mask\n+    def test_decoder_model_past_with_3d_attn_mask(self):\n+        (\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+        ) = self.model_tester.prepare_config_and_inputs()\n+\n+        attention_mask = ids_tensor(\n+            [self.model_tester.batch_size, self.model_tester.encoder_seq_length, self.model_tester.encoder_seq_length],\n+            vocab_size=2,\n+        )\n+        decoder_attention_mask = ids_tensor(\n+            [self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.seq_length],\n+            vocab_size=2,\n+        )\n+\n+        self.model_tester.create_and_check_decoder_model_attention_mask_past(\n+            config,\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+        )\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_decoder_model_past_with_large_inputs\n+    def test_decoder_model_past_with_large_inputs(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n+\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_generate_with_past_key_values\n+    def test_generate_with_past_key_values(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n+\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n+    # Copied from tests.models.t5.test_modeling_t5.T5ModelTest.test_model_fp16_forward\n+    def test_model_fp16_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model with Gemma -> T5Gemma (Add is_encoder_decoder option)\n+    def test_T5Gemma_sequence_classification_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+\n+        for is_encoder_decoder in [True, False]:\n+            model = (\n+                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                .to(torch_device)\n+                .eval()\n+            )\n+            result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model_for_single_label with Gemma -> T5Gemma (Add is_encoder_decoder option)\n+    def test_T5Gemma_sequence_classification_model_for_single_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"single_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+\n+        for is_encoder_decoder in [True, False]:\n+            model = (\n+                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                .to(torch_device)\n+                .eval()\n+            )\n+            result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_sequence_classification_model_for_multi_label with Gemma -> T5Gemma (Add is_encoder_decoder option)\n+    def test_T5Gemma_sequence_classification_model_for_multi_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"multi_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor(\n+            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n+        ).to(torch.float)\n+\n+        for is_encoder_decoder in [True, False]:\n+            model = (\n+                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                .to(torch_device)\n+                .eval()\n+            )\n+            result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+            self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_Gemma_token_classification_model with Gemma -> T5Gemma (Add is_encoder_decoder option)\n+    def test_T5Gemma_token_classification_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        token_labels = ids_tensor([self.model_tester.batch_size, self.model_tester.seq_length], config.num_labels)\n+\n+        for is_encoder_decoder in [True, False]:\n+            model = (\n+                self.model_tester.for_token_class(config, is_encoder_decoder=is_encoder_decoder)\n+                .to(torch_device)\n+                .eval()\n+            )\n+\n+            result = model(input_ids, attention_mask=attention_mask, labels=token_labels)\n+            self.assertEqual(\n+                result.logits.shape,\n+                (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n+            )\n+\n+    # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_sdpa_equivalence\n+    # Add decoder_input_ids and adjust hidden states.\n+    @require_torch_sdpa\n+    @require_torch_accelerator\n+    def test_sdpa_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(reason=\"Model does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(torch_device)\n+            dummy_input = inputs_dict[model_class.main_input_name].to(torch_device)\n+            decoder_dummy_input = torch.ones_like(dummy_input)\n+\n+            model.config._attn_implementation = \"sdpa\"\n+            states_sdpa = model(dummy_input, decoder_input_ids=decoder_dummy_input, output_hidden_states=True)\n+\n+            model.config._attn_implementation = \"eager\"\n+            states_eager = model(dummy_input, decoder_input_ids=decoder_dummy_input, output_hidden_states=True)\n+\n+            if hasattr(states_sdpa, \"decoder_hidden_states\"):\n+                states_sdpa = states_sdpa.decoder_hidden_states[-1]\n+                states_eager = states_eager.decoder_hidden_states[-1]\n+            else:\n+                states_sdpa = states_sdpa.hidden_states[-1]\n+                states_eager = states_eager.hidden_states[-1]\n+\n+            torch.testing.assert_close(states_sdpa, states_eager, atol=1e-5, rtol=1e-5)\n+\n+    @unittest.skip(\"T5Gemma eager/FA2 attention outputs are expected to be different\")\n+    def test_flash_attn_2_equivalence(self):\n+        pass\n+\n+    # Based on tests.test_modeling_common.ModelTesterMixin.test_attention_outputs\n+    # Skip token classification\n+    def test_attention_outputs(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model does not output attentions\")\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+        decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", seq_len)\n+        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n+        decoder_key_length = getattr(self.model_tester, \"decoder_key_length\", decoder_seq_length)\n+        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n+        chunk_length = getattr(self.model_tester, \"chunk_length\", None)\n+        if chunk_length is not None and hasattr(self.model_tester, \"num_hashes\"):\n+            encoder_seq_length = encoder_seq_length * self.model_tester.num_hashes\n+\n+        for model_class in self.all_model_classes:\n+            # Skip token and sequence classification.\n+            if model_class in [self.model_tester.for_token_class, self.model_tester.for_sequence_class]:\n+                continue\n+\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config._attn_implementation = \"eager\"\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            if chunk_length is not None:\n+                self.assertListEqual(\n+                    list(attentions[0].shape[-4:]),\n+                    [self.model_tester.num_attention_heads, encoder_seq_length, chunk_length, encoder_key_length],\n+                )\n+            else:\n+                self.assertListEqual(\n+                    list(attentions[0].shape[-3:]),\n+                    [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+                )\n+            out_len = len(outputs)\n+\n+            if self.is_encoder_decoder:\n+                correct_outlen = 5\n+\n+                # loss is at first position\n+                if \"labels\" in inputs_dict:\n+                    correct_outlen += 1  # loss is added to beginning\n+                if \"past_key_values\" in outputs:\n+                    correct_outlen += 1  # past_key_values have been returned\n+\n+                self.assertEqual(out_len, correct_outlen)\n+\n+                # decoder attentions\n+                decoder_attentions = outputs.decoder_attentions\n+                self.assertIsInstance(decoder_attentions, (list, tuple))\n+                self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n+                self.assertListEqual(\n+                    list(decoder_attentions[0].shape[-3:]),\n+                    [self.model_tester.num_attention_heads, decoder_seq_length, decoder_key_length],\n+                )\n+\n+                # cross attentions\n+                cross_attentions = outputs.cross_attentions\n+                self.assertIsInstance(cross_attentions, (list, tuple))\n+                self.assertEqual(len(cross_attentions), self.model_tester.num_hidden_layers)\n+                self.assertListEqual(\n+                    list(cross_attentions[0].shape[-3:]),\n+                    [\n+                        self.model_tester.num_attention_heads,\n+                        decoder_seq_length,\n+                        encoder_key_length,\n+                    ],\n+                )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if hasattr(self.model_tester, \"num_hidden_states_types\"):\n+                added_hidden_states = self.model_tester.num_hidden_states_types\n+            elif self.is_encoder_decoder:\n+                added_hidden_states = 2\n+            else:\n+                added_hidden_states = 1\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+            if chunk_length is not None:\n+                self.assertListEqual(\n+                    list(self_attentions[0].shape[-4:]),\n+                    [self.model_tester.num_attention_heads, encoder_seq_length, chunk_length, encoder_key_length],\n+                )\n+            else:\n+                self.assertListEqual(\n+                    list(self_attentions[0].shape[-3:]),\n+                    [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+                )\n+\n+    # Based on tests.generation.test_utils.GenerationTesterMixin.test_past_key_values_format\n+    # Adjust encoder attention number for cross-attention caching and update attention head dimension\n+    @pytest.mark.generate\n+    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n+        \"\"\"\n+        Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test, or pass the\n+        expected cache shapes.\n+        Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # 1. If it doesn't support cache, skip the test\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n+\n+            model = model_class(config).to(torch_device)\n+            model = model.eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            if \"past_key_values\" not in outputs:\n+                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n+\n+            # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n+            past_kv = outputs[\"past_key_values\"]\n+            is_legacy_cache = not isinstance(past_kv, Cache)\n+\n+            text_config = config.get_text_config().decoder\n+            num_decoder_layers = text_config.num_hidden_layers\n+\n+            if custom_all_cache_shapes is None:\n+                num_query_attention_heads = getattr(\n+                    text_config, \"decoder_attention_heads\", text_config.num_attention_heads\n+                )\n+                per_head_embed_dim = text_config.head_dim\n+                num_key_value_heads = (\n+                    text_config.num_key_value_heads\n+                    if getattr(text_config, \"num_key_value_heads\", None) is not None\n+                    else num_query_attention_heads\n+                )\n+                if config.is_encoder_decoder:\n+                    encoder_num_attention_heads = num_key_value_heads\n+                    encoder_per_head_embed_dim = per_head_embed_dim\n+                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n+                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n+                    # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n+                    default_cross_attention_shape = (\n+                        batch_size,\n+                        encoder_num_attention_heads,\n+                        encoder_per_head_embed_dim,\n+                    )\n+                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+                    all_cache_shapes = [\n+                        [\n+                            default_self_attention_shape,\n+                            default_self_attention_shape,\n+                            default_cross_attention_shape,\n+                            default_cross_attention_shape,\n+                        ]\n+                        for _ in range(num_decoder_layers)\n+                    ]\n+                else:\n+                    batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+                    all_cache_shapes = [\n+                        [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n+                    ]\n+\n+            else:\n+                all_cache_shapes = custom_all_cache_shapes\n+\n+            # 3. Check cache shapes\n+            # 3.1. Encoder-Decoder checks\n+            if config.is_encoder_decoder:\n+                num_cache_decoder_layers = (\n+                    len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache.key_cache)\n+                )\n+                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n+\n+                for i in range(num_decoder_layers):\n+                    if is_legacy_cache:\n+                        self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n+\n+                    # Self attention\n+                    self_attention_layer_key_cache = (\n+                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.key_cache[i]\n+                    )\n+                    self_attention_layer_value_cache = (\n+                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.value_cache[i]\n+                    )\n+                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+\n+                    # Cross attention (ignore 3rd dim, see default shape preparation)\n+                    cross_attention_layer_key_cache = (\n+                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.key_cache[i]\n+                    )\n+                    cross_attention_layer_value_cache = (\n+                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.value_cache[i]\n+                    )\n+                    cross_attention_layer_key_cache = cross_attention_layer_key_cache[:, :, 0, :]\n+                    cross_attention_layer_value_cache = cross_attention_layer_value_cache[:, :, 0, :]\n+                    self.assertEqual(cross_attention_layer_key_cache.shape, all_cache_shapes[i][2])\n+                    self.assertEqual(cross_attention_layer_value_cache.shape, all_cache_shapes[i][3])\n+\n+            # 3.2. Decoder-only checks\n+            else:\n+                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.key_cache)\n+                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n+\n+                for i in range(num_decoder_layers):\n+                    if is_legacy_cache:\n+                        self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n+\n+                    # Self attention\n+                    self_attention_layer_key_cache = past_kv[i][0] if is_legacy_cache else past_kv.key_cache[i]\n+                    self_attention_layer_value_cache = past_kv[i][1] if is_legacy_cache else past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+\n+    @unittest.skip(\"Mismatch issue doesn't exist in T5Gemma.\")\n+    def test_load_with_mismatched_shapes(self):\n+        pass\n+\n+    # Based on tests.generation.test_utils.GenerationTesterMixin.test_generate_continue_from_past_key_values\n+    # Updated decoder_attention_mask to consider the appended bos token\n+    @pytest.mark.generate\n+    def test_generate_continue_from_past_key_values(self):\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            if model_class == self.model_tester.for_token_class:\n+                continue\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n+                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n+                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n+\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            if \"token_type_ids\" in inputs:\n+                del inputs[\"token_type_ids\"]\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n+            outputs = model(**inputs)\n+            if \"past_key_values\" not in outputs:\n+                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n+\n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+\n+            # It must be encoder-decoder models\n+            self.assertTrue(config.is_encoder_decoder)\n+\n+            inputs[\"decoder_input_ids\"] = outputs_cached.sequences\n+            if \"decoder_attention_mask\" in inputs:\n+                decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n+\n+                # Add BOS mask: the new sequence comes with a new BOS token, which is not included in the original inputs\n+                padding_tensor = torch.ones_like(decoder_attention_mask[:, :1])\n+                decoder_attention_mask = torch.cat([padding_tensor, decoder_attention_mask], dim=1)\n+\n+                inputs[\"decoder_attention_mask\"] = torch.nn.functional.pad(\n+                    decoder_attention_mask,\n+                    (0, new_attention_len - decoder_attention_mask.shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            for layer_idx in range(len(outputs_cached.past_key_values)):\n+                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            outputs_cached.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n+    # Based on tests.test_modeling_common.ModelTesterMixin.test_inputs_embeds_matches_input_ids\n+    # Update encoder and decoder embeddings\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        model_class = self.model_tester.model_class\n+\n+        model = model_class(config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        model_forward_args = inspect.signature(model.forward).parameters\n+        if \"inputs_embeds\" not in model_forward_args:\n+            self.skipTest(reason=\"This model doesn't use `inputs_embeds`\")\n+\n+        inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n+        pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1\n+\n+        encoder_embedding = model.get_encoder().get_input_embeddings()\n+        decoder_embedding = model.get_decoder().get_input_embeddings()\n+\n+        encoder_input_ids = inputs[\"input_ids\"]\n+        decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n+        encoder_input_ids[encoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)\n+        decoder_input_ids[decoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)\n+        del inputs[\"input_ids\"]\n+        inputs.pop(\"decoder_input_ids\", None)\n+\n+        inputs_embeds = encoder_embedding(encoder_input_ids)\n+        decoder_inputs_embeds = decoder_embedding(decoder_input_ids)\n+        with torch.no_grad():\n+            out_ids = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids, **inputs)[0]\n+            out_embeds = model(inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, **inputs)[0]\n+\n+        torch.testing.assert_close(out_embeds, out_ids)\n+\n+    # Based on tests.test_modeling_common.ModelTesterMixin.test_inputs_embeds_matches_input_ids\n+    # Adjust token classiifcation\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            if model_class in [self.model_tester.for_token_class, self.model_tester.for_sequence_class]:\n+                model = model_class(config, is_encoder_decoder=False)\n+            else:\n+                model = model_class(config)\n+\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            if hasattr(self.model_tester, \"encoder_seq_length\"):\n+                seq_length = self.model_tester.encoder_seq_length\n+                if hasattr(self.model_tester, \"chunk_length\") and self.model_tester.chunk_length > 1:\n+                    seq_length = seq_length * self.model_tester.chunk_length\n+            else:\n+                seq_length = self.model_tester.seq_length\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [seq_length, self.model_tester.hidden_size],\n+            )\n+\n+            if config.is_encoder_decoder:\n+                hidden_states = outputs.decoder_hidden_states\n+\n+                self.assertIsInstance(hidden_states, (list, tuple))\n+                self.assertEqual(len(hidden_states), expected_num_layers)\n+                seq_len = getattr(self.model_tester, \"seq_length\", None)\n+                decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", seq_len)\n+\n+                self.assertListEqual(\n+                    list(hidden_states[0].shape[-2:]),\n+                    [decoder_seq_length, self.model_tester.hidden_size],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    # Based on tests.models.t5.test_modeling_t5.T5ModelTest.test_custom_4d_attention_mask\n+    # Excluding the final token from input_ids\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_ids[:, :-1],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_ids[:1, :-1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n+    # Based on tests.test_modeling_common.ModelTesterMixin.test_flex_attention_with_grads\n+    # Update hidden size for encoder and decoder\n+    @require_torch_gpu\n+    def test_flex_attention_with_grads(self):\n+        for model_class in self.all_model_classes:\n+            # TODO: raushan, fix for composite models after making VLMs support new attn API\n+            if not model_class._supports_flex_attn or self._is_composite:\n+                self.skipTest(reason=\"This model does not support flex attention\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config._attn_implementation = \"flex_attention\"\n+            # Flex Attention cannot use dropout\n+            config.encoder.attention_dropout = 0\n+            config.decoder.attention_dropout = 0\n+\n+            # Flex attention relies on triton on compilation\n+            # However, triton cannot handle hidden dimensions of less than 16\n+            # --> forcing at least a hidden dim of 16\n+            config.encoder.hidden_size *= max(\n+                16\n+                // getattr(\n+                    config.encoder, \"head_dim\", config.encoder.hidden_size // config.encoder.num_attention_heads\n+                ),\n+                1,\n+            )\n+            config.decoder.hidden_size *= max(\n+                16\n+                // getattr(\n+                    config.decoder, \"head_dim\", config.decoder.hidden_size // config.decoder.num_attention_heads\n+                ),\n+                1,\n+            )\n+            config.decoder.cross_attention_hidden_size = config.encoder.hidden_size\n+\n+            config.decoder.head_dim = max(16, config.decoder.head_dim)\n+            config.encoder.head_dim = max(16, config.encoder.head_dim)\n+\n+            model = model_class(config).to(device=torch_device)\n+            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+\n+            # Elaborate workaround for encoder-decoder models as some do not specify their main input\n+            dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n+            if config.is_encoder_decoder:\n+                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n+                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n+\n+            # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n+            _ = model(**dummy_inputs)\n+\n+    @unittest.skip(\"EncoderDecoderCache can't be gathered because it is not iterable.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+\n+class T5GemmaEncoderOnlyModelTester:\n+    config_class = T5GemmaConfig\n+    module_config_class = T5GemmaModuleConfig\n+\n+    if is_torch_available():\n+        model_class = T5GemmaEncoderModel\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        is_training=True,\n+        use_attention_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        seq_length=7,\n+        # default to encoders\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_key_value_heads=2,\n+        intermediate_size=37,\n+        # common\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        scope=None,\n+        # special ids\n+        eos_token_id=1,\n+        pad_token_id=0,\n+        bos_token_id=2,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.use_attention_mask = use_attention_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        # encoder\n+        self.seq_length = seq_length\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.intermediate_size = intermediate_size\n+        # common\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.scope = scope\n+        self.head_dim = self.hidden_size // self.num_attention_heads\n+        # special ids\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+\n+    def get_encoder_config(self):\n+        return self.module_config_class(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            head_dim=self.head_dim,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def get_config(self):\n+        return self.config_class(\n+            encoder=self.get_encoder_config(),\n+            decoder=None,\n+            is_encoder_decoder=False,\n+            # Used for generation test.\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        # Remove BOS symbols from inputs.\n+        input_ids = torch.where(input_ids == self.bos_token_id, 42, input_ids)\n+\n+        attention_mask = None\n+        if self.use_attention_mask:\n+            attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n+\n+        config = self.get_config()\n+\n+        return (\n+            config,\n+            input_ids,\n+            attention_mask,\n+        )\n+\n+    def create_and_check_model(\n+        self,\n+        config,\n+        input_ids,\n+        attention_mask,\n+    ):\n+        model = self.model_class(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+        )\n+        result = model(input_ids=input_ids)\n+        encoder_output = result.last_hidden_state\n+\n+        self.parent.assertEqual(encoder_output.size(), (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_model_fp16_forward(\n+        self,\n+        config,\n+        input_ids,\n+        attention_mask,\n+    ):\n+        model = self.model_class(config=config).to(torch_device).half().eval()\n+        output = model(input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n+        self.parent.assertFalse(torch.isnan(output).any().item())\n+\n+    def create_and_check_with_token_classification_head(\n+        self,\n+        config,\n+        input_ids,\n+        attention_mask,\n+    ):\n+        labels = torch.tensor([1] * self.seq_length * self.batch_size, dtype=torch.long, device=torch_device)\n+        model = T5GemmaForTokenClassification(config=config, is_encoder_decoder=False).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            labels=labels,\n+            attention_mask=attention_mask,\n+        )\n+        self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, config.num_labels))\n+        self.parent.assertEqual(outputs[\"loss\"].size(), ())\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            attention_mask,\n+        ) = config_and_inputs\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class T5GemmaEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (T5GemmaEncoderModel, T5GemmaForTokenClassification) if is_torch_available() else ()\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_headmasking = False\n+    _is_stateful = True\n+    is_encoder_decoder = False\n+    model_split_percents = [0.4, 0.5]\n+\n+    def setUp(self):\n+        self.model_tester = T5GemmaEncoderOnlyModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=T5GemmaConfig,\n+            # For faking the testing.\n+            hidden_size=37,\n+            vocab_size=self.model_tester.vocab_size,\n+            num_attention_heads=self.model_tester.num_attention_heads,\n+            num_hidden_layers=self.model_tester.num_hidden_layers,\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n+    def test_model_fp16_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n+\n+    def test_with_token_classification_head(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_with_token_classification_head(*config_and_inputs)\n+\n+    @unittest.skip(\"No loss in the output of T5GemmaEncoderModel\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(\"No loss in the output of T5GemmaEncoderModel\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\"No loss in the output of T5GemmaEncoderModel\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\"No loss in the output of T5GemmaEncoderModel\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    # Based on tests.test_modeling_common.ModelTesterMixin.test_flex_attention_with_grads\n+    # Update hidden size for encoder\n+    @require_torch_gpu\n+    def test_flex_attention_with_grads(self):\n+        for model_class in self.all_model_classes:\n+            # TODO: raushan, fix for composite models after making VLMs support new attn API\n+            if not model_class._supports_flex_attn or self._is_composite:\n+                self.skipTest(reason=\"This model does not support flex attention\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config._attn_implementation = \"flex_attention\"\n+            # Flex Attention cannot use dropout\n+            config.encoder.attention_dropout = 0\n+\n+            # Flex attention relies on triton on compilation\n+            # However, triton cannot handle hidden dimensions of less than 16\n+            # --> forcing at least a hidden dim of 16\n+            config.encoder.hidden_size *= max(\n+                16\n+                // getattr(\n+                    config.encoder, \"head_dim\", config.encoder.hidden_size // config.encoder.num_attention_heads\n+                ),\n+                1,\n+            )\n+            config.encoder.head_dim = max(16, config.encoder.head_dim)\n+\n+            model = model_class(config).to(device=torch_device)\n+            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+\n+            # Elaborate workaround for encoder-decoder models as some do not specify their main input\n+            dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n+\n+            # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n+            _ = model(**dummy_inputs)\n+\n+\n+# Based on tests.models.t5.test_modeling_t5.TestAsymmetricT5\n+# Adapted for T5Gemma\n+@require_torch\n+class TestAsymmetricT5Gemma(unittest.TestCase):\n+    def build_model_and_check_forward_pass(self, **kwargs):\n+        tester = T5GemmaModelTester(self, **kwargs)\n+        config, *inputs = tester.prepare_config_and_inputs()\n+        (\n+            input_ids,\n+            decoder_input_ids,\n+            attention_mask,\n+            decoder_attention_mask,\n+            lm_labels,\n+        ) = inputs\n+        model = T5GemmaForConditionalGeneration(config=config).to(torch_device).eval()\n+        outputs = model(\n+            input_ids=input_ids,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            labels=lm_labels,\n+        )\n+        # outputs = model(*inputs)\n+        assert len(outputs) == 4\n+        assert outputs[\"logits\"].size() == (tester.batch_size, tester.seq_length, tester.vocab_size)\n+        assert outputs[\"loss\"].size() == ()\n+        return model.model\n+\n+    def test_small_decoder(self):\n+        model = self.build_model_and_check_forward_pass(num_hidden_layers=1, encoder_num_hidden_layers=2)\n+        assert len(model.encoder.layers) == 2\n+        assert len(model.decoder.layers) == 1\n+\n+    def test_defaulting_to_symmetry(self):\n+        model = self.build_model_and_check_forward_pass(num_hidden_layers=2, encoder_num_hidden_layers=2)\n+        assert len(model.decoder.layers) == len(model.encoder.layers) == 2"
        }
    ],
    "stats": {
        "total": 5148,
        "additions": 5148,
        "deletions": 0
    }
}