{
    "author": "guangy10",
    "message": "Gemma3 is Torch Exportable (#37728)\n\n* Gemma3 is Torch Exportable\n\n* Expand the support to other mdoels using HybridCache\n\n---------\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "816b37010cb6fd963433c6c5681b18be6475592e",
    "files": [
        {
            "sha": "7a5f1fd79763fed830e938d122d169c39cc7ee73",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 284,
            "deletions": 4,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -20,15 +20,207 @@\n \n \n if is_torch_available():\n-    from transformers import PreTrainedModel, StaticCache\n+    from transformers import HybridCache, PreTrainedModel, StaticCache\n     from transformers.pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n \n \n+class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n+    \"\"\"\n+    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n+    specifically for decoder-only LM with cache. This module ensures that the\n+    exported model is compatible with further lowering and execution in `ExecuTorch`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model: PreTrainedModel,\n+        max_batch_size: int = 1,\n+        max_cache_len: int = 4096,\n+    ):\n+        \"\"\"\n+        Initializes the exportable module with `HybridCache`.\n+\n+        Args:\n+            model (`PreTrainedModel`): The pretrained model to wrap.\n+            max_batch_size (int): Maximum batch size for the cache.\n+            max_cache_len (int): Maximum sequence length for the cache.\n+\n+        Raises:\n+            ValueError: If the model is configured with a unsupported cache implementation.\n+        \"\"\"\n+        super().__init__()\n+\n+        if model.config.cache_implementation == \"static\":\n+            self.model = TorchExportableModuleWithStaticCache(model)\n+        elif model.config.cache_implementation == \"hybrid\":\n+            self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n+        else:\n+            raise ValueError(\n+                f\"Unsupported cache implementation in this export recipe: '{model.config.cache_implementation}'\"\n+            )\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        cache_position: torch.Tensor,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n+\n+        Args:\n+            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n+\n+        Returns:\n+            torch.Tensor: Logits output from the model.\n+        \"\"\"\n+        return self.model.forward(input_ids, cache_position)\n+\n+    def export(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        dynamic_shapes: Optional[dict] = None,\n+        strict: Optional[bool] = None,\n+    ) -> torch.export.ExportedProgram:\n+        \"\"\"\n+        Export the wrapped module using `torch.export`.\n+\n+        Args:\n+            input_ids (`Optional[torch.Tensor]`):\n+                Tensor representing current input token id to the module. If not provided, a default tensor will be used.\n+            cache_position (`Optional[torch.Tensor]`):\n+                Tensor representing current input position in the cache. If not provided, a default tensor will be used.\n+            dynamic_shapes (`Optional[dict]`):\n+                Dynamic shapes to use for export if specified.\n+            strict(`Optional[bool]`):\n+                Flag to instruct `torch.export` to use `torchdynamo`.\n+        \"\"\"\n+        example_input_ids = input_ids if input_ids is not None else torch.tensor([[1]], dtype=torch.long)\n+        example_cache_position = cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long)\n+\n+        return torch.export.export(\n+            self.model,\n+            args=(example_input_ids, example_cache_position),\n+            kwargs={},\n+            dynamic_shapes=dynamic_shapes,\n+            strict=strict if strict is not None else True,\n+        )\n+\n+    @staticmethod\n+    def generate(\n+        exported_program: torch.export.ExportedProgram,\n+        tokenizer,\n+        prompt: str,\n+        max_new_tokens: int = 20,\n+        do_sample: bool = False,\n+        temperature: float = 1.0,\n+        top_k: int = 50,\n+        top_p: float = 1.0,\n+        device: str = \"cpu\",\n+    ) -> str:\n+        \"\"\"\n+        Generate a sequence of tokens using an exported program.\n+\n+        Args:\n+            exported_program (`torch.export.ExportedProgram`): The exported model being used for generate.\n+            tokenizer: The tokenizer to use.\n+            prompt (str): The input prompt.\n+            max_new_tokens (int): Maximum number of new tokens to generate.\n+            do_sample (bool): Whether to use sampling or greedy decoding.\n+            temperature (float): The temperature for sampling.\n+            top_k (int): The number of highest probability tokens to keep for top-k sampling.\n+            top_p (float): The cumulative probability for nucleus sampling.\n+            device (str): The device to use.\n+\n+        Returns:\n+            str: The generated text.\n+        \"\"\"\n+        # Get the module from the exported program\n+        exported_module = exported_program.module()\n+\n+        # Tokenize the prompt\n+        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n+\n+        # Initialize with the prompt\n+        generated_ids = input_ids.clone()\n+\n+        # Process the prompt tokens first\n+        curr_position = 0\n+        for i in range(input_ids.shape[1]):\n+            # Process one token at a time\n+            curr_input_ids = input_ids[:, i : i + 1]\n+            curr_cache_position = torch.tensor([curr_position], dtype=torch.long, device=device)\n+\n+            # Forward pass\n+            _ = exported_module(curr_input_ids, curr_cache_position)\n+            curr_position += 1\n+\n+        # Generate new tokens\n+        for _ in range(max_new_tokens):\n+            # Get the last token as input\n+            curr_input_ids = generated_ids[:, -1:]\n+            curr_cache_position = torch.tensor([curr_position], dtype=torch.long, device=device)\n+\n+            # Forward pass to get next token logits\n+            outputs = exported_module(curr_input_ids, curr_cache_position)\n+\n+            # Get the next token ID\n+            if do_sample:\n+                # Apply temperature\n+                if temperature > 0:\n+                    logits = outputs / temperature\n+                else:\n+                    logits = outputs\n+\n+                # Apply top-k filtering\n+                if top_k > 0:\n+                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n+                    logits[indices_to_remove] = float(\"-inf\")\n+\n+                # Apply top-p (nucleus) filtering\n+                if top_p < 1.0:\n+                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n+                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n+\n+                    # Remove tokens with cumulative probability above the threshold\n+                    sorted_indices_to_remove = cumulative_probs > top_p\n+                    # Shift the indices to the right to keep also the first token above the threshold\n+                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n+                    sorted_indices_to_remove[..., 0] = 0\n+\n+                    # Scatter sorted tensors to original indexing\n+                    indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n+                    logits[indices_to_remove] = float(\"-inf\")\n+\n+                # Sample from the filtered distribution\n+                probs = torch.softmax(logits, dim=-1)\n+                next_token_id = torch.multinomial(probs, num_samples=1)\n+            else:\n+                # Greedy decoding\n+                next_token_id = outputs.argmax(dim=-1, keepdim=True)\n+\n+            # Ensure next_token_id has the right shape before concatenation\n+            if next_token_id.dim() > 2:\n+                next_token_id = next_token_id.squeeze(-1)\n+\n+            # Append to the generated sequence\n+            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n+            curr_position += 1\n+\n+            # Stop if we generate an EOS token\n+            if next_token_id.item() == tokenizer.eos_token_id:\n+                break\n+\n+        # Decode the generated text\n+        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+\n+\n class TorchExportableModuleWithStaticCache(torch.nn.Module):\n     \"\"\"\n-    A wrapper module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for use with static caching. This module ensures that the exported model\n-    is compatible with further lowering and execution in `ExecuTorch`.\n+    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n+    specifically for decoder-only LM to `StaticCache`. This module ensures that the\n+    exported model is compatible with further lowering and execution in `ExecuTorch`.\n \n     Note:\n         This class is specifically designed to support export process using `torch.export`\n@@ -178,6 +370,94 @@ def generate(\n         return torch.tensor([response_tokens], dtype=torch.long)\n \n \n+class TorchExportableModuleWithHybridCache(torch.nn.Module):\n+    \"\"\"\n+    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n+    specifically for decoder-only LM to `HybridCache`. This module ensures that the\n+    exported model is compatible with further lowering and execution in `ExecuTorch`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model: PreTrainedModel,\n+        max_batch_size: int = 1,\n+        max_cache_len: int = 4096,\n+    ):\n+        \"\"\"\n+        Initializes the exportable module with `HybridCache`.\n+\n+        Args:\n+            model (`PreTrainedModel`): The pretrained model to wrap.\n+            max_batch_size (int): Maximum batch size for the cache.\n+            max_cache_len (int): Maximum sequence length for the cache.\n+\n+        Raises:\n+            AssertionError: If the model doesn't have the expected configuration for HybridCache.\n+        \"\"\"\n+        super().__init__()\n+        self.model = model\n+\n+        # Verify the model is configured for HybridCache\n+        if not self.model.config.use_cache:\n+            raise AssertionError(\"Model must have caching enabled\")\n+\n+        if (\n+            not hasattr(self.model.config, \"cache_implementation\")\n+            or self.model.config.cache_implementation != \"hybrid\"\n+        ):\n+            raise AssertionError(\"Model must use 'hybrid' cache implementation\")\n+\n+        # Initialize the HybridCache\n+        self.cache = HybridCache(\n+            config=self.model.config,\n+            max_batch_size=max_batch_size,\n+            max_cache_len=max_cache_len,\n+            device=self.model.device,\n+            dtype=self.model.dtype,\n+        )\n+\n+        # Register all key and value cache tensors as buffers\n+        for i in range(len(self.cache.key_cache)):\n+            self.register_buffer(f\"key_cache_{i}\", self.cache.key_cache[i], persistent=False)\n+            self.register_buffer(f\"value_cache_{i}\", self.cache.value_cache[i], persistent=False)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        cache_position: torch.Tensor,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n+\n+        Args:\n+            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n+\n+        Returns:\n+            torch.Tensor: Logits output from the model.\n+        \"\"\"\n+        batch_size, seq_len = input_ids.shape\n+\n+        # Generate position_ids from cache_position\n+        position_ids = cache_position.unsqueeze(0).expand(batch_size, -1)\n+\n+        # Create attention mask (always ones for token-by-token generation)\n+        attention_mask = torch.ones((batch_size, seq_len), dtype=torch.long, device=input_ids.device)\n+\n+        # Forward pass with the model\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=self.cache,\n+            use_cache=True,\n+            cache_position=cache_position,\n+        )\n+\n+        # Return only the logits to simplify the export\n+        return outputs.logits\n+\n+\n def convert_and_export_with_cache(\n     model: PreTrainedModel,\n     example_input_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "da68159e770e254c8453d48f771bab710342138c",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -351,7 +351,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "9a47f493930f216f191c974da12c2d25a5492dba",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -400,7 +400,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "a1a308f1d81ee1fa29b8d62790c1350087074f63",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -317,7 +317,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "5234938dc72b4cfa3d5927b54af0a3fbe534b36a",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -364,7 +364,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "0e009d8887be4f11bd8171ec90b4a150a26f6b3f",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -410,7 +410,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "b4e7301964fd1a0de9d8f2c2bbb12608e964ec89",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -494,7 +494,7 @@ def forward(\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n                 offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = max(0, offset)\n+                offset = torch.clamp(offset, min=0)\n                 # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n                 # but without data-dependent slicing (i.e. torch.compile friendly)\n                 mask_indexes = torch.arange("
        },
        {
            "sha": "c05f319d5a9561162fbb719a8c10dbd2aee42691",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -337,6 +337,44 @@ def test_export_static_cache(self):\n         ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n \n+    @slow\n+    @require_read_token\n+    def test_export_hybrid_cache(self):\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+        from transformers.pytorch_utils import is_torch_greater_or_equal\n+\n+        if not is_torch_greater_or_equal(\"2.6.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")\n+\n+        model_id = \"google/gemma-2-2b\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n+        self.assertEqual(model.config.cache_implementation, \"hybrid\")\n+\n+        # Export + HybridCache\n+        model.eval()\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n+\n+        # Test generation with the exported model\n+        prompt = \"What is the capital of France?\"\n+        max_new_tokens_to_generate = 20\n+        # Generate text with the exported model\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        export_generated_text = TorchExportableModuleForDecoderOnlyLM.generate(\n+            exported_program, tokenizer, prompt, max_new_tokens=max_new_tokens_to_generate\n+        )\n+\n+        input_text = tokenizer(prompt, return_tensors=\"pt\")\n+        with torch.no_grad():\n+            eager_outputs = model.generate(\n+                **input_text,\n+                max_new_tokens=max_new_tokens_to_generate,\n+                do_sample=False,  # Use greedy decoding to match the exported model\n+            )\n+\n+        eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)\n+        self.assertEqual(export_generated_text, eager_generated_text)\n+\n     @require_read_token\n     @tooslow\n     def test_model_9b_bf16_flex_attention(self):"
        },
        {
            "sha": "39b7abeaea806b40b5b6320b360ca7e97abb7212",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816b37010cb6fd963433c6c5681b18be6475592e/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=816b37010cb6fd963433c6c5681b18be6475592e",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Gemma3 model.\"\"\"\n \n+import logging\n import tempfile\n import unittest\n \n@@ -52,6 +53,7 @@\n         Gemma3Processor,\n         Gemma3TextModel,\n     )\n+    from transformers.pytorch_utils import is_torch_greater_or_equal\n \n \n class Gemma3ModelTester(GemmaModelTester):\n@@ -664,3 +666,42 @@ def test_generation_beyond_sliding_window_with_generation_config(self):\n         model.generation_config.transformers_version = \"4.49.0\"\n         with self.assertRaises(RuntimeError):  # errors out because it is not using hybrid cache\n             out = model.generate(**inputs, generation_config=generation_config)\n+\n+    def test_export_text_only_with_hybrid_cache(self):\n+        if not is_torch_greater_or_equal(\"2.6.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")\n+\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        model_id = \"google/gemma-3-1b-it\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n+        self.assertEqual(model.config.cache_implementation, \"hybrid\")\n+\n+        # Export + HybridCache\n+        model.eval()\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export()\n+        logging.info(f\"\\nExported program: {exported_program}\")\n+\n+        # Test generation with the exported model\n+        prompt = \"What is the capital of France?\"\n+        max_new_tokens_to_generate = 20\n+        # Generate text with the exported model\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        export_generated_text = TorchExportableModuleForDecoderOnlyLM.generate(\n+            exported_program, tokenizer, prompt, max_new_tokens=max_new_tokens_to_generate\n+        )\n+        logging.info(f\"\\nExport generated texts: '{export_generated_text}'\")\n+\n+        input_text = tokenizer(prompt, return_tensors=\"pt\")\n+        with torch.no_grad():\n+            eager_outputs = model.generate(\n+                **input_text,\n+                max_new_tokens=max_new_tokens_to_generate,\n+                do_sample=False,  # Use greedy decoding to match the exported model\n+            )\n+\n+        eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)\n+        logging.info(f\"\\nEager generated texts: '{eager_generated_text}'\")\n+\n+        self.assertEqual(export_generated_text, eager_generated_text)"
        }
    ],
    "stats": {
        "total": 379,
        "additions": 369,
        "deletions": 10
    }
}