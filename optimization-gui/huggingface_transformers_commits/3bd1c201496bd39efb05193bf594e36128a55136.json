{
    "author": "yao-matrix",
    "message": "enable misc cases on XPU & use device agnostic APIs for cases in tests (#38192)\n\n* use device agnostic APIs in tests\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* more\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* add reset_peak_memory_stats API\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "3bd1c201496bd39efb05193bf594e36128a55136",
    "files": [
        {
            "sha": "094bc4f232fdd06193597a50c93887a46015a277",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -3024,6 +3024,11 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n         \"cpu\": 0,\n         \"default\": 0,\n     }\n+    BACKEND_RESET_PEAK_MEMORY_STATS = {\n+        \"cuda\": torch.cuda.reset_peak_memory_stats,\n+        \"cpu\": None,\n+        \"default\": None,\n+    }\n     BACKEND_MEMORY_ALLOCATED = {\n         \"cuda\": torch.cuda.memory_allocated,\n         \"cpu\": 0,\n@@ -3044,6 +3049,7 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_EMPTY_CACHE = {\"default\": None}\n     BACKEND_DEVICE_COUNT = {\"default\": lambda: 0}\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED = {\"default\": None}\n+    BACKEND_RESET_PEAK_MEMORY_STATS = {\"default\": None}\n     BACKEND_MAX_MEMORY_ALLOCATED = {\"default\": 0}\n     BACKEND_MEMORY_ALLOCATED = {\"default\": 0}\n     BACKEND_SYNCHRONIZE = {\"default\": None}\n@@ -3072,6 +3078,7 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_MANUAL_SEED[\"xpu\"] = torch.xpu.manual_seed\n     BACKEND_DEVICE_COUNT[\"xpu\"] = torch.xpu.device_count\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.reset_peak_memory_stats\n+    BACKEND_RESET_PEAK_MEMORY_STATS[\"xpu\"] = torch.xpu.reset_peak_memory_stats\n     BACKEND_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.max_memory_allocated\n     BACKEND_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.memory_allocated\n     BACKEND_SYNCHRONIZE[\"xpu\"] = torch.xpu.synchronize\n@@ -3100,6 +3107,10 @@ def backend_reset_max_memory_allocated(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_RESET_MAX_MEMORY_ALLOCATED)\n \n \n+def backend_reset_peak_memory_stats(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_RESET_PEAK_MEMORY_STATS)\n+\n+\n def backend_max_memory_allocated(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_MAX_MEMORY_ALLOCATED)\n "
        },
        {
            "sha": "4a86e8e79ff9cc49ee663cec43db5fc2202f4acd",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -30,6 +30,7 @@\n )\n from transformers.models.idefics3 import Idefics3VisionConfig\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_large_accelerator,\n@@ -302,7 +303,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     @slow\n     @require_torch_large_accelerator"
        },
        {
            "sha": "9dd22d7b8ba86ba7f6926b7485b88ba371444c4e",
            "filename": "tests/models/cohere/test_tokenization_cohere.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -17,7 +17,11 @@\n from functools import lru_cache\n \n from transformers import CohereTokenizerFast\n-from transformers.testing_utils import require_jinja, require_tokenizers, require_torch_multi_gpu\n+from transformers.testing_utils import (\n+    require_jinja,\n+    require_tokenizers,\n+    require_torch_multi_accelerator,\n+)\n \n from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n@@ -55,7 +59,7 @@ def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n         return CohereTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     # This gives CPU OOM on a single-gpu runner (~60G RAM). On multi-gpu runner, it has ~180G RAM which is enough.\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_torch_encode_plus_sent_to_model(self):\n         super().test_torch_encode_plus_sent_to_model()\n "
        },
        {
            "sha": "2819592fe09d6b8c89cdc03d783aabcb40ec326a",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -29,6 +29,7 @@\n from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliForRetrievalOutput\n from transformers.models.colpali.processing_colpali import ColPaliProcessor\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_torch,\n     require_vision,\n     slow,\n@@ -303,7 +304,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     @slow\n     def test_model_integration_test(self):"
        },
        {
            "sha": "eea0c10d4bf4d527fa5fbd591e5affff0bb42cff",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -35,7 +35,7 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     require_torch_sdpa,\n     slow,\n     torch_device,\n@@ -583,7 +583,7 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_integration_test(self):\n         model = Idefics2ForConditionalGeneration.from_pretrained(\n             \"HuggingFaceM4/idefics2-8b-base\","
        },
        {
            "sha": "b29a6b4db95c1dec8f92b49457f0a7f8fe4b3f4a",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -31,7 +31,7 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n+from transformers.testing_utils import backend_empty_cache, require_soundfile, require_torch, slow, torch_device\n from transformers.utils import is_soundfile_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -296,7 +296,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_text_only_generation(self):\n         model = AutoModelForCausalLM.from_pretrained("
        },
        {
            "sha": "3a0f6458adaee9390522928469a622e33545840d",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -29,6 +29,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     is_flaky,\n     require_cv2,\n     require_flash_attn,\n@@ -421,7 +422,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     @slow\n     def test_small_model_integration_test(self):"
        },
        {
            "sha": "92b6d7f87f9aa9a16c9842fa416c64a091d7ffc2",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -28,6 +28,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -367,7 +368,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     @slow\n     def test_small_model_integration_test(self):"
        },
        {
            "sha": "0446fb2052d47cfb0bd43272790f2d634a7322e6",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_flash_attn,\n-    require_non_xpu,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -42,7 +41,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_available, is_torchaudio_available\n+from transformers.utils import cached_property, is_torch_available, is_torch_xpu_available, is_torchaudio_available\n from transformers.utils.import_utils import is_datasets_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -2431,11 +2430,10 @@ def test_default_multilingual_transcription_long_form(self):\n             \" How many different species are there in the chilli? How many different species are there in the chilli?\",\n         )\n \n-    @require_non_xpu\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_speculative_decoding_distil(self):\n-        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n+        torch_dtype = torch.float16 if (torch.cuda.is_available() or is_torch_xpu_available()) else torch.float32\n         model_id = \"openai/whisper-large-v2\"\n         model = WhisperForConditionalGeneration.from_pretrained(\n             model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True"
        },
        {
            "sha": "6174a613295408f88e3a175b87b74c7d0bcc9ddc",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -21,9 +21,11 @@\n from transformers.integrations.tensor_parallel import get_packed_weights, repack_weights\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n     get_torch_dist_unique_port,\n     require_huggingface_hub_greater_or_equal,\n     require_torch_multi_gpu,\n+    torch_device,\n )\n \n \n@@ -168,4 +170,4 @@ def test_model_save(self):\n \n @require_torch_multi_gpu\n class TestTensorParallelCuda(TestTensorParallel):\n-    nproc_per_node = torch.cuda.device_count()\n+    nproc_per_node = backend_device_count(torch_device)"
        },
        {
            "sha": "67da1ad857e51ce99022e3ca4d543a8e9bb39751",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -73,6 +73,7 @@\n )\n from transformers.testing_utils import (\n     CaptureLogger,\n+    backend_empty_cache,\n     get_device_properties,\n     hub_retry,\n     is_flaky,\n@@ -2652,7 +2653,7 @@ def get_current_gpu_memory_use():\n         config = self.model_tester.get_large_model_config()\n \n         for model_class in self.all_parallelizable_model_classes:\n-            torch.cuda.empty_cache()\n+            backend_empty_cache(torch_device)\n \n             # 1. single gpu memory load + unload + memory measurements\n             # Retrieve initial memory usage (can easily be ~0.6-1.5GB if cuda-kernels have been preloaded by previous tests)\n@@ -2668,7 +2669,7 @@ def get_current_gpu_memory_use():\n \n             del model\n             gc.collect()\n-            torch.cuda.empty_cache()\n+            backend_empty_cache(torch_device)\n \n             # 2. MP test\n             # it's essential to re-calibrate the usage before the next stage\n@@ -2692,7 +2693,7 @@ def get_current_gpu_memory_use():\n \n             del model\n             gc.collect()\n-            torch.cuda.empty_cache()\n+            backend_empty_cache(torch_device)\n \n     @require_torch_gpu\n     @require_torch_multi_gpu"
        },
        {
            "sha": "21b8622473fbfabbf674053068e8def4dbf987c3",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -66,6 +66,7 @@\n     backend_max_memory_allocated,\n     backend_memory_allocated,\n     backend_reset_max_memory_allocated,\n+    backend_reset_peak_memory_stats,\n     evaluate_side_effect_factory,\n     execute_subprocess_async,\n     get_gpu_count,\n@@ -1654,7 +1655,7 @@ def is_any_loss_nan_or_inf(log_history):\n         self.assertFalse(is_any_loss_nan_or_inf(log_history_filter))\n \n     def test_train_and_eval_dataloaders(self):\n-        if torch_device == \"cuda\":\n+        if torch_device in [\"cuda\", \"xpu\"]:\n             n_gpu = max(1, backend_device_count(torch_device))\n         else:\n             n_gpu = 1\n@@ -4106,7 +4107,7 @@ def forward(self, x):\n         mod = MyModule()\n \n         # 1. without TorchDynamo (eager baseline)\n-        a = torch.ones(1024, 1024, device=\"cuda\", requires_grad=True)\n+        a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n         a.grad = None\n         trainer = CustomTrainer(model=mod)\n         # warmup\n@@ -4115,17 +4116,17 @@ def forward(self, x):\n \n         # resets\n         gc.collect()\n-        torch.cuda.empty_cache()\n-        torch.cuda.reset_peak_memory_stats()\n+        backend_empty_cache(torch_device)\n+        backend_reset_peak_memory_stats(torch_device)\n \n         orig_loss = trainer.training_step(mod, {\"x\": a})\n-        orig_peak_mem = torch.cuda.max_memory_allocated()\n+        orig_peak_mem = backend_max_memory_allocated(torch_device)\n         torchdynamo.reset()\n         del trainer\n \n         # 2. TorchDynamo nvfuser\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            a = torch.ones(1024, 1024, device=\"cuda\", requires_grad=True)\n+            a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n             a.grad = None\n             args = TrainingArguments(output_dir=tmp_dir, torch_compile_backend=\"nvfuser\")\n             trainer = CustomTrainer(model=mod, args=args)\n@@ -4135,11 +4136,11 @@ def forward(self, x):\n \n             # resets\n             gc.collect()\n-            torch.cuda.empty_cache()\n-            torch.cuda.reset_peak_memory_stats()\n+            backend_empty_cache(torch_device)\n+            backend_reset_peak_memory_stats(torch_device)\n \n             loss = trainer.training_step(mod, {\"x\": a})\n-            peak_mem = torch.cuda.max_memory_allocated()\n+            peak_mem = backend_max_memory_allocated(torch_device)\n             torchdynamo.reset()\n             del trainer\n "
        },
        {
            "sha": "089b45c192bfbe0160a05a7c7312dae283980eed",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bd1c201496bd39efb05193bf594e36128a55136/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=3bd1c201496bd39efb05193bf594e36128a55136",
            "patch": "@@ -21,6 +21,7 @@\n from transformers.generation.configuration_utils import ALL_CACHE_IMPLEMENTATIONS\n from transformers.testing_utils import (\n     CaptureStderr,\n+    backend_device_count,\n     cleanup,\n     get_gpu_count,\n     is_torch_available,\n@@ -210,8 +211,8 @@ def _skip_on_failed_cache_prerequisites(self, cache_implementation):\n             if not has_accelerator:\n                 self.skipTest(\"Offloaded caches require an accelerator\")\n             if cache_implementation in [\"offloaded_static\", \"offloaded_hybrid_chunked\"]:\n-                if torch.cuda.device_count() != 1:\n-                    self.skipTest(\"Offloaded static caches require exactly 1 GPU\")\n+                if backend_device_count(torch_device) != 1:\n+                    self.skipTest(\"Offloaded static caches require exactly 1 accelerator\")\n \n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_batched(self, cache_implementation):"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 52,
        "deletions": 30
    }
}