{
    "author": "MekkCyber",
    "message": "Add bnb to the list of supported quantization methods for LLama4 (#37348)\n\n* add bnb\n\n* style\n\n* update\n\n* add pre_quantized check",
    "sha": "f8301051833e927227a5597d4d3e2709057ca72a",
    "files": [
        {
            "sha": "08cd1e2e7d3466455e6eb4cc62a6cc203a89a171",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8301051833e927227a5597d4d3e2709057ca72a/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8301051833e927227a5597d4d3e2709057ca72a/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=f8301051833e927227a5597d4d3e2709057ca72a",
            "patch": "@@ -220,7 +220,9 @@ def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         \"\"\"\n         model.is_quantized = True\n         model.quantization_method = self.quantization_config.quant_method\n-        self._convert_model_for_quantization(model)\n+        print(\"self.pre_quantized\", self.pre_quantized)\n+        if self.pre_quantized:\n+            self._convert_model_for_quantization(model)\n         return self._process_model_before_weight_loading(model, **kwargs)\n \n     def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n@@ -303,13 +305,13 @@ def _convert_model_for_quantization(self, model):\n \n         for name, module in model.named_modules():\n             module_class_name = module.__class__.__name__\n-            if (\n-                module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys()\n-                and self.quantization_config.quant_method == QuantizationMethod.COMPRESSED_TENSORS\n+            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys() and (\n+                self.quantization_config.quant_method\n+                in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n             ):\n                 with init_empty_weights():\n                     parent_module, name = get_module_from_name(model, name)\n-                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name](\n+                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"module_name\"](\n                         model.config.get_text_config()\n                     )\n \n@@ -337,4 +339,9 @@ def forward(\n         return routed_out\n \n \n-MODULES_TO_PATCH_FOR_QUANTIZATION = {\"Llama4TextExperts\": SequentialLlama4TextExperts}\n+MODULES_TO_PATCH_FOR_QUANTIZATION = {\n+    \"Llama4TextExperts\": {\n+        \"module_name\": SequentialLlama4TextExperts,\n+        \"quantization_methods\": [QuantizationMethod.COMPRESSED_TENSORS, QuantizationMethod.BITS_AND_BYTES],\n+    }\n+}"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 13,
        "deletions": 6
    }
}