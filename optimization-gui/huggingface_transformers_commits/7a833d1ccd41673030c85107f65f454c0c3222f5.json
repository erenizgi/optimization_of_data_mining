{
    "author": "vasqu",
    "message": ":rotating_light: [`Clip`] Fix masking and enable flash attention on all model types (#41750)\n\n* fix\n\n* make kwargs fully passed and adjust with outputs xxx\n\n* propogate metaclip 2\n\n* propogate mlcd and fix test\n\n* style\n\n* fix repo consistency, need to add ignore rules as those are building blocks\n\n* style\n\n* oops\n\n* fix mlcd",
    "sha": "7a833d1ccd41673030c85107f65f454c0c3222f5",
    "files": [
        {
            "sha": "33a85df063c70d75336cabec2b62c763bfdd5aa4",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 42,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -310,7 +310,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -324,15 +323,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -344,13 +334,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -384,16 +373,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -497,7 +484,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -512,21 +498,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -563,17 +541,19 @@ def forward(\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -618,7 +598,6 @@ class CLIPTextModel(CLIPPreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n@@ -632,8 +611,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -726,7 +704,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -766,7 +743,6 @@ def forward(\n class CLIPModel(CLIPPreTrainedModel):\n     config: CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPConfig):\n         super().__init__(config)\n@@ -966,7 +942,6 @@ class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPTextConfig):\n@@ -986,8 +961,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1049,7 +1023,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1117,8 +1090,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "f352ce30e2bec0133bfa1ce629ff8123e8bd041d",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 52,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -12,7 +12,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -200,7 +200,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -214,15 +213,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # METACLIP_2 text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -234,13 +224,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -274,16 +263,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -387,7 +374,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -402,21 +388,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -437,36 +415,32 @@ def __init__(self, config: MetaClip2TextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -527,7 +501,6 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n@@ -541,16 +514,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -630,7 +600,6 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2TextConfig):\n@@ -650,16 +619,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2TextModelOutput:\n         r\"\"\"\n@@ -792,7 +758,6 @@ class MetaClip2Model(MetaClip2PreTrainedModel):\n \n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)\n@@ -1078,7 +1043,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1187,7 +1152,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1254,8 +1218,7 @@ def __init__(self, config: MetaClip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ae465d40a3aa02dbd062c1172bb22ef735c5be9e",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 59,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -3,19 +3,18 @@\n import torch\n from torch import nn\n \n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n-    CLIPEncoderLayer,\n     CLIPForImageClassification,\n     CLIPModel,\n+    CLIPPreTrainedModel,\n     CLIPTextEmbeddings,\n     CLIPTextModel,\n     CLIPTextModelWithProjection,\n@@ -214,24 +213,9 @@ class MetaClip2MLP(CLIPMLP):\n     pass\n \n \n-class MetaClip2EncoderLayer(CLIPEncoderLayer):\n-    pass\n-\n-\n @auto_docstring\n-class MetaClip2PreTrainedModel(PreTrainedModel):\n-    config: MetaClip2Config\n+class MetaClip2PreTrainedModel(CLIPPreTrainedModel):\n     base_model_prefix = \"metaclip_2\"\n-    input_modalities = [\"image\", \"text\"]\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-    _can_record_outputs = {\n-        \"hidden_states\": MetaClip2EncoderLayer,\n-        \"attentions\": MetaClip2Attention,\n-    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -291,36 +275,32 @@ def _init_weights(self, module):\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):\n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -372,22 +352,13 @@ class MetaClip2TextModel(CLIPTextModel):\n     >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-        self.text_model = MetaClip2TextTransformer(config)\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -409,8 +380,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -446,24 +415,13 @@ class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n     >>> text_embeds = outputs.text_embeds\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-\n-        text_model = MetaClip2TextModel._from_config(config)\n-        self.text_model = text_model.text_model\n-\n-        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -484,8 +442,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -550,6 +506,8 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -694,7 +652,7 @@ class MetaClip2VisionModel(CLIPVisionModel):\n     ```\"\"\"\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -764,6 +722,8 @@ class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n     >>> image_embeds = outputs.image_embeds\n     ```\"\"\"\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "fe7e8682b469d4ee172b1b5f8b84ae03ef4fc778",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 65,
            "deletions": 124,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -25,12 +25,12 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_mlcd import MLCDVisionConfig\n \n \n@@ -259,7 +259,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         batch_size, seq_length = hidden_states.shape[:-1]\n@@ -316,7 +316,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -328,18 +328,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -348,12 +345,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(nn.Module):\n@@ -377,9 +369,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -395,53 +385,68 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n+@auto_docstring\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    config: MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n class MLCDVisionTransformer(nn.Module):\n     def __init__(self, config: MLCDVisionConfig):\n         super().__init__()\n@@ -459,16 +464,8 @@ def __init__(self, config: MLCDVisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -486,66 +483,19 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs[0]\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n-@auto_docstring\n-class MLCDPreTrainedModel(PreTrainedModel):\n-    config: MLCDVisionConfig\n-    base_model_prefix = \"mlcd\"\n-    supports_gradient_checkpointing = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_factor\n-        if isinstance(module, MLCDVisionEmbeddings):\n-            factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-        elif isinstance(module, MLCDAttention):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n-        elif isinstance(module, MLCDMLP):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n-        elif isinstance(module, MLCDVisionTransformer):\n-            factor = self.config.initializer_factor\n-            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The vision model from M_L_C_D without any head or projection on top.\n@@ -566,13 +516,12 @@ def __init__(self, config: MLCDVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -596,17 +545,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
        },
        {
            "sha": "2be712febf2f66f868ea168beb1d20d09ff20a44",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 66,
            "deletions": 125,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -19,11 +19,11 @@\n import torch.nn as nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n@@ -206,7 +206,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_length = hidden_states.shape[:-1]\n \n@@ -258,7 +258,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -270,18 +270,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -290,12 +287,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(CLIPEncoder):\n@@ -316,9 +308,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -334,107 +324,18 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n-\n-\n-class MLCDVisionTransformer(CLIPVisionTransformer):\n-    def __init__(self, config: MLCDVisionConfig):\n-        super().__init__(config)\n-        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n-        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n-        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n-        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n-        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n-        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n-        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-        position_embeddings = (emb.cos(), emb.sin())\n-\n-        hidden_states = self.embeddings(pixel_values)\n-        hidden_states = self.pre_layrnorm(hidden_states)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -443,8 +344,15 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config: MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -478,14 +386,55 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n+class MLCDVisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n class MLCDVisionModel(CLIPVisionModel):\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -509,17 +458,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
        },
        {
            "sha": "c3c9109ed5e1bc7eab17bcaf733738b9d4e01561",
            "filename": "tests/models/mlcd/test_modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a833d1ccd41673030c85107f65f454c0c3222f5/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a833d1ccd41673030c85107f65f454c0c3222f5/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py?ref=7a833d1ccd41673030c85107f65f454c0c3222f5",
            "patch": "@@ -146,7 +146,7 @@ class MLCDVisionModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference(self):\n         model_name = \"DeepGlint-AI/mlcd-vit-bigG-patch14-448\"\n-        model = MLCDVisionModel.from_pretrained(model_name).to(torch_device)\n+        model = MLCDVisionModel.from_pretrained(model_name, attn_implementation=\"eager\").to(torch_device)\n         processor = AutoProcessor.from_pretrained(model_name)\n \n         # process single image"
        }
    ],
    "stats": {
        "total": 583,
        "additions": 180,
        "deletions": 403
    }
}