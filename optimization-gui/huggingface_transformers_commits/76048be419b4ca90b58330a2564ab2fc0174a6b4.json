{
    "author": "maximevtush",
    "message": "fix: typos in documentation files (#36122)\n\n* Update tools.py\r\n\r\n* Update text_generation.py\r\n\r\n* Update question_answering.py",
    "sha": "76048be419b4ca90b58330a2564ab2fc0174a6b4",
    "files": [
        {
            "sha": "fd1473a8c9687460b2575ccde45cb23ec6044ce9",
            "filename": "src/transformers/agents/tools.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=76048be419b4ca90b58330a2564ab2fc0174a6b4",
            "patch": "@@ -479,7 +479,7 @@ def __init__(\n                 if api_name is None:\n                     api_name = list(space_description.keys())[0]\n                     logger.warning(\n-                        f\"Since `api_name` was not defined, it was automatically set to the first avilable API: `{api_name}`.\"\n+                        f\"Since `api_name` was not defined, it was automatically set to the first available API: `{api_name}`.\"\n                     )\n                 self.api_name = api_name\n "
        },
        {
            "sha": "eee05b9f2c526aae4435bb331394fb92820b6d20",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=76048be419b4ca90b58330a2564ab2fc0174a6b4",
            "patch": "@@ -399,7 +399,7 @@ def __call__(self, *args, **kwargs):\n         return super().__call__(examples, **kwargs)\n \n     def preprocess(self, example, padding=\"do_not_pad\", doc_stride=None, max_question_len=64, max_seq_len=None):\n-        # XXX: This is specal, args_parser will not handle anything generator or dataset like\n+        # XXX: This is special, args_parser will not handle anything generator or dataset like\n         # For those we expect user to send a simple valid example either directly as a SquadExample or simple dict.\n         # So we still need a little sanitation here.\n         if isinstance(example, dict):"
        },
        {
            "sha": "0c81e4cc7cea56ad82665eb256abac269b134503",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76048be419b4ca90b58330a2564ab2fc0174a6b4/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=76048be419b4ca90b58330a2564ab2fc0174a6b4",
            "patch": "@@ -244,7 +244,7 @@ def __call__(self, text_inputs, **kwargs):\n                 Prefix added to prompt.\n             handle_long_generation (`str`, *optional*):\n                 By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n-                the model maximum length). There is no perfect way to adress this (more info\n+                the model maximum length). There is no perfect way to address this (more info\n                 :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n                 strategies to work around that problem depending on your use case.\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}