{
    "author": "molbap",
    "message": "Fix position embeddings singular/plural (#33678)\n\n* fix position embeddings\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo\r\n\r\n* fix init\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo\r\n\r\n* fix copies\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo\r\n\r\n* handle exception where list + tensors are cat'd\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo\r\n\r\n* add missing default\r\n\r\n* [run-slow] blip, blip_2, instructblip, instructblipvideo",
    "sha": "9f97c3938430190c15a0c1fdd6eec757a7224823",
    "files": [
        {
            "sha": "e7df057858865332b8780c867a363dad6c8d6e92",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=9f97c3938430190c15a0c1fdd6eec757a7224823",
            "patch": "@@ -233,7 +233,6 @@ def __init__(self, config: BlipVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n-    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n@@ -245,14 +244,14 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        num_positions = self.position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = self.position_embedding[:, :1]\n+        patch_pos_embed = self.position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "4b0ed4f71d9c95be2c8c6b04663b9622d40bab2c",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=9f97c3938430190c15a0c1fdd6eec757a7224823",
            "patch": "@@ -200,7 +200,6 @@ def __init__(self, config: Blip2VisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n-    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n@@ -212,14 +211,14 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        num_positions = self.position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = self.position_embedding[:, :1]\n+        patch_pos_embed = self.position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "de4e84b82f83773c4855661622ca918b2a6c6eff",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=9f97c3938430190c15a0c1fdd6eec757a7224823",
            "patch": "@@ -104,7 +104,6 @@ def __init__(self, config: InstructBlipVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n-    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n@@ -116,14 +115,14 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        num_positions = self.position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = self.position_embedding[:, :1]\n+        patch_pos_embed = self.position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "dc6c9deaf17781923d91a9df832e9a5eb7d277ff",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=9f97c3938430190c15a0c1fdd6eec757a7224823",
            "patch": "@@ -122,8 +122,10 @@ def __call__(\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-\n+            # we have to concatenate lists - so we keep track of return_tensors here\n+            return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+            output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors\n             # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n             # because BLIP expects image tokens to be at the beginning even before BOS token\n             if self.num_query_tokens is not None and images is not None:\n@@ -145,9 +147,7 @@ def __call__(\n                     )\n \n             # cast to desired return tensors type after concatenating\n-            text_encoding = BatchEncoding(\n-                text_encoding, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\")\n-            )\n+            text_encoding = BatchEncoding(text_encoding, tensor_type=return_tensors)\n \n             encoding.update(text_encoding)\n             qformer_text_encoding = self.qformer_tokenizer(text, **output_kwargs[\"text_kwargs\"])"
        },
        {
            "sha": "0808aa58b855fe889a2330a596a1ea4890408eeb",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f97c3938430190c15a0c1fdd6eec757a7224823/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=9f97c3938430190c15a0c1fdd6eec757a7224823",
            "patch": "@@ -111,7 +111,6 @@ def __init__(self, config: InstructBlipVideoVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n-    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n@@ -123,14 +122,14 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        num_positions = self.position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = self.position_embedding[:, :1]\n+        patch_pos_embed = self.position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        }
    ],
    "stats": {
        "total": 46,
        "additions": 21,
        "deletions": 25
    }
}