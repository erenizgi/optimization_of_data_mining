{
    "author": "guang-yng",
    "message": "fix(generation): stop beam search per-instance when heuristic satisfied (#38778)\n\n* fix(decoding): stop beam search per-instance when heuristic satisfied\n\nPreviously, when early_stopping is set to `False`, the early-stopping heuristic only halted generation when **all** batch instances reached the criterion. This caused instances that are impossible (suggested by the heuristic) to improve keep generating, leading to inconsistent and overlong outputs across the batch.\n\nNow we apply the heuristic **per-instance**: once a certain instance of batch has its all beams impossibe to improve, we mark that instance finished while letting others continue. This restores expected behavior and ensures consistency in batched generation.\n\n* Add test case GenerationIntegrationTests.test_beam_search_early_stop_heuristic\n\n* Update naming improvement_possibility -> is_early_stop_heuristic_unsatisfied\n\n* Add comments for early stop heuristic\n\n* Update src/transformers/generation/utils.py\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "356fd681098f4b33a6f95660a5a0252eae313348",
    "files": [
        {
            "sha": "c778e9e012f634cfd92b141c998161229a22b545",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 61,
            "deletions": 29,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/356fd681098f4b33a6f95660a5a0252eae313348/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/356fd681098f4b33a6f95660a5a0252eae313348/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=356fd681098f4b33a6f95660a5a0252eae313348",
            "patch": "@@ -3764,46 +3764,64 @@ def _gather_beams(tensor: torch.Tensor, beam_indices: torch.Tensor) -> torch.Ten\n         return gathered_tensor\n \n     @staticmethod\n-    def _beam_search_has_unfinished_sequences(\n+    def _check_early_stop_heuristic(\n+        is_early_stop_heuristic_unsatisfied: torch.Tensor,\n         running_beam_scores: torch.Tensor,\n         beam_scores: torch.Tensor,\n         is_sent_finished: torch.Tensor,\n-        next_token_hits_stopping_criteria: torch.Tensor,\n         cur_len: int,\n         max_length: int,\n         decoder_prompt_len: int,\n         early_stopping: Union[bool, str],\n         length_penalty: float,\n     ):\n         \"\"\"\n-        Beam Search stopping condition -- halts the generation loop if any of these conditions becomes False\n+        Determine whether early stopping is possible by checking if the best possible score of running beams\n+        could still improve upon the finished ones.\n+\n+        Mechanism:\n+        - Without a length penalty, beam scores typically decrease as more tokens are generated.\n+        So, if the *best possible* score from any running beam is already worse than the *worst* finished beam,\n+        we can safely stop early.\n+        - With a length penalty, scores may increase with longer sequences. In this case, we use heuristics\n+        to estimate the best possible score — though this estimate may not always be correct — and stop\n+        if no further improvement seems likely.\n+\n+        We apply different heuristics depending on the value of `early_stopping`:\n+        1. `early_stopping == False`:\n+        -> Use a heuristic that assumes the best score comes from the current length minus the decoder prompt length.\n+        -> See detailed discussion: https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n+\n+        2. `early_stopping == \"never\"`:\n+        -> Estimate the best score using either `max_length` or `cur_len`, depending on the sign of `length_penalty`.\n+        -> A positive length penalty favors longer sequences, so we use `max_length` in that case.\n+\n+        NOTE: the canonical beam search implementation can be replicated with `early_stopping=\"never\"` and\n+        `length_penalty=0.0`, which are NOT the default flags. The default behavior was empirically found to produce\n+        better sequences (prior to 2022), and changing it is BC breaking.\n         \"\"\"\n-        # a. Can the open beams improve the top completed scores?\n-        # early_stopping == False -> apply heuristic = always get the best score from `cur_len - decoder_prompt_len`.\n-        # early_stopping == \"never\" -> compute the best score from `max_length` or `cur_len`, depending on the\n-        #   sign of `length_penalty`. Positive `length_penalty` favors longer sequences, thus we use\n-        #   `max_length` there.\n-        # !!\n-        # Be sure to check the docstring for `early_stopping` and `length_penalty`. The default parameterization\n-        # does NOT correspond to a canonical beam search implementation, and tends to favor shorter output sequences\n-        # compared to it (the heuristic active by default underestimates the maximum achievable score, and thus cut\n-        # generation short). Also, be mindful that length penalty > 0.0 actually favors longer sequences, despite\n-        # its name. These modifications were empirically found in the past (prior to 2022) to produce better quality\n-        # generations, and changing them is BC breaking.\n-        # For a canonical beam search implementation, set `early_stopping=\"never\"` and `length_penalty=0.0`.\n-        # See the discussion below for more details.\n-        # https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n-        # !!\n         if early_stopping == \"never\" and length_penalty > 0.0:\n             best_hypothetical_length = max_length - decoder_prompt_len\n         else:\n             best_hypothetical_length = cur_len - decoder_prompt_len\n-\n-        # best-case scenario: the next tokens have logprobs=0 (probability=1), and the score stays the same before\n-        # applying length penalty\n         best_possible_running_score = running_beam_scores[:, :1] / (best_hypothetical_length**length_penalty)\n         worst_finished_score = torch.where(is_sent_finished, torch.min(beam_scores, dim=1, keepdim=True)[0], -1.0e9)\n-        improvement_possible = torch.any(best_possible_running_score > worst_finished_score)\n+        return is_early_stop_heuristic_unsatisfied & torch.any(\n+            best_possible_running_score > worst_finished_score, dim=-1, keepdim=True\n+        )\n+\n+    @staticmethod\n+    def _beam_search_has_unfinished_sequences(\n+        is_early_stop_heuristic_unsatisfied: torch.Tensor,\n+        is_sent_finished: torch.Tensor,\n+        next_token_hits_stopping_criteria: torch.Tensor,\n+        early_stopping: Union[bool, str],\n+    ):\n+        \"\"\"\n+        Beam Search stopping condition -- halts the generation loop if any of these conditions becomes False\n+        \"\"\"\n+        # a. Can the open beams improve the top completed scores?\n+        improvement_possible = torch.any(is_early_stop_heuristic_unsatisfied)\n \n         # b. Is there still a beam without fully completed sequences? This is only relevant if early_stopping is\n         # enabled, where we want to finish as soon as all beams have a completed sequence.\n@@ -3899,6 +3917,7 @@ def _update_finished_beams(\n         topk_log_probs: torch.Tensor,\n         beam_indices: torch.Tensor,\n         topk_running_beam_indices: torch.Tensor,\n+        is_early_stop_heuristic_unsatisfied: torch.Tensor,\n         is_sent_finished: torch.Tensor,\n         next_token_hits_stopping_criteria: torch.Tensor,\n         top_num_beam_mask: torch.Tensor,\n@@ -3923,6 +3942,9 @@ def _update_finished_beams(\n         # - make sure no scores can be added anymore if beam is full and early stopping is on\n         beams_in_batch_are_full = torch.all(is_sent_finished, axis=-1, keepdims=True) & (early_stopping is True)\n         topk_log_probs += beams_in_batch_are_full.to(torch.float32) * -1.0e9\n+        # - make sure no scores can be added anymore if improvement is not possible\n+        topk_log_probs += (~is_early_stop_heuristic_unsatisfied).to(torch.float32) * -1.0e9\n+\n         # - make sure still running sequences cannot be chosen as finalized beam\n         topk_log_probs += (~did_top_num_beams_just_finished) * -1.0e9\n \n@@ -4074,6 +4096,9 @@ def _beam_search(\n         # per batch, beam-item state bit indicating if sentence has finished.\n         is_sent_finished = torch.zeros((batch_size, num_beams), dtype=torch.bool, device=input_ids.device)\n \n+        # per batch state bit indicating if there is a possibility to improve the best finished sentence.\n+        is_early_stop_heuristic_unsatisfied = torch.ones((batch_size, 1), dtype=torch.bool, device=input_ids.device)\n+\n         # per batch, beam-item state bit indicating if there are valid continuations.\n         next_token_hits_stopping_criteria = torch.zeros(\n             (batch_size, num_beams), dtype=torch.bool, device=input_ids.device\n@@ -4186,6 +4211,7 @@ def _beam_search(\n                 topk_log_probs=topk_log_probs,\n                 beam_indices=beam_indices,\n                 topk_running_beam_indices=topk_running_beam_indices,\n+                is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n                 is_sent_finished=is_sent_finished,\n                 next_token_hits_stopping_criteria=next_token_hits_stopping_criteria,\n                 top_num_beam_mask=top_num_beam_mask,\n@@ -4207,16 +4233,22 @@ def _beam_search(\n                 )\n \n             cur_len = cur_len + 1\n+            is_early_stop_heuristic_unsatisfied = self._check_early_stop_heuristic(\n+                is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n+                running_beam_scores=running_beam_scores,\n+                beam_scores=beam_scores,\n+                is_sent_finished=is_sent_finished,\n+                cur_len=cur_len,\n+                max_length=max_length,\n+                decoder_prompt_len=decoder_prompt_len,\n+                early_stopping=early_stopping,\n+                length_penalty=length_penalty,\n+            )\n             this_peer_finished = not self._beam_search_has_unfinished_sequences(\n-                running_beam_scores,\n-                beam_scores,\n+                is_early_stop_heuristic_unsatisfied,\n                 is_sent_finished,\n                 next_token_hits_stopping_criteria,\n-                cur_len,\n-                max_length,\n-                decoder_prompt_len,\n                 early_stopping,\n-                length_penalty,\n             )\n \n         # 5. prepare outputs"
        },
        {
            "sha": "9edb1fe99f29eb33119de6989d8f25881a100e4f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/356fd681098f4b33a6f95660a5a0252eae313348/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/356fd681098f4b33a6f95660a5a0252eae313348/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=356fd681098f4b33a6f95660a5a0252eae313348",
            "patch": "@@ -2887,6 +2887,53 @@ def test_diverse_beam_search(self):\n             ],\n         )\n \n+    @slow\n+    def test_beam_search_early_stop_heuristic(self):\n+        \"\"\"Regression test for #38778 (early stopping needs to be tracked at a batch level)\"\"\"\n+        EXPECTED_OUTPUT = (\n+            \"<|user|>\\nWhat is 3+5?\\n<|assistant|>\\nThe sum of 3 and 5 is 8. \\n\\nSo, 3 + 5 = 8. \\n\\n\"\n+            \"Let's confirm this using Python code:\\n\\n```python\\n# Define the numbers\\nnum1 = 3\\nnum2 = 5\\n\\n\"\n+            \"# Calculate the sum\\nresult = num1 + num2\\n\\n# Print the result\\nprint(result)\\n```\\n\"\n+            \"```output\\n8\\n```\\nThe sum of 3 and 5 is \\\\(\\\\boxed{8}\\\\).\"\n+        )\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B-Instruct\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-0425-1B-Instruct\", padding_side=\"left\")\n+        generation_config = GenerationConfig(\n+            num_beams=10,\n+            max_new_tokens=256,\n+            length_penalty=2,\n+        )\n+        # batch of 1\n+        question = [{\"role\": \"user\", \"content\": \"What is 3+5?\"}]\n+        question = tokenizer.apply_chat_template(\n+            question, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n+        )\n+        inputs = tokenizer(question, return_tensors=\"pt\", padding=True).to(\"cuda\")\n+        outputs = model.generate(**inputs, generation_config=generation_config)\n+        responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+        self.assertEqual(responses[0], EXPECTED_OUTPUT)\n+\n+        # batch of 2\n+        question = [{\"role\": \"user\", \"content\": \"What is 3+5?\"}]\n+        cot_question = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"What is 3+5? Explain your reasoning step by step, and provide the final answer at the end.\",\n+            }\n+        ]\n+        question = tokenizer.apply_chat_template(\n+            question, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n+        )\n+        cot_question = tokenizer.apply_chat_template(\n+            cot_question, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"\n+        )\n+        inputs = tokenizer([question, cot_question], return_tensors=\"pt\", padding=True).to(\"cuda\")\n+\n+        outputs = model.generate(**inputs, generation_config=generation_config)\n+        responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+        self.assertEqual(responses[0], EXPECTED_OUTPUT)\n+\n     def test_max_length_if_input_embeds(self):\n         article = \"Today a dragon flew over Paris.\"\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)"
        },
        {
            "sha": "0c3c90768deb96cb1fb89642706c92b9605d3fac",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/356fd681098f4b33a6f95660a5a0252eae313348/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/356fd681098f4b33a6f95660a5a0252eae313348/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=356fd681098f4b33a6f95660a5a0252eae313348",
            "patch": "@@ -840,7 +840,7 @@ def test_hybrid_cache_exportability(self):\n         input_ids = torch.zeros((1, 3), dtype=torch.long)\n         cache_position = torch.tensor([0, 1, 2], dtype=torch.long)\n         dynamic_shapes = {\"input_ids\": {1: torch.export.Dim.DYNAMIC}, \"cache_position\": {0: torch.export.Dim.DYNAMIC}}\n-        strict = version.parse(torch.__version__) != version.parse(\"2.7.0\")\n+        strict = version.parse(torch.__version__) < version.parse(\"2.7.0\")\n         exported_program = exportable_module.export(\n             input_ids=input_ids,\n             cache_position=cache_position,"
        }
    ],
    "stats": {
        "total": 139,
        "additions": 109,
        "deletions": 30
    }
}