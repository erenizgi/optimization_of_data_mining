{
    "author": "zucchini-nlp",
    "message": "Audio Flamingo3 - fix attention masking (#42278)\n\n* fix attention masking\n\n* use `input_features_mask` for paddinf",
    "sha": "6db2dd319c4b063836784bb4e75f664771956e6b",
    "files": [
        {
            "sha": "50e10b702532d1412277fe025c3bb4eee02b1d9d",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6db2dd319c4b063836784bb4e75f664771956e6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6db2dd319c4b063836784bb4e75f664771956e6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=6db2dd319c4b063836784bb4e75f664771956e6b",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import eager_mask, padding_mask_function\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n@@ -336,20 +336,10 @@ def forward(\n                 - 0 for tokens that are **masked**.\n         \"\"\"\n \n-        # Prepare attention mask for transformer layers\n-        batch_size = input_features.shape[0]\n         seq_len = (input_features.shape[-1] - 1) // 2 + 1  # After conv2 downsampling\n-\n         input_features_lengths = input_features_mask.sum(-1)\n         input_features_lengths = (input_features_lengths - 1) // 2 + 1  # conv2 downsampling\n         input_features_mask = torch.arange(seq_len, device=input_features.device) < input_features_lengths[:, None]\n-        attention_mask = eager_mask(\n-            batch_size=batch_size,\n-            cache_position=torch.arange(seq_len, device=input_features.device),\n-            kv_length=seq_len,\n-            mask_function=padding_mask_function(input_features_mask),\n-            dtype=self.conv1.weight.dtype,\n-        )\n \n         # Conv front-end\n         inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n@@ -360,6 +350,12 @@ def forward(\n         hidden_states = inputs_embeds + self.embed_positions.weight\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=input_features_mask,\n+        )\n+\n         # Transformer stack\n         for layer in self.layers:\n             drop = self.training and torch.rand([]) < self.layerdrop"
        },
        {
            "sha": "f8535ebbe4397bd583d22f52d2b0fe56e9591dab",
            "filename": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6db2dd319c4b063836784bb4e75f664771956e6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6db2dd319c4b063836784bb4e75f664771956e6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py?ref=6db2dd319c4b063836784bb4e75f664771956e6b",
            "patch": "@@ -21,7 +21,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...masking_utils import eager_mask, padding_mask_function\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -73,20 +73,10 @@ def forward(\n                 - 0 for tokens that are **masked**.\n         \"\"\"\n \n-        # Prepare attention mask for transformer layers\n-        batch_size = input_features.shape[0]\n         seq_len = (input_features.shape[-1] - 1) // 2 + 1  # After conv2 downsampling\n-\n         input_features_lengths = input_features_mask.sum(-1)\n         input_features_lengths = (input_features_lengths - 1) // 2 + 1  # conv2 downsampling\n         input_features_mask = torch.arange(seq_len, device=input_features.device) < input_features_lengths[:, None]\n-        attention_mask = eager_mask(\n-            batch_size=batch_size,\n-            cache_position=torch.arange(seq_len, device=input_features.device),\n-            kv_length=seq_len,\n-            mask_function=padding_mask_function(input_features_mask),\n-            dtype=self.conv1.weight.dtype,\n-        )\n \n         # Conv front-end\n         inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n@@ -97,6 +87,12 @@ def forward(\n         hidden_states = inputs_embeds + self.embed_positions.weight\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=input_features_mask,\n+        )\n+\n         # Transformer stack\n         for layer in self.layers:\n             drop = self.training and torch.rand([]) < self.layerdrop"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 14,
        "deletions": 22
    }
}