{
    "author": "IlyasMoutawwakil",
    "message": "Fixes in check_model_inputs, GPTBigCodeModel and ImageGPTModel (#40811)\n\n* misc fixes\n\n* fix\n\n* Update src/transformers/models/imagegpt/modeling_imagegpt.py\n\n* Apply suggestion from @IlyasMoutawwakil\n\n* pickup use_cache from args input as well\n\n* fix",
    "sha": "7a1aeec36e3ad667d3ff864778c33d14c483d590",
    "files": [
        {
            "sha": "96201cb88606a4ce5566077139b55fbcf2412499",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=7a1aeec36e3ad667d3ff864778c33d14c483d590",
            "patch": "@@ -472,14 +472,7 @@ def forward(\n             raise ValueError(\"batch_size has to be defined and > 0\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)"
        },
        {
            "sha": "092be32b7175f452c9a743e451aefb8344326bfb",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=7a1aeec36e3ad667d3ff864778c33d14c483d590",
            "patch": "@@ -517,24 +517,20 @@ def forward(\n                 )\n                 use_cache = False\n \n-        if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n-        past_length = past_key_values.get_seq_length() if past_key_values is not None else past_key_values\n-\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + input_shape[-1], device=device\n+            )\n+\n         if position_ids is None:\n-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n-            position_ids = position_ids.unsqueeze(0)\n+            position_ids = cache_position.unsqueeze(0)\n \n         # ImageGPTAttention mask.\n         if attention_mask is not None:"
        },
        {
            "sha": "5bdcc929dcddd2a35f42593e0103829d122d35a2",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 23,
            "deletions": 5,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a1aeec36e3ad667d3ff864778c33d14c483d590/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=7a1aeec36e3ad667d3ff864778c33d14c483d590",
            "patch": "@@ -797,17 +797,34 @@ def check_model_inputs(tie_last_hidden_states=True):\n     def wrapped_fn(func):\n         @wraps(func)\n         def wrapper(self, *args, **kwargs):\n-            use_cache = (\n-                kwargs[\"use_cache\"] if kwargs.get(\"use_cache\") is not None else getattr(self.config, \"use_cache\", None)\n-            )\n+            use_cache_arg_index = None\n+            if \"use_cache\" in func.__code__.co_varnames:\n+                use_cache_arg_index = func.__code__.co_varnames.index(\"use_cache\") - 1  # -1 for self\n+\n+            if (\n+                use_cache_arg_index is not None\n+                and len(args) > use_cache_arg_index\n+                and args[use_cache_arg_index] is not None\n+            ):\n+                use_cache = args[use_cache_arg_index]\n+            elif kwargs.get(\"use_cache\") is not None:\n+                use_cache = kwargs[\"use_cache\"]\n+            else:\n+                use_cache = getattr(self.config, \"use_cache\", None)\n+\n             if use_cache is not None:\n                 if getattr(self, \"gradient_checkpointing\", False) and self.training and use_cache:\n                     logger.warning_once(\n                         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n                     )\n                     use_cache = False\n \n-                kwargs[\"use_cache\"] = use_cache\n+                if use_cache_arg_index is not None and len(args) > use_cache_arg_index:\n+                    args = list(args)\n+                    args[use_cache_arg_index] = use_cache\n+                    args = tuple(args)\n+                else:\n+                    kwargs[\"use_cache\"] = use_cache\n \n             return_dict = kwargs.pop(\"return_dict\", None)\n             if return_dict is None:\n@@ -818,7 +835,8 @@ def wrapper(self, *args, **kwargs):\n                 for k, v in all_args[\"kwargs\"].items():\n                     all_args[k] = v\n \n-            capture_flags = _CAN_RECORD_REGISTRY.get(str(self.__class__), {})  # there is a weak ref for executorch\n+            # _can_record_outputs is None by default\n+            capture_flags = _CAN_RECORD_REGISTRY.get(str(self.__class__)) or {}  # there is a weak ref for executorch\n             recordable_keys = {\n                 f\"output_{k}\": all_args.get(\n                     f\"output_{k}\","
        }
    ],
    "stats": {
        "total": 61,
        "additions": 34,
        "deletions": 27
    }
}