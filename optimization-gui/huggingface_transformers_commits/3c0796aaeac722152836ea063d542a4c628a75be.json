{
    "author": "ariG23498",
    "message": "[Fast Processor] BEiT (#37005)\n\n* adding fast processor for beit\n\n* adding resample\n\n* address review issues and add segmentation maps logic\n\n* style\n\n* chore: adding tests\n\n* reduce label test\n\n* adding batched tests\n\n* Update src/transformers/models/beit/image_processing_beit_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* fix imports and make segmentation masks\n\n* fix tests\n\n* build segmentation maps\n\n* all tests pass\n\n* style\n\n* style fix\n\n* style\n\n* chore: delete demo.py file\n\n* review suggestions\n\n* Update docs/source/en/model_doc/beit.md\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "3c0796aaeac722152836ea063d542a4c628a75be",
    "files": [
        {
            "sha": "e40fbdc9c8892100423c64b765d4bd710423af27",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -150,6 +150,11 @@ If you're interested in submitting a resource to be included here, please feel f\n [[autodoc]] BeitImageProcessor\n     - preprocess\n     - post_process_semantic_segmentation\n+## BeitImageProcessorFast\n+\n+[[autodoc]] BeitImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n \n <frameworkcontent>\n <pt>"
        },
        {
            "sha": "e0b94693a36fbab0ed4842b27c114bc72d700064",
            "filename": "docs/source/ja/model_doc/beit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -105,6 +105,11 @@ BEiT の使用を開始するのに役立つ公式 Hugging Face およびコミ\n \n [[autodoc]] BeitImageProcessor\n     - preprocess\n+\n+## BeitImageProcessorFast\n+\n+[[autodoc]] BeitImageProcessorFast\n+    - preprocess\n     - post_process_semantic_segmentation\n \n ## BeitModel"
        },
        {
            "sha": "8a57682de7b12421619ecf3e2f91a650f4489244",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -57,8 +57,8 @@\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n             (\"align\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n-            (\"aria\", (\"AriaImageProcessor\",)),\n-            (\"beit\", (\"BeitImageProcessor\",)),\n+            (\"aria\", (\"AriaImageProcessor\")),\n+            (\"beit\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n             (\"bit\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"blip-2\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n@@ -71,7 +71,7 @@\n             (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"cvt\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n-            (\"data2vec-vision\", (\"BeitImageProcessor\",)),\n+            (\"data2vec-vision\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n             (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\",)),"
        },
        {
            "sha": "3f412a3500683fa9ac5ebac12c860ae91ab4422b",
            "filename": "src/transformers/models/beit/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_beit import *\n     from .feature_extraction_beit import *\n     from .image_processing_beit import *\n+    from .image_processing_beit_fast import *\n     from .modeling_beit import *\n     from .modeling_flax_beit import *\n else:"
        },
        {
            "sha": "50dd6fe7f5bf9aff2b4f900d2c8c0852593fde5b",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "added",
            "additions": 284,
            "deletions": 0,
            "changes": 284,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -0,0 +1,284 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Beit.\"\"\"\n+\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from torchvision.transforms import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    is_torch_tensor,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, add_start_docstrings\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+class BeitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    do_reduce_labels: Optional[bool]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Beit image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\",\n+)\n+class BeitImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 224, \"width\": 224}\n+    default_to_square = True\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    do_normalize = True\n+    do_reduce_labels = False\n+    valid_kwargs = BeitFastImageProcessorKwargs\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"reduce_labels\" in image_processor_dict:\n+            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+        return label\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return processed_images\n+\n+    def _preprocess_segmentation_maps(\n+        self,\n+        segmentation_maps,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses a single segmentation map.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_normalize\"] = False\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n+        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+        return processed_segmentation_maps\n+\n+    def __call__(self, images, segmentation_maps=None, **kwargs):\n+        # Overrides the `__call__` method of the `Preprocessor` class such that the images and segmentation maps can both\n+        # be passed in as positional arguments.\n+        return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n+\n+    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n+    @add_start_docstrings(\n+        \"Constructs a fast Beit image processor.\",\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+        \"\"\"\n+        do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+            is used for background, and background itself is not included in all classes of a dataset (e.g.\n+            ADE20k). The background label will be replaced by 255.\n+        \"\"\",\n+    )\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        images = self._preprocess(\n+            images=images,\n+            **kwargs,\n+        )\n+        data = {\"pixel_values\": images}\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_segmentation_maps(\n+                segmentation_maps=segmentation_maps,\n+                **kwargs,\n+            )\n+            data[\"labels\"] = segmentation_maps\n+\n+        return BatchFeature(data=data)\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+        \"\"\"\n+        Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`BeitForSemanticSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+\n+        Returns:\n+            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        # TODO: add support for other frameworks\n+        logits = outputs.logits\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            if is_torch_tensor(target_sizes):\n+                target_sizes = target_sizes.numpy()\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(logits)):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = logits.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+\n+__all__ = [\"BeitImageProcessorFast\"]"
        },
        {
            "sha": "5a46279a6829449ef7c7aafd5ce23cc323955cbf",
            "filename": "tests/models/beit/test_image_processing_beit.py",
            "status": "modified",
            "additions": 192,
            "deletions": 151,
            "changes": 343,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c0796aaeac722152836ea063d542a4c628a75be/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c0796aaeac722152836ea063d542a4c628a75be/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py?ref=3c0796aaeac722152836ea063d542a4c628a75be",
            "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import BeitImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import BeitImageProcessorFast\n+\n \n class BeitImageProcessingTester:\n     def __init__(\n@@ -118,6 +121,7 @@ def prepare_semantic_batch_inputs():\n @require_vision\n class BeitImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = BeitImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = BeitImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -128,159 +132,196 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n-        self.assertEqual(image_processor.do_reduce_labels, False)\n-\n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, crop_size=84, do_reduce_labels=True\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+            self.assertEqual(image_processor.do_reduce_labels, False)\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, crop_size=84, do_reduce_labels=True\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n \n     def test_call_segmentation_maps(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        maps = []\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-            maps.append(torch.zeros(image.shape[-2:]).long())\n-\n-        # Test not batched input\n-        encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched\n-        encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test not batched input (PIL images)\n-        image, segmentation_map = prepare_semantic_single_inputs()\n-\n-        encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched input (PIL images)\n-        images, segmentation_maps = prepare_semantic_batch_inputs()\n-\n-        encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n     def test_reduce_labels(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-\n-        # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n-        image, map = prepare_semantic_single_inputs()\n-        encoding = image_processing(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n-\n-        image_processing.do_reduce_labels = True\n-        encoding = image_processing(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-    def test_removed_deprecated_kwargs(self):\n-        image_processor_dict = dict(self.image_processor_dict)\n-        image_processor_dict.pop(\"do_reduce_labels\", None)\n-        image_processor_dict[\"reduce_labels\"] = True\n-\n-        # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = self.image_processing_class(**image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n-        # test we still support reduce_labels with config\n-        image_processor = self.image_processing_class.from_dict(image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+            image, map = prepare_semantic_single_inputs()\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n+\n+            image_processing.do_reduce_labels = True\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(image_encoding_slow.pixel_values - image_encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+        self.assertTrue(torch.allclose(image_encoding_slow.labels, image_encoding_fast.labels, atol=1e-1))\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )"
        }
    ],
    "stats": {
        "total": 644,
        "additions": 490,
        "deletions": 154
    }
}