{
    "author": "manuelsh",
    "message": "adding positional encoder changes and tests (#32600)\n\n* adding positional encoder changes and tests\r\n\r\n* adding ruff suggestions\r\n\r\n* changes added by python utils/check_copies.py --fix_and_overwrite\r\n\r\n* removing pos_encoding added by script\r\n\r\n* adding interpolation to clipseg\r\n\r\n* formatting\r\n\r\n* adding further testing to altclip and better documentation to kosmos2\r\n\r\n* skipping test_inputs_embeds_matches_input_ids_with_generate in git model\r\n\r\n* fixing clipseg comment suggestions\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* fixing bridgetower test\r\n\r\n* fixing altclip tensor output POS test\r\n\r\n* adding ruff formatting\r\n\r\n* fixing several tests\r\n\r\n* formatting with ruff\r\n\r\n* adding positional encoder changes and tests\r\n\r\n* adding ruff suggestions\r\n\r\n* changes added by python utils/check_copies.py --fix_and_overwrite\r\n\r\n* removing pos_encoding added by script\r\n\r\n* adding interpolation to clipseg\r\n\r\n* formatting\r\n\r\n* adding further testing to altclip and better documentation to kosmos2\r\n\r\n* skipping test_inputs_embeds_matches_input_ids_with_generate in git model\r\n\r\n* fixing clipseg comment suggestions\r\n\r\n* fixing bridgetower test\r\n\r\n* fixing altclip tensor output POS test\r\n\r\n* adding ruff formatting\r\n\r\n* fixing several tests\r\n\r\n* formatting with ruff\r\n\r\n* adding right pretrained model\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* fixing test_inference_image_segmentation\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* fixing test_inference_interpolate_pos_encoding for the git model as there is no vision_model_output\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* adding ruff formatting\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* adding new interpolate_pos_encoding function\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* fixing interpolate_POS funciton\r\n\r\n* adapting output tensor in teests\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* modifying output tensor\r\n\r\n* [run_slow] altclip, bridgetower, chinese_clip, clip, clipseg, git, kosmos2, x_clip\r\n\r\n* adding the correct tensor\r\n\r\n* [run_slow]  clipseg\r\n\r\n* fixing spaces\r\n\r\n* [run_slow]  clipseg\r\n\r\n* [run_slow]  clipseg\r\n\r\n---------\r\n\r\nCo-authored-by: Manuel Sanchez Hernandez <manuel.sanchez.hernandez@schibsted.com>",
    "sha": "a55adee890b0757461e1588d8847fd385ce5c214",
    "files": [
        {
            "sha": "84e368379e292a525d20f569ca6242e10906e659",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 64,
            "deletions": 5,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -32,7 +32,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import ModelOutput, add_start_docstrings_to_model_forward, logging, replace_return_docstrings, torch_int\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n \n@@ -100,6 +100,8 @@\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -137,6 +139,8 @@\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -1009,15 +1013,63 @@ def __init__(self, config: AltCLIPVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -1097,6 +1149,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n@@ -1111,7 +1164,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1156,6 +1209,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1186,6 +1240,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1546,6 +1601,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1578,6 +1634,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1598,6 +1655,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, AltCLIPOutput]:\n         r\"\"\"\n@@ -1642,6 +1700,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n "
        },
        {
            "sha": "c7b32de0d08e1719776d912b5d7caa09200e7207",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 79,
            "deletions": 11,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -34,7 +34,13 @@\n )\n from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n@@ -111,6 +117,8 @@\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -276,15 +284,63 @@ def __init__(self, config: BridgeTowerVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -302,8 +358,13 @@ def __init__(self, config):\n                 [nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) for _ in range(config.num_hidden_layers)]\n             )\n \n-    def forward(self, pixel_values: torch.Tensor, attention_mask):\n-        hidden_states = self.embeddings(pixel_values)\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        attention_mask,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding)\n         hidden_states = self.ln_pre(hidden_states)\n         # NLD -> LND\n         hidden_states = hidden_states.permute(1, 0, 2)\n@@ -324,8 +385,12 @@ def forward(self, pixel_values: torch.Tensor, attention_mask):\n             hidden_states = torch.stack(hidden_states_stack, dim=0)\n         return hidden_states\n \n-    def forward_pre(self, pixel_values: torch.Tensor):\n-        hidden_states = self.embeddings(pixel_values)\n+    def forward_pre(\n+        self,\n+        pixel_values: torch.Tensor,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.ln_pre(hidden_states)\n         # NLD -> LND\n         hidden_states = hidden_states.permute(1, 0, 2)\n@@ -1015,8 +1080,8 @@ def __init__(self, config):\n     def dtype(self):\n         return self.visual.embeddings.patch_embedding.weight.dtype\n \n-    def forward(self, image, image_mask=None):\n-        return self.visual(image.type(self.dtype), image_mask)\n+    def forward(self, image, image_mask=None, interpolate_pos_encoding=False):\n+        return self.visual(image.type(self.dtype), image_mask, interpolate_pos_encoding)\n \n \n class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n@@ -1280,6 +1345,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple[torch.Tensor], BridgeTowerModelOutput]:\n         r\"\"\"\n         output_hidden_states (`bool`, *optional*):\n@@ -1352,7 +1418,9 @@ def forward(\n                 all_hidden_states_text += (text_embeds,)\n \n         if image_embeds is None:\n-            image_embeds = self.vision_model.visual.forward_pre(pixel_values.type(self.vision_model.dtype))\n+            image_embeds = self.vision_model.visual.forward_pre(\n+                pixel_values.type(self.vision_model.dtype), interpolate_pos_encoding=interpolate_pos_encoding\n+            )\n         else:\n             # Permute as BridgeTowerResidualAttention has batch_first=True\n             image_embeds = image_embeds.permute(1, 0, 2)"
        },
        {
            "sha": "393d5784bb440a4835c9a5f19c1b1fb16261d569",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 64,
            "deletions": 4,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_chinese_clip import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig\n \n@@ -188,15 +189,63 @@ def __init__(self, config: ChineseCLIPVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -798,6 +847,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -813,6 +864,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -1052,6 +1105,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1066,7 +1120,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1299,6 +1353,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1329,6 +1384,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1425,6 +1481,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1461,6 +1518,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1481,6 +1539,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ChineseCLIPOutput]:\n         r\"\"\"\n@@ -1516,6 +1575,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n "
        },
        {
            "sha": "370f17f479650a8db0efa72d0eaf82262cd0062f",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 66,
            "deletions": 4,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -36,6 +36,7 @@\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n@@ -196,15 +197,63 @@ def __init__(self, config: CLIPVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -704,6 +753,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -741,6 +792,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -1023,6 +1076,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n@@ -1037,7 +1091,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1087,6 +1141,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1118,6 +1173,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n \n@@ -1214,6 +1270,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1249,6 +1306,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1268,6 +1326,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPOutput]:\n         r\"\"\"\n@@ -1305,6 +1364,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1466,6 +1526,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPVisionModelOutput]:\n         r\"\"\"\n@@ -1495,6 +1556,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n "
        },
        {
            "sha": "90520524fa8843577dfbd80de68cdf1f6f6e9449",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 60,
            "deletions": 24,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -33,6 +33,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_clipseg import CLIPSegConfig, CLIPSegTextConfig, CLIPSegVisionConfig\n \n@@ -163,40 +164,62 @@ def __init__(self, config: CLIPSegVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def interpolate_position_embeddings(self, new_size):\n-        if len(new_size) != 2:\n-            raise ValueError(\"new_size should consist of 2 values\")\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        num_patches_one_direction = int(self.num_patches**0.5)\n-        # we interpolate the position embeddings in 2D\n-        a = self.position_embedding.weight[1:].T.view(\n-            1, self.config.hidden_size, num_patches_one_direction, num_patches_one_direction\n-        )\n-        b = (\n-            nn.functional.interpolate(a, new_size, mode=\"bicubic\", align_corners=False)\n-            .squeeze(0)\n-            .view(self.config.hidden_size, new_size[0] * new_size[1])\n-            .T\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n         )\n-        result = torch.cat([self.position_embedding.weight[:1], b])\n \n-        return result\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-\n-        if embeddings.shape[1] != self.num_positions:\n-            new_shape = int(math.sqrt(embeddings.shape[1] - 1))\n-            embeddings = embeddings + self.interpolate_position_embeddings((new_shape, new_shape))\n-            embeddings = embeddings.to(embeddings.dtype)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n         else:\n             embeddings = embeddings + self.position_embedding(self.position_ids)\n-\n         return embeddings\n \n \n@@ -512,6 +535,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -549,6 +574,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -825,6 +852,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n@@ -839,7 +867,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -884,6 +912,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -912,6 +941,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1005,6 +1035,7 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1040,6 +1071,7 @@ def get_image_features(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1059,6 +1091,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPSegOutput]:\n         r\"\"\"\n@@ -1096,6 +1129,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1363,6 +1397,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPSegOutput]:\n         r\"\"\"\n@@ -1402,6 +1437,7 @@ def forward(\n                 pixel_values=pixel_values,\n                 output_attentions=output_attentions,\n                 output_hidden_states=True,  # we need the intermediate hidden states\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n                 return_dict=return_dict,\n             )\n             pooled_output = self.clip.visual_projection(vision_outputs[1])"
        },
        {
            "sha": "cf8edfe474880f02a3c2400e50dcd1c6d7da2644",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 75,
            "deletions": 7,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -37,7 +37,13 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n from .configuration_git import GitConfig, GitVisionConfig\n \n \n@@ -602,6 +608,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -631,15 +639,63 @@ def __init__(self, config: GitVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -924,6 +980,8 @@ def forward(\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -948,6 +1006,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutput]:\n         r\"\"\"\n@@ -963,7 +1022,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1012,6 +1071,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutput]:\n         r\"\"\"\n@@ -1041,6 +1101,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1167,6 +1228,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1235,13 +1297,17 @@ def forward(\n         if pixel_values is not None:\n             if pixel_values.ndim == 4:\n                 # here we assume pixel_values is of shape (batch_size, num_channels, height, width)\n-                visual_features = self.image_encoder(pixel_values).last_hidden_state\n+                visual_features = self.image_encoder(\n+                    pixel_values, interpolate_pos_encoding=interpolate_pos_encoding\n+                ).last_hidden_state\n \n             elif pixel_values.ndim == 5:\n                 # here we assume pixel_values is of shape (batch_size, num_frames, num_channels, height, width)\n                 visual_features = []\n                 for frame_idx in range(pixel_values.shape[1]):\n-                    visual_features_frame = self.image_encoder(pixel_values[:, frame_idx, :, :]).last_hidden_state\n+                    visual_features_frame = self.image_encoder(\n+                        pixel_values[:, frame_idx, :, :], interpolate_pos_encoding=interpolate_pos_encoding\n+                    ).last_hidden_state\n                     visual_features_frame += self.img_temperal_embedding[frame_idx]\n                     visual_features.append(visual_features_frame)\n \n@@ -1358,6 +1424,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1511,6 +1578,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n "
        },
        {
            "sha": "5adc48a3a2ef599969c7a0d4a3a7dd64928ba8fa",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 62,
            "deletions": 4,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_kosmos2 import Kosmos2Config, Kosmos2TextConfig, Kosmos2VisionConfig\n \n@@ -121,6 +122,8 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -259,6 +262,8 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -401,15 +406,63 @@ def __init__(self, config: Kosmos2VisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -701,6 +754,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -712,7 +766,7 @@ def forward(\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1443,6 +1497,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1453,6 +1508,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n \n@@ -1769,6 +1825,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Kosmos2ModelOutput]:\n         r\"\"\"\n@@ -1820,6 +1877,7 @@ def forward(\n                 pixel_values=pixel_values,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n                 return_dict=return_dict,\n             )\n             # The whole `last_hidden_state` through `post_layernorm` instead of just `pooled_output`."
        },
        {
            "sha": "d05db378443e0b94f256cc5f27d2f0c10d6e3c0a",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 60,
            "deletions": 4,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -32,6 +32,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_x_clip import XCLIPConfig, XCLIPTextConfig, XCLIPVisionConfig\n \n@@ -121,15 +122,63 @@ def __init__(self, config: XCLIPVisionConfig):\n         self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n         self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        batch_size = pixel_values.shape[0]\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n \n         class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n         embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n-        embeddings = embeddings + self.position_embedding(self.position_ids)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n         return embeddings\n \n \n@@ -567,6 +616,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -604,6 +655,8 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n@@ -954,6 +1007,7 @@ def forward(\n         pixel_values: torch.FloatTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -966,7 +1020,7 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layernorm(hidden_states)\n \n         encoder_outputs = self.encoder(\n@@ -1455,6 +1509,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, XCLIPOutput]:\n         r\"\"\"\n@@ -1555,6 +1610,7 @@ def forward(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n         )\n "
        },
        {
            "sha": "f4ac29479c5f1f068fa93e983f9443449621ec6b",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -597,3 +597,44 @@ def test_inference(self):\n         expected_probs = torch.tensor([[9.9942e-01, 5.7805e-04]], device=torch_device)\n \n         self.assertTrue(torch.allclose(probs, expected_probs, atol=5e-3))\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model_name = \"BAAI/AltCLIP\"\n+        model = AltCLIPModel.from_pretrained(model_name).to(torch_device)\n+\n+        image_processor = AltCLIPProcessor.from_pretrained(\n+            model_name, size={\"shortest_edge\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = image_processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 145, 1024))\n+        print(\"nilesh \")\n+        print(outputs.vision_model_output.last_hidden_state.shape)\n+        print(outputs.vision_model_output.last_hidden_state[0, :3, :3])\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.3589, -0.5939, 0.3534], [0.4346, 0.1647, 0.7071], [1.1404, -0.4716, 0.1664]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        },
        {
            "sha": "cceeee4912dc3fbe4a746c3efab7f88e5c5d07d5",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -656,3 +656,37 @@ def test_training(self):\n             for name, param in model.named_parameters():\n                 if self._is_layer_used(model_class, name):\n                     self.assertIsNotNone(param.grad, f\"Gradients should not be None - got {param.grad} for {name}\")\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model_name = \"BridgeTower/bridgetower-base\"\n+        model = BridgeTowerModel.from_pretrained(model_name).to(torch_device)\n+\n+        image_processor = BridgeTowerProcessor.from_pretrained(model_name, size={\"shortest_edge\": 180})\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = image_processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 122, 768))\n+\n+        self.assertEqual(outputs.image_features.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.6518, 0.4978, -0.4544], [-2.6672, -0.0843, -0.4210], [-2.4510, -0.1002, -0.3458]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.image_features[0, :3, :3], expected_slice, atol=1e-4))"
        },
        {
            "sha": "647b3ac7b73a3d2fdf4af94cb69d75e8be2860f3",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -740,3 +740,41 @@ def test_inference(self):\n         expected_probs = torch.tensor([[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]], device=torch_device)\n \n         self.assertTrue(torch.allclose(probs, expected_probs, atol=5e-3))\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model_name = \"OFA-Sys/chinese-clip-vit-base-patch16\"\n+        model = ChineseCLIPModel.from_pretrained(model_name).to(torch_device)\n+\n+        image_processor = ChineseCLIPProcessor.from_pretrained(\n+            model_name, size={\"height\": 180, \"width\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = image_processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 122, 768))\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.3990, 0.2983, -0.1239], [-0.1452, -0.2759, 0.0403], [-0.3149, -0.4763, 0.8555]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        },
        {
            "sha": "88824756a6fb542e35b48681ae1b2fd3084aece0",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -1182,3 +1182,40 @@ def test_inference(self):\n         expected_logits = torch.tensor([[24.5701, 19.3049]], device=torch_device)\n \n         self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # CLIP models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(torch_device)\n+\n+        processor = CLIPProcessor.from_pretrained(\n+            \"openai/clip-vit-base-patch32\", size={\"height\": 180, \"width\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 26, 768))\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.1538, 0.0322, -0.3235], [0.2893, 0.1135, -0.5708], [0.0461, 0.1540, -0.6018]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        },
        {
            "sha": "c5edf7cb757b309f70545dcc8e358a0a4cf312b1",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 40,
            "deletions": 2,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -796,20 +796,58 @@ def test_inference_image_segmentation(self):\n \n         # forward pass\n         with torch.no_grad():\n-            outputs = model(**inputs)\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n \n         # verify the predicted masks\n         self.assertEqual(\n             outputs.logits.shape,\n             torch.Size((3, 352, 352)),\n         )\n         expected_masks_slice = torch.tensor(\n-            [[-7.4613, -7.4785, -7.3628], [-7.3268, -7.0899, -7.1333], [-6.9838, -6.7900, -6.8913]]\n+            [[-7.4613, -7.4785, -7.3627], [-7.3268, -7.0898, -7.1333], [-6.9838, -6.7900, -6.8913]]\n         ).to(torch_device)\n+\n         self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_masks_slice, atol=1e-3))\n \n         # verify conditional and pooled output\n         expected_conditional = torch.tensor([0.5601, -0.0314, 0.1980]).to(torch_device)\n         expected_pooled_output = torch.tensor([0.5036, -0.2681, -0.2644]).to(torch_device)\n         self.assertTrue(torch.allclose(outputs.conditional_embeddings[0, :3], expected_conditional, atol=1e-3))\n         self.assertTrue(torch.allclose(outputs.pooled_output[0, :3], expected_pooled_output, atol=1e-3))\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = CLIPSegModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(torch_device)\n+\n+        processor = CLIPSegProcessor.from_pretrained(\n+            \"openai/clip-vit-base-patch32\", size={\"height\": 180, \"width\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 26, 768))\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.1538, 0.0322, -0.3235], [0.2893, 0.1135, -0.5708], [0.0461, 0.1540, -0.6018]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        },
        {
            "sha": "33da9e26cba03d2b34c50dac3c994d384e653d5f",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -614,3 +614,38 @@ def test_batched_generation(self):\n         generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n         self.assertEqual(generated_captions, [\"two cats sleeping on a pink blanket next to remotes.\"] * 2)\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # CLIP family models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = GitModel.from_pretrained(\"microsoft/git-base\").to(torch_device)\n+\n+        processor = GitProcessor.from_pretrained(\n+            \"microsoft/git-base\", size={\"height\": 180, \"width\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 130, 768))\n+\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-1.0296, 2.5960, 0.8703], [1.7027, 1.3302, -0.4543], [-1.4932, -0.1084, 0.0502]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))"
        },
        {
            "sha": "913111c0a088e1e11ff6a52c803ab560db9d8bbd",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -762,3 +762,40 @@ def test_snowman_image_captioning_batch(self):\n         self.assertEqual(processed_text[0], EXPECTED_PROCESSED_TEXT_0)\n         self.assertEqual(all_final_text[0], EXPECTED_FINAL_TEXT_0)\n         self.assertListEqual(all_entities[0], EXPECTED_ENTITIES_0)\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = Kosmos2Model.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(torch_device)\n+\n+        processor = AutoProcessor.from_pretrained(\n+            \"microsoft/kosmos-2-patch14-224\", size={\"shortest_edge\": 180}, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        inputs = processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 145, 1024))\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[1.0022, -1.1901, 3.2887], [2.6164, 0.0515, -0.8270], [1.8315, 0.1272, -0.8590]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        },
        {
            "sha": "8b91019bae18ccd49d1d39bae5ffbf90e386a6d6",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a55adee890b0757461e1588d8847fd385ce5c214/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=a55adee890b0757461e1588d8847fd385ce5c214",
            "patch": "@@ -731,3 +731,39 @@ def test_inference(self):\n         expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)\n \n         self.assertTrue(torch.allclose(outputs.logits_per_video, expected_logits, atol=1e-3))\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # XCLIP models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = XCLIPModel.from_pretrained(\"microsoft/xclip-base-patch32\").to(torch_device)\n+\n+        processor = XCLIPProcessor.from_pretrained(\n+            \"microsoft/xclip-base-patch32\", size=180, crop_size={\"height\": 180, \"width\": 180}\n+        )\n+\n+        video = prepare_video()\n+        inputs = processor(text=\"what's in the video\", videos=video, return_tensors=\"pt\").to(torch_device)\n+\n+        # interpolate_pos_encodiung false should return value error\n+        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n+            with torch.no_grad():\n+                model(**inputs, interpolate_pos_encoding=False)\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((8, 26, 768))\n+\n+        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[0.0126, 0.2109, 0.0609], [0.0448, 0.5862, -0.1688], [-0.0881, 0.8525, -0.3044]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(\n+            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        )"
        }
    ],
    "stats": {
        "total": 893,
        "additions": 828,
        "deletions": 65
    }
}