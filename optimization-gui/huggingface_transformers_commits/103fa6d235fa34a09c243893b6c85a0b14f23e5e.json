{
    "author": "SunMarc",
    "message": "[v5] Remove deprecated prediction loop (#41123)\n\n* rem deprecated\n\n* more\n\n* rm all instances of legacy arg",
    "sha": "103fa6d235fa34a09c243893b6c85a0b14f23e5e",
    "files": [
        {
            "sha": "d8bfbf09355a88febc06fe7b0d1fda24e0ddcbf8",
            "filename": "docs/source/en/internal/trainer_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fen%2Finternal%2Ftrainer_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fen%2Finternal%2Ftrainer_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ftrainer_utils.md?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -36,10 +36,6 @@ Most of those are only useful if you are studying the code of the Trainer in the\n \n [[autodoc]] trainer_callback.CallbackHandler\n \n-## Distributed Evaluation\n-\n-[[autodoc]] trainer_pt_utils.DistributedTensorGatherer\n-\n ## Trainer Argument Parser\n \n [[autodoc]] HfArgumentParser"
        },
        {
            "sha": "6b984bf618fd0bc287b33bfe0a933aa13adb4bbb",
            "filename": "docs/source/ja/internal/trainer_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fja%2Finternal%2Ftrainer_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fja%2Finternal%2Ftrainer_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Ftrainer_utils.md?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -36,10 +36,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] trainer_callback.CallbackHandler\n \n-## Distributed Evaluation\n-\n-[[autodoc]] trainer_pt_utils.DistributedTensorGatherer\n-\n ## Trainer Argument Parser\n \n [[autodoc]] HfArgumentParser"
        },
        {
            "sha": "bdce2c34198cc94a2ce00b7838083117dc556dfb",
            "filename": "docs/source/ko/internal/trainer_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fko%2Finternal%2Ftrainer_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fko%2Finternal%2Ftrainer_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Ftrainer_utils.md?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -36,10 +36,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] trainer_callback.CallbackHandler\n \n-## 분산 평가 (Distributed Evaluation) [[transformers.trainer_pt_utils.DistributedTensorGatherer]]\n-\n-[[autodoc]] trainer_pt_utils.DistributedTensorGatherer\n-\n ## Trainer 인자 파서 (Trainer Argument Parser) [[transformers.HfArgumentParser]]\n \n [[autodoc]] HfArgumentParser"
        },
        {
            "sha": "a62c64a76cb674ca0128413533b59fbb7e5259bd",
            "filename": "docs/source/zh/internal/trainer_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fzh%2Finternal%2Ftrainer_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/docs%2Fsource%2Fzh%2Finternal%2Ftrainer_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Ftrainer_utils.md?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -37,10 +37,6 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] trainer_callback.CallbackHandler\n \n-## 分布式评估\n-\n-[[autodoc]] trainer_pt_utils.DistributedTensorGatherer\n-\n ## Trainer参数解析\n \n [[autodoc]] HfArgumentParser"
        },
        {
            "sha": "f315afc2918ccf4b7be2f529e663ff24fb41896c",
            "filename": "examples/pytorch/question-answering/trainer_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -41,10 +41,9 @@ def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metr\n         # Temporarily disable metric computation, we will do it in the loop here.\n         compute_metrics = self.compute_metrics\n         self.compute_metrics = None\n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n         start_time = time.time()\n         try:\n-            output = eval_loop(\n+            output = self.evaluation_loop(\n                 eval_dataloader,\n                 description=\"Evaluation\",\n                 # No point gathering the predictions if there are no metrics, otherwise we defer to\n@@ -96,10 +95,9 @@ def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_ke\n         # Temporarily disable metric computation, we will do it in the loop here.\n         compute_metrics = self.compute_metrics\n         self.compute_metrics = None\n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n         start_time = time.time()\n         try:\n-            output = eval_loop(\n+            output = self.evaluation_loop(\n                 predict_dataloader,\n                 description=\"Prediction\",\n                 # No point gathering the predictions if there are no metrics, otherwise we defer to"
        },
        {
            "sha": "806dbb7bd57f48c91966bd9691d9953d14b9afaf",
            "filename": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -63,9 +63,8 @@ def evaluate(\n         compute_metrics = self.compute_metrics\n         self.compute_metrics = None\n         start_time = time.time()\n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n         try:\n-            output = eval_loop(\n+            output = self.evaluation_loop(\n                 eval_dataloader,\n                 description=\"Evaluation\",\n                 # No point gathering the predictions if there are no metrics, otherwise we defer to\n@@ -124,9 +123,8 @@ def predict(\n         compute_metrics = self.compute_metrics\n         self.compute_metrics = None\n         start_time = time.time()\n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n         try:\n-            output = eval_loop(\n+            output = self.evaluation_loop(\n                 predict_dataloader,\n                 description=\"Prediction\",\n                 # No point gathering the predictions if there are no metrics, otherwise we defer to"
        },
        {
            "sha": "dd0fb29a3025b855892d65782ff0392e806d34dd",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 231,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -87,22 +87,18 @@\n     TrainerState,\n )\n from .trainer_pt_utils import (\n-    DistributedTensorGatherer,\n     EvalLoopContainer,\n     IterableDatasetShard,\n     LabelSmoother,\n     LayerWiseDummyOptimizer,\n     LengthGroupedSampler,\n-    SequentialDistributedSampler,\n     distributed_broadcast_scalars,\n     distributed_concat,\n     find_batch_size,\n     get_model_param_count,\n     get_module_class_from_name,\n     get_parameter_names,\n-    nested_concat,\n     nested_detach,\n-    nested_numpify,\n     nested_xla_mesh_reduce,\n     reissue_pt_warnings,\n     remove_dummy_checkpoint,\n@@ -1093,23 +1089,6 @@ def get_train_dataloader(self) -> DataLoader:\n     def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:\n         if eval_dataset is None or not has_length(eval_dataset):\n             return None\n-        # Build the sampler.\n-\n-        # Deprecated code\n-        if self.args.use_legacy_prediction_loop:\n-            if is_torch_xla_available():\n-                return SequentialDistributedSampler(\n-                    eval_dataset, num_replicas=xr.world_size(), rank=xr.global_ordinal()\n-                )\n-            elif is_sagemaker_mp_enabled():\n-                return SequentialDistributedSampler(\n-                    eval_dataset,\n-                    num_replicas=smp.dp_size(),\n-                    rank=smp.dp_rank(),\n-                    batch_size=self.args.per_device_eval_batch_size,\n-                )\n-            else:\n-                return SequentialSampler(eval_dataset)\n \n         if self.args.group_by_length:\n             if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n@@ -4358,8 +4337,7 @@ def evaluate(\n \n         start_time = time.time()\n \n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n-        output = eval_loop(\n+        output = self.evaluation_loop(\n             eval_dataloader,\n             description=\"Evaluation\",\n             # No point gathering the predictions if there are no metrics, otherwise we defer to\n@@ -4436,8 +4414,7 @@ def predict(\n         test_dataloader = self.get_test_dataloader(test_dataset)\n         start_time = time.time()\n \n-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n-        output = eval_loop(\n+        output = self.evaluation_loop(\n             test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix\n         )\n         total_batch_size = self.args.eval_batch_size * self.args.world_size\n@@ -5067,212 +5044,6 @@ def push_to_hub(\n             revision=revision,\n         )\n \n-    #\n-    # Deprecated code\n-    #\n-\n-    def prediction_loop(\n-        self,\n-        dataloader: DataLoader,\n-        description: str,\n-        prediction_loss_only: Optional[bool] = None,\n-        ignore_keys: Optional[list[str]] = None,\n-        metric_key_prefix: str = \"eval\",\n-    ) -> EvalLoopOutput:\n-        \"\"\"\n-        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n-\n-        Works both with or without labels.\n-        \"\"\"\n-        args = self.args\n-\n-        if not has_length(dataloader):\n-            raise ValueError(\"dataloader must implement a working __len__\")\n-\n-        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n-\n-        # if eval is called w/o train, handle model prep here\n-        if self.is_deepspeed_enabled and self.deepspeed is None:\n-            _, _ = deepspeed_init(self, num_training_steps=0, inference=True)\n-\n-        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n-\n-        if len(self.accelerator._models) == 0 and model is self.model:\n-            model = (\n-                self.accelerator.prepare(model)\n-                if self.is_deepspeed_enabled or self.is_fsdp_enabled\n-                else self.accelerator.prepare_model(model, evaluation_mode=True)\n-            )\n-\n-            if self.is_fsdp_enabled:\n-                self.model = model\n-\n-            # for the rest of this function `model` is the outside model, whether it was wrapped or not\n-            if model is not self.model:\n-                self.model_wrapped = model\n-\n-            # backward compatibility\n-            if self.is_deepspeed_enabled:\n-                self.deepspeed = self.model_wrapped\n-\n-        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n-        # while ``train`` is running, cast it to the right dtype first and then put on device\n-        if not self.is_in_train:\n-            if args.fp16_full_eval:\n-                model = model.to(dtype=torch.float16, device=args.device)\n-            elif args.bf16_full_eval:\n-                model = model.to(dtype=torch.bfloat16, device=args.device)\n-\n-        batch_size = (\n-            dataloader.total_batch_size\n-            if getattr(dataloader, \"_is_accelerate_prepared\", False)\n-            else dataloader.batch_size\n-        )\n-\n-        if batch_size is None:\n-            raise ValueError(\n-                \"Batch size cannot be None. Ensure the dataloader has a valid batch_size or total_batch_size.\"\n-            )\n-\n-        num_examples = self.num_examples(dataloader)\n-        logger.info(f\"\\n***** Running {description} *****\")\n-        logger.info(f\"  Num examples = {num_examples}\")\n-        logger.info(f\"  Batch size = {batch_size}\")\n-\n-        losses_host: Optional[torch.Tensor] = None\n-        preds_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n-        labels_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n-        inputs_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n-        metrics: Optional[dict] = None\n-        eval_set_kwargs: dict = {}\n-\n-        world_size = max(1, args.world_size)\n-\n-        eval_losses_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=batch_size)\n-        if not prediction_loss_only:\n-            # The actual number of eval_sample can be greater than num_examples in distributed settings (when we pass\n-            # a batch size to the sampler)\n-            make_multiple_of = None\n-            if hasattr(dataloader, \"sampler\") and isinstance(dataloader.sampler, SequentialDistributedSampler):\n-                make_multiple_of = dataloader.sampler.batch_size\n-            preds_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n-            labels_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n-            inputs_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n-\n-        model.eval()\n-        if hasattr(self.optimizer, \"eval\") and callable(self.optimizer.eval):\n-            self.optimizer.eval()\n-\n-        if args.past_index >= 0:\n-            self._past = None\n-\n-        self.callback_handler.eval_dataloader = dataloader\n-\n-        for step, inputs in enumerate(dataloader):\n-            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n-            main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n-            inputs_decode = (\n-                self._prepare_input(inputs[main_input_name]) if \"inputs\" in args.include_for_metrics else None\n-            )\n-\n-            if loss is not None:\n-                losses = loss.repeat(batch_size)\n-                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n-            if logits is not None:\n-                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n-            if labels is not None:\n-                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n-            if inputs_decode is not None:\n-                inputs_host = (\n-                    inputs_decode\n-                    if inputs_host is None\n-                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n-                )\n-            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n-\n-            if self.args.batch_eval_metrics:\n-                if self.compute_metrics is not None and preds_host is not None and labels_host is not None:\n-                    is_last_step = self.accelerator.gradient_state.end_of_dataloader\n-                    batch_kwargs = {}\n-                    batch_kwargs[\"losses\"] = losses_host if \"loss\" in args.include_for_metrics else None\n-                    batch_kwargs[\"inputs\"] = inputs_host if \"inputs\" in args.include_for_metrics else None\n-                    metrics = self.compute_metrics(\n-                        EvalPrediction(predictions=preds_host, label_ids=labels_host, **batch_kwargs),\n-                        compute_result=is_last_step,\n-                    )\n-\n-            if self.args.batch_eval_metrics or (\n-                args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0\n-            ):\n-                # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n-                eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n-                if not prediction_loss_only:\n-                    preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n-                    labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n-                    inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n-\n-                # Set back to None to begin a new accumulation\n-                del losses_host, preds_host, labels_host, inputs_host\n-                torch.cuda.empty_cache()\n-                losses_host, preds_host, labels_host, inputs_host = None, None, None, None\n-\n-        if args.past_index and hasattr(self, \"_past\"):\n-            # Clean the state at the end of the evaluation loop\n-            delattr(self, \"_past\")\n-\n-        # Gather all remaining tensors and put them back on the CPU\n-        eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n-        if not prediction_loss_only:\n-            preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n-            labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n-            inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n-\n-        eval_loss = eval_losses_gatherer.finalize()\n-        preds = preds_gatherer.finalize() if not prediction_loss_only else None\n-        label_ids = labels_gatherer.finalize() if not prediction_loss_only else None\n-        inputs_ids = inputs_gatherer.finalize() if not prediction_loss_only else None\n-\n-        if (\n-            self.compute_metrics is not None\n-            and preds is not None\n-            and label_ids is not None\n-            and not self.args.batch_eval_metrics\n-        ):\n-            eval_set_kwargs[\"losses\"] = eval_loss if \"loss\" in args.include_for_metrics else None\n-            eval_set_kwargs[\"inputs\"] = inputs_ids if \"inputs\" in args.include_for_metrics else None\n-            metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids, **eval_set_kwargs))\n-        elif metrics is None:\n-            metrics = {}\n-\n-        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n-        metrics = denumpify_detensorize(metrics)\n-\n-        if eval_loss is not None:\n-            metrics[f\"{metric_key_prefix}_loss\"] = eval_loss.mean().item()\n-\n-        # Prefix all keys with metric_key_prefix + '_'\n-        for key in list(metrics.keys()):\n-            if not key.startswith(f\"{metric_key_prefix}_\"):\n-                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n-\n-        return EvalLoopOutput(predictions=preds, label_ids=label_ids, metrics=metrics, num_samples=num_examples)\n-\n-    def _gather_and_numpify(self, tensors, name):\n-        \"\"\"\n-        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\n-        concatenating them to `gathered`\n-        \"\"\"\n-        if tensors is None:\n-            return\n-        if is_torch_xla_available():\n-            tensors = nested_xla_mesh_reduce(tensors, name)\n-        elif is_sagemaker_mp_enabled():\n-            tensors = smp_gather(tensors)\n-        elif self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n-            tensors = distributed_concat(tensors)\n-\n-        return nested_numpify(tensors)\n-\n     def _add_sm_patterns_to_gitignore(self) -> None:\n         \"\"\"Add SageMaker Checkpointing patterns to .gitignore file.\"\"\"\n         # Make sure we only do this on the main process"
        },
        {
            "sha": "b1cb1f551ac5fe5d88edae414bf882099ce759b6",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 164,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -340,62 +340,6 @@ def get_arrays(self):\n         return self.arrays\n \n \n-class SequentialDistributedSampler(Sampler):\n-    \"\"\"\n-    Distributed Sampler that subsamples indices sequentially, making it easier to collate all results at the end.\n-\n-    Even though we only use this sampler for eval and predict (no training), which means that the model params won't\n-    have to be synced (i.e. will not hang for synchronization even if varied number of forward passes), we still add\n-    extra samples to the sampler to make it evenly divisible (like in `DistributedSampler`) to make it easy to `gather`\n-    or `reduce` resulting tensors at the end of the loop.\n-    \"\"\"\n-\n-    def __init__(self, dataset, num_replicas=None, rank=None, batch_size=None):\n-        warnings.warn(\n-            \"SequentialDistributedSampler is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        if num_replicas is None:\n-            if not dist.is_available():\n-                raise RuntimeError(\"Requires distributed package to be available\")\n-            num_replicas = dist.get_world_size()\n-        if rank is None:\n-            if not dist.is_available():\n-                raise RuntimeError(\"Requires distributed package to be available\")\n-            rank = dist.get_rank()\n-        self.dataset = dataset\n-        self.num_replicas = num_replicas\n-        self.rank = rank\n-        num_samples = len(self.dataset)\n-        # Add extra samples to make num_samples a multiple of batch_size if passed\n-        if batch_size is not None:\n-            self.num_samples = int(math.ceil(num_samples / (batch_size * num_replicas))) * batch_size\n-        else:\n-            self.num_samples = int(math.ceil(num_samples / num_replicas))\n-        self.total_size = self.num_samples * self.num_replicas\n-        self.batch_size = batch_size\n-\n-    def __iter__(self):\n-        indices = list(range(len(self.dataset)))\n-\n-        # add extra samples to make it evenly divisible\n-        indices += indices[: (self.total_size - len(indices))]\n-        assert len(indices) == self.total_size, (\n-            f\"Indices length {len(indices)} and total size {self.total_size} mismatched\"\n-        )\n-\n-        # subsample\n-        indices = indices[self.rank * self.num_samples : (self.rank + 1) * self.num_samples]\n-        assert len(indices) == self.num_samples, (\n-            f\"Indices length {len(indices)} and sample number {self.num_samples} mismatched\"\n-        )\n-\n-        return iter(indices)\n-\n-    def __len__(self):\n-        return self.num_samples\n-\n-\n def get_tpu_sampler(dataset: torch.utils.data.Dataset, batch_size: int):\n     if xr.world_size() <= 1:\n         return RandomSampler(dataset)\n@@ -426,114 +370,6 @@ def nested_truncate(tensors, limit):\n     return tensors[:limit]\n \n \n-class DistributedTensorGatherer:\n-    \"\"\"\n-    A class responsible for properly gathering tensors (or nested list/tuple of tensors) on the CPU by chunks.\n-\n-    If our dataset has 16 samples with a batch size of 2 on 3 processes and we gather then transfer on CPU at every\n-    step, our sampler will generate the following indices:\n-\n-        `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1]`\n-\n-    to get something of size a multiple of 3 (so that each process gets the same dataset length). Then process 0, 1 and\n-    2 will be responsible of making predictions for the following samples:\n-\n-        - P0: `[0, 1, 2, 3, 4, 5]`\n-        - P1: `[6, 7, 8, 9, 10, 11]`\n-        - P2: `[12, 13, 14, 15, 0, 1]`\n-\n-    The first batch treated on each process will be:\n-\n-        - P0: `[0, 1]`\n-        - P1: `[6, 7]`\n-        - P2: `[12, 13]`\n-\n-    So if we gather at the end of the first batch, we will get a tensor (nested list/tuple of tensor) corresponding to\n-    the following indices:\n-\n-        `[0, 1, 6, 7, 12, 13]`\n-\n-    If we directly concatenate our results without taking any precautions, the user will then get the predictions for\n-    the indices in this order at the end of the prediction loop:\n-\n-        `[0, 1, 6, 7, 12, 13, 2, 3, 8, 9, 14, 15, 4, 5, 10, 11, 0, 1]`\n-\n-    For some reason, that's not going to roll their boat. This class is there to solve that problem.\n-\n-    Args:\n-        world_size (`int`):\n-            The number of processes used in the distributed training.\n-        num_samples (`int`):\n-            The number of samples in our dataset.\n-        make_multiple_of (`int`, *optional*):\n-            If passed, the class assumes the datasets passed to each process are made to be a multiple of this argument\n-            (by adding samples).\n-        padding_index (`int`, *optional*, defaults to -100):\n-            The padding index to use if the arrays don't all have the same sequence length.\n-    \"\"\"\n-\n-    def __init__(self, world_size, num_samples, make_multiple_of=None, padding_index=-100):\n-        warnings.warn(\n-            \"DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\",\n-            FutureWarning,\n-        )\n-        self.world_size = world_size\n-        self.num_samples = num_samples\n-        total_size = world_size if make_multiple_of is None else world_size * make_multiple_of\n-        self.total_samples = int(np.ceil(num_samples / total_size)) * total_size\n-        self.process_length = self.total_samples // world_size\n-        self._storage = None\n-        self._offsets = None\n-        self.padding_index = padding_index\n-\n-    def add_arrays(self, arrays):\n-        \"\"\"\n-        Add `arrays` to the internal storage, Will initialize the storage to the full size at the first arrays passed\n-        so that if we're bound to get an OOM, it happens at the beginning.\n-        \"\"\"\n-        if arrays is None:\n-            return\n-        if self._storage is None:\n-            self._storage = nested_new_like(arrays, self.total_samples, padding_index=self.padding_index)\n-            self._offsets = list(range(0, self.total_samples, self.process_length))\n-\n-        slice_len, self._storage = self._nested_set_tensors(self._storage, arrays)\n-        for i in range(self.world_size):\n-            self._offsets[i] += slice_len\n-\n-    def _nested_set_tensors(self, storage, arrays):\n-        if isinstance(arrays, (list, tuple)):\n-            result = [self._nested_set_tensors(x, y) for x, y in zip(storage, arrays)]\n-            return result[0][0], type(arrays)(r[1] for r in result)\n-        assert arrays.shape[0] % self.world_size == 0, (\n-            f\"Arrays passed should all have a first dimension multiple of {self.world_size}, found {arrays.shape[0]}.\"\n-        )\n-\n-        slice_len = arrays.shape[0] // self.world_size\n-        for i in range(self.world_size):\n-            if len(arrays.shape) == 1:\n-                storage[self._offsets[i] : self._offsets[i] + slice_len] = arrays[i * slice_len : (i + 1) * slice_len]\n-            else:\n-                # Expand the array on the fly if needed.\n-                if len(storage.shape) > 1 and storage.shape[1] < arrays.shape[1]:\n-                    storage = expand_like(storage, arrays.shape[1], padding_index=self.padding_index)\n-                storage[self._offsets[i] : self._offsets[i] + slice_len, : arrays.shape[1]] = arrays[\n-                    i * slice_len : (i + 1) * slice_len\n-                ]\n-        return slice_len, storage\n-\n-    def finalize(self):\n-        \"\"\"\n-        Return the properly gathered arrays and truncate to the number of samples (since the sampler added some extras\n-        to get each process a dataset of the same length).\n-        \"\"\"\n-        if self._storage is None:\n-            return\n-        if self._offsets[0] != self.process_length:\n-            logger.warning(\"Not all data has been set. Are you sure you passed all values?\")\n-        return nested_truncate(self._storage, self.num_samples)\n-\n-\n @dataclass\n class LabelSmoother:\n     \"\"\""
        },
        {
            "sha": "8bb913922a34ba2ee07d970fc3f23bbb0d7a4dfe",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -1253,9 +1253,6 @@ class TrainingArguments:\n     skip_memory_metrics: bool = field(\n         default=True, metadata={\"help\": \"Whether or not to skip adding of memory profiler reports to metrics.\"}\n     )\n-    use_legacy_prediction_loop: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to use the legacy prediction_loop in the Trainer.\"}\n-    )\n     push_to_hub: bool = field(\n         default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n     )"
        },
        {
            "sha": "3978af66eef28c8078e721af284a406a5d9c3e10",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -179,7 +179,6 @@ def test_trainer(self):\n     parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n     training_args = parser.parse_args_into_dataclasses()[0]\n     training_args.per_device_eval_batch_size = 1\n-    training_args.use_legacy_prediction_loop = False\n     training_args.predict_with_generate = True\n     training_args.generation_config = GenerationConfig(max_length=30)\n "
        },
        {
            "sha": "af80fc73650e3022c9ecd1339193c2b14f9c8e51",
            "filename": "tests/trainer/test_trainer_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/103fa6d235fa34a09c243893b6c85a0b14f23e5e/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/103fa6d235fa34a09c243893b6c85a0b14f23e5e/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_utils.py?ref=103fa6d235fa34a09c243893b6c85a0b14f23e5e",
            "patch": "@@ -35,12 +35,10 @@\n     from transformers.trainer_pt_utils import (\n         DistributedLengthGroupedSampler,\n         DistributedSamplerWithLoop,\n-        DistributedTensorGatherer,\n         EvalLoopContainer,\n         IterableDatasetShard,\n         LabelSmoother,\n         LengthGroupedSampler,\n-        SequentialDistributedSampler,\n         ShardSampler,\n         get_parameter_names,\n         numpy_pad_and_concatenate,\n@@ -80,79 +78,6 @@ def __iter__(self):\n \n @require_torch\n class TrainerUtilsTest(unittest.TestCase):\n-    def test_distributed_tensor_gatherer(self):\n-        # Simulate a result with a dataset of size 21, 4 processes and chunks of lengths 2, 3, 1\n-        world_size = 4\n-        num_samples = 21\n-        input_indices = [\n-            [0, 1, 6, 7, 12, 13, 18, 19],\n-            [2, 3, 4, 8, 9, 10, 14, 15, 16, 20, 0, 1],\n-            [5, 11, 17, 2],\n-        ]\n-\n-        predictions = np.random.normal(size=(num_samples, 13))\n-        gatherer = DistributedTensorGatherer(world_size=world_size, num_samples=num_samples)\n-        for indices in input_indices:\n-            gatherer.add_arrays(predictions[indices])\n-        result = gatherer.finalize()\n-        self.assertTrue(np.array_equal(result, predictions))\n-\n-        # With nested tensors\n-        gatherer = DistributedTensorGatherer(world_size=world_size, num_samples=num_samples)\n-        for indices in input_indices:\n-            gatherer.add_arrays([predictions[indices], [predictions[indices], predictions[indices]]])\n-        result = gatherer.finalize()\n-        self.assertTrue(isinstance(result, list))\n-        self.assertEqual(len(result), 2)\n-        self.assertTrue(isinstance(result[1], list))\n-        self.assertEqual(len(result[1]), 2)\n-        self.assertTrue(np.array_equal(result[0], predictions))\n-        self.assertTrue(np.array_equal(result[1][0], predictions))\n-        self.assertTrue(np.array_equal(result[1][1], predictions))\n-\n-    def test_distributed_tensor_gatherer_different_shapes(self):\n-        # Simulate a result with a dataset of size 21, 4 processes and chunks of lengths 2, 3, 1\n-        world_size = 4\n-        num_samples = 21\n-        input_indices = [\n-            [0, 1, 6, 7, 12, 13, 18, 19],\n-            [2, 3, 4, 8, 9, 10, 14, 15, 16, 20, 0, 1],\n-            [5, 11, 17, 2],\n-        ]\n-        sequence_lengths = [8, 10, 13]\n-\n-        predictions = np.random.normal(size=(num_samples, 13))\n-        gatherer = DistributedTensorGatherer(world_size=world_size, num_samples=num_samples)\n-        for indices, seq_length in zip(input_indices, sequence_lengths):\n-            gatherer.add_arrays(predictions[indices, :seq_length])\n-        result = gatherer.finalize()\n-\n-        # Remove the extra samples added at the end for a round multiple of num processes.\n-        actual_indices = [input_indices[0], input_indices[1][:-2], input_indices[2][:-1]]\n-        for indices, seq_length in zip(actual_indices, sequence_lengths):\n-            self.assertTrue(np.array_equal(result[indices, :seq_length], predictions[indices, :seq_length]))\n-\n-        # With nested tensors\n-        predictions = np.random.normal(size=(num_samples, 13))\n-        gatherer = DistributedTensorGatherer(world_size=world_size, num_samples=num_samples)\n-        for indices, seq_length in zip(input_indices, sequence_lengths):\n-            gatherer.add_arrays([predictions[indices, :seq_length], predictions[indices]])\n-        result = gatherer.finalize()\n-\n-        for indices, seq_length in zip(actual_indices, sequence_lengths):\n-            self.assertTrue(np.array_equal(result[0][indices, :seq_length], predictions[indices, :seq_length]))\n-        self.assertTrue(np.array_equal(result[1], predictions))\n-\n-        # Check if works if varying seq_length is second\n-        gatherer = DistributedTensorGatherer(world_size=world_size, num_samples=num_samples)\n-        for indices, seq_length in zip(input_indices, sequence_lengths):\n-            gatherer.add_arrays([predictions[indices], predictions[indices, :seq_length]])\n-        result = gatherer.finalize()\n-\n-        self.assertTrue(np.array_equal(result[0], predictions))\n-        for indices, seq_length in zip(actual_indices, sequence_lengths):\n-            self.assertTrue(np.array_equal(result[1][indices, :seq_length], predictions[indices, :seq_length]))\n-\n     def test_label_smoothing(self):\n         epsilon = 0.1\n         num_labels = 12\n@@ -297,38 +222,6 @@ def test_distributed_sampler_with_loop(self):\n             self.assertEqual(set(total[:length]), set(dataset))\n             self.assertEqual(set(total[length:]), set(total[: (len(total) - length)]))\n \n-    def test_sequential_distributed_sampler(self):\n-        batch_size = 16\n-        for length in [23, 64, 123]:\n-            dataset = list(range(length))\n-            shard1 = SequentialDistributedSampler(dataset, num_replicas=2, rank=0)\n-            shard2 = SequentialDistributedSampler(dataset, num_replicas=2, rank=1)\n-\n-            # Sample\n-            samples1 = list(shard1)\n-            samples2 = list(shard2)\n-\n-            total = samples1 + samples2\n-\n-            self.assertListEqual(total[:length], dataset)\n-            self.assertListEqual(total[length:], dataset[: (len(total) - length)])\n-\n-            # With a batch_size passed\n-            shard1 = SequentialDistributedSampler(dataset, num_replicas=2, rank=0, batch_size=batch_size)\n-            shard2 = SequentialDistributedSampler(dataset, num_replicas=2, rank=1, batch_size=batch_size)\n-\n-            # Sample\n-            samples1 = list(shard1)\n-            samples2 = list(shard2)\n-\n-            self.assertTrue(len(samples1) % batch_size == 0)\n-            self.assertTrue(len(samples2) % batch_size == 0)\n-\n-            total = samples1 + samples2\n-\n-            self.assertListEqual(total[:length], dataset)\n-            self.assertListEqual(total[length:], dataset[: (len(total) - length)])\n-\n     def check_iterable_dataset_shard(self, dataset, batch_size, drop_last, num_processes=2, epoch=0):\n         # Set the seed for the base dataset to get the proper reference.\n         dataset.generator.manual_seed(epoch)"
        }
    ],
    "stats": {
        "total": 536,
        "additions": 6,
        "deletions": 530
    }
}