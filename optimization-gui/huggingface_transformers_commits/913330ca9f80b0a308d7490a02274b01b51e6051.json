{
    "author": "zucchini-nlp",
    "message": "VLMs: fix number of image tokens (#34332)\n\n* fix\r\n\r\n* fix tests\r\n\r\n* add tests\r\n\r\n* style\r\n\r\n* style\r\n\r\n* fix qwen after rebase\r\n\r\n* fix video llava",
    "sha": "913330ca9f80b0a308d7490a02274b01b51e6051",
    "files": [
        {
            "sha": "0661da8727996f70a99e6ea29bf557b40bbd73e6",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -1288,7 +1288,7 @@ def forward(\n         if pixel_values is not None:\n             image_tokens = self.get_image_tokens(pixel_values)\n             n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum().item()\n-            n_image_features = image_tokens.shape[0]\n+            n_image_features = image_tokens.shape[0] * image_tokens.shape[1]\n             if n_image_tokens_in_text != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens_in_text}, features {n_image_features}\""
        },
        {
            "sha": "6d6bf4a6f38e3f4a0f2f08813070824bf6fe22a8",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -527,8 +527,9 @@ def forward(\n \n         # TODO: @raushan retain only the new behavior after v4.47\n         elif image_features is not None:\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-            n_image_features = image_features.shape[1]\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "c40ee1f70f900c76f87472823a740d61b39c7b68",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -1020,6 +1020,7 @@ def forward(\n             if image_features is not None:\n                 n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n                 n_image_features = image_features.shape[0]\n+\n                 if n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "1425a017dc05584c6ef22ac569fe491c9fb62f39",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -533,6 +533,7 @@ def forward(\n             if image_features is not None:\n                 n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n                 n_image_features = image_features.shape[0]\n+\n                 if n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "f8bdb5bf8d5a7c0e0504609305c4c212293f2269",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -679,6 +679,7 @@ def forward(\n             )\n             n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n             n_image_features = image_features.shape[0]\n+\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -704,6 +705,7 @@ def forward(\n             )\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n+\n             n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n             n_video_features = video_features.shape[0]\n             if n_video_tokens != n_video_features:"
        },
        {
            "sha": "9c0d0b45ee8e51fc346540c1b2373cc3d38e9136",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -1503,13 +1503,14 @@ def get_rope_index(\n         mrope_position_deltas = []\n         if image_grid_thw is not None or video_grid_thw is not None:\n             total_input_ids = input_ids\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(total_input_ids)\n             position_ids = torch.ones(\n                 3, input_ids.shape[0], input_ids.shape[1], dtype=input_ids.dtype, device=input_ids.device\n             )\n             image_index, video_index = 0, 0\n             for i, input_ids in enumerate(total_input_ids):\n-                if attention_mask is not None:\n-                    input_ids = input_ids[attention_mask[i] == 1]\n+                input_ids = input_ids[attention_mask[i] == 1]\n                 image_nums, video_nums = 0, 0\n                 vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n                 vision_tokens = input_ids[vision_start_indices + 1]"
        },
        {
            "sha": "02efc7c344f7b886a013037bf5d767d5a18f2c81",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -628,8 +628,8 @@ def forward(\n         # TODO: @raushan retain only the new behavior after v4.47\n         else:\n             if pixel_values_images is not None:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-                n_image_features = image_features.shape[1]\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n                 if n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -644,8 +644,8 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n             if pixel_values_videos is not None:\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum(dim=-1)[0].item()\n-                n_video_features = video_features.shape[1]\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_features = video_features.shape[0] * video_features.shape[1]\n                 if n_video_tokens != n_video_features:\n                     raise ValueError(\n                         f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "4060f8c8ecd1bf772da23977ba092565cb6ee009",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -517,8 +517,8 @@ def forward(\n \n         # TODO: @raushan retain only the new behavior after v4.47\n         elif image_features is not None:\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-            n_image_features = image_features.shape[1]\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "1a17f18de342345cc82494052839e32f678a8c57",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -235,6 +235,35 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "e088b2505366f6e7be198c4d1f49a84db99a5cc1",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -283,6 +283,38 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            input_dict[\"image_sizes\"] = input_dict[\"image_sizes\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            image_sizes = input_dict[\"image_sizes\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "edf1dd2d4c07a45e14a5bfca80f5df40385c3ee1",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -303,6 +303,38 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            input_dict[\"image_sizes\"] = input_dict[\"image_sizes\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            image_sizes = input_dict[\"image_sizes\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "95ae59dfc08fca31d7f94b6c248694fc5120adbd",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -236,6 +236,36 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "e1cd715f8f1d34c39381e43accddc8c5bf8946b3",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -58,7 +58,7 @@ class Qwen2VLVisionText2TextModelTester:\n     def __init__(\n         self,\n         parent,\n-        batch_size=2,\n+        batch_size=3,\n         seq_length=7,\n         num_channels=3,\n         ignore_index=-100,\n@@ -245,6 +245,40 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            patch_size = config.vision_config.patch_size\n+            one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            input_dict[\"image_grid_thw\"] = input_dict[\"image_grid_thw\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = input_dict[\"image_grid_thw\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values, image_grid_thw=image_grid_thw)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values, image_grid_thw=image_grid_thw)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "e25ad1d44460c7079e0e935f34da1464304a4caf",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 32,
            "deletions": 3,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -123,9 +123,9 @@ def __init__(\n         self.batch_size = 5\n         self.num_channels = 3\n         self.image_size = 224\n-        self.encoder_seq_length = 64\n+        self.encoder_seq_length = 246\n         self.num_image_tokens = 25\n-        self.num_video_tokens = 26\n+        self.num_video_tokens = 26 * self.num_frames\n         self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n \n     def get_config(self):\n@@ -267,7 +267,7 @@ def test_mixed_input(self):\n             # if we remove some images from inputs leaving only one\n             # image number mismatch error should raise\n             inputs[\"pixel_values_images\"] = inputs[\"pixel_values_images\"][:1]\n-            with self.assertRaises(RuntimeError):\n+            with self.assertRaises(ValueError):\n                 _ = model(**inputs)\n \n     def test_video_only_input(self):\n@@ -401,6 +401,35 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values_images\"] = input_dict[\"pixel_values_images\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values_images\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values_images=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values_images=pixel_values)\n+\n \n @require_torch\n class VideoLlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "a976e3cb51f54d535149d3ce24f9db86ec628f7f",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/913330ca9f80b0a308d7490a02274b01b51e6051/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=913330ca9f80b0a308d7490a02274b01b51e6051",
            "patch": "@@ -217,6 +217,36 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        }
    ],
    "stats": {
        "total": 252,
        "additions": 237,
        "deletions": 15
    }
}