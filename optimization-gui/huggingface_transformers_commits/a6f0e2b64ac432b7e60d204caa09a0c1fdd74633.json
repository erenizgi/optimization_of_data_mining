{
    "author": "daviswer",
    "message": "Add z-loss to Bamba for v2 (#37842)\n\n* Remove const\n\n* Fix arg ref\n\n* Sharded save\n\n* Add z_loss flag\n\n* Add modeling zloss\n\n* Demodularize clm forward for zloss\n\n* Also demodularize init for z_loss flag\n\n* PR comments (mostly modularizing right)\n\n* Demodularize forward\n\n* Better name zloss and explain typematch\n\n* Fully propagate coeff name\n\n* style fixes\n\n* zloss default float\n\n* Remove conflicting annotations\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "a6f0e2b64ac432b7e60d204caa09a0c1fdd74633",
    "files": [
        {
            "sha": "5255f7cdf57f37e82db25ef8a9bab170020d01a9",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=a6f0e2b64ac432b7e60d204caa09a0c1fdd74633",
            "patch": "@@ -100,6 +100,8 @@ class BambaConfig(PretrainedConfig):\n             Flag indicating whether or not to use bias in the convolution layer of the mamba mixer block.\n         mamba_proj_bias (`bool`, *optional*, defaults to `False`):\n             Flag indicating whether or not to use bias in the input and output projections ([\"in_proj\", \"out_proj\"]) of the mamba mixer block\n+        z_loss_coefficient (`float`, *optional*, defaults to 0.0):\n+            Coefficient for auxiliary z-loss used to control logit growth during training\n \n     \"\"\"\n \n@@ -135,6 +137,7 @@ def __init__(\n         mamba_chunk_size=256,\n         mamba_conv_bias=True,\n         mamba_proj_bias=False,\n+        z_loss_coefficient=0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -186,6 +189,7 @@ def __init__(\n         self.mamba_chunk_size = mamba_chunk_size\n         self.mamba_conv_bias = mamba_conv_bias\n         self.mamba_proj_bias = mamba_proj_bias\n+        self.z_loss_coefficient = z_loss_coefficient\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "d2487b4e5bee90c4f46f8399a516c87c306526f7",
            "filename": "src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py?ref=a6f0e2b64ac432b7e60d204caa09a0c1fdd74633",
            "patch": "@@ -248,7 +248,6 @@ def convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(\n         \"--precision\",\n         type=str,\n         default=\"fp16\",\n-        const=\"fp16\",\n         required=True,\n         choices=(\"fp32\", \"fp16\", \"bf16\"),\n         help=\"The precision the model will be saved in. Select from fp32, fp16 or bf16.\",\n@@ -267,7 +266,8 @@ def convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(\n     args = parser.parse_args()\n \n     convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(\n-        args.mamba2_checkpoint_directory,\n+        args.mamba_ssm_checkpoint_directory,\n         args.precision,\n         args.output_dir,\n+        save_model=\"sharded\",\n     )"
        },
        {
            "sha": "5432260c7ddfab84a97f76fd1b94a18f8d420cf1",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=a6f0e2b64ac432b7e60d204caa09a0c1fdd74633",
            "patch": "@@ -1330,6 +1330,7 @@ def __init__(self, config):\n         self.model = BambaModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.z_loss_coefficient = config.z_loss_coefficient\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1418,6 +1419,10 @@ def forward(\n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+            if self.z_loss_coefficient > 0:\n+                # Type-match loss, but avoid upcasting large logits tensor until after it's been reduced on dim -1\n+                z_loss = logits.logsumexp(dim=-1).to(dtype=loss.dtype).pow(2).mean()\n+                loss = loss + self.z_loss_coefficient * z_loss\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "810eef9868d8f0683697111daa27017a5afc0757",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 44,
            "deletions": 12,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6f0e2b64ac432b7e60d204caa09a0c1fdd74633/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=a6f0e2b64ac432b7e60d204caa09a0c1fdd74633",
            "patch": "@@ -1093,6 +1093,13 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class BambaForCausalLM(LlamaForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.z_loss_coefficient = config.z_loss_coefficient\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1130,21 +1137,46 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        return super().forward(\n-            input_ids,\n-            attention_mask,\n-            position_ids,\n-            past_key_values,\n-            inputs_embeds,\n-            labels,\n-            use_cache,\n-            output_attentions,\n-            output_hidden_states,\n-            cache_position,\n-            logits_to_keep,\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             **kwargs,\n         )\n \n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+            if self.z_loss_coefficient > 0:\n+                # Type-match loss, but avoid upcasting large logits tensor until after it's been reduced on dim -1\n+                z_loss = logits.logsumexp(dim=-1).to(dtype=loss.dtype).pow(2).mean()\n+                loss = loss + self.z_loss_coefficient * z_loss\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 55,
        "deletions": 14
    }
}