{
    "author": "ArthurZucker",
    "message": "Fix Gradient Accumulation issue (#34191)\n\n* quick fix\r\n\r\n* 3 losses\r\n\r\n* oups\r\n\r\n* fix\r\n\r\n* nits\r\n\r\n* check how it scales for special models\r\n\r\n* propagate for conditiona detr\r\n\r\n* propagate\r\n\r\n* propagate\r\n\r\n* propagate\r\n\r\n* fixes\r\n\r\n* propagate changes\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* nits\r\n\r\n* f string\r\n\r\n* fixes\r\n\r\n* more fixes\r\n\r\n* ?\r\n\r\n* nit\r\n\r\n* arg annoying f string\r\n\r\n* nits\r\n\r\n* grumble\r\n\r\n* update\r\n\r\n* nit\r\n\r\n* refactor\r\n\r\n* fix fetch tests\r\n\r\n* nit\r\n\r\n* nit\r\n\r\n* Update src/transformers/loss/loss_utils.py\r\n\r\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\r\n\r\n* update\r\n\r\n* nit\r\n\r\n* fixup\r\n\r\n* make pass\r\n\r\n* nits\r\n\r\n* port code to more models\r\n\r\n* fixup\r\n\r\n* ntis\r\n\r\n* arf\r\n\r\n* update\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* fix\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* fine\r\n\r\n* agjkfslga.jsdlkgjklas\r\n\r\n* nits\r\n\r\n* fix fx?\r\n\r\n* update\r\n\r\n* update\r\n\r\n* styel\r\n\r\n* fix imports\r\n\r\n* update\r\n\r\n* update\r\n\r\n* fixup to fix the torch fx?\r\n\r\n---------\r\n\r\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>",
    "sha": "c1c7e89620b43f0924148e267a6da5d38450ce1f",
    "files": [
        {
            "sha": "e48b2599d4c29882992fdb5c0d2912dd79d63027",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -142,7 +142,9 @@\n         \"is_tensorboard_available\",\n         \"is_wandb_available\",\n     ],\n+    \"loss\": [],\n     \"modelcard\": [\"ModelCard\"],\n+    # Losses\n     \"modeling_tf_pytorch_utils\": [\n         \"convert_tf_weight_name_to_pt_weight_name\",\n         \"load_pytorch_checkpoint_in_tf2_model\","
        },
        {
            "sha": "8bc08ca625961e97993882ecff7a4a15b65e8b69",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -184,6 +184,9 @@ class PretrainedConfig(PushToHubMixin):\n             Whether the model should use legacy TensorFlow losses. Legacy losses have variable output shapes and may\n             not be XLA-compatible. This option is here for backward compatibility and will be removed in Transformers\n             v5.\n+        loss_type (`str`, *optional*):\n+            The type of loss that the model should use. It should be in `LOSS_MAPPING`'s keys, otherwise the loss will\n+            be automatically infered from the model architecture.\n     \"\"\"\n \n     model_type: str = \"\""
        },
        {
            "sha": "196860c9f1c60559826cf6eca845e79834b9c8fd",
            "filename": "src/transformers/loss/__init__.py",
            "status": "added",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2F__init__.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -0,0 +1,13 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License."
        },
        {
            "sha": "62080bcb3fd94fc0fc9051640744a81e03657d4e",
            "filename": "src/transformers/loss/loss_deformable_detr.py",
            "status": "added",
            "additions": 178,
            "deletions": 0,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_deformable_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -0,0 +1,178 @@\n+import torch\n+import torch.nn as nn\n+\n+from ..image_transforms import center_to_corners_format\n+from ..utils import is_scipy_available\n+from .loss_for_object_detection import (\n+    HungarianMatcher,\n+    ImageLoss,\n+    _set_aux_loss,\n+    generalized_box_iou,\n+    sigmoid_focal_loss,\n+)\n+\n+\n+if is_scipy_available():\n+    from scipy.optimize import linear_sum_assignment\n+\n+\n+class DeformableDetrHungarianMatcher(HungarianMatcher):\n+    @torch.no_grad()\n+    def forward(self, outputs, targets):\n+        \"\"\"\n+        Differences:\n+        - out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid() instead of softmax\n+        - class_cost uses alpha and gamma\n+        \"\"\"\n+        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n+\n+        # We flatten to compute the cost matrices in a batch\n+        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n+        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n+\n+        # Also concat the target labels and boxes\n+        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n+        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n+\n+        # Compute the classification cost.\n+        alpha = 0.25\n+        gamma = 2.0\n+        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n+        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n+        class_cost = pos_cost_class[:, target_ids] - neg_cost_class[:, target_ids]\n+\n+        # Compute the L1 cost between boxes\n+        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n+\n+        # Compute the giou cost between boxes\n+        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n+\n+        # Final cost matrix\n+        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n+        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n+\n+        sizes = [len(v[\"boxes\"]) for v in targets]\n+        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n+        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n+\n+\n+class DeformableDetrImageLoss(ImageLoss):\n+    def __init__(self, matcher, num_classes, focal_alpha, losses):\n+        nn.Module.__init__(self)\n+        self.matcher = matcher\n+        self.num_classes = num_classes\n+        self.focal_alpha = focal_alpha\n+        self.losses = losses\n+\n+    # removed logging parameter, which was part of the original implementation\n+    def loss_labels(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n+        of dim [nb_target_boxes]\n+        \"\"\"\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No logits were found in the outputs\")\n+        source_logits = outputs[\"logits\"]\n+\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n+        )\n+        target_classes[idx] = target_classes_o\n+\n+        target_classes_onehot = torch.zeros(\n+            [source_logits.shape[0], source_logits.shape[1], source_logits.shape[2] + 1],\n+            dtype=source_logits.dtype,\n+            layout=source_logits.layout,\n+            device=source_logits.device,\n+        )\n+        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n+\n+        target_classes_onehot = target_classes_onehot[:, :, :-1]\n+        loss_ce = (\n+            sigmoid_focal_loss(source_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2)\n+            * source_logits.shape[1]\n+        )\n+        losses = {\"loss_ce\": loss_ce}\n+\n+        return losses\n+\n+\n+def DeformableDetrForSegmentationLoss(\n+    logits, labels, device, pred_boxes, pred_masks, config, outputs_class=None, outputs_coord=None, **kwargs\n+):\n+    # First: create the matcher\n+    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n+    # Second: create the criterion\n+    losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n+    criterion = DeformableDetrImageLoss(\n+        matcher=matcher,\n+        num_classes=config.num_labels,\n+        focal_alpha=config.focal_alpha,\n+        losses=losses,\n+    )\n+    criterion.to(device)\n+    # Third: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    outputs_loss[\"pred_masks\"] = pred_masks\n+\n+    auxiliary_outputs = None\n+    if config.auxiliary_loss:\n+        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+    # Fourth: compute total loss, as a weighted sum of the various losses\n+    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n+    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n+    weight_dict[\"loss_mask\"] = config.mask_loss_coefficient\n+    weight_dict[\"loss_dice\"] = config.dice_loss_coefficient\n+    if config.auxiliary_loss:\n+        aux_weight_dict = {}\n+        for i in range(config.decoder_layers - 1):\n+            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n+        weight_dict.update(aux_weight_dict)\n+\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    return loss, loss_dict, auxiliary_outputs\n+\n+\n+def DeformableDetrForObjectDetectionLoss(\n+    logits, labels, device, pred_boxes, config, outputs_class=None, outputs_coord=None, **kwargs\n+):\n+    # First: create the matcher\n+    matcher = DeformableDetrHungarianMatcher(\n+        class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost\n+    )\n+    # Second: create the criterion\n+    losses = [\"labels\", \"boxes\", \"cardinality\"]\n+    criterion = DeformableDetrImageLoss(\n+        matcher=matcher,\n+        num_classes=config.num_labels,\n+        focal_alpha=config.focal_alpha,\n+        losses=losses,\n+    )\n+    criterion.to(device)\n+    # Third: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    auxiliary_outputs = None\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    if config.auxiliary_loss:\n+        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+    # Fourth: compute total loss, as a weighted sum of the various losses\n+    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n+    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n+    if config.auxiliary_loss:\n+        aux_weight_dict = {}\n+        for i in range(config.decoder_layers - 1):\n+            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n+        weight_dict.update(aux_weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "b820f6daed1224dace31fa30be24ac26094effd2",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "added",
            "additions": 562,
            "deletions": 0,
            "changes": 562,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -0,0 +1,562 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import List, Optional\n+\n+import torch\n+import torch.nn as nn\n+from torch import Tensor\n+\n+from ..utils import is_accelerate_available, is_scipy_available, is_vision_available, requires_backends\n+\n+\n+if is_accelerate_available():\n+    from accelerate import PartialState\n+    from accelerate.utils import reduce\n+\n+if is_scipy_available():\n+    from scipy.optimize import linear_sum_assignment\n+\n+\n+if is_vision_available():\n+    from transformers.image_transforms import center_to_corners_format\n+\n+\n+def dice_loss(inputs, targets, num_boxes):\n+    \"\"\"\n+    Compute the DICE loss, similar to generalized IOU for masks\n+\n+    Args:\n+        inputs: A float tensor of arbitrary shape.\n+                The predictions for each example.\n+        targets: A float tensor with the same shape as inputs. Stores the binary\n+                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n+                 class).\n+    \"\"\"\n+    inputs = inputs.sigmoid()\n+    inputs = inputs.flatten(1)\n+    numerator = 2 * (inputs * targets).sum(1)\n+    denominator = inputs.sum(-1) + targets.sum(-1)\n+    loss = 1 - (numerator + 1) / (denominator + 1)\n+    return loss.sum() / num_boxes\n+\n+\n+def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n+    \"\"\"\n+    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n+\n+    Args:\n+        inputs (`torch.FloatTensor` of arbitrary shape):\n+            The predictions for each example.\n+        targets (`torch.FloatTensor` with the same shape as `inputs`)\n+            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n+            and 1 for the positive class).\n+        alpha (`float`, *optional*, defaults to `0.25`):\n+            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n+        gamma (`int`, *optional*, defaults to `2`):\n+            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n+\n+    Returns:\n+        Loss tensor\n+    \"\"\"\n+    prob = inputs.sigmoid()\n+    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n+    # add modulating factor\n+    p_t = prob * targets + (1 - prob) * (1 - targets)\n+    loss = ce_loss * ((1 - p_t) ** gamma)\n+\n+    if alpha >= 0:\n+        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n+        loss = alpha_t * loss\n+\n+    return loss.mean(1).sum() / num_boxes\n+\n+\n+# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+class ImageLoss(nn.Module):\n+    \"\"\"\n+    This class computes the losses for DetrForObjectDetection/DetrForSegmentation. The process happens in two steps: 1)\n+    we compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair\n+    of matched ground-truth / prediction (supervise class and box).\n+\n+    A note on the `num_classes` argument (copied from original repo in detr.py): \"the naming of the `num_classes`\n+    parameter of the criterion is somewhat misleading. It indeed corresponds to `max_obj_id` + 1, where `max_obj_id` is\n+    the maximum id for a class in your dataset. For example, COCO has a `max_obj_id` of 90, so we pass `num_classes` to\n+    be 91. As another example, for a dataset that has a single class with `id` 1, you should pass `num_classes` to be 2\n+    (`max_obj_id` + 1). For more details on this, check the following discussion\n+    https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\"\n+\n+\n+    Args:\n+        matcher (`DetrHungarianMatcher`):\n+            Module able to compute a matching between targets and proposals.\n+        num_classes (`int`):\n+            Number of object categories, omitting the special no-object category.\n+        eos_coef (`float`):\n+            Relative classification weight applied to the no-object category.\n+        losses (`List[str]`):\n+            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n+    \"\"\"\n+\n+    def __init__(self, matcher, num_classes, eos_coef, losses):\n+        super().__init__()\n+        self.matcher = matcher\n+        self.num_classes = num_classes\n+        self.eos_coef = eos_coef\n+        self.losses = losses\n+        empty_weight = torch.ones(self.num_classes + 1)\n+        empty_weight[-1] = self.eos_coef\n+        self.register_buffer(\"empty_weight\", empty_weight)\n+\n+    # removed logging parameter, which was part of the original implementation\n+    def loss_labels(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n+        [nb_target_boxes]\n+        \"\"\"\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No logits were found in the outputs\")\n+        source_logits = outputs[\"logits\"]\n+\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n+        )\n+        target_classes[idx] = target_classes_o\n+\n+        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n+        losses = {\"loss_ce\": loss_ce}\n+\n+        return losses\n+\n+    @torch.no_grad()\n+    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n+\n+        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n+        \"\"\"\n+        logits = outputs[\"logits\"]\n+        device = logits.device\n+        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n+        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n+        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n+        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n+        losses = {\"cardinality_error\": card_err}\n+        return losses\n+\n+    def loss_boxes(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n+\n+        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n+        are expected in format (center_x, center_y, w, h), normalized by the image size.\n+        \"\"\"\n+        if \"pred_boxes\" not in outputs:\n+            raise KeyError(\"No predicted boxes found in outputs\")\n+        idx = self._get_source_permutation_idx(indices)\n+        source_boxes = outputs[\"pred_boxes\"][idx]\n+        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n+\n+        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n+\n+        losses = {}\n+        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n+\n+        loss_giou = 1 - torch.diag(\n+            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n+        )\n+        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n+        return losses\n+\n+    def loss_masks(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the losses related to the masks: the focal loss and the dice loss.\n+\n+        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n+        \"\"\"\n+        if \"pred_masks\" not in outputs:\n+            raise KeyError(\"No predicted masks found in outputs\")\n+\n+        source_idx = self._get_source_permutation_idx(indices)\n+        target_idx = self._get_target_permutation_idx(indices)\n+        source_masks = outputs[\"pred_masks\"]\n+        source_masks = source_masks[source_idx]\n+        masks = [t[\"masks\"] for t in targets]\n+        # TODO use valid to mask invalid areas due to padding in loss\n+        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n+        target_masks = target_masks.to(source_masks)\n+        target_masks = target_masks[target_idx]\n+\n+        # upsample predictions to the target size\n+        source_masks = nn.functional.interpolate(\n+            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n+        )\n+        source_masks = source_masks[:, 0].flatten(1)\n+\n+        target_masks = target_masks.flatten(1)\n+        target_masks = target_masks.view(source_masks.shape)\n+        losses = {\n+            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n+            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n+        }\n+        return losses\n+\n+    def _get_source_permutation_idx(self, indices):\n+        # permute predictions following indices\n+        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n+        source_idx = torch.cat([source for (source, _) in indices])\n+        return batch_idx, source_idx\n+\n+    def _get_target_permutation_idx(self, indices):\n+        # permute targets following indices\n+        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n+        target_idx = torch.cat([target for (_, target) in indices])\n+        return batch_idx, target_idx\n+\n+    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n+        loss_map = {\n+            \"labels\": self.loss_labels,\n+            \"cardinality\": self.loss_cardinality,\n+            \"boxes\": self.loss_boxes,\n+            \"masks\": self.loss_masks,\n+        }\n+        if loss not in loss_map:\n+            raise ValueError(f\"Loss {loss} not supported\")\n+        return loss_map[loss](outputs, targets, indices, num_boxes)\n+\n+    def forward(self, outputs, targets):\n+        \"\"\"\n+        This performs the loss computation.\n+\n+        Args:\n+             outputs (`dict`, *optional*):\n+                Dictionary of tensors, see the output specification of the model for the format.\n+             targets (`List[dict]`, *optional*):\n+                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n+                losses applied, see each loss' doc.\n+        \"\"\"\n+        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n+\n+        # Retrieve the matching between the outputs of the last layer and the targets\n+        indices = self.matcher(outputs_without_aux, targets)\n+\n+        # Compute the average number of target boxes across all nodes, for normalization purposes\n+        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n+        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n+        world_size = 1\n+        if is_accelerate_available():\n+            if PartialState._shared_state != {}:\n+                num_boxes = reduce(num_boxes)\n+                world_size = PartialState().num_processes\n+        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n+\n+        # Compute all the requested losses\n+        losses = {}\n+        for loss in self.losses:\n+            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n+\n+        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n+        if \"auxiliary_outputs\" in outputs:\n+            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n+                indices = self.matcher(auxiliary_outputs, targets)\n+                for loss in self.losses:\n+                    if loss == \"masks\":\n+                        # Intermediate masks losses are too costly to compute, we ignore them.\n+                        continue\n+                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n+                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n+                    losses.update(l_dict)\n+\n+        return losses\n+\n+\n+# taken from https://github.com/facebookresearch/detr/blob/master/models/matcher.py\n+class HungarianMatcher(nn.Module):\n+    \"\"\"\n+    This class computes an assignment between the targets and the predictions of the network.\n+\n+    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n+    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n+    un-matched (and thus treated as non-objects).\n+\n+    Args:\n+        class_cost:\n+            The relative weight of the classification error in the matching cost.\n+        bbox_cost:\n+            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n+        giou_cost:\n+            The relative weight of the giou loss of the bounding box in the matching cost.\n+    \"\"\"\n+\n+    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n+        super().__init__()\n+        requires_backends(self, [\"scipy\"])\n+\n+        self.class_cost = class_cost\n+        self.bbox_cost = bbox_cost\n+        self.giou_cost = giou_cost\n+        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n+            raise ValueError(\"All costs of the Matcher can't be 0\")\n+\n+    @torch.no_grad()\n+    def forward(self, outputs, targets):\n+        \"\"\"\n+        Args:\n+            outputs (`dict`):\n+                A dictionary that contains at least these entries:\n+                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n+                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n+            targets (`List[dict]`):\n+                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n+                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n+                  ground-truth\n+                 objects in the target) containing the class labels\n+                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n+\n+        Returns:\n+            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n+            - index_i is the indices of the selected predictions (in order)\n+            - index_j is the indices of the corresponding selected targets (in order)\n+            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n+        \"\"\"\n+        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n+\n+        # We flatten to compute the cost matrices in a batch\n+        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n+        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n+\n+        # Also concat the target labels and boxes\n+        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n+        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n+\n+        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n+        # but approximate it in 1 - proba[target class].\n+        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+        class_cost = -out_prob[:, target_ids]\n+\n+        # Compute the L1 cost between boxes\n+        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n+\n+        # Compute the giou cost between boxes\n+        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n+\n+        # Final cost matrix\n+        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n+        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n+\n+        sizes = [len(v[\"boxes\"]) for v in targets]\n+        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n+        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n+\n+\n+# below: bounding box utilities taken from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\n+\n+\n+def _upcast(t: Tensor) -> Tensor:\n+    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n+    if t.is_floating_point():\n+        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n+    else:\n+        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n+\n+\n+def box_area(boxes: Tensor) -> Tensor:\n+    \"\"\"\n+    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n+\n+    Args:\n+        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n+            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n+            < x2` and `0 <= y1 < y2`.\n+\n+    Returns:\n+        `torch.FloatTensor`: a tensor containing the area for each box.\n+    \"\"\"\n+    boxes = _upcast(boxes)\n+    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n+\n+\n+# modified from torchvision to also return the union\n+def box_iou(boxes1, boxes2):\n+    area1 = box_area(boxes1)\n+    area2 = box_area(boxes2)\n+\n+    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n+    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n+\n+    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n+    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n+\n+    union = area1[:, None] + area2 - inter\n+\n+    iou = inter / union\n+    return iou, union\n+\n+\n+def generalized_box_iou(boxes1, boxes2):\n+    \"\"\"\n+    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n+\n+    Returns:\n+        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n+    \"\"\"\n+    # degenerate boxes gives inf / nan results\n+    # so do an early check\n+    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n+        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n+    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n+        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n+    iou, union = box_iou(boxes1, boxes2)\n+\n+    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n+    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n+\n+    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n+    area = width_height[:, :, 0] * width_height[:, :, 1]\n+\n+    return iou - (area - union) / area\n+\n+\n+# below: taken from https://github.com/facebookresearch/detr/blob/master/util/misc.py#L306\n+def _max_by_axis(the_list):\n+    # type: (List[List[int]]) -> List[int]\n+    maxes = the_list[0]\n+    for sublist in the_list[1:]:\n+        for index, item in enumerate(sublist):\n+            maxes[index] = max(maxes[index], item)\n+    return maxes\n+\n+\n+class NestedTensor:\n+    def __init__(self, tensors, mask: Optional[Tensor]):\n+        self.tensors = tensors\n+        self.mask = mask\n+\n+    def to(self, device):\n+        cast_tensor = self.tensors.to(device)\n+        mask = self.mask\n+        if mask is not None:\n+            cast_mask = mask.to(device)\n+        else:\n+            cast_mask = None\n+        return NestedTensor(cast_tensor, cast_mask)\n+\n+    def decompose(self):\n+        return self.tensors, self.mask\n+\n+    def __repr__(self):\n+        return str(self.tensors)\n+\n+\n+def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n+    if tensor_list[0].ndim == 3:\n+        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n+        batch_shape = [len(tensor_list)] + max_size\n+        batch_size, num_channels, height, width = batch_shape\n+        dtype = tensor_list[0].dtype\n+        device = tensor_list[0].device\n+        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n+        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n+        for img, pad_img, m in zip(tensor_list, tensor, mask):\n+            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n+            m[: img.shape[1], : img.shape[2]] = False\n+    else:\n+        raise ValueError(\"Only 3-dimensional tensors are supported\")\n+    return NestedTensor(tensor, mask)\n+\n+\n+# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+@torch.jit.unused\n+def _set_aux_loss(outputs_class, outputs_coord):\n+    # this is a workaround to make torchscript happy, as torchscript\n+    # doesn't support dictionary with non-homogeneous values, such\n+    # as a dict having both a Tensor and a list.\n+    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n+\n+\n+def ForSegmentationLoss(\n+    logits, labels, device, pred_boxes, pred_masks, config, outputs_class=None, outputs_coord=None, **kwargs\n+):\n+    # First: create the matcher\n+    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n+    # Second: create the criterion\n+    losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n+    criterion = ImageLoss(\n+        matcher=matcher,\n+        num_classes=config.num_labels,\n+        eos_coef=config.eos_coefficient,\n+        losses=losses,\n+    )\n+    criterion.to(device)\n+    # Third: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    outputs_loss[\"pred_masks\"] = pred_masks\n+\n+    auxiliary_outputs = None\n+    if config.auxiliary_loss:\n+        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+    # Fourth: compute total loss, as a weighted sum of the various losses\n+    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n+    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n+    weight_dict[\"loss_mask\"] = config.mask_loss_coefficient\n+    weight_dict[\"loss_dice\"] = config.dice_loss_coefficient\n+    if config.auxiliary_loss:\n+        aux_weight_dict = {}\n+        for i in range(config.decoder_layers - 1):\n+            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n+        weight_dict.update(aux_weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    return loss, loss_dict, auxiliary_outputs\n+\n+\n+def ForObjectDetectionLoss(\n+    logits, labels, device, pred_boxes, config, outputs_class=None, outputs_coord=None, **kwargs\n+):\n+    # First: create the matcher\n+    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n+    # Second: create the criterion\n+    losses = [\"labels\", \"boxes\", \"cardinality\"]\n+    criterion = ImageLoss(\n+        matcher=matcher,\n+        num_classes=config.num_labels,\n+        eos_coef=config.eos_coefficient,\n+        losses=losses,\n+    )\n+    criterion.to(device)\n+    # Third: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    auxiliary_outputs = None\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    if config.auxiliary_loss:\n+        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+    # Fourth: compute total loss, as a weighted sum of the various losses\n+    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n+    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n+    if config.auxiliary_loss:\n+        aux_weight_dict = {}\n+        for i in range(config.decoder_layers - 1):\n+            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n+        weight_dict.update(aux_weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "3aea87c5f5a75a9aa99dbbfcfc5a50091e22d8c8",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "added",
            "additions": 463,
            "deletions": 0,
            "changes": 463,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -0,0 +1,463 @@\n+# Copyright 2020 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ..utils import is_scipy_available, is_vision_available, requires_backends\n+from .loss_for_object_detection import (\n+    _set_aux_loss,\n+    box_iou,\n+    dice_loss,\n+    generalized_box_iou,\n+    nested_tensor_from_tensor_list,\n+    sigmoid_focal_loss,\n+)\n+\n+\n+if is_scipy_available():\n+    from scipy.optimize import linear_sum_assignment\n+\n+\n+if is_vision_available():\n+    from transformers.image_transforms import center_to_corners_format\n+\n+\n+class RTDetrHungarianMatcher(nn.Module):\n+    \"\"\"This class computes an assignment between the targets and the predictions of the network\n+\n+    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n+    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n+    un-matched (and thus treated as non-objects).\n+\n+    Args:\n+        config: RTDetrConfig\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        requires_backends(self, [\"scipy\"])\n+\n+        self.class_cost = config.matcher_class_cost\n+        self.bbox_cost = config.matcher_bbox_cost\n+        self.giou_cost = config.matcher_giou_cost\n+\n+        self.use_focal_loss = config.use_focal_loss\n+        self.alpha = config.matcher_alpha\n+        self.gamma = config.matcher_gamma\n+\n+        if self.class_cost == self.bbox_cost == self.giou_cost == 0:\n+            raise ValueError(\"All costs of the Matcher can't be 0\")\n+\n+    @torch.no_grad()\n+    def forward(self, outputs, targets):\n+        \"\"\"Performs the matching\n+\n+        Params:\n+            outputs: This is a dict that contains at least these entries:\n+                 \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n+                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n+\n+            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n+                 \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n+                           objects in the target) containing the class labels\n+                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n+\n+        Returns:\n+            A list of size batch_size, containing tuples of (index_i, index_j) where:\n+                - index_i is the indices of the selected predictions (in order)\n+                - index_j is the indices of the corresponding selected targets (in order)\n+            For each batch element, it holds:\n+                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n+        \"\"\"\n+        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n+\n+        # We flatten to compute the cost matrices in a batch\n+        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n+        # Also concat the target labels and boxes\n+        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n+        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n+        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n+        # but approximate it in 1 - proba[target class].\n+        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+        if self.use_focal_loss:\n+            out_prob = F.sigmoid(outputs[\"logits\"].flatten(0, 1))\n+            out_prob = out_prob[:, target_ids]\n+            neg_cost_class = (1 - self.alpha) * (out_prob**self.gamma) * (-(1 - out_prob + 1e-8).log())\n+            pos_cost_class = self.alpha * ((1 - out_prob) ** self.gamma) * (-(out_prob + 1e-8).log())\n+            class_cost = pos_cost_class - neg_cost_class\n+        else:\n+            out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n+            class_cost = -out_prob[:, target_ids]\n+\n+        # Compute the L1 cost between boxes\n+        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n+        # Compute the giou cost betwen boxes\n+        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n+        # Compute the final cost matrix\n+        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n+        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n+\n+        sizes = [len(v[\"boxes\"]) for v in targets]\n+        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n+\n+        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n+\n+\n+class RTDetrLoss(nn.Module):\n+    \"\"\"\n+    This class computes the losses for RTDetr. The process happens in two steps: 1) we compute hungarian assignment\n+    between ground truth boxes and the outputs of the model 2) we supervise each pair of matched ground-truth /\n+    prediction (supervise class and box).\n+\n+    Args:\n+        matcher (`DetrHungarianMatcher`):\n+            Module able to compute a matching between targets and proposals.\n+        weight_dict (`Dict`):\n+            Dictionary relating each loss with its weights. These losses are configured in RTDetrConf as\n+            `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`\n+        losses (`List[str]`):\n+            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n+        alpha (`float`):\n+            Parameter alpha used to compute the focal loss.\n+        gamma (`float`):\n+            Parameter gamma used to compute the focal loss.\n+        eos_coef (`float`):\n+            Relative classification weight applied to the no-object category.\n+        num_classes (`int`):\n+            Number of object categories, omitting the special no-object category.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.matcher = RTDetrHungarianMatcher(config)\n+        self.num_classes = config.num_labels\n+        self.weight_dict = {\n+            \"loss_vfl\": config.weight_loss_vfl,\n+            \"loss_bbox\": config.weight_loss_bbox,\n+            \"loss_giou\": config.weight_loss_giou,\n+        }\n+        self.losses = [\"vfl\", \"boxes\"]\n+        self.eos_coef = config.eos_coefficient\n+        empty_weight = torch.ones(config.num_labels + 1)\n+        empty_weight[-1] = self.eos_coef\n+        self.register_buffer(\"empty_weight\", empty_weight)\n+        self.alpha = config.focal_loss_alpha\n+        self.gamma = config.focal_loss_gamma\n+\n+    def loss_labels_vfl(self, outputs, targets, indices, num_boxes, log=True):\n+        if \"pred_boxes\" not in outputs:\n+            raise KeyError(\"No predicted boxes found in outputs\")\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No predicted logits found in outputs\")\n+        idx = self._get_source_permutation_idx(indices)\n+\n+        src_boxes = outputs[\"pred_boxes\"][idx]\n+        target_boxes = torch.cat([_target[\"boxes\"][i] for _target, (_, i) in zip(targets, indices)], dim=0)\n+        ious, _ = box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n+        ious = torch.diag(ious).detach()\n+\n+        src_logits = outputs[\"logits\"]\n+        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n+        )\n+        target_classes[idx] = target_classes_original\n+        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n+\n+        target_score_original = torch.zeros_like(target_classes, dtype=src_logits.dtype)\n+        target_score_original[idx] = ious.to(target_score_original.dtype)\n+        target_score = target_score_original.unsqueeze(-1) * target\n+\n+        pred_score = F.sigmoid(src_logits).detach()\n+        weight = self.alpha * pred_score.pow(self.gamma) * (1 - target) + target_score\n+\n+        loss = F.binary_cross_entropy_with_logits(src_logits, target_score, weight=weight, reduction=\"none\")\n+        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n+        return {\"loss_vfl\": loss}\n+\n+    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n+        \"\"\"Classification loss (NLL)\n+        targets dicts must contain the key \"class_labels\" containing a tensor of dim [nb_target_boxes]\n+        \"\"\"\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No logits were found in the outputs\")\n+\n+        src_logits = outputs[\"logits\"]\n+\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n+        )\n+        target_classes[idx] = target_classes_original\n+\n+        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.class_weight)\n+        losses = {\"loss_ce\": loss_ce}\n+        return losses\n+\n+    @torch.no_grad()\n+    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes. This is not\n+        really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n+        \"\"\"\n+        logits = outputs[\"logits\"]\n+        device = logits.device\n+        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n+        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n+        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n+        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n+        losses = {\"cardinality_error\": card_err}\n+        return losses\n+\n+    def loss_boxes(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss. Targets dicts must\n+        contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes are expected in\n+        format (center_x, center_y, w, h), normalized by the image size.\n+        \"\"\"\n+        if \"pred_boxes\" not in outputs:\n+            raise KeyError(\"No predicted boxes found in outputs\")\n+        idx = self._get_source_permutation_idx(indices)\n+        src_boxes = outputs[\"pred_boxes\"][idx]\n+        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n+\n+        losses = {}\n+\n+        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\"none\")\n+        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n+\n+        loss_giou = 1 - torch.diag(\n+            generalized_box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n+        )\n+        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n+        return losses\n+\n+    def loss_masks(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Compute the losses related to the masks: the focal loss and the dice loss. Targets dicts must contain the key\n+        \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n+        \"\"\"\n+        if \"pred_masks\" not in outputs:\n+            raise KeyError(\"No predicted masks found in outputs\")\n+\n+        source_idx = self._get_source_permutation_idx(indices)\n+        target_idx = self._get_target_permutation_idx(indices)\n+        source_masks = outputs[\"pred_masks\"]\n+        source_masks = source_masks[source_idx]\n+        masks = [t[\"masks\"] for t in targets]\n+        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n+        target_masks = target_masks.to(source_masks)\n+        target_masks = target_masks[target_idx]\n+\n+        # upsample predictions to the target size\n+        source_masks = nn.functional.interpolate(\n+            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n+        )\n+        source_masks = source_masks[:, 0].flatten(1)\n+\n+        target_masks = target_masks.flatten(1)\n+        target_masks = target_masks.view(source_masks.shape)\n+        losses = {\n+            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n+            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n+        }\n+        return losses\n+\n+    def loss_labels_bce(self, outputs, targets, indices, num_boxes, log=True):\n+        src_logits = outputs[\"logits\"]\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n+        )\n+        target_classes[idx] = target_classes_original\n+\n+        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n+        loss = F.binary_cross_entropy_with_logits(src_logits, target * 1.0, reduction=\"none\")\n+        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n+        return {\"loss_bce\": loss}\n+\n+    def _get_source_permutation_idx(self, indices):\n+        # permute predictions following indices\n+        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n+        source_idx = torch.cat([source for (source, _) in indices])\n+        return batch_idx, source_idx\n+\n+    def _get_target_permutation_idx(self, indices):\n+        # permute targets following indices\n+        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n+        target_idx = torch.cat([target for (_, target) in indices])\n+        return batch_idx, target_idx\n+\n+    def loss_labels_focal(self, outputs, targets, indices, num_boxes, log=True):\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No logits found in outputs\")\n+\n+        src_logits = outputs[\"logits\"]\n+\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n+        target_classes = torch.full(\n+            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n+        )\n+        target_classes[idx] = target_classes_original\n+\n+        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n+        loss = sigmoid_focal_loss(src_logits, target, self.alpha, self.gamma)\n+        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n+        return {\"loss_focal\": loss}\n+\n+    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n+        loss_map = {\n+            \"labels\": self.loss_labels,\n+            \"cardinality\": self.loss_cardinality,\n+            \"boxes\": self.loss_boxes,\n+            \"masks\": self.loss_masks,\n+            \"bce\": self.loss_labels_bce,\n+            \"focal\": self.loss_labels_focal,\n+            \"vfl\": self.loss_labels_vfl,\n+        }\n+        if loss not in loss_map:\n+            raise ValueError(f\"Loss {loss} not supported\")\n+        return loss_map[loss](outputs, targets, indices, num_boxes)\n+\n+    @staticmethod\n+    def get_cdn_matched_indices(dn_meta, targets):\n+        dn_positive_idx, dn_num_group = dn_meta[\"dn_positive_idx\"], dn_meta[\"dn_num_group\"]\n+        num_gts = [len(t[\"class_labels\"]) for t in targets]\n+        device = targets[0][\"class_labels\"].device\n+\n+        dn_match_indices = []\n+        for i, num_gt in enumerate(num_gts):\n+            if num_gt > 0:\n+                gt_idx = torch.arange(num_gt, dtype=torch.int64, device=device)\n+                gt_idx = gt_idx.tile(dn_num_group)\n+                assert len(dn_positive_idx[i]) == len(gt_idx)\n+                dn_match_indices.append((dn_positive_idx[i], gt_idx))\n+            else:\n+                dn_match_indices.append(\n+                    (\n+                        torch.zeros(0, dtype=torch.int64, device=device),\n+                        torch.zeros(0, dtype=torch.int64, device=device),\n+                    )\n+                )\n+\n+        return dn_match_indices\n+\n+    def forward(self, outputs, targets):\n+        \"\"\"\n+        This performs the loss computation.\n+\n+        Args:\n+             outputs (`dict`, *optional*):\n+                Dictionary of tensors, see the output specification of the model for the format.\n+             targets (`List[dict]`, *optional*):\n+                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n+                losses applied, see each loss' doc.\n+        \"\"\"\n+        outputs_without_aux = {k: v for k, v in outputs.items() if \"auxiliary_outputs\" not in k}\n+\n+        # Retrieve the matching between the outputs of the last layer and the targets\n+        indices = self.matcher(outputs_without_aux, targets)\n+\n+        # Compute the average number of target boxes across all nodes, for normalization purposes\n+        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n+        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n+        num_boxes = torch.clamp(num_boxes, min=1).item()\n+\n+        # Compute all the requested losses\n+        losses = {}\n+        for loss in self.losses:\n+            l_dict = self.get_loss(loss, outputs, targets, indices, num_boxes)\n+            l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n+            losses.update(l_dict)\n+\n+        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n+        if \"auxiliary_outputs\" in outputs:\n+            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n+                indices = self.matcher(auxiliary_outputs, targets)\n+                for loss in self.losses:\n+                    if loss == \"masks\":\n+                        # Intermediate masks losses are too costly to compute, we ignore them.\n+                        continue\n+                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n+                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n+                    l_dict = {k + f\"_aux_{i}\": v for k, v in l_dict.items()}\n+                    losses.update(l_dict)\n+\n+        # In case of cdn auxiliary losses. For rtdetr\n+        if \"dn_auxiliary_outputs\" in outputs:\n+            if \"denoising_meta_values\" not in outputs:\n+                raise ValueError(\n+                    \"The output must have the 'denoising_meta_values` key. Please, ensure that 'outputs' includes a 'denoising_meta_values' entry.\"\n+                )\n+            indices = self.get_cdn_matched_indices(outputs[\"denoising_meta_values\"], targets)\n+            num_boxes = num_boxes * outputs[\"denoising_meta_values\"][\"dn_num_group\"]\n+\n+            for i, auxiliary_outputs in enumerate(outputs[\"dn_auxiliary_outputs\"]):\n+                # indices = self.matcher(auxiliary_outputs, targets)\n+                for loss in self.losses:\n+                    if loss == \"masks\":\n+                        # Intermediate masks losses are too costly to compute, we ignore them.\n+                        continue\n+                    kwargs = {}\n+                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes, **kwargs)\n+                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n+                    l_dict = {k + f\"_dn_{i}\": v for k, v in l_dict.items()}\n+                    losses.update(l_dict)\n+\n+        return losses\n+\n+\n+def RTDetrForObjectDetectionLoss(\n+    logits,\n+    labels,\n+    device,\n+    pred_boxes,\n+    config,\n+    outputs_class=None,\n+    outputs_coord=None,\n+    enc_topk_logits=None,\n+    enc_topk_bboxes=None,\n+    denoising_meta_values=None,\n+    **kwargs,\n+):\n+    criterion = RTDetrLoss(config)\n+    criterion.to(device)\n+    # Second: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    if config.auxiliary_loss:\n+        if denoising_meta_values is not None:\n+            dn_out_coord, outputs_coord = torch.split(outputs_coord, denoising_meta_values[\"dn_num_split\"], dim=2)\n+            dn_out_class, outputs_class = torch.split(outputs_class, denoising_meta_values[\"dn_num_split\"], dim=2)\n+\n+        auxiliary_outputs = _set_aux_loss(outputs_class[:, :-1].transpose(0, 1), outputs_coord[:, :-1].transpose(0, 1))\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+        outputs_loss[\"auxiliary_outputs\"].extend(_set_aux_loss([enc_topk_logits], [enc_topk_bboxes]))\n+        if denoising_meta_values is not None:\n+            outputs_loss[\"dn_auxiliary_outputs\"] = _set_aux_loss(\n+                dn_out_class.transpose(0, 1), dn_out_coord.transpose(0, 1)\n+            )\n+            outputs_loss[\"denoising_meta_values\"] = denoising_meta_values\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+\n+    loss = sum(loss_dict.values())\n+    return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "efa23d24e360b4dce21121a301e8ba7a6f79230b",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -0,0 +1,114 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, MSELoss\n+\n+from .loss_deformable_detr import DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n+from .loss_for_object_detection import ForObjectDetectionLoss, ForSegmentationLoss\n+from .loss_rt_detr import RTDetrForObjectDetectionLoss\n+\n+\n+def fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs):\n+    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n+    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n+    if reduction == \"sum\":\n+        loss = loss / num_items_in_batch\n+    return loss\n+\n+\n+def ForCausalLMLoss(\n+    logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs\n+):\n+    # Upcast to float if we need to compute the loss to avoid potential precision issues\n+    logits = logits.float()\n+    # Shift so that tokens < n predict n\n+    shift_logits = logits[..., :-1, :].contiguous()\n+    shift_labels = labels[..., 1:].contiguous()\n+\n+    # Flatten the tokens\n+    shift_logits = shift_logits.view(-1, vocab_size)\n+    shift_labels = shift_labels.view(-1)\n+    # Enable model parallelism\n+    shift_labels = shift_labels.to(shift_logits.device)\n+    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n+    return loss\n+\n+\n+def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n+    num_labels = config.num_labels\n+    if config.problem_type is None:\n+        if num_labels == 1:\n+            config.problem_type = \"regression\"\n+        elif num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+            config.problem_type = \"single_label_classification\"\n+        else:\n+            config.problem_type = \"multi_label_classification\"\n+\n+    if config.problem_type == \"regression\":\n+        loss_fct = MSELoss()\n+        if num_labels == 1:\n+            loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n+        else:\n+            loss = loss_fct(pooled_logits, labels)\n+    elif config.problem_type == \"single_label_classification\":\n+        loss = fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)\n+    elif config.problem_type == \"multi_label_classification\":\n+        loss_fct = BCEWithLogitsLoss()\n+        loss = loss_fct(pooled_logits, labels)\n+    return loss\n+\n+\n+def ForQuestionAnsweringLoss(start_logits, end_logits, start_positions, end_positions, **kwargs):\n+    total_loss = None\n+    if start_positions is not None and end_positions is not None:\n+        # If we are on multi-GPU, split add a dimension\n+        if len(start_positions.size()) > 1:\n+            start_positions = start_positions.squeeze(-1).to(start_logits.device)\n+        if len(end_positions.size()) > 1:\n+            end_positions = end_positions.squeeze(-1).to(end_logits.device)\n+        # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+        ignored_index = start_logits.size(1)\n+        start_positions = start_positions.clamp(0, ignored_index)\n+        end_positions = end_positions.clamp(0, ignored_index)\n+\n+        start_loss = fixed_cross_entropy(start_logits, start_positions, ignore_index=ignored_index, **kwargs)\n+        end_loss = fixed_cross_entropy(end_logits, end_positions, ignore_index=ignored_index, **kwargs)\n+        total_loss = (start_loss + end_loss) / 2\n+    return total_loss\n+\n+\n+def ForTokenClassification(logits, labels, config, **kwargs):\n+    # Upcast to float if we need to compute the loss to avoid potential precision issues\n+    logits = logits.view(-1, config.num_labels)\n+    labels = labels.view(-1)\n+    logits = logits.float()\n+    # Flatten the tokens\n+    return fixed_cross_entropy(logits, labels, **kwargs)\n+\n+\n+LOSS_MAPPING = {\n+    \"ForCausalLM\": ForCausalLMLoss,\n+    \"ForQuestionAnswering\": ForQuestionAnsweringLoss,\n+    \"ForSequenceClassification\": ForSequenceClassificationLoss,\n+    \"ForTokenClassification\": ForTokenClassification,\n+    \"ForSegmentation\": ForSegmentationLoss,\n+    \"ForObjectDetection\": ForObjectDetectionLoss,\n+    \"DeformableDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"GroundingDinoForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n+    \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n+}"
        },
        {
            "sha": "c84aec21a32663d90bfdc63862ce5daf6b8bedf9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -28,7 +28,7 @@\n import warnings\n from contextlib import contextmanager\n from dataclasses import dataclass\n-from functools import partial, wraps\n+from functools import lru_cache, partial, wraps\n from threading import Thread\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n from zipfile import is_zipfile\n@@ -45,6 +45,7 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n+from .loss.loss_utils import LOSS_MAPPING\n from .pytorch_utils import (  # noqa: F401\n     Conv1D,\n     apply_chunking_to_forward,\n@@ -4979,6 +4980,28 @@ def _is_quantized_training_enabled(self):\n \n         return self.hf_quantizer.is_trainable\n \n+    @property\n+    @lru_cache\n+    def loss_function(self):\n+        if getattr(self.config, \"loss_type\", None) is not None:\n+            loss_type = self.config.loss_type\n+        else:\n+            loss_type = self.__class__.__name__\n+            if loss_type not in LOSS_MAPPING:\n+                loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n+                loss_type = re.findall(loss_groups, self.__class__.__name__)\n+                if len(loss_type) > 0:\n+                    loss_type = loss_type[0]\n+                else:\n+                    loss_type = None\n+        if loss_type is None or loss_type not in LOSS_MAPPING and getattr(self.config, \"loss_type\", None) is not None:\n+            logger.warning_once(\n+                f\"`loss_type={loss_type}` was set in the config but it is unrecognised.\"\n+                f\"Using the default loss: `ForCausalLMLoss`.\"\n+            )\n+            loss_type = \"ForCausalLM\"\n+        return LOSS_MAPPING[loss_type]\n+\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        },
        {
            "sha": "3abe6ef86445004c95c722f268c21381f1efd4d2",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -28,7 +28,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1173,18 +1172,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "d633b92547d7db64fcfc735d55e2345dd577b395",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 32,
            "deletions": 560,
            "changes": 592,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -29,10 +29,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_accelerate_available,\n-    is_scipy_available,\n     is_timm_available,\n-    is_vision_available,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -41,18 +38,9 @@\n from .configuration_conditional_detr import ConditionalDetrConfig\n \n \n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n-\n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n if is_timm_available():\n     from timm import create_model\n \n-if is_vision_available():\n-    from ...image_transforms import center_to_corners_format\n \n logger = logging.get_logger(__name__)\n \n@@ -1610,6 +1598,28 @@ def forward(\n         )\n \n \n+# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->ConditionalDetr\n+class ConditionalDetrMLPPredictionHead(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+\n+    \"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [hidden_dim] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     CONDITIONAL_DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n@@ -1723,7 +1733,7 @@ def forward(\n \n         reference = outputs.reference_points if return_dict else outputs[-1]\n         reference_before_sigmoid = inverse_sigmoid(reference).transpose(0, 1)\n-        outputs_coords = []\n+\n         hs = sequence_output\n         tmp = self.bbox_predictor(hs)\n         tmp[..., :2] += reference_before_sigmoid\n@@ -1732,47 +1742,20 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = ConditionalDetrHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = ConditionalDetrLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                focal_alpha=self.config.focal_alpha,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n+                outputs_coords = []\n                 intermediate = outputs.intermediate_hidden_states if return_dict else outputs[4]\n                 outputs_class = self.class_labels_classifier(intermediate)\n-\n                 for lvl in range(intermediate.shape[0]):\n                     tmp = self.bbox_predictor(intermediate[lvl])\n                     tmp[..., :2] += reference_before_sigmoid\n                     outputs_coord = tmp.sigmoid()\n                     outputs_coords.append(outputs_coord)\n                 outputs_coord = torch.stack(outputs_coords)\n-\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": self.config.cls_loss_coefficient, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -1977,43 +1960,14 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = ConditionalDetrHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n-            criterion = ConditionalDetrLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                focal_alpha=self.config.focal_alpha,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n-            outputs_loss[\"pred_masks\"] = pred_masks\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n                 intermediate = decoder_outputs.intermediate_hidden_states if return_dict else decoder_outputs[-1]\n                 outputs_class = self.conditional_detr.class_labels_classifier(intermediate)\n                 outputs_coord = self.conditional_detr.bbox_predictor(intermediate).sigmoid()\n-                auxiliary_outputs = self.conditional_detr._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            weight_dict[\"loss_mask\"] = self.config.mask_loss_coefficient\n-            weight_dict[\"loss_dice\"] = self.config.dice_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, pred_masks, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -2151,485 +2105,3 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-class ConditionalDetrLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for ConditionalDetrForObjectDetection/ConditionalDetrForSegmentation. The process\n-    happens in two steps: 1) we compute hungarian assignment between ground truth boxes and the outputs of the model 2)\n-    we supervise each pair of matched ground-truth / prediction (supervise class and box).\n-\n-    Args:\n-        matcher (`ConditionalDetrHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        focal_alpha (`float`):\n-            Alpha parameter in focal loss.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss.__init__\n-    def __init__(self, matcher, num_classes, focal_alpha, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.focal_alpha = focal_alpha\n-        self.losses = losses\n-\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss.loss_labels\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n-        of dim [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        target_classes_onehot = torch.zeros(\n-            [source_logits.shape[0], source_logits.shape[1], source_logits.shape[2] + 1],\n-            dtype=source_logits.dtype,\n-            layout=source_logits.layout,\n-            device=source_logits.device,\n-        )\n-        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n-\n-        target_classes_onehot = target_classes_onehot[:, :, :-1]\n-        loss_ce = (\n-            sigmoid_focal_loss(source_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2)\n-            * source_logits.shape[1]\n-        )\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss.loss_cardinality\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss.loss_boxes\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.loss_masks\n-    def loss_masks(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the masks: the focal loss and the dice loss.\n-\n-        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n-        \"\"\"\n-        if \"pred_masks\" not in outputs:\n-            raise KeyError(\"No predicted masks found in outputs\")\n-\n-        source_idx = self._get_source_permutation_idx(indices)\n-        target_idx = self._get_target_permutation_idx(indices)\n-        source_masks = outputs[\"pred_masks\"]\n-        source_masks = source_masks[source_idx]\n-        masks = [t[\"masks\"] for t in targets]\n-        # TODO use valid to mask invalid areas due to padding in loss\n-        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n-        target_masks = target_masks.to(source_masks)\n-        target_masks = target_masks[target_idx]\n-\n-        # upsample predictions to the target size\n-        source_masks = nn.functional.interpolate(\n-            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n-        )\n-        source_masks = source_masks[:, 0].flatten(1)\n-\n-        target_masks = target_masks.flatten(1)\n-        target_masks = target_masks.view(source_masks.shape)\n-        losses = {\n-            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n-            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n-        }\n-        return losses\n-\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss._get_source_permutation_idx\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss._get_target_permutation_idx\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.get_loss\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-            \"masks\": self.loss_masks,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.forward\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes across all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        return losses\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->ConditionalDetr\n-class ConditionalDetrMLPPredictionHead(nn.Module):\n-    \"\"\"\n-    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n-    height and width of a bounding box w.r.t. an image.\n-\n-    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-\n-    \"\"\"\n-\n-    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        h = [hidden_dim] * (num_layers - 1)\n-        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n-\n-    def forward(self, x):\n-        for i, layer in enumerate(self.layers):\n-            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n-        return x\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrHungarianMatcher with DeformableDetr->ConditionalDetr\n-class ConditionalDetrHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost.\n-        alpha = 0.25\n-        gamma = 2.0\n-        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n-        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n-        class_cost = pos_cost_class[:, target_ids] - neg_cost_class[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)"
        },
        {
            "sha": "f47221e55ad8adaaacfbb34e282605aec3bc5d18",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 24,
            "deletions": 509,
            "changes": 533,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -37,12 +37,9 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_accelerate_available,\n     is_ninja_available,\n-    is_scipy_available,\n     is_timm_available,\n     is_torch_cuda_available,\n-    is_vision_available,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -86,23 +83,10 @@ def load_cuda_kernels():\n     )\n \n \n-if is_vision_available():\n-    from transformers.image_transforms import center_to_corners_format\n-\n-\n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n-\n-\n if is_timm_available():\n     from timm import create_model\n \n \n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"DeformableDetrConfig\"\n@@ -1869,6 +1853,28 @@ def forward(\n         )\n \n \n+# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead\n+class DeformableDetrMLPPredictionHead(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+\n+    \"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [hidden_dim] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     Deformable DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n@@ -1887,7 +1893,6 @@ def __init__(self, config: DeformableDetrConfig):\n \n         # Deformable DETR encoder-decoder model\n         self.model = DeformableDetrModel(config)\n-\n         # Detection heads on top\n         self.class_embed = nn.Linear(config.d_model, config.num_labels)\n         self.bbox_embed = DeformableDetrMLPPredictionHead(\n@@ -1922,14 +1927,6 @@ def __init__(self, config: DeformableDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-    @torch.jit.unused\n-    def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n-        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n-\n     @add_start_docstrings_to_model_forward(DEFORMABLE_DETR_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=DeformableDetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -2034,41 +2031,9 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = DeformableDetrHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = DeformableDetrLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                focal_alpha=self.config.focal_alpha,\n-                losses=losses,\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n             )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n-            if self.config.auxiliary_loss:\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-            if self.config.two_stage:\n-                enc_outputs_coord = outputs.enc_outputs_coord_logits.sigmoid()\n-                outputs_loss[\"enc_outputs\"] = {\"logits\": outputs.enc_outputs_class, \"pred_boxes\": enc_outputs_coord}\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n-\n         if not return_dict:\n             if auxiliary_outputs is not None:\n                 output = (logits, pred_boxes) + auxiliary_outputs + outputs\n@@ -2099,453 +2064,3 @@ def forward(\n         )\n \n         return dict_outputs\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-class DeformableDetrLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for `DeformableDetrForObjectDetection`. The process happens in two steps: 1) we\n-    compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair of\n-    matched ground-truth / prediction (supervise class and box).\n-\n-    Args:\n-        matcher (`DeformableDetrHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        focal_alpha (`float`):\n-            Alpha parameter in focal loss.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    def __init__(self, matcher, num_classes, focal_alpha, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.focal_alpha = focal_alpha\n-        self.losses = losses\n-\n-    # removed logging parameter, which was part of the original implementation\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n-        of dim [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        target_classes_onehot = torch.zeros(\n-            [source_logits.shape[0], source_logits.shape[1], source_logits.shape[2] + 1],\n-            dtype=source_logits.dtype,\n-            layout=source_logits.layout,\n-            device=source_logits.device,\n-        )\n-        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n-\n-        target_classes_onehot = target_classes_onehot[:, :, :-1]\n-        loss_ce = (\n-            sigmoid_focal_loss(source_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2)\n-            * source_logits.shape[1]\n-        )\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.loss_cardinality\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.loss_boxes\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss._get_source_permutation_idx\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss._get_target_permutation_idx\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\" and k != \"enc_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes accross all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        if \"enc_outputs\" in outputs:\n-            enc_outputs = outputs[\"enc_outputs\"]\n-            bin_targets = copy.deepcopy(targets)\n-            for bt in bin_targets:\n-                bt[\"class_labels\"] = torch.zeros_like(bt[\"class_labels\"])\n-            indices = self.matcher(enc_outputs, bin_targets)\n-            for loss in self.losses:\n-                l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes)\n-                l_dict = {k + \"_enc\": v for k, v in l_dict.items()}\n-                losses.update(l_dict)\n-\n-        return losses\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead\n-class DeformableDetrMLPPredictionHead(nn.Module):\n-    \"\"\"\n-    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n-    height and width of a bounding box w.r.t. an image.\n-\n-    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-\n-    \"\"\"\n-\n-    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        h = [hidden_dim] * (num_layers - 1)\n-        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n-\n-    def forward(self, x):\n-        for i, layer in enumerate(self.layers):\n-            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n-        return x\n-\n-\n-class DeformableDetrHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost.\n-        alpha = 0.25\n-        gamma = 2.0\n-        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n-        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n-        class_cost = pos_cost_class[:, target_ids] - neg_cost_class[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)"
        },
        {
            "sha": "f51362d94e1722ca61b8973db6d621fdbdada472",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 30,
            "deletions": 553,
            "changes": 583,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -29,10 +29,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_accelerate_available,\n-    is_scipy_available,\n     is_timm_available,\n-    is_vision_available,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -41,21 +38,10 @@\n from .configuration_detr import DetrConfig\n \n \n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n-\n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n-\n if is_timm_available():\n     from timm import create_model\n \n \n-if is_vision_available():\n-    from transformers.image_transforms import center_to_corners_format\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"DetrConfig\"\n@@ -1343,6 +1329,28 @@ def forward(\n         )\n \n \n+# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+class DetrMLPPredictionHead(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+\n+    \"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [hidden_dim] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top, for tasks\n@@ -1368,14 +1376,6 @@ def __init__(self, config: DetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-    @torch.jit.unused\n-    def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n-        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n-\n     @add_start_docstrings_to_model_forward(DETR_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=DetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1458,40 +1458,14 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = DetrHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = DetrLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                eos_coef=self.config.eos_coefficient,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n                 intermediate = outputs.intermediate_hidden_states if return_dict else outputs[4]\n                 outputs_class = self.class_labels_classifier(intermediate)\n                 outputs_coord = self.bbox_predictor(intermediate).sigmoid()\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -1542,7 +1516,6 @@ def __init__(self, config: DetrConfig):\n         self.bbox_attention = DetrMHAttentionMap(\n             hidden_size, hidden_size, number_of_heads, dropout=0.0, std=config.init_xavier_std\n         )\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1688,43 +1661,14 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = DetrHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n-            criterion = DetrLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                eos_coef=self.config.eos_coefficient,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n-            outputs_loss[\"pred_masks\"] = pred_masks\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n                 intermediate = decoder_outputs.intermediate_hidden_states if return_dict else decoder_outputs[-1]\n                 outputs_class = self.detr.class_labels_classifier(intermediate)\n                 outputs_coord = self.detr.bbox_predictor(intermediate).sigmoid()\n-                auxiliary_outputs = self.detr._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            weight_dict[\"loss_mask\"] = self.config.mask_loss_coefficient\n-            weight_dict[\"loss_dice\"] = self.config.dice_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, device, pred_boxes, pred_masks, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -1861,470 +1805,3 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights\n-\n-\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-class DetrLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for DetrForObjectDetection/DetrForSegmentation. The process happens in two steps: 1)\n-    we compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair\n-    of matched ground-truth / prediction (supervise class and box).\n-\n-    A note on the `num_classes` argument (copied from original repo in detr.py): \"the naming of the `num_classes`\n-    parameter of the criterion is somewhat misleading. It indeed corresponds to `max_obj_id` + 1, where `max_obj_id` is\n-    the maximum id for a class in your dataset. For example, COCO has a `max_obj_id` of 90, so we pass `num_classes` to\n-    be 91. As another example, for a dataset that has a single class with `id` 1, you should pass `num_classes` to be 2\n-    (`max_obj_id` + 1). For more details on this, check the following discussion\n-    https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\"\n-\n-\n-    Args:\n-        matcher (`DetrHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        eos_coef (`float`):\n-            Relative classification weight applied to the no-object category.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    def __init__(self, matcher, num_classes, eos_coef, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.eos_coef = eos_coef\n-        self.losses = losses\n-        empty_weight = torch.ones(self.num_classes + 1)\n-        empty_weight[-1] = self.eos_coef\n-        self.register_buffer(\"empty_weight\", empty_weight)\n-\n-    # removed logging parameter, which was part of the original implementation\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n-        [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    def loss_masks(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the masks: the focal loss and the dice loss.\n-\n-        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n-        \"\"\"\n-        if \"pred_masks\" not in outputs:\n-            raise KeyError(\"No predicted masks found in outputs\")\n-\n-        source_idx = self._get_source_permutation_idx(indices)\n-        target_idx = self._get_target_permutation_idx(indices)\n-        source_masks = outputs[\"pred_masks\"]\n-        source_masks = source_masks[source_idx]\n-        masks = [t[\"masks\"] for t in targets]\n-        # TODO use valid to mask invalid areas due to padding in loss\n-        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n-        target_masks = target_masks.to(source_masks)\n-        target_masks = target_masks[target_idx]\n-\n-        # upsample predictions to the target size\n-        source_masks = nn.functional.interpolate(\n-            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n-        )\n-        source_masks = source_masks[:, 0].flatten(1)\n-\n-        target_masks = target_masks.flatten(1)\n-        target_masks = target_masks.view(source_masks.shape)\n-        losses = {\n-            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n-            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n-        }\n-        return losses\n-\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-            \"masks\": self.loss_masks,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes across all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        return losses\n-\n-\n-# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-class DetrMLPPredictionHead(nn.Module):\n-    \"\"\"\n-    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n-    height and width of a bounding box w.r.t. an image.\n-\n-    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-\n-    \"\"\"\n-\n-    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        h = [hidden_dim] * (num_layers - 1)\n-        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n-\n-    def forward(self, x):\n-        for i, layer in enumerate(self.layers):\n-            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n-        return x\n-\n-\n-# taken from https://github.com/facebookresearch/detr/blob/master/models/matcher.py\n-class DetrHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n-        # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n-        class_cost = -out_prob[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# below: bounding box utilities taken from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\n-\n-\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# modified from torchvision to also return the union\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# below: taken from https://github.com/facebookresearch/detr/blob/master/util/misc.py#L306\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)"
        },
        {
            "sha": "43882e7f8c0596cc8dd601b76e6f6f76026687c1",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -25,7 +25,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1084,18 +1083,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1199,27 +1187,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1302,8 +1271,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "ca1de9a880fef5be1204d1c6e26e03471a0a5372",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1003,18 +1002,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "28f5f5da7ba00374390172c0e755c8fed2d5ece0",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -24,7 +24,6 @@\n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n@@ -1065,18 +1064,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1258,27 +1246,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1361,8 +1330,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "9d7f047e1a849453ef62131b25675e363cec26b2",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -18,7 +18,6 @@\n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n@@ -806,18 +805,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "9c01ce19f3239935f754f6bf24d1dc54efd7b11a",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 477,
            "changes": 480,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch Grounding DINO model.\"\"\"\n \n-import copy\n import math\n import os\n import warnings\n@@ -33,31 +32,19 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_scipy_available,\n     is_timm_available,\n     is_torch_cuda_available,\n-    is_vision_available,\n     replace_return_docstrings,\n     requires_backends,\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import meshgrid\n-from ...utils import is_accelerate_available, is_ninja_available, logging\n+from ...utils import is_ninja_available, logging\n from ...utils.backbone_utils import load_backbone\n from ..auto import AutoModel\n from .configuration_grounding_dino import GroundingDinoConfig\n \n \n-if is_vision_available():\n-    from transformers.image_transforms import center_to_corners_format\n-\n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n-\n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n if is_timm_available():\n     from timm import create_model\n \n@@ -2488,436 +2475,6 @@ def forward(self, x):\n         return x\n \n \n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrHungarianMatcher with DeformableDetr->GroundingDino\n-class GroundingDinoHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost.\n-        alpha = 0.25\n-        gamma = 2.0\n-        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n-        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n-        class_cost = pos_cost_class[:, target_ids] - neg_cost_class[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrLoss with DeformableDetr->GroundingDino\n-class GroundingDinoLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for `GroundingDinoForObjectDetection`. The process happens in two steps: 1) we\n-    compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair of\n-    matched ground-truth / prediction (supervise class and box).\n-\n-    Args:\n-        matcher (`GroundingDinoHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        focal_alpha (`float`):\n-            Alpha parameter in focal loss.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    def __init__(self, matcher, num_classes, focal_alpha, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.focal_alpha = focal_alpha\n-        self.losses = losses\n-\n-    # removed logging parameter, which was part of the original implementation\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n-        of dim [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        target_classes_onehot = torch.zeros(\n-            [source_logits.shape[0], source_logits.shape[1], source_logits.shape[2] + 1],\n-            dtype=source_logits.dtype,\n-            layout=source_logits.layout,\n-            device=source_logits.device,\n-        )\n-        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n-\n-        target_classes_onehot = target_classes_onehot[:, :, :-1]\n-        loss_ce = (\n-            sigmoid_focal_loss(source_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2)\n-            * source_logits.shape[1]\n-        )\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.loss_cardinality\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss.loss_boxes\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss._get_source_permutation_idx\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    # Copied from transformers.models.detr.modeling_detr.DetrLoss._get_target_permutation_idx\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\" and k != \"enc_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes accross all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        if \"enc_outputs\" in outputs:\n-            enc_outputs = outputs[\"enc_outputs\"]\n-            bin_targets = copy.deepcopy(targets)\n-            for bt in bin_targets:\n-                bt[\"class_labels\"] = torch.zeros_like(bt[\"class_labels\"])\n-            indices = self.matcher(enc_outputs, bin_targets)\n-            for loss in self.losses:\n-                l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes)\n-                l_dict = {k + \"_enc\": v for k, v in l_dict.items()}\n-                losses.update(l_dict)\n-\n-        return losses\n-\n-\n @add_start_docstrings(\n     \"\"\"\n     Grounding DINO Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top,\n@@ -3079,40 +2636,9 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = GroundingDinoHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = GroundingDinoLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                focal_alpha=self.config.focal_alpha,\n-                losses=losses,\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n             )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n-            if self.config.auxiliary_loss:\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-            if self.config.two_stage:\n-                enc_outputs_coord = outputs[-1].sigmoid()\n-                outputs_loss[\"enc_outputs\"] = {\"logits\": outputs[-2], \"pred_boxes\": enc_outputs_coord}\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n \n         if not return_dict:\n             if auxiliary_outputs is not None:"
        },
        {
            "sha": "818c6acb3f796166cd2cae94ecfb22158e707c9d",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 34,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -26,7 +26,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache  # we need __iter__ and __len__ of pkv\n@@ -1543,18 +1542,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         aux_loss = None\n         if output_router_logits:\n@@ -1729,27 +1717,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "a4bb1d78fdc5ceb16a6af7a0f44656d0e53bc65e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -20,7 +20,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n from torch.nn import functional as F\n \n from ...activations import ACT2FN\n@@ -1436,27 +1436,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "e9064ff3ae5b22890b05f657dcb7a7e9bc30d0c5",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 53,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -24,7 +24,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1147,6 +1146,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n+        **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1209,18 +1209,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1324,27 +1313,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1396,6 +1366,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1427,29 +1398,16 @@ def forward(\n         start_logits = start_logits.squeeze(-1).contiguous()\n         end_logits = end_logits.squeeze(-1).contiguous()\n \n-        total_loss = None\n+        loss = None\n         if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, split add a dimension\n-            if len(start_positions.size()) > 1:\n-                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n-            if len(end_positions.size()) > 1:\n-                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n-            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n-            ignored_index = start_logits.size(1)\n-            start_positions = start_positions.clamp(0, ignored_index)\n-            end_positions = end_positions.clamp(0, ignored_index)\n-\n-            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n         if not return_dict:\n             output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n+            return ((loss,) + output) if loss is not None else output\n \n         return QuestionAnsweringModelOutput(\n-            loss=total_loss,\n+            loss=loss,\n             start_logits=start_logits,\n             end_logits=end_logits,\n             hidden_states=outputs.hidden_states,\n@@ -1526,8 +1484,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "15eea1ae1f502bc5995dc0fa1176e5c0c93d9639",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -25,7 +25,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1220,27 +1220,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1324,8 +1305,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "7fbfb90cd322b5b93f867b6b1f4c1784c8fa25aa",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -26,7 +26,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1327,18 +1326,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         aux_loss = None\n         if output_router_logits:\n@@ -1458,27 +1446,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1562,8 +1531,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "d1cc3a13bf3cc38abc74775bf917a3e91e682448",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n@@ -1950,18 +1949,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "6cac7ecdfbe5d912dce8b819f0b246100abcb391",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 9,
            "deletions": 53,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -22,7 +22,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import Size, Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n@@ -1084,18 +1083,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1200,27 +1188,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1273,6 +1242,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1304,29 +1274,16 @@ def forward(\n         start_logits = start_logits.squeeze(-1).contiguous()\n         end_logits = end_logits.squeeze(-1).contiguous()\n \n-        total_loss = None\n+        loss = None\n         if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, split add a dimension\n-            if len(start_positions.size()) > 1:\n-                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n-            if len(end_positions.size()) > 1:\n-                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n-            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n-            ignored_index = start_logits.size(1)\n-            start_positions = start_positions.clamp(0, ignored_index)\n-            end_positions = end_positions.clamp(0, ignored_index)\n-\n-            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n         if not return_dict:\n             output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n+            return ((loss,) + output) if loss is not None else output\n \n         return QuestionAnsweringModelOutput(\n-            loss=total_loss,\n+            loss=loss,\n             start_logits=start_logits,\n             end_logits=end_logits,\n             hidden_states=outputs.hidden_states,\n@@ -1404,8 +1361,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "6c7dc59cdbff38840bb0e5ea7e906a56044d4ad4",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -26,7 +26,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1127,18 +1126,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "32f7ded42e8901edf2e7833d550eb5a54b893c4a",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -18,7 +18,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1291,18 +1290,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         aux_loss = None\n         if output_router_logits:"
        },
        {
            "sha": "d773396010a3cb2006b0a1ba2f3b01e6124dde56",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -98,7 +98,7 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n-# Copied from transformers.models.detr.modeling_detr._upcast\n+# Copied from transformers.loss.loss_for_object_detection._upcast\n def _upcast(t: Tensor) -> Tensor:\n     # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n     if t.is_floating_point():\n@@ -107,7 +107,7 @@ def _upcast(t: Tensor) -> Tensor:\n         return t if t.dtype in (torch.int32, torch.int64) else t.int()\n \n \n-# Copied from transformers.models.detr.modeling_detr.box_area\n+# Copied from transformers.loss.loss_for_object_detection.box_area\n def box_area(boxes: Tensor) -> Tensor:\n     \"\"\"\n     Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n@@ -124,7 +124,7 @@ def box_area(boxes: Tensor) -> Tensor:\n     return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n \n \n-# Copied from transformers.models.detr.modeling_detr.box_iou\n+# Copied from transformers.loss.loss_for_object_detection.box_iou\n def box_iou(boxes1, boxes2):\n     area1 = box_area(boxes1)\n     area2 = box_area(boxes2)\n@@ -141,7 +141,7 @@ def box_iou(boxes1, boxes2):\n     return iou, union\n \n \n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n+# Copied from transformers.loss.loss_for_object_detection.generalized_box_iou\n def generalized_box_iou(boxes1, boxes2):\n     \"\"\"\n     Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format."
        },
        {
            "sha": "7c3e124a207ff7a5c6473efe87cb5fc0484b6e46",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -98,7 +98,7 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n-# Copied from transformers.models.detr.modeling_detr._upcast\n+# Copied from transformers.loss.loss_for_object_detection._upcast\n def _upcast(t: Tensor) -> Tensor:\n     # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n     if t.is_floating_point():\n@@ -107,7 +107,7 @@ def _upcast(t: Tensor) -> Tensor:\n         return t if t.dtype in (torch.int32, torch.int64) else t.int()\n \n \n-# Copied from transformers.models.detr.modeling_detr.box_area\n+# Copied from transformers.loss.loss_for_object_detection.box_area\n def box_area(boxes: Tensor) -> Tensor:\n     \"\"\"\n     Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n@@ -124,7 +124,7 @@ def box_area(boxes: Tensor) -> Tensor:\n     return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n \n \n-# Copied from transformers.models.detr.modeling_detr.box_iou\n+# Copied from transformers.loss.loss_for_object_detection.box_iou\n def box_iou(boxes1, boxes2):\n     area1 = box_area(boxes1)\n     area2 = box_area(boxes2)\n@@ -141,7 +141,7 @@ def box_iou(boxes1, boxes2):\n     return iou, union\n \n \n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n+# Copied from transformers.loss.loss_for_object_detection.generalized_box_iou\n def generalized_box_iou(boxes1, boxes2):\n     \"\"\"\n     Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format."
        },
        {
            "sha": "ddd26729164df2c7e13cde090ea7eee79860132e",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -25,7 +25,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1073,27 +1073,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1177,8 +1158,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "ef1a5b4d0ec243445ae3fb4cf7b7637b13f4ae18",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 34,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -22,7 +22,7 @@\n import torch.utils.checkpoint\n from packaging import version\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1250,18 +1250,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1366,27 +1355,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + model_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "d0865065db1882ce2370b999367ace3e788faea2",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 34,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -22,7 +22,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1300,18 +1300,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1455,27 +1444,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + model_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "9e24e59c64c2fe23809419d77d099a700d03c8f8",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 34,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1474,18 +1473,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         aux_loss = None\n         if output_router_logits:\n@@ -1644,27 +1632,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "d6f7cd94288a7723528f268be2cff0832d3710e9",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -1204,18 +1204,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1423,8 +1412,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "5b0441e02cfbeab35555f1ab570beb932ce92af0",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -26,7 +26,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1393,18 +1392,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         aux_loss = None\n         if output_router_logits:\n@@ -1524,27 +1512,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1628,8 +1597,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "21644c4a869a0afe9a5b92c57344a89c9edc3957",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 37,
            "deletions": 614,
            "changes": 651,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -37,19 +37,14 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_ninja_available,\n-    is_scipy_available,\n     is_torch_cuda_available,\n     logging,\n     replace_return_docstrings,\n-    requires_backends,\n )\n from ...utils.backbone_utils import load_backbone\n from .configuration_rt_detr import RTDetrConfig\n \n \n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n logger = logging.get_logger(__name__)\n \n MultiScaleDeformableAttention = None\n@@ -1616,6 +1611,29 @@ def wrapper(self, *args, **kwargs):\n     return decorator\n \n \n+# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+class RTDetrMLPPredictionHead(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+    Origin from https://github.com/lyuwenyu/RT-DETR/blob/94f5e16708329d2f2716426868ec89aa774af016/rtdetr_paddle/ppdet/modeling/transformers/utils.py#L453\n+\n+    \"\"\"\n+\n+    def __init__(self, config, input_dim, d_model, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [d_model] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting raw hidden states without any head on top.\n@@ -1950,588 +1968,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-class RTDetrLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for RTDetr. The process happens in two steps: 1) we compute hungarian assignment\n-    between ground truth boxes and the outputs of the model 2) we supervise each pair of matched ground-truth /\n-    prediction (supervise class and box).\n-\n-    Args:\n-        matcher (`DetrHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        weight_dict (`Dict`):\n-            Dictionary relating each loss with its weights. These losses are configured in RTDetrConf as\n-            `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-        alpha (`float`):\n-            Parameter alpha used to compute the focal loss.\n-        gamma (`float`):\n-            Parameter gamma used to compute the focal loss.\n-        eos_coef (`float`):\n-            Relative classification weight applied to the no-object category.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-\n-        self.matcher = RTDetrHungarianMatcher(config)\n-        self.num_classes = config.num_labels\n-        self.weight_dict = {\n-            \"loss_vfl\": config.weight_loss_vfl,\n-            \"loss_bbox\": config.weight_loss_bbox,\n-            \"loss_giou\": config.weight_loss_giou,\n-        }\n-        self.losses = [\"vfl\", \"boxes\"]\n-        self.eos_coef = config.eos_coefficient\n-        empty_weight = torch.ones(config.num_labels + 1)\n-        empty_weight[-1] = self.eos_coef\n-        self.register_buffer(\"empty_weight\", empty_weight)\n-        self.alpha = config.focal_loss_alpha\n-        self.gamma = config.focal_loss_gamma\n-\n-    def loss_labels_vfl(self, outputs, targets, indices, num_boxes, log=True):\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No predicted logits found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-\n-        src_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([_target[\"boxes\"][i] for _target, (_, i) in zip(targets, indices)], dim=0)\n-        ious, _ = box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n-        ious = torch.diag(ious).detach()\n-\n-        src_logits = outputs[\"logits\"]\n-        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n-        )\n-        target_classes[idx] = target_classes_original\n-        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n-\n-        target_score_original = torch.zeros_like(target_classes, dtype=src_logits.dtype)\n-        target_score_original[idx] = ious.to(target_score_original.dtype)\n-        target_score = target_score_original.unsqueeze(-1) * target\n-\n-        pred_score = F.sigmoid(src_logits).detach()\n-        weight = self.alpha * pred_score.pow(self.gamma) * (1 - target) + target_score\n-\n-        loss = F.binary_cross_entropy_with_logits(src_logits, target_score, weight=weight, reduction=\"none\")\n-        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n-        return {\"loss_vfl\": loss}\n-\n-    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n-        \"\"\"Classification loss (NLL)\n-        targets dicts must contain the key \"class_labels\" containing a tensor of dim [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-\n-        src_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n-        )\n-        target_classes[idx] = target_classes_original\n-\n-        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.class_weight)\n-        losses = {\"loss_ce\": loss_ce}\n-        return losses\n-\n-    @torch.no_grad()\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes. This is not\n-        really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss. Targets dicts must\n-        contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes are expected in\n-        format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        src_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        losses = {}\n-\n-        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\"none\")\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    def loss_masks(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the masks: the focal loss and the dice loss. Targets dicts must contain the key\n-        \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n-        \"\"\"\n-        if \"pred_masks\" not in outputs:\n-            raise KeyError(\"No predicted masks found in outputs\")\n-\n-        source_idx = self._get_source_permutation_idx(indices)\n-        target_idx = self._get_target_permutation_idx(indices)\n-        source_masks = outputs[\"pred_masks\"]\n-        source_masks = source_masks[source_idx]\n-        masks = [t[\"masks\"] for t in targets]\n-        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n-        target_masks = target_masks.to(source_masks)\n-        target_masks = target_masks[target_idx]\n-\n-        # upsample predictions to the target size\n-        source_masks = nn.functional.interpolate(\n-            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n-        )\n-        source_masks = source_masks[:, 0].flatten(1)\n-\n-        target_masks = target_masks.flatten(1)\n-        target_masks = target_masks.view(source_masks.shape)\n-        losses = {\n-            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n-            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n-        }\n-        return losses\n-\n-    def loss_labels_bce(self, outputs, targets, indices, num_boxes, log=True):\n-        src_logits = outputs[\"logits\"]\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n-        )\n-        target_classes[idx] = target_classes_original\n-\n-        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n-        loss = F.binary_cross_entropy_with_logits(src_logits, target * 1.0, reduction=\"none\")\n-        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n-        return {\"loss_bce\": loss}\n-\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def loss_labels_focal(self, outputs, targets, indices, num_boxes, log=True):\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits found in outputs\")\n-\n-        src_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n-        )\n-        target_classes[idx] = target_classes_original\n-\n-        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n-        loss = sigmoid_focal_loss(src_logits, target, self.alpha, self.gamma)\n-        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n-        return {\"loss_focal\": loss}\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-            \"masks\": self.loss_masks,\n-            \"bce\": self.loss_labels_bce,\n-            \"focal\": self.loss_labels_focal,\n-            \"vfl\": self.loss_labels_vfl,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    @staticmethod\n-    def get_cdn_matched_indices(dn_meta, targets):\n-        dn_positive_idx, dn_num_group = dn_meta[\"dn_positive_idx\"], dn_meta[\"dn_num_group\"]\n-        num_gts = [len(t[\"class_labels\"]) for t in targets]\n-        device = targets[0][\"class_labels\"].device\n-\n-        dn_match_indices = []\n-        for i, num_gt in enumerate(num_gts):\n-            if num_gt > 0:\n-                gt_idx = torch.arange(num_gt, dtype=torch.int64, device=device)\n-                gt_idx = gt_idx.tile(dn_num_group)\n-                assert len(dn_positive_idx[i]) == len(gt_idx)\n-                dn_match_indices.append((dn_positive_idx[i], gt_idx))\n-            else:\n-                dn_match_indices.append(\n-                    (\n-                        torch.zeros(0, dtype=torch.int64, device=device),\n-                        torch.zeros(0, dtype=torch.int64, device=device),\n-                    )\n-                )\n-\n-        return dn_match_indices\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if \"auxiliary_outputs\" not in k}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes across all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        num_boxes = torch.clamp(num_boxes, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            l_dict = self.get_loss(loss, outputs, targets, indices, num_boxes)\n-            l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n-            losses.update(l_dict)\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n-                    l_dict = {k + f\"_aux_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        # In case of cdn auxiliary losses. For rtdetr\n-        if \"dn_auxiliary_outputs\" in outputs:\n-            if \"denoising_meta_values\" not in outputs:\n-                raise ValueError(\n-                    \"The output must have the 'denoising_meta_values` key. Please, ensure that 'outputs' includes a 'denoising_meta_values' entry.\"\n-                )\n-            indices = self.get_cdn_matched_indices(outputs[\"denoising_meta_values\"], targets)\n-            num_boxes = num_boxes * outputs[\"denoising_meta_values\"][\"dn_num_group\"]\n-\n-            for i, auxiliary_outputs in enumerate(outputs[\"dn_auxiliary_outputs\"]):\n-                # indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    kwargs = {}\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes, **kwargs)\n-                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n-                    l_dict = {k + f\"_dn_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        return losses\n-\n-\n-# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-class RTDetrMLPPredictionHead(nn.Module):\n-    \"\"\"\n-    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n-    height and width of a bounding box w.r.t. an image.\n-\n-    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-    Origin from https://github.com/lyuwenyu/RT-DETR/blob/94f5e16708329d2f2716426868ec89aa774af016/rtdetr_paddle/ppdet/modeling/transformers/utils.py#L453\n-\n-    \"\"\"\n-\n-    def __init__(self, config, input_dim, d_model, output_dim, num_layers):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        h = [d_model] * (num_layers - 1)\n-        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n-\n-    def forward(self, x):\n-        for i, layer in enumerate(self.layers):\n-            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n-        return x\n-\n-\n-class RTDetrHungarianMatcher(nn.Module):\n-    \"\"\"This class computes an assignment between the targets and the predictions of the network\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        config: RTDetrConfig\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = config.matcher_class_cost\n-        self.bbox_cost = config.matcher_bbox_cost\n-        self.giou_cost = config.matcher_giou_cost\n-\n-        self.use_focal_loss = config.use_focal_loss\n-        self.alpha = config.matcher_alpha\n-        self.gamma = config.matcher_gamma\n-\n-        if self.class_cost == self.bbox_cost == self.giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"Performs the matching\n-\n-        Params:\n-            outputs: This is a dict that contains at least these entries:\n-                 \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n-\n-            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                 \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n-                           objects in the target) containing the class labels\n-                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n-\n-        Returns:\n-            A list of size batch_size, containing tuples of (index_i, index_j) where:\n-                - index_i is the indices of the selected predictions (in order)\n-                - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds:\n-                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n-        # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n-        if self.use_focal_loss:\n-            out_prob = F.sigmoid(outputs[\"logits\"].flatten(0, 1))\n-            out_prob = out_prob[:, target_ids]\n-            neg_cost_class = (1 - self.alpha) * (out_prob**self.gamma) * (-(1 - out_prob + 1e-8).log())\n-            pos_cost_class = self.alpha * ((1 - out_prob) ** self.gamma) * (-(out_prob + 1e-8).log())\n-            class_cost = pos_cost_class - neg_cost_class\n-        else:\n-            out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n-            class_cost = -out_prob[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-        # Compute the giou cost betwen boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-        # Compute the final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)\n-\n-\n @add_start_docstrings(\n     \"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting bounding boxes and logits to be further\n@@ -2673,39 +2109,26 @@ def forward(\n         outputs_class = outputs.intermediate_logits if return_dict else outputs[2]\n         outputs_coord = outputs.intermediate_reference_points if return_dict else outputs[3]\n \n-        if self.training and denoising_meta_values is not None:\n-            dn_out_coord, outputs_coord = torch.split(outputs_coord, denoising_meta_values[\"dn_num_split\"], dim=2)\n-            dn_out_class, outputs_class = torch.split(outputs_class, denoising_meta_values[\"dn_num_split\"], dim=2)\n-\n         logits = outputs_class[:, -1]\n         pred_boxes = outputs_coord[:, -1]\n \n-        loss, loss_dict, auxiliary_outputs = None, None, None\n+        loss, loss_dict, auxiliary_outputs, enc_topk_logits, enc_topk_bboxes = None, None, None, None, None\n         if labels is not None:\n-            # First: create the criterion\n-            criterion = RTDetrLoss(self.config)\n-            criterion.to(self.device)\n-            # Second: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n-            if self.config.auxiliary_loss:\n+            if self.training and denoising_meta_values is not None:\n                 enc_topk_logits = outputs.enc_topk_logits if return_dict else outputs[-5]\n                 enc_topk_bboxes = outputs.enc_topk_bboxes if return_dict else outputs[-4]\n-                auxiliary_outputs = self._set_aux_loss(\n-                    outputs_class[:, :-1].transpose(0, 1), outputs_coord[:, :-1].transpose(0, 1)\n-                )\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-                outputs_loss[\"auxiliary_outputs\"].extend(self._set_aux_loss([enc_topk_logits], [enc_topk_bboxes]))\n-                if self.training and denoising_meta_values is not None:\n-                    outputs_loss[\"dn_auxiliary_outputs\"] = self._set_aux_loss(\n-                        dn_out_class.transpose(0, 1), dn_out_coord.transpose(0, 1)\n-                    )\n-                    outputs_loss[\"denoising_meta_values\"] = denoising_meta_values\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-\n-            loss = sum(loss_dict.values())\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits,\n+                labels,\n+                self.device,\n+                pred_boxes,\n+                self.config,\n+                outputs_class,\n+                outputs_coord,\n+                enc_topk_logits=enc_topk_logits,\n+                enc_topk_bboxes=enc_topk_bboxes,\n+                denoising_meta_values=denoising_meta_values,\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:"
        },
        {
            "sha": "a2356258ce38ed4ba58c8868447a48f9046e7098",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -25,7 +25,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1349,27 +1349,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1453,8 +1434,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "1779337b1a0093f1d2d2acc369c8ca03d7124449",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -25,7 +25,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -1295,27 +1295,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1399,8 +1380,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "be57ab46016ccf9e261ce4044b1505c481ec048b",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 499,
            "changes": 503,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -29,10 +29,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_accelerate_available,\n-    is_scipy_available,\n     is_timm_available,\n-    is_vision_available,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -41,18 +38,9 @@\n from .configuration_table_transformer import TableTransformerConfig\n \n \n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n if is_timm_available():\n     from timm import create_model\n \n-if is_vision_available():\n-    from transformers.image_transforms import center_to_corners_format\n-\n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n \n logger = logging.get_logger(__name__)\n \n@@ -1312,14 +1300,6 @@ def __init__(self, config: TableTransformerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.jit.unused\n-    # Copied from transformers.models.detr.modeling_detr.DetrForObjectDetection._set_aux_loss\n-    def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n-        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n-\n     @add_start_docstrings_to_model_forward(TABLE_TRANSFORMER_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TableTransformerObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1398,40 +1378,14 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = TableTransformerHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = TableTransformerLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                eos_coef=self.config.eos_coefficient,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n                 intermediate = outputs.intermediate_hidden_states if return_dict else outputs[4]\n                 outputs_class = self.class_labels_classifier(intermediate)\n                 outputs_coord = self.bbox_predictor(intermediate).sigmoid()\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -1456,258 +1410,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrLoss with Detr->TableTransformer,detr->table_transformer\n-class TableTransformerLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for TableTransformerForObjectDetection/TableTransformerForSegmentation. The process happens in two steps: 1)\n-    we compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair\n-    of matched ground-truth / prediction (supervise class and box).\n-\n-    A note on the `num_classes` argument (copied from original repo in table_transformer.py): \"the naming of the `num_classes`\n-    parameter of the criterion is somewhat misleading. It indeed corresponds to `max_obj_id` + 1, where `max_obj_id` is\n-    the maximum id for a class in your dataset. For example, COCO has a `max_obj_id` of 90, so we pass `num_classes` to\n-    be 91. As another example, for a dataset that has a single class with `id` 1, you should pass `num_classes` to be 2\n-    (`max_obj_id` + 1). For more details on this, check the following discussion\n-    https://github.com/facebookresearch/table_transformer/issues/108#issuecomment-650269223\"\n-\n-\n-    Args:\n-        matcher (`TableTransformerHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        eos_coef (`float`):\n-            Relative classification weight applied to the no-object category.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    def __init__(self, matcher, num_classes, eos_coef, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.eos_coef = eos_coef\n-        self.losses = losses\n-        empty_weight = torch.ones(self.num_classes + 1)\n-        empty_weight[-1] = self.eos_coef\n-        self.register_buffer(\"empty_weight\", empty_weight)\n-\n-    # removed logging parameter, which was part of the original implementation\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n-        [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    def loss_masks(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the masks: the focal loss and the dice loss.\n-\n-        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n-        \"\"\"\n-        if \"pred_masks\" not in outputs:\n-            raise KeyError(\"No predicted masks found in outputs\")\n-\n-        source_idx = self._get_source_permutation_idx(indices)\n-        target_idx = self._get_target_permutation_idx(indices)\n-        source_masks = outputs[\"pred_masks\"]\n-        source_masks = source_masks[source_idx]\n-        masks = [t[\"masks\"] for t in targets]\n-        # TODO use valid to mask invalid areas due to padding in loss\n-        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n-        target_masks = target_masks.to(source_masks)\n-        target_masks = target_masks[target_idx]\n-\n-        # upsample predictions to the target size\n-        source_masks = nn.functional.interpolate(\n-            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n-        )\n-        source_masks = source_masks[:, 0].flatten(1)\n-\n-        target_masks = target_masks.flatten(1)\n-        target_masks = target_masks.view(source_masks.shape)\n-        losses = {\n-            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n-            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n-        }\n-        return losses\n-\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-            \"masks\": self.loss_masks,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes across all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        return losses\n-\n-\n # Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->TableTransformer,detr->table_transformer\n class TableTransformerMLPPredictionHead(nn.Module):\n     \"\"\"\n@@ -1728,200 +1430,3 @@ def forward(self, x):\n         for i, layer in enumerate(self.layers):\n             x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n         return x\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrHungarianMatcher with Detr->TableTransformer\n-class TableTransformerHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n-        # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n-        class_cost = -out_prob[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)"
        },
        {
            "sha": "2d00c973b85c18edfe971c005a539c29163ea52c",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 27,
            "deletions": 516,
            "changes": 543,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -21,7 +21,7 @@\n \n import torch\n import torch.utils.checkpoint\n-from torch import Tensor, nn\n+from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -32,26 +32,12 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_accelerate_available,\n-    is_scipy_available,\n-    is_vision_available,\n     logging,\n     replace_return_docstrings,\n-    requires_backends,\n )\n from .configuration_yolos import YolosConfig\n \n \n-if is_scipy_available():\n-    from scipy.optimize import linear_sum_assignment\n-\n-if is_vision_available():\n-    from transformers.image_transforms import center_to_corners_format\n-\n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import reduce\n-\n logger = logging.get_logger(__name__)\n \n # General docstring\n@@ -728,6 +714,28 @@ def forward(self, hidden_states):\n         return pooled_output\n \n \n+# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->Yolos\n+class YolosMLPPredictionHead(nn.Module):\n+    \"\"\"\n+    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n+    height and width of a bounding box w.r.t. an image.\n+\n+    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n+\n+    \"\"\"\n+\n+    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        h = [hidden_dim] * (num_layers - 1)\n+        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n+\n+    def forward(self, x):\n+        for i, layer in enumerate(self.layers):\n+            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n+        return x\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     YOLOS Model (consisting of a ViT encoder) with object detection heads on top, for tasks such as COCO detection.\n@@ -837,40 +845,14 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n-            # First: create the matcher\n-            matcher = YolosHungarianMatcher(\n-                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n-            )\n-            # Second: create the criterion\n-            losses = [\"labels\", \"boxes\", \"cardinality\"]\n-            criterion = YolosLoss(\n-                matcher=matcher,\n-                num_classes=self.config.num_labels,\n-                eos_coef=self.config.eos_coefficient,\n-                losses=losses,\n-            )\n-            criterion.to(self.device)\n-            # Third: compute the losses, based on outputs and labels\n-            outputs_loss = {}\n-            outputs_loss[\"logits\"] = logits\n-            outputs_loss[\"pred_boxes\"] = pred_boxes\n+            outputs_class, outputs_coord = None, None\n             if self.config.auxiliary_loss:\n                 intermediate = outputs.intermediate_hidden_states if return_dict else outputs[4]\n                 outputs_class = self.class_labels_classifier(intermediate)\n                 outputs_coord = self.bbox_predictor(intermediate).sigmoid()\n-                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n-                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n-\n-            loss_dict = criterion(outputs_loss, labels)\n-            # Fourth: compute total loss, as a weighted sum of the various losses\n-            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n-            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n-            if self.config.auxiliary_loss:\n-                aux_weight_dict = {}\n-                for i in range(self.config.decoder_layers - 1):\n-                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n-                weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss, loss_dict, auxiliary_outputs = self.loss_function(\n+                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+            )\n \n         if not return_dict:\n             if auxiliary_outputs is not None:\n@@ -889,474 +871,3 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.dice_loss\n-def dice_loss(inputs, targets, num_boxes):\n-    \"\"\"\n-    Compute the DICE loss, similar to generalized IOU for masks\n-\n-    Args:\n-        inputs: A float tensor of arbitrary shape.\n-                The predictions for each example.\n-        targets: A float tensor with the same shape as inputs. Stores the binary\n-                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n-                 class).\n-    \"\"\"\n-    inputs = inputs.sigmoid()\n-    inputs = inputs.flatten(1)\n-    numerator = 2 * (inputs * targets).sum(1)\n-    denominator = inputs.sum(-1) + targets.sum(-1)\n-    loss = 1 - (numerator + 1) / (denominator + 1)\n-    return loss.sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss\n-def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n-    \"\"\"\n-    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n-\n-    Args:\n-        inputs (`torch.FloatTensor` of arbitrary shape):\n-            The predictions for each example.\n-        targets (`torch.FloatTensor` with the same shape as `inputs`)\n-            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n-            and 1 for the positive class).\n-        alpha (`float`, *optional*, defaults to `0.25`):\n-            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n-        gamma (`int`, *optional*, defaults to `2`):\n-            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n-\n-    Returns:\n-        Loss tensor\n-    \"\"\"\n-    prob = inputs.sigmoid()\n-    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n-    # add modulating factor\n-    p_t = prob * targets + (1 - prob) * (1 - targets)\n-    loss = ce_loss * ((1 - p_t) ** gamma)\n-\n-    if alpha >= 0:\n-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n-        loss = alpha_t * loss\n-\n-    return loss.mean(1).sum() / num_boxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrLoss with Detr->Yolos\n-class YolosLoss(nn.Module):\n-    \"\"\"\n-    This class computes the losses for YolosForObjectDetection/YolosForSegmentation. The process happens in two steps: 1)\n-    we compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair\n-    of matched ground-truth / prediction (supervise class and box).\n-\n-    A note on the `num_classes` argument (copied from original repo in detr.py): \"the naming of the `num_classes`\n-    parameter of the criterion is somewhat misleading. It indeed corresponds to `max_obj_id` + 1, where `max_obj_id` is\n-    the maximum id for a class in your dataset. For example, COCO has a `max_obj_id` of 90, so we pass `num_classes` to\n-    be 91. As another example, for a dataset that has a single class with `id` 1, you should pass `num_classes` to be 2\n-    (`max_obj_id` + 1). For more details on this, check the following discussion\n-    https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\"\n-\n-\n-    Args:\n-        matcher (`YolosHungarianMatcher`):\n-            Module able to compute a matching between targets and proposals.\n-        num_classes (`int`):\n-            Number of object categories, omitting the special no-object category.\n-        eos_coef (`float`):\n-            Relative classification weight applied to the no-object category.\n-        losses (`List[str]`):\n-            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n-    \"\"\"\n-\n-    def __init__(self, matcher, num_classes, eos_coef, losses):\n-        super().__init__()\n-        self.matcher = matcher\n-        self.num_classes = num_classes\n-        self.eos_coef = eos_coef\n-        self.losses = losses\n-        empty_weight = torch.ones(self.num_classes + 1)\n-        empty_weight[-1] = self.eos_coef\n-        self.register_buffer(\"empty_weight\", empty_weight)\n-\n-    # removed logging parameter, which was part of the original implementation\n-    def loss_labels(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n-        [nb_target_boxes]\n-        \"\"\"\n-        if \"logits\" not in outputs:\n-            raise KeyError(\"No logits were found in the outputs\")\n-        source_logits = outputs[\"logits\"]\n-\n-        idx = self._get_source_permutation_idx(indices)\n-        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n-        target_classes = torch.full(\n-            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n-        )\n-        target_classes[idx] = target_classes_o\n-\n-        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n-        losses = {\"loss_ce\": loss_ce}\n-\n-        return losses\n-\n-    @torch.no_grad()\n-    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n-\n-        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n-        \"\"\"\n-        logits = outputs[\"logits\"]\n-        device = logits.device\n-        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n-        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n-        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n-        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n-        losses = {\"cardinality_error\": card_err}\n-        return losses\n-\n-    def loss_boxes(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n-\n-        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n-        are expected in format (center_x, center_y, w, h), normalized by the image size.\n-        \"\"\"\n-        if \"pred_boxes\" not in outputs:\n-            raise KeyError(\"No predicted boxes found in outputs\")\n-        idx = self._get_source_permutation_idx(indices)\n-        source_boxes = outputs[\"pred_boxes\"][idx]\n-        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n-\n-        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n-\n-        losses = {}\n-        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n-\n-        loss_giou = 1 - torch.diag(\n-            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n-        )\n-        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n-        return losses\n-\n-    def loss_masks(self, outputs, targets, indices, num_boxes):\n-        \"\"\"\n-        Compute the losses related to the masks: the focal loss and the dice loss.\n-\n-        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n-        \"\"\"\n-        if \"pred_masks\" not in outputs:\n-            raise KeyError(\"No predicted masks found in outputs\")\n-\n-        source_idx = self._get_source_permutation_idx(indices)\n-        target_idx = self._get_target_permutation_idx(indices)\n-        source_masks = outputs[\"pred_masks\"]\n-        source_masks = source_masks[source_idx]\n-        masks = [t[\"masks\"] for t in targets]\n-        # TODO use valid to mask invalid areas due to padding in loss\n-        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n-        target_masks = target_masks.to(source_masks)\n-        target_masks = target_masks[target_idx]\n-\n-        # upsample predictions to the target size\n-        source_masks = nn.functional.interpolate(\n-            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n-        )\n-        source_masks = source_masks[:, 0].flatten(1)\n-\n-        target_masks = target_masks.flatten(1)\n-        target_masks = target_masks.view(source_masks.shape)\n-        losses = {\n-            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n-            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n-        }\n-        return losses\n-\n-    def _get_source_permutation_idx(self, indices):\n-        # permute predictions following indices\n-        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n-        source_idx = torch.cat([source for (source, _) in indices])\n-        return batch_idx, source_idx\n-\n-    def _get_target_permutation_idx(self, indices):\n-        # permute targets following indices\n-        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n-        target_idx = torch.cat([target for (_, target) in indices])\n-        return batch_idx, target_idx\n-\n-    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n-        loss_map = {\n-            \"labels\": self.loss_labels,\n-            \"cardinality\": self.loss_cardinality,\n-            \"boxes\": self.loss_boxes,\n-            \"masks\": self.loss_masks,\n-        }\n-        if loss not in loss_map:\n-            raise ValueError(f\"Loss {loss} not supported\")\n-        return loss_map[loss](outputs, targets, indices, num_boxes)\n-\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        This performs the loss computation.\n-\n-        Args:\n-             outputs (`dict`, *optional*):\n-                Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n-                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n-                losses applied, see each loss' doc.\n-        \"\"\"\n-        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n-\n-        # Retrieve the matching between the outputs of the last layer and the targets\n-        indices = self.matcher(outputs_without_aux, targets)\n-\n-        # Compute the average number of target boxes across all nodes, for normalization purposes\n-        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n-        world_size = 1\n-        if is_accelerate_available():\n-            if PartialState._shared_state != {}:\n-                num_boxes = reduce(num_boxes)\n-                world_size = PartialState().num_processes\n-        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n-\n-        # Compute all the requested losses\n-        losses = {}\n-        for loss in self.losses:\n-            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n-\n-        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n-        if \"auxiliary_outputs\" in outputs:\n-            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n-                indices = self.matcher(auxiliary_outputs, targets)\n-                for loss in self.losses:\n-                    if loss == \"masks\":\n-                        # Intermediate masks losses are too costly to compute, we ignore them.\n-                        continue\n-                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n-                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n-                    losses.update(l_dict)\n-\n-        return losses\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->Yolos\n-class YolosMLPPredictionHead(nn.Module):\n-    \"\"\"\n-    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n-    height and width of a bounding box w.r.t. an image.\n-\n-    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-\n-    \"\"\"\n-\n-    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        h = [hidden_dim] * (num_layers - 1)\n-        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n-\n-    def forward(self, x):\n-        for i, layer in enumerate(self.layers):\n-            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n-        return x\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.DetrHungarianMatcher with Detr->Yolos\n-class YolosHungarianMatcher(nn.Module):\n-    \"\"\"\n-    This class computes an assignment between the targets and the predictions of the network.\n-\n-    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n-    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n-    un-matched (and thus treated as non-objects).\n-\n-    Args:\n-        class_cost:\n-            The relative weight of the classification error in the matching cost.\n-        bbox_cost:\n-            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n-        giou_cost:\n-            The relative weight of the giou loss of the bounding box in the matching cost.\n-    \"\"\"\n-\n-    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n-        super().__init__()\n-        requires_backends(self, [\"scipy\"])\n-\n-        self.class_cost = class_cost\n-        self.bbox_cost = bbox_cost\n-        self.giou_cost = giou_cost\n-        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n-            raise ValueError(\"All costs of the Matcher can't be 0\")\n-\n-    @torch.no_grad()\n-    def forward(self, outputs, targets):\n-        \"\"\"\n-        Args:\n-            outputs (`dict`):\n-                A dictionary that contains at least these entries:\n-                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n-                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n-                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n-                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n-                  ground-truth\n-                 objects in the target) containing the class labels\n-                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n-\n-        Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n-            - index_i is the indices of the selected predictions (in order)\n-            - index_j is the indices of the corresponding selected targets (in order)\n-            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n-        \"\"\"\n-        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n-\n-        # We flatten to compute the cost matrices in a batch\n-        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n-        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n-\n-        # Also concat the target labels and boxes\n-        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n-        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n-\n-        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n-        # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n-        class_cost = -out_prob[:, target_ids]\n-\n-        # Compute the L1 cost between boxes\n-        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-\n-        # Compute the giou cost between boxes\n-        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n-\n-        # Final cost matrix\n-        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n-        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n-\n-        sizes = [len(v[\"boxes\"]) for v in targets]\n-        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n-        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._upcast\n-def _upcast(t: Tensor) -> Tensor:\n-    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n-    if t.is_floating_point():\n-        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n-    else:\n-        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_area\n-def box_area(boxes: Tensor) -> Tensor:\n-    \"\"\"\n-    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n-\n-    Args:\n-        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n-            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n-            < x2` and `0 <= y1 < y2`.\n-\n-    Returns:\n-        `torch.FloatTensor`: a tensor containing the area for each box.\n-    \"\"\"\n-    boxes = _upcast(boxes)\n-    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.box_iou\n-def box_iou(boxes1, boxes2):\n-    area1 = box_area(boxes1)\n-    area2 = box_area(boxes2)\n-\n-    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n-    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n-\n-    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n-    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n-\n-    union = area1[:, None] + area2 - inter\n-\n-    iou = inter / union\n-    return iou, union\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.generalized_box_iou\n-def generalized_box_iou(boxes1, boxes2):\n-    \"\"\"\n-    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n-\n-    Returns:\n-        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n-    \"\"\"\n-    # degenerate boxes gives inf / nan results\n-    # so do an early check\n-    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n-        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n-    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n-        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n-    iou, union = box_iou(boxes1, boxes2)\n-\n-    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n-    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n-\n-    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n-    area = width_height[:, :, 0] * width_height[:, :, 1]\n-\n-    return iou - (area - union) / area\n-\n-\n-# Copied from transformers.models.detr.modeling_detr._max_by_axis\n-def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n-    maxes = the_list[0]\n-    for sublist in the_list[1:]:\n-        for index, item in enumerate(sublist):\n-            maxes[index] = max(maxes[index], item)\n-    return maxes\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.NestedTensor\n-class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n-        self.tensors = tensors\n-        self.mask = mask\n-\n-    def to(self, device):\n-        cast_tensor = self.tensors.to(device)\n-        mask = self.mask\n-        if mask is not None:\n-            cast_mask = mask.to(device)\n-        else:\n-            cast_mask = None\n-        return NestedTensor(cast_tensor, cast_mask)\n-\n-    def decompose(self):\n-        return self.tensors, self.mask\n-\n-    def __repr__(self):\n-        return str(self.tensors)\n-\n-\n-# Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list\n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n-    if tensor_list[0].ndim == 3:\n-        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n-        batch_shape = [len(tensor_list)] + max_size\n-        batch_size, num_channels, height, width = batch_shape\n-        dtype = tensor_list[0].dtype\n-        device = tensor_list[0].device\n-        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n-        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n-        for img, pad_img, m in zip(tensor_list, tensor, mask):\n-            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n-            m[: img.shape[1], : img.shape[2]] = False\n-    else:\n-        raise ValueError(\"Only 3-dimensional tensors are supported\")\n-    return NestedTensor(tensor, mask)"
        },
        {
            "sha": "8a61c15e30a0a992e9dcffd1ce60a3b3db9b8f25",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -1483,18 +1483,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "9b8244c243fc4a2280372262c15b8fe2fdc5166c",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1c7e89620b43f0924148e267a6da5d38450ce1f/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1c7e89620b43f0924148e267a6da5d38450ce1f/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=c1c7e89620b43f0924148e267a6da5d38450ce1f",
            "patch": "@@ -144,6 +144,57 @@\n         \"initializer_range\",\n         \"supported_aspect_ratios\",\n     ],\n+    \"ConditionalDetrConfig\": [\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"cls_loss_coefficient\",\n+        \"dice_loss_coefficient\",\n+        \"focal_alpha\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+        \"mask_loss_coefficient\",\n+    ],\n+    \"DetrConfig\": [\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"dice_loss_coefficient\",\n+        \"eos_coefficient\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+        \"mask_loss_coefficient\",\n+    ],\n+    \"GroundingDinoConfig\": [\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"focal_alpha\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+    ],\n+    \"RTDetrConfig\": [\n+        \"eos_coefficient\",\n+        \"focal_loss_alpha\",\n+        \"focal_loss_gamma\",\n+        \"matcher_alpha\",\n+        \"matcher_bbox_cost\",\n+        \"matcher_class_cost\",\n+        \"matcher_gamma\",\n+        \"matcher_giou_cost\",\n+        \"use_focal_loss\",\n+        \"weight_loss_bbox\",\n+        \"weight_loss_giou\",\n+        \"weight_loss_vfl\",\n+    ],\n+    \"YolosConfig\": [\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"eos_coefficient\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+    ],\n }\n \n "
        }
    ],
    "stats": {
        "total": 5997,
        "additions": 1652,
        "deletions": 4345
    }
}