{
    "author": "HyunZ118",
    "message": "ğŸŒ [i18n-KO] Translated clipseg.md to Korean (#39903)\n\n* docs: ko: model_doc/clipseg.md\n\n* fix: manual edits\n\n* Apply suggestions from code review\n\nCo-authored-by: Kim Juwon <81630351+Kim-Ju-won@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Kim Juwon <81630351+Kim-Ju-won@users.noreply.github.com>",
    "sha": "c81f426f9a1a6bb860532201d3efb2f5c0bd087c",
    "files": [
        {
            "sha": "74833464ccd108475ee6670b8a1280f73315e2ba",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c81f426f9a1a6bb860532201d3efb2f5c0bd087c/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c81f426f9a1a6bb860532201d3efb2f5c0bd087c/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=c81f426f9a1a6bb860532201d3efb2f5c0bd087c",
            "patch": "@@ -1037,7 +1037,7 @@\n         title: Chinese-CLIP\n       - local: model_doc/clip\n         title: CLIP\n-      - local: in_translation\n+      - local: model_doc/clipseg\n         title: CLIPSeg\n       - local: in_translation\n         title: CLVP"
        },
        {
            "sha": "a2a28ca64a69d54d34540449e6214c692f589ef3",
            "filename": "docs/source/ko/model_doc/clipseg.md",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/c81f426f9a1a6bb860532201d3efb2f5c0bd087c/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c81f426f9a1a6bb860532201d3efb2f5c0bd087c/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md?ref=c81f426f9a1a6bb860532201d3efb2f5c0bd087c",
            "patch": "@@ -0,0 +1,92 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# CLIPSeg[[clipseg]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## ê°œìš”[[overview]]\n+\n+CLIPSeg ëª¨ë¸ì€ Timo LÃ¼ddeckeì™€ Alexander Eckerê°€ [Image Segmentation Using Text and Image Prompts](https://huggingface.co/papers/2112.10003) ë…¼ë¬¸ì—ì„œ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. CLIPSegëŠ” ê°€ì¤‘ì¹˜ê°€ ê³ ì •ëœ CLIP ëª¨ë¸ì— ìµœì†Œí•œì˜ ë””ì½”ë”ë¥¼ ê²°í•©í•˜ì—¬ ì œë¡œìƒ· ë° ì›ìƒ· ì´ë¯¸ì§€ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+*ì´ë¯¸ì§€ ë¶„í• ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „ì— ì •ì˜ëœ ê°ì²´ í´ë˜ìŠ¤ ì§‘í•©ì— ëŒ€í•´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë³´ë‹¤ ë³µì¡í•œ ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ë ¤ë©´, í•´ë‹¹ ë‚´ìš©ì„ í¬í•¨í•œ ë°ì´í„° ì„¸íŠ¸ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•´ì•¼ í•˜ë¯€ë¡œ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. ì´ì— ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ì‹œì ì— í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ì„ì˜ì˜ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„í• ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ ê³¼ì œë¥¼ ê°–ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì´ë¯¸ì§€ ë¶„í•  íƒœìŠ¤í¬â€”ì§€ì‹œ í‘œí˜„ ë¶„í• (referring expression segmentation), ì œë¡œìƒ· ë¶„í• (zero-shot segmentation), ì›ìƒ· ë¶„í• (one-shot segmentation)â€”ì„ ë‹¨ì¼ í†µí•© ëª¨ë¸ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” CLIP ëª¨ë¸ì„ ë°±ë³¸ìœ¼ë¡œ ì‚¼ê³ , ê³ í•´ìƒë„ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ë””ì½”ë”ë¥¼ ì¶”ê°€í•´ ì´ë¥¼ í™•ì¥í–ˆìŠµë‹ˆë‹¤. í™•ì¥ëœ PhraseCut ë°ì´í„° ì„¸íŠ¸ë¥¼ í™œìš©í•´ í›ˆë ¨í•œ ë³¸ ì‹œìŠ¤í…œì€ ììœ  í˜•ì‹ì˜ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë‚˜ íŠ¹ì • ëª©ì ì„ í‘œí˜„í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ì´ì§„ ë¶„í•  ë§µì„ ìƒì„±í•©ë‹ˆë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ì˜ ë‹¤ì–‘í•œ êµ¬ì„± ë°©ì‹ê³¼ ê·¸ íš¨ê³¼ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì…ë ¥ ë°©ì‹ì€ ì•ì„œ ì–¸ê¸‰í•œ ì„¸ ê°€ì§€ íƒœìŠ¤í¬ë¿ë§Œ ì•„ë‹ˆë¼, í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ë¡œ ì§ˆì˜í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì´ì§„ ë¶„í•  ë¬¸ì œì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë³¸ ì‹œìŠ¤í…œì´ ì–´í¬ë˜ìŠ¤(affordance)ë‚˜ ê°ì²´ ì†ì„±ê³¼ ê°™ì€ ì¼ë°˜í™”ëœ ì§ˆì˜ì—ë„ ë†’ì€ ì ì‘ë ¥ì„ ë³´ì„ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.*\n+\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\n+alt=\"drawing\" width=\"600\"/> \n+\n+<small> CLIPSeg ê°œìš”. <a href=\"https://huggingface.co/papers/2112.10003\">ì›ë³¸ ë…¼ë¬¸</a>ì—ì„œ ë°œì·Œ. </small>\n+\n+ì´ ëª¨ë¸ì€ [nielsr](https://huggingface.co/nielsr)ë‹˜ì´ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n+ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/timojl/clipseg)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ[[usage-tips]]\n+\n+- [`CLIPSegForImageSegmentation`]ì€ [`CLIPSegModel`]ê³¼ ë™ì¼í•œ, [`CLIPSegModel`] ìœ„ì— ë””ì½”ë”ë¥¼ ì¶”ê°€í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n+- [`CLIPSegForImageSegmentation`]ì€ í…ŒìŠ¤íŠ¸ ì‹œì ì— ì„ì˜ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„í• ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ í”„ë¡¬í”„íŠ¸ëŠ” í…ìŠ¤íŠ¸(`input_ids`), ì´ë¯¸ì§€(`conditional_pixel_values`), ì‚¬ìš©ì ì •ì˜ ì¡°ê±´ë¶€ ì„ë² ë”©(`conditional_embeddings`)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+\n+## ë¦¬ì†ŒìŠ¤[[resources]]\n+\n+CLIPSegë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë  Hugging Face ê³µì‹ ìë£Œì™€ ì»¤ë®¤ë‹ˆí‹°(ğŸŒ ì•„ì´ì½˜ìœ¼ë¡œ í‘œì‹œ)ì˜ ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤ ëª©ë¡ì„ ì•„ë˜ì— ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. í˜¹ì‹œ ëª©ë¡ì— ì—†ëŠ” ìƒˆë¡œìš´ ìë£Œë‚˜ íŠœí† ë¦¬ì–¼ì„ ê³µìœ í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ì–¸ì œë“ ì§€ Pull Requestë¥¼ í†µí•´ ì œì•ˆí•´ ì£¼ì„¸ìš”. ì €í¬ê°€ ê²€í†  í›„ ì†Œì¤‘íˆ ë°˜ì˜í•˜ê² ìŠµë‹ˆë‹¤! ê¸°ì¡´ ìë£Œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ë‚´ìš©ì´ë¼ë©´ ë”ìš± ì¢‹ìŠµë‹ˆë‹¤.\n+\n+\n+<PipelineTag pipeline=\"image-segmentation\"/>\n+\n+- [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb)ì„ ì‹œì—°í•˜ëŠ” ë…¸íŠ¸ë¶.\n+\n+## CLIPSegConfig[[transformers.CLIPSegConfig]]\n+\n+[[autodoc]] CLIPSegConfig\n+    - from_text_vision_configs\n+\n+## CLIPSegTextConfig[[transformers.CLIPSegTextConfig]]\n+\n+[[autodoc]] CLIPSegTextConfig\n+\n+## CLIPSegVisionConfig[[transformers.CLIPSegVisionConfig]]\n+\n+[[autodoc]] CLIPSegVisionConfig\n+\n+## CLIPSegProcessor[[transformers.CLIPSegProcessor]]\n+\n+[[autodoc]] CLIPSegProcessor\n+\n+## CLIPSegModel[[transformers.CLIPSegModel]]\n+\n+[[autodoc]] CLIPSegModel\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## CLIPSegTextModel[[transformers.CLIPSegTextModel]]\n+\n+[[autodoc]] CLIPSegTextModel\n+    - forward\n+\n+## CLIPSegVisionModel[[transformers.CLIPSegVisionModel]]\n+\n+[[autodoc]] CLIPSegVisionModel\n+    - forward\n+\n+## CLIPSegForImageSegmentation[[transformers.CLIPSegForImageSegmentation]]\n+\n+[[autodoc]] CLIPSegForImageSegmentation\n+    - forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 93,
        "deletions": 1
    }
}