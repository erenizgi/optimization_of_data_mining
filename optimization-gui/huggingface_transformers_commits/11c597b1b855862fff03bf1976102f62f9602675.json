{
    "author": "SunMarc",
    "message": "Remove deprecated args in Trainer for v5 (#41404)\n\nremove deprecated code",
    "sha": "11c597b1b855862fff03bf1976102f62f9602675",
    "files": [
        {
            "sha": "aeb8fde81467afdaf3adb75a16ddc64c8b9a0ec2",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/11c597b1b855862fff03bf1976102f62f9602675/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11c597b1b855862fff03bf1976102f62f9602675/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=11c597b1b855862fff03bf1976102f62f9602675",
            "patch": "@@ -1013,12 +1013,7 @@ class TrainingArguments:\n         default=False,\n         metadata={\"help\": \"Whether to use fp16 (mixed) precision instead of 32-bit\"},\n     )\n-    half_precision_backend: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"The backend to be used for half precision. This argument is deprecated. We will always use CPU/CUDA AMP from torch\",\n-        },\n-    )\n+\n     bf16_full_eval: bool = field(\n         default=False,\n         metadata={\n@@ -1376,14 +1371,6 @@ class TrainingArguments:\n             \"help\": \"Which mode to use with `torch.compile`, passing one will trigger a model compilation.\",\n         },\n     )\n-\n-    include_tokens_per_second: Optional[bool] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"This arg is deprecated and will be removed in v5 , use `include_num_input_tokens_seen` instead.\"\n-        },\n-    )\n-\n     include_num_input_tokens_seen: Union[str, bool] = field(\n         default=\"no\",\n         metadata={\n@@ -1585,11 +1572,6 @@ def __post_init__(self):\n                         # gpu\n                         raise ValueError(error_message)\n \n-        if self.half_precision_backend is not None:\n-            raise ValueError(\n-                \"half_precision_backend is deprecated. For mixed precision, we will always use CPU/CUDA AMP from torch\"\n-            )\n-\n         if self.fp16 and self.bf16:\n             raise ValueError(\"At most one of fp16 and bf16 can be True, but not both\")\n \n@@ -1887,12 +1869,6 @@ def __post_init__(self):\n                 \" when --dataloader_num_workers > 1.\"\n             )\n \n-        if self.include_tokens_per_second is not None:\n-            logger.warning(\n-                \"include_tokens_per_second is deprecated and will be removed in v5. Use `include_num_input_tokens_seen` instead. \"\n-            )\n-            self.include_num_input_tokens_seen = self.include_tokens_per_second\n-\n         if isinstance(self.include_num_input_tokens_seen, bool):\n             self.include_num_input_tokens_seen = \"all\" if self.include_num_input_tokens_seen else \"no\"\n "
        }
    ],
    "stats": {
        "total": 26,
        "additions": 1,
        "deletions": 25
    }
}