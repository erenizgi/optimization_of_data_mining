{
    "author": "Cyrilvallez",
    "message": "Fix Layer device placement in Caches (#39732)\n\n* fix device placement\n\n* style\n\n* typo in comment",
    "sha": "fc2bd1eac0327e849c6125033a13e3f6af6ee69e",
    "files": [
        {
            "sha": "8f9a007a10d1ac99a863a24872e18ac6f4f9d935",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc2bd1eac0327e849c6125033a13e3f6af6ee69e/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc2bd1eac0327e849c6125033a13e3f6af6ee69e/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=fc2bd1eac0327e849c6125033a13e3f6af6ee69e",
            "patch": "@@ -263,6 +263,14 @@ def update(\n         key_states = key_states.to(self.keys.dtype)\n         value_states = value_states.to(self.values.dtype)\n \n+        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n+        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n+        # will always have the same device\n+        if self.device != key_states.device:\n+            self.device = key_states.device\n+            self.keys = self.keys.to(self.device)\n+            self.values = self.values.to(self.device)\n+\n         if cache_position is None:\n             # Prefill phase where seq_len potentially equals max_cache_len. Directly copy.\n             self.keys.copy_(key_states)\n@@ -341,6 +349,14 @@ def update(\n         if cache_position is None:\n             raise ValueError(\"`cache_position` must be provided for SlidingWindowLayer.\")\n \n+        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n+        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n+        # will always have the same device\n+        if self.device != key_states.device:\n+            self.device = key_states.device\n+            self.keys = self.keys.to(self.device)\n+            self.values = self.values.to(self.device)\n+\n         key_states = key_states.to(self.keys.dtype)\n         value_states = value_states.to(self.values.dtype)\n \n@@ -354,7 +370,7 @@ def update(\n             return key_states, value_states\n \n         # Sliding window logic for generation phase or prefill < window\n-        slicing = torch.arange(self.max_cache_len, device=value_states.device)\n+        slicing = torch.arange(self.max_cache_len, device=self.device)\n         current_seq_len = cache_position[-1] + 1  # Use last position to determine current length\n         to_shift = current_seq_len > self.max_cache_len\n         indices = (slicing + to_shift.sum()) % self.max_cache_len\n@@ -411,6 +427,14 @@ def update(\n         if cache_position is None:\n             raise ValueError(\"`cache_position` must be provided for ChunkedSlidingLayer.\")\n \n+        # This may be needed if the Layer was not created with the right device in the beginning, i.e. if it did not respect\n+        # the device_map. However, even if it is the case, this will only run once, because then the new states received\n+        # will always have the same device\n+        if self.device != key_states.device:\n+            self.device = key_states.device\n+            self.keys = self.keys.to(self.device)\n+            self.values = self.values.to(self.device)\n+\n         cumulative_length = self.cumulative_length\n         self.cumulative_length += key_states.shape[-2]\n         is_full = cumulative_length >= self.max_cache_len"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 25,
        "deletions": 1
    }
}