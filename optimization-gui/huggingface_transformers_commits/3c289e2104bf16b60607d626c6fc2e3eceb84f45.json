{
    "author": "FightingZhen",
    "message": "[performance_optim] reduce frequency of declaring attention_mask in Ascend NPU flash attention (#38278)\n\n[performance_optim] reduce frequency of declaring attention_mask in ASCEND NPU flash attention",
    "sha": "3c289e2104bf16b60607d626c6fc2e3eceb84f45",
    "files": [
        {
            "sha": "e32af9f4bc9ed6b47f9dfe86e5713a776def94fa",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c289e2104bf16b60607d626c6fc2e3eceb84f45/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c289e2104bf16b60607d626c6fc2e3eceb84f45/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=3c289e2104bf16b60607d626c6fc2e3eceb84f45",
            "patch": "@@ -37,6 +37,8 @@\n         \"or 3 (down-right aligned causal mask).\"\n     )\n \n+ATTN_MASK_NPU = None\n+\n \n def is_npu_fa2_top_left_aligned_causal_mask():\n     return SPARSE_MODE == TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE if is_torch_npu_available() else False\n@@ -171,7 +173,9 @@ def npu_flash_attn_func(\n         head_num = q.shape[2]\n         output = torch_npu.npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n     else:\n-        attn_mask_npu = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n+        global ATTN_MASK_NPU\n+        if ATTN_MASK_NPU is None:\n+            ATTN_MASK_NPU = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n         head_num = q.shape[2]\n         output = torch_npu.npu_fusion_attention(\n             q,\n@@ -181,7 +185,7 @@ def npu_flash_attn_func(\n             \"BSND\",\n             keep_prob=keep_prob,\n             scale=softmax_scale,\n-            atten_mask=attn_mask_npu,\n+            atten_mask=ATTN_MASK_NPU,\n             sparse_mode=SPARSE_MODE,\n         )[0]\n \n@@ -222,7 +226,9 @@ def npu_flash_attn_varlen_func(\n             actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),\n         )[0]\n     else:\n-        attn_mask_npu = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n+        global ATTN_MASK_NPU\n+        if ATTN_MASK_NPU is None:\n+            ATTN_MASK_NPU = torch.triu(torch.ones([2048, 2048], device=q.device), diagonal=1).bool()\n         head_num = q.shape[1]\n         output = torch_npu.npu_fusion_attention(\n             q,\n@@ -231,7 +237,7 @@ def npu_flash_attn_varlen_func(\n             head_num,\n             pse=None,\n             padding_mask=None,\n-            atten_mask=attn_mask_npu,\n+            atten_mask=ATTN_MASK_NPU,\n             scale=softmax_scale,\n             keep_prob=keep_prob,\n             input_layout=\"TND\","
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}