{
    "author": "rdonggroq",
    "message": "Fix smart resize (#38706)\n\n* Fix smart_resize bug\n\n* Add smart_resize test\n\n* Remove unnecessary error checking\n\n* Fix smart_resize tests\n\n---------\n\nCo-authored-by: Richard Dong <rdong@rdong.c.groq-143208.internal>",
    "sha": "afdb821318e06e670c7238a9059e7e031065e319",
    "files": [
        {
            "sha": "c82f2dc42aeb77d1fa1bbef32245214152038113",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/afdb821318e06e670c7238a9059e7e031065e319/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afdb821318e06e670c7238a9059e7e031065e319/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=afdb821318e06e670c7238a9059e7e031065e319",
            "patch": "@@ -81,18 +81,16 @@ def smart_resize(\n     3. The aspect ratio of the image is maintained as closely as possible.\n \n     \"\"\"\n-    if height < factor or width < factor:\n-        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n-    elif max(height, width) / min(height, width) > 200:\n+    if max(height, width) / min(height, width) > 200:\n         raise ValueError(\n             f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n         )\n     h_bar = round(height / factor) * factor\n     w_bar = round(width / factor) * factor\n     if h_bar * w_bar > max_pixels:\n         beta = math.sqrt((height * width) / max_pixels)\n-        h_bar = math.floor(height / beta / factor) * factor\n-        w_bar = math.floor(width / beta / factor) * factor\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n     elif h_bar * w_bar < min_pixels:\n         beta = math.sqrt(min_pixels / (height * width))\n         h_bar = math.ceil(height * beta / factor) * factor"
        },
        {
            "sha": "a4826428acab4a7fbcd9444173a56d7355e76971",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/afdb821318e06e670c7238a9059e7e031065e319/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afdb821318e06e670c7238a9059e7e031065e319/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=afdb821318e06e670c7238a9059e7e031065e319",
            "patch": "@@ -64,18 +64,16 @@ def smart_resize(\n     3. The aspect ratio of the image is maintained as closely as possible.\n \n     \"\"\"\n-    if height < factor or width < factor:\n-        raise ValueError(f\"height:{height} and width:{width} must be larger than factor:{factor}\")\n-    elif max(height, width) / min(height, width) > 200:\n+    if max(height, width) / min(height, width) > 200:\n         raise ValueError(\n             f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n         )\n     h_bar = round(height / factor) * factor\n     w_bar = round(width / factor) * factor\n     if h_bar * w_bar > max_pixels:\n         beta = math.sqrt((height * width) / max_pixels)\n-        h_bar = math.floor(height / beta / factor) * factor\n-        w_bar = math.floor(width / beta / factor) * factor\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n     elif h_bar * w_bar < min_pixels:\n         beta = math.sqrt(min_pixels / (height * width))\n         h_bar = math.ceil(height * beta / factor) * factor"
        },
        {
            "sha": "2171a7ddb6ba7d3fd1c6e0c5c72d63aef1383c9b",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 43,
            "deletions": 30,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/afdb821318e06e670c7238a9059e7e031065e319/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afdb821318e06e670c7238a9059e7e031065e319/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=afdb821318e06e670c7238a9059e7e031065e319",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import itertools\n import tempfile\n import unittest\n \n@@ -169,18 +170,18 @@ def test_call_pil(self):\n                 self.assertIsInstance(image[0], Image.Image)\n \n             # Test not batched input\n-            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (4900, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n             self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n             # Test batched\n-            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (34300, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n@@ -196,18 +197,18 @@ def test_call_numpy(self):\n                 self.assertIsInstance(image[0], np.ndarray)\n \n             # Test not batched input\n-            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (4900, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n             self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n             # Test batched\n-            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (34300, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n@@ -224,18 +225,18 @@ def test_call_pytorch(self):\n                 self.assertIsInstance(image[0], torch.Tensor)\n \n             # Test not batched input\n-            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (4900, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n             self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n             # Test batched\n-            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (34300, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n@@ -251,19 +252,19 @@ def test_nested_input(self):\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n \n             # Test batched as a list of images\n-            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-            encoded_images = prcocess_out.pixel_values\n-            image_grid_thws = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n             expected_output_image_shape = (34300, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n             self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n             # Test batched as a nested list of images, where each sublist is one batch\n             image_inputs_nested = image_inputs[:3] + image_inputs[3:]\n-            prcocess_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n-            encoded_images_nested = prcocess_out.pixel_values\n-            image_grid_thws_nested = prcocess_out.image_grid_thw\n+            process_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n+            encoded_images_nested = process_out.pixel_values\n+            image_grid_thws_nested = process_out.image_grid_thw\n             expected_output_image_shape = (34300, 1176)\n             expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n             self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n@@ -281,8 +282,8 @@ def test_video_inputs(self):\n             for num_frames, expected_dims in expected_dims_by_frames.items():\n                 image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n                 video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = prcocess_out.pixel_values_videos\n+                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+                encoded_video = process_out.pixel_values_videos\n                 expected_output_video_shape = (expected_dims, 1176)\n                 self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n \n@@ -293,8 +294,8 @@ def test_custom_patch_size(self):\n             for patch_size in (1, 3, 5, 7):\n                 image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n                 video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = prcocess_out.pixel_values_videos\n+                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+                encoded_video = process_out.pixel_values_videos\n                 expected_output_video_shape = (171500, 1176)\n                 self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n \n@@ -308,9 +309,21 @@ def test_custom_image_size(self):\n                 )\n \n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-            prcocess_out = image_processor_loaded(image_inputs, return_tensors=\"pt\")\n+            process_out = image_processor_loaded(image_inputs, return_tensors=\"pt\")\n             expected_output_video_shape = [112, 1176]\n-            self.assertListEqual(list(prcocess_out.pixel_values.shape), expected_output_video_shape)\n+            self.assertListEqual(list(process_out.pixel_values.shape), expected_output_video_shape)\n+\n+    def test_custom_pixels(self):\n+        pixel_choices = frozenset(itertools.product((100, 150, 200, 20000), (100, 150, 200, 20000)))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_dict = self.image_processor_dict.copy()\n+            for a_pixels, b_pixels in pixel_choices:\n+                image_processor_dict[\"min_pixels\"] = min(a_pixels, b_pixels)\n+                image_processor_dict[\"max_pixels\"] = max(a_pixels, b_pixels)\n+                image_processor = image_processing_class(**image_processor_dict)\n+                image_inputs = self.image_processor_tester.prepare_image_inputs()\n+                # Just checking that it doesn't raise an error\n+                image_processor(image_inputs, return_tensors=\"pt\")\n \n     def test_temporal_padding(self):\n         for image_processing_class in self.image_processor_list:"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 49,
        "deletions": 40
    }
}