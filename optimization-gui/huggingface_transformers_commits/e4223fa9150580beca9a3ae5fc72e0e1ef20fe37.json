{
    "author": "nevertmr",
    "message": "ğŸŒ [i18n-KO] Translated `main_classes/optimizer_schedules.md` to Korean (#39713)\n\n* docs: ko: main_classes/optimizer_schedules\n\n* feat: nmt draft\n\n* fix: improve TOC anchors and expressions in optimizer_schedules\n\n- Add TOC anchors to all section headers\n- Fix terminology and improve Korean expressions\n\n* fix: Correct translation of 'weight decay fixed' to 'ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ ì ìš©ëœ'\n\nChanged 'ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ ìˆ˜ì •ëœ' to 'ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ ì ìš©ëœ' for more accurate translation of 'weight decay fixed' in the context of optimization.\n\n* fix: Use more natural Korean inheritance expression\n\nChanged 'ì—ì„œ ìƒì†ë°›ëŠ”' to 'ì„ ìƒì†ë°›ëŠ”' to follow natural Korean grammar patterns for inheritance terminology.\n\n* fix: Use consistent 'ë¯¸ì„¸ ì¡°ì •' translation for 'finetuned models'\n\nChanged 'íŒŒì¸íŠœë‹ëœ' to 'ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸' to follow the established translation glossary for 'finetuned models' terminology.",
    "sha": "e4223fa9150580beca9a3ae5fc72e0e1ef20fe37",
    "files": [
        {
            "sha": "fcb8f51fcb28b2a472fce711ea3c81f1cef1aef7",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4223fa9150580beca9a3ae5fc72e0e1ef20fe37/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4223fa9150580beca9a3ae5fc72e0e1ef20fe37/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=e4223fa9150580beca9a3ae5fc72e0e1ef20fe37",
            "patch": "@@ -412,8 +412,8 @@\n       title: í…ìŠ¤íŠ¸ ìƒì„±\n     - local: main_classes/onnx\n       title: ONNX\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Optimization\n+    - local: main_classes/optimizer_schedules\n+      title: ìµœì í™”\n     - local: main_classes/output\n       title: ëª¨ë¸ ì¶œë ¥\n     - local: main_classes/peft"
        },
        {
            "sha": "24e1009e8274c70fdd09264b70d43d85f2ab57ca",
            "filename": "docs/source/ko/main_classes/optimizer_schedules.md",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4223fa9150580beca9a3ae5fc72e0e1ef20fe37/docs%2Fsource%2Fko%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4223fa9150580beca9a3ae5fc72e0e1ef20fe37/docs%2Fsource%2Fko%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Foptimizer_schedules.md?ref=e4223fa9150580beca9a3ae5fc72e0e1ef20fe37",
            "patch": "@@ -0,0 +1,76 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ìµœì í™”[[optimization]]\n+\n+`.optimization` ëª¨ë“ˆì€ ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤:\n+\n+- ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¤‘ì¹˜ ê°ì‡ ê°€ ì ìš©ëœ ì˜µí‹°ë§ˆì´ì €\n+- `_LRSchedule`ì„ ìƒì†ë°›ëŠ” ìŠ¤ì¼€ì¤„ ê°ì²´ í˜•íƒœì˜ ì—¬ëŸ¬ ìŠ¤ì¼€ì¤„\n+- ì—¬ëŸ¬ ë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  í´ë˜ìŠ¤\n+\n+\n+## AdaFactor (PyTorch)[[transformers.Adafactor]]\n+\n+[[autodoc]] Adafactor\n+\n+## AdamWeightDecay (TensorFlow)[[transformers.AdamWeightDecay]]\n+\n+[[autodoc]] AdamWeightDecay\n+\n+[[autodoc]] create_optimizer\n+\n+## ìŠ¤ì¼€ì¤„[[schedules]]\n+\n+### í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ (PyTorch)[[transformers.SchedulerType]]\n+\n+[[autodoc]] SchedulerType\n+\n+[[autodoc]] get_scheduler\n+\n+[[autodoc]] get_constant_schedule\n+\n+[[autodoc]] get_constant_schedule_with_warmup\n+\n+<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png\"/>\n+\n+[[autodoc]] get_cosine_schedule_with_warmup\n+\n+<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png\"/>\n+\n+[[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup\n+\n+<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png\"/>\n+\n+[[autodoc]] get_linear_schedule_with_warmup\n+\n+<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png\"/>\n+\n+[[autodoc]] get_polynomial_decay_schedule_with_warmup\n+\n+[[autodoc]] get_inverse_sqrt_schedule\n+\n+[[autodoc]] get_wsd_schedule\n+\n+### ì›œì—… (TensorFlow)[[transformers.WarmUp]]\n+\n+[[autodoc]] WarmUp\n+\n+## ê·¸ë˜ë””ì–¸íŠ¸ ì „ëµ[[gradient-strategies]]\n+\n+### GradientAccumulator (TensorFlow)[[transformers.GradientAccumulator]]\n+\n+[[autodoc]] GradientAccumulator"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 78,
        "deletions": 2
    }
}