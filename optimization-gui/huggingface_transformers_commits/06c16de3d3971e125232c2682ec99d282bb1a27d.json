{
    "author": "cyyever",
    "message": "Enable RUF013 to enforce optional typing (#37266)\n\n* Enable RUF013 for Optional typing\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Add Optional to types\n\n* Format code\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "06c16de3d3971e125232c2682ec99d282bb1a27d",
    "files": [
        {
            "sha": "f49cb1cc738ffa8b6c8d7c01d6446d0c60d2f637",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -20,7 +20,9 @@ line-length = 119\n [tool.ruff.lint]\n # Never enforce `E501` (line length violations).\n ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\" ]\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\"]\n+# RUF013: Checks for the use of implicit Optional\n+#  in type annotations when the default parameter value is None.\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\"]\n \n # Ignore import violations in all `__init__.py` files.\n [tool.ruff.lint.per-file-ignores]"
        },
        {
            "sha": "8a344c520ee7a7bea11340801a4493372a42d374",
            "filename": "src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -17,6 +17,7 @@\n import json\n import re\n from pathlib import Path\n+from typing import Optional\n \n import requests\n import torch\n@@ -319,7 +320,7 @@ def load_original_state_dict(repo_id, model_name):\n }\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     # Use the mapping to rename keys\n     for original_key, converted_key in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n         for key in list(state_dict_keys.keys()):"
        },
        {
            "sha": "784061a9ec6dfb6f1f0ea7c13e5de4680181b145",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -1972,8 +1972,8 @@ def __init__(\n         kernel_size: int,\n         stride: int,\n         groups: int = 1,\n-        padding: int = None,\n-        activation: str = None,\n+        padding: Optional[int] = None,\n+        activation: Optional[str] = None,\n     ):\n         super().__init__()\n         self.conv = nn.Conv2d("
        },
        {
            "sha": "c252f8224c9f5d0171a7178db2119b1f1098a0b6",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -1054,8 +1054,8 @@ def __init__(\n         kernel_size: int,\n         stride: int,\n         groups: int = 1,\n-        padding: int = None,\n-        activation: str = None,\n+        padding: Optional[int] = None,\n+        activation: Optional[str] = None,\n     ):\n         super().__init__(config, in_channels, out_channels, kernel_size, stride, padding=None, activation=activation)\n         self.conv = nn.Conv2d("
        },
        {
            "sha": "28d1715820746c817defd4e28d3709d2abd4ba60",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -66,7 +66,7 @@ def __init__(\n         mask_group_max_patches: Optional[int] = None,\n         mask_group_min_patches: int = 16,\n         mask_group_min_aspect_ratio: Optional[float] = 0.3,\n-        mask_group_max_aspect_ratio: float = None,\n+        mask_group_max_aspect_ratio: Optional[float] = None,\n     ):\n         if not isinstance(input_size, tuple):\n             input_size = (input_size,) * 2"
        },
        {
            "sha": "f687a2e7146ac84a2e8d0df72464e4b44d09d494",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -15,6 +15,7 @@\n import gc\n import os\n import re\n+from typing import Optional\n \n import torch\n from einops import rearrange\n@@ -116,7 +117,7 @@\n CONTEXT_LENGTH = 8192\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None, path: str = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None, path: Optional[str] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings.\n@@ -303,7 +304,9 @@ def write_model(\n     del model\n \n \n-def write_tokenizer(save_dir: str, push_to_hub: bool = False, path: str = None, hub_dir: str = None):\n+def write_tokenizer(\n+    save_dir: str, push_to_hub: bool = False, path: Optional[str] = None, hub_dir: Optional[str] = None\n+):\n     if LM_TYPE_CORRESPONDENCE[path] == \"qwen2\":\n         tokenizer = AutoTokenizer.from_pretrained(\n             \"Qwen/Qwen2.5-VL-7B-Instruct\",\n@@ -355,7 +358,7 @@ def write_tokenizer(save_dir: str, push_to_hub: bool = False, path: str = None,\n         tokenizer.push_to_hub(hub_dir, use_temp_dir=True)\n \n \n-def write_image_processor(save_dir: str, push_to_hub: bool = False, hub_dir: str = None):\n+def write_image_processor(save_dir: str, push_to_hub: bool = False, hub_dir: Optional[str] = None):\n     image_processor = GotOcr2ImageProcessorFast(\n         do_resize=True,\n         size={\"height\": 448, \"width\": 448},"
        },
        {
            "sha": "a79c34b08c00e3b06cab66feac548d8c245dd4fb",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -269,7 +269,7 @@ def __call__(\n         return BatchFeature(data={**text_inputs, **image_videos_inputs}, tensor_type=return_tensors)\n \n     def sample_indices_fn(\n-        self, metadata: VideoMetadata, num_frames: int = None, initial_shift: Union[bool, float, int] = True\n+        self, metadata: VideoMetadata, num_frames: Optional[int] = None, initial_shift: Union[bool, float, int] = True\n     ):\n         \"\"\"\n         The function to generate indices of frames to sample from a video."
        },
        {
            "sha": "99ae35a4b3133bc0003035807fbb71b7e2b2fd66",
            "filename": "src/transformers/models/janus/convert_janus_weights_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -25,6 +25,7 @@\n import json\n import os\n import re\n+from typing import Optional\n \n import torch\n from accelerate import init_empty_weights\n@@ -167,7 +168,9 @@ def convert_state_dict_to_hf(state_dict):\n     return converted_state_dict\n \n \n-def ensure_model_downloaded(repo_id: str = None, revision: str = None, local_dir: str = None) -> str:\n+def ensure_model_downloaded(\n+    repo_id: Optional[str] = None, revision: Optional[str] = None, local_dir: Optional[str] = None\n+) -> str:\n     \"\"\"\n     Ensures model files are downloaded locally, downloads them if not.\n     Returns path to local files."
        },
        {
            "sha": "5138768453ff0ab83a52db65c3021dfd458cb9dd",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -98,15 +98,15 @@ class JanusImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         min_size: int = 14,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -411,13 +411,13 @@ def pad_to_square(\n     def postprocess(\n         self,\n         images: ImageInput,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n-        image_mean: List[float] = None,\n-        image_std: List[float] = None,\n-        input_data_format: str = None,\n-        return_tensors: str = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[List[float]] = None,\n+        image_std: Optional[List[float]] = None,\n+        input_data_format: Optional[str] = None,\n+        return_tensors: Optional[str] = None,\n     ):\n         \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale"
        },
        {
            "sha": "c9dec37ad1141eb6686de8d89e5927c4cabe47e6",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -1508,15 +1508,15 @@ class JanusImageProcessor(BlipImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         min_size: int = 14,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -1673,13 +1673,13 @@ def resize(\n     def postprocess(\n         self,\n         images: ImageInput,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n-        image_mean: List[float] = None,\n-        image_std: List[float] = None,\n-        input_data_format: str = None,\n-        return_tensors: str = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[List[float]] = None,\n+        image_std: Optional[List[float]] = None,\n+        input_data_format: Optional[str] = None,\n+        return_tensors: Optional[str] = None,\n     ):\n         \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale"
        },
        {
            "sha": "aaa77f68a45a1975af0d2feb0157842bac9c4d1e",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c16de3d3971e125232c2682ec99d282bb1a27d/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=06c16de3d3971e125232c2682ec99d282bb1a27d",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for MobileNetV2.\"\"\"\n \n-from typing import List, Tuple\n+from typing import List, Optional, Tuple\n \n from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n@@ -42,7 +42,7 @@ class MobileNetV2ImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     do_convert_rgb = None\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        }
    ],
    "stats": {
        "total": 73,
        "additions": 41,
        "deletions": 32
    }
}