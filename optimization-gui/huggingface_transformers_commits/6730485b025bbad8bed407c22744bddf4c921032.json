{
    "author": "itazap",
    "message": "clean_up_tokenization_spaces=False if unset (#31938)\n\n* clean_up_tokenization_spaces=False if unset\r\n\r\n* deprecate warning\r\n\r\n* updating param for old models\r\n\r\n* update models\r\n\r\n* make fix-copies\r\n\r\n* fix-copies and update bert models\r\n\r\n* warning msg\r\n\r\n* update prophet and clvp\r\n\r\n* updating test since space before is arbitrarily removed\r\n\r\n* remove warning for 4.45",
    "sha": "6730485b025bbad8bed407c22744bddf4c921032",
    "files": [
        {
            "sha": "07583b949661de12a9fca3afc231c02ff0a8f798",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -88,6 +88,9 @@ class BertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -105,6 +108,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -136,6 +140,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "10bbc096bf54d317f1a7222ae362aacd43247d74",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -91,6 +91,9 @@ class ConvBertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original ConvBERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -108,6 +111,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -139,6 +143,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "610000ce813a58f7a8dd5a3062e2e436b0b180a2",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -90,6 +90,9 @@ class DistilBertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -108,6 +111,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -138,6 +142,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "2acd86ca083997ea3193abd87c4c18d8a6cd127e",
            "filename": "src/transformers/models/electra/tokenization_electra.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -90,6 +90,9 @@ class ElectraTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original Electra).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -107,6 +110,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -138,6 +142,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "78499cbee4ec26c47c4a9c085845e88c1bb7b274",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -107,6 +107,9 @@ class FunnelTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -127,6 +130,7 @@ def __init__(\n         eos_token=\"</s>\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -159,6 +163,7 @@ def __init__(\n             eos_token=eos_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "62fb4c524f22dc8f33871922bcd9141215e8373f",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -91,6 +91,9 @@ class LayoutLMTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original LayoutLM).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -108,6 +111,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -139,6 +143,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8310993160a3250b1f2d86bc7ff709e304286081",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -90,6 +90,9 @@ class LxmertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original Lxmert).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -107,6 +110,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -138,6 +142,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "e4faaf12d3834ce623db5e73a2b3219024582ff2",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -92,6 +92,9 @@ class MobileBertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original MobileBERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -109,6 +112,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -140,6 +144,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8d46381f05677f3931d6607fa595989562c20b68",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -108,6 +108,9 @@ class MPNetTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -128,6 +131,7 @@ def __init__(\n         mask_token=\"<mask>\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         bos_token = AddedToken(bos_token, special=True) if isinstance(bos_token, str) else bos_token\n@@ -170,6 +174,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "dc8956da0935cc5ea80a51f8c0e46ec7ade5a7d9",
            "filename": "src/transformers/models/prophetnet/tokenization_prophetnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -308,6 +308,9 @@ class ProphetNetTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -330,6 +333,7 @@ def __init__(\n         mask_token: Optional[str] = \"[MASK]\",\n         tokenize_chinese_chars: Optional[bool] = True,\n         strip_accents: Optional[bool] = None,\n+        clean_up_tokenization_spaces: bool = True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -360,6 +364,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "9ac72fcc2608fea7a602aac70dce1434745ff264",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -91,6 +91,9 @@ class SqueezeBertTokenizer(PreTrainedTokenizer):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original SqueezeBERT).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -108,6 +111,7 @@ def __init__(\n         mask_token=\"[MASK]\",\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not os.path.isfile(vocab_file):\n@@ -139,6 +143,7 @@ def __init__(\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "69950396079aa1e21b6af673ca193e915fd894c0",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -225,6 +225,9 @@ class TapasTokenizer(PreTrainedTokenizer):\n             Minimum length of each question in terms of tokens (will be skipped otherwise).\n         max_question_length (`int`, *optional*):\n             Maximum length of each question in terms of tokens (will be skipped otherwise).\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -252,6 +255,7 @@ def __init__(\n         max_question_length=None,\n         model_max_length: int = 512,\n         additional_special_tokens: Optional[List[str]] = None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         if not is_pandas_available():\n@@ -322,6 +326,7 @@ def __init__(\n             max_question_length=max_question_length,\n             model_max_length=model_max_length,\n             additional_special_tokens=additional_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "c96f7a33147969c05994f7b0163e98df338fb654",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -1622,7 +1622,7 @@ def __init__(self, **kwargs):\n             )\n \n         # By default, cleaning tokenization spaces for both fast and slow tokenizers\n-        self.clean_up_tokenization_spaces = kwargs.pop(\"clean_up_tokenization_spaces\", True)\n+        self.clean_up_tokenization_spaces = kwargs.pop(\"clean_up_tokenization_spaces\", False)\n \n         # By default, do not split special tokens for both fast and slow tokenizers\n         self.split_special_tokens = kwargs.pop(\"split_special_tokens\", False)"
        },
        {
            "sha": "aa8d2d22a5bedd1aeb0cce062977b7e9e96b471a",
            "filename": "tests/models/clvp/test_tokenization_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -79,7 +79,7 @@ def get_tokenizer(self, **kwargs):\n     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_input_output_texts\n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n-        output_text = \"lower newer\"\n+        output_text = \"lower[SPACE]newer\"\n         return input_text, output_text\n \n     # Copied from transformers.tests.models.layoutxlm.test_tokenization_layoutxlm.LayoutXLMTokenizationTest.test_add_special_tokens"
        },
        {
            "sha": "4a4058891d3d7663cd5d593077e8a1e9fdfc050f",
            "filename": "tests/models/wav2vec2/test_tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -147,8 +147,8 @@ def test_tokenizer_decode_added_tokens(self):\n         batch_tokens = tokenizer.batch_decode(sample_ids)\n         batch_tokens_2 = tokenizer.batch_decode(sample_ids, skip_special_tokens=True)\n \n-        self.assertEqual(batch_tokens, [\"HELLO<unk>!?!?$$$\", \"BYE BYE<unk>$$$\"])\n-        self.assertEqual(batch_tokens_2, [\"HELO!?!?\", \"BYE BYE\"])\n+        self.assertEqual(batch_tokens, [\"HELLO<unk>!? !?$$$\", \"BYE BYE<unk>$$$\"])\n+        self.assertEqual(batch_tokens_2, [\"HELO!? !?\", \"BYE BYE\"])\n \n     def test_call(self):\n         # Tests that all call wrap to encode_plus and batch_encode_plus\n@@ -467,8 +467,8 @@ def test_tokenizer_decode_added_tokens(self):\n         batch_tokens = tokenizer.batch_decode(sample_ids)\n         batch_tokens_2 = tokenizer.batch_decode(sample_ids, skip_special_tokens=True)\n \n-        self.assertEqual(batch_tokens, [\"HELLO<unk>!?!?<new_tokens>$$$\", \"BYE BYE<unk><new_tokens>$$$\"])\n-        self.assertEqual(batch_tokens_2, [\"HELO!?!?<new_tokens>\", \"BYE BYE<new_tokens>\"])\n+        self.assertEqual(batch_tokens, [\"HELLO<unk>!? !?<new_tokens>$$$\", \"BYE BYE<unk><new_tokens>$$$\"])\n+        self.assertEqual(batch_tokens_2, [\"HELO!? !?<new_tokens>\", \"BYE BYE<new_tokens>\"])\n \n     def test_special_characters_in_vocab(self):\n         sent = \"ʈʰ æ æ̃ ˧ kʰ\""
        },
        {
            "sha": "96bed25ad16a2c2d03cfe50f63d9b30201c8b98b",
            "filename": "tests/models/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6730485b025bbad8bed407c22744bddf4c921032/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py?ref=6730485b025bbad8bed407c22744bddf4c921032",
            "patch": "@@ -249,7 +249,7 @@ def test_tokenizer_decode_added_tokens(self):\n         # fmt: on\n \n         batch_tokens = tokenizer.batch_decode(sample_ids)\n-        self.assertEqual(batch_tokens, [\"k s ɾ ɾ l ɭʲ!?!? $$$\", \"j ð s j ð s oːɹ $$$\"])\n+        self.assertEqual(batch_tokens, [\"k s ɾ ɾ l ɭʲ ! ? ! ? $$$\", \"j ð s j ð s oːɹ $$$\"])\n \n     @staticmethod\n     def get_from_offsets(offsets, key):"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 67,
        "deletions": 7
    }
}