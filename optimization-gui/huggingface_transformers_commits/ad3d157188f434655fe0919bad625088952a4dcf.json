{
    "author": "gante",
    "message": "[RoPE] abstract dynamic RoPE update under a decorator âœ¨  (#37249)\n\n* dynamic rope decorator\n\n* longrope; shorter fwd pass\n\n* propper docstring\n\n* make fixup",
    "sha": "ad3d157188f434655fe0919bad625088952a4dcf",
    "files": [
        {
            "sha": "35b8b2e88eb5ce37159196ab5e325a96203c3aeb",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -25,6 +25,10 @@ Most of those are only useful if you are studying the code of the models in the\n [[autodoc]] AttentionInterface\n     - register\n \n+## Rotary Position Embedding Functions\n+\n+[[autodoc]] dynamic_rope_update\n+\n ## Pytorch custom modules\n \n [[autodoc]] pytorch_utils.Conv1D"
        },
        {
            "sha": "96f993e7e595c01f031be648efcba783a13d0966",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -1483,7 +1483,7 @@\n \n     _import_structure[\"modeling_flash_attention_utils\"] = []\n     _import_structure[\"modeling_outputs\"] = []\n-    _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\"]\n+    _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\", \"dynamic_rope_update\"]\n     _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\", \"AttentionInterface\"]\n \n     # PyTorch models structure\n@@ -6762,7 +6762,7 @@\n             model_addition_debugger,\n             model_addition_debugger_context,\n         )\n-        from .modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+        from .modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n         from .modeling_utils import AttentionInterface, PreTrainedModel\n         from .models.albert import (\n             AlbertForMaskedLM,"
        },
        {
            "sha": "9a9cfd072b2788ebde08f26e3337ef6eb933d0c2",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import math\n+from functools import wraps\n from typing import Optional\n \n from .configuration_utils import PretrainedConfig\n@@ -26,6 +27,68 @@\n     import torch\n \n \n+def dynamic_rope_update(rope_forward):\n+    \"\"\"\n+    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE\n+    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).\n+\n+    Args:\n+        rope_forward (Callable):\n+            The forward pass of the RoPE implementation.\n+\n+    Returns:\n+        The decorated forward pass.\n+    \"\"\"\n+\n+    def longrope_frequency_update(self, position_ids, device):\n+        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if hasattr(self.config, \"original_max_position_embeddings\"):\n+            original_max_position_embeddings = self.config.original_max_position_embeddings\n+        else:\n+            original_max_position_embeddings = self.config.max_position_embeddings\n+        if seq_len > original_max_position_embeddings:\n+            if not hasattr(self, \"long_inv_freq\"):\n+                self.long_inv_freq, _ = self.rope_init_fn(\n+                    self.config, device, seq_len=original_max_position_embeddings + 1\n+                )\n+            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n+        else:\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+\n+    def dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @wraps(rope_forward)\n+    def wrapper(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            dynamic_frequency_update(self, position_ids, device=x.device)\n+        elif self.rope_type == \"longrope\":\n+            longrope_frequency_update(self, position_ids, device=x.device)\n+        return rope_forward(self, x, position_ids)\n+\n+    return wrapper\n+\n+\n def _compute_default_rope_parameters(\n     config: Optional[PretrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,"
        },
        {
            "sha": "c26fa30e0d4fd2f8f27d71440d1bfcc481cce6fb",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -752,47 +752,18 @@ def __init__(self, config: AriaTextConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "47c0ee5a931317c91735cd5a367585470788a2ce",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -142,47 +142,18 @@ def __init__(self, config: BambaConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "bd527959794d845e2a6549cf5a4c280ced9a6ac8",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 7,
            "deletions": 34,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -101,45 +101,18 @@ def __init__(self, config: CohereConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "17644ff4f8c9d0c35175599c039f8f8ba2f29e20",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -31,6 +31,7 @@\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n@@ -73,25 +74,17 @@ def forward(self, hidden_states):\n \n class CohereRotaryEmbedding(LlamaRotaryEmbedding):\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "18a3a50ac157fee0a7dfd8db9984c61cd83b236e",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 34,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -30,7 +30,7 @@\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -68,45 +68,18 @@ def __init__(self, config: Cohere2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "7d932a4f15854ba2f7d53bfe6e863472ad2dcd63",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -18,7 +18,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -82,47 +82,18 @@ def __init__(self, config: DeepseekV3Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "e444a423234123064a4a9729973005cbbde6a5a3",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -44,7 +44,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -644,47 +644,18 @@ def __init__(self, config: DiffLlamaConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "15d89e36770771f845e9e55b3ee2fa47195af0af",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -1225,47 +1225,18 @@ def __init__(self, config: Emu3Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "ee76f7cbb02603cafe146507b86b01338f01fe1e",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -127,47 +127,18 @@ def __init__(self, config: FalconConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "9b349a43816303b437746a016897f1ca6682890e",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -35,7 +35,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -118,47 +118,18 @@ def __init__(self, config: GemmaConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "144a94ef33e9c9a7646001ddd19f6f0640c5e9dc",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -35,7 +35,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -365,47 +365,18 @@ def __init__(self, config: Gemma2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "0988e2692aa430a1ec9e6bbd8c645bc2e4c58d99",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -33,7 +33,7 @@\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -160,47 +160,18 @@ def __init__(self, config: Gemma3TextConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "07365a495f9bc2d72b7f35321945f54994720d52",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -36,7 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -282,47 +282,18 @@ def __init__(self, config: GlmConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "86ad00d3c2ec10540ea39f501e0e081edb746546",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -21,7 +21,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -306,47 +306,18 @@ def __init__(self, config: GPTNeoXConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "83955ff050108a20b7fcb3754f9fa6281eef0e12",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -27,7 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     is_torch_flex_attn_available,\n@@ -250,47 +250,18 @@ def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "e65f43229146af41903fbdb4653d6d9ce1817418",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -336,47 +336,18 @@ def __init__(self, config: GraniteConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "3b71ca4da633a5da1ff3f084a4204ea863ccb9fd",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -30,7 +30,7 @@\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -180,47 +180,18 @@ def __init__(self, config: GraniteMoeConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "81644fc3c38874d01daf209fee7f07db33b0855a",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -770,47 +770,18 @@ def __init__(self, config: GraniteMoeSharedConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "37350ae4626b3febe8e846017d8bc7e37999016f",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -101,47 +101,18 @@ def __init__(self, config: HeliumConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "7a4b6b36f1a7e4ea3081d4c5d5c640bdc70a540a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -32,7 +32,7 @@\n     MoeModelOutputWithPast,\n     SequenceClassifierOutputWithPast,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -412,47 +412,18 @@ def __init__(self, config: JetMoeConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "938fd7d3265a69d1213f56a1052ea45ec77cfb6b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -36,7 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n@@ -107,47 +107,18 @@ def __init__(self, config: LlamaConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "6313ff4b65653ce653d751c679dfc449ac239d66",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -381,47 +381,18 @@ def __init__(self, config: MimiConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "494f94dd3480d26e410b6aabaebc8cc32abdfc3f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -22,7 +22,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -290,47 +290,18 @@ def __init__(self, config: MistralConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "5c6bea2b58d04c239461631fabf5b8fb57ff347a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -45,7 +45,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -412,47 +412,18 @@ def __init__(self, config: MixtralConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "48e863d68689e1dda8cfa76eb7a2717233bd2bf8",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 33,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -28,7 +28,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -998,44 +998,18 @@ def __init__(self, config: MllamaTextConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "5b2486e2c283d583b2a331ac08338b47c506f9d1",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -264,47 +264,18 @@ def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Opti\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "317c7ac6df6827d2ddfd2d05732d2b50982f6e19",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -41,7 +41,7 @@\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -335,47 +335,18 @@ def __init__(self, config: MoonshineConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "0cf575187f70f5b6a6d20e25748d4e20197e4ed5",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     ModelOutput,\n     Seq2SeqLMOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -325,47 +325,18 @@ def __init__(self, config: MoshiConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "b29b4bf0ad303e54b3b454d3cf7071a73ba039b7",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -35,7 +35,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -112,47 +112,18 @@ def __init__(\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "4ad98556eea7c6e17a3be489e4a0e590ed356c8e",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -17,7 +17,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -301,47 +301,18 @@ def __init__(self, config: OlmoConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "3404c5e817776804ba64e1a9ce6d096f4a921753",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -16,7 +16,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -302,47 +302,18 @@ def __init__(self, config: Olmo2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "f6d21fb05eead864951df07760316aefa345c6d8",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -28,7 +28,7 @@\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -175,47 +175,18 @@ def __init__(self, config: OlmoeConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "508e0c80733aa669ddac393b742b5e98e9566abc",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -36,7 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -82,47 +82,18 @@ def __init__(self, config: PersimmonConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "51e518ffc203bda72662defa846a82c938d1d45d",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -21,7 +21,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -297,47 +297,18 @@ def __init__(self, config: PhiConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "1c8688b28f859d2e0441ad11ea638f43fa95ed5d",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 83,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -325,88 +325,6 @@ def forward(\n         return outputs\n \n \n-class Phi3RotaryEmbedding(nn.Module):\n-    def __init__(self, config: Phi3Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-        elif self.rope_type == \"longrope\":\n-            self._longrope_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-    def _longrope_frequency_update(self, position_ids, device):\n-        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if hasattr(self.config, \"original_max_position_embeddings\"):\n-            original_max_position_embeddings = self.config.original_max_position_embeddings\n-        else:\n-            original_max_position_embeddings = self.config.max_position_embeddings\n-        if seq_len > original_max_position_embeddings:\n-            if not hasattr(self, \"long_inv_freq\"):\n-                self.long_inv_freq, _ = self.rope_init_fn(\n-                    self.config, device, seq_len=original_max_position_embeddings + 1\n-                )\n-            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n-        else:\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-\n-\n PHI3_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -455,6 +373,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Phi3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Phi3Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI3_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "4e38e3164df9fa36c5d2eb1dbb2c34ea4a7a90c0",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -33,7 +33,6 @@\n     MistralForSequenceClassification,\n     MistralForTokenClassification,\n     MistralPreTrainedModel,\n-    MistralRotaryEmbedding,\n     eager_attention_forward,\n     rotate_half,\n )\n@@ -244,55 +243,6 @@ def forward(\n         return outputs\n \n \n-class Phi3RotaryEmbedding(MistralRotaryEmbedding):\n-    def __init__(self, config: Phi3Config, device=None):\n-        super().__init__(config, device)\n-\n-    def _longrope_frequency_update(self, position_ids, device):\n-        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if hasattr(self.config, \"original_max_position_embeddings\"):\n-            original_max_position_embeddings = self.config.original_max_position_embeddings\n-        else:\n-            original_max_position_embeddings = self.config.max_position_embeddings\n-        if seq_len > original_max_position_embeddings:\n-            if not hasattr(self, \"long_inv_freq\"):\n-                self.long_inv_freq, _ = self.rope_init_fn(\n-                    self.config, device, seq_len=original_max_position_embeddings + 1\n-                )\n-            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n-        else:\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-        elif self.rope_type == \"longrope\":\n-            self._longrope_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n class Phi3PreTrainedModel(MistralPreTrainedModel):\n     _version = \"0.0.5\"\n "
        },
        {
            "sha": "fb399f6d83dffc5cdf5fd97fc3b87341ffefcb75",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 35,
            "deletions": 83,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -41,7 +41,7 @@\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithPast,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -1605,88 +1605,6 @@ def forward(\n         return inputs_embeds\n \n \n-class Phi4MultimodalRotaryEmbedding(nn.Module):\n-    def __init__(self, config: Phi4MultimodalConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-        elif self.rope_type == \"longrope\":\n-            self._longrope_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-    def _longrope_frequency_update(self, position_ids, device):\n-        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if hasattr(self.config, \"original_max_position_embeddings\"):\n-            original_max_position_embeddings = self.config.original_max_position_embeddings\n-        else:\n-            original_max_position_embeddings = self.config.max_position_embeddings\n-        if seq_len > original_max_position_embeddings:\n-            if not hasattr(self, \"long_inv_freq\"):\n-                self.long_inv_freq, _ = self.rope_init_fn(\n-                    self.config, device, seq_len=original_max_position_embeddings + 1\n-                )\n-            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n-        else:\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-\n-\n PHI4_MULTIMODAL_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -1735,6 +1653,40 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class Phi4MultimodalRotaryEmbedding(nn.Module):\n+    def __init__(self, config: Phi4MultimodalConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "1a3e3aee0fdd13a2c33e52f5551b7ed70f9d28c9",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 26,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -23,6 +23,7 @@\n from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n+from ...modeling_rope_utils import dynamic_rope_update\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -83,38 +84,17 @@ def __init__(self, config, device=None):\n         self.register_buffer(\"inv_freq\", torch.cat((inv_freq, inv_freq), dim=-1), persistent=False)\n \n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n         freqs = self.inv_freq[position_ids]\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             emb = freqs\n             cos = emb.cos()\n             sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half"
        },
        {
            "sha": "16a7316e2d0e56eafe301a7f2d8693d6cc6c73ec",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -22,7 +22,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -303,47 +303,18 @@ def __init__(self, config: Qwen2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "eb455444e5cbdaedb013b4bbd3de64de35093bc1",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 33,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -39,7 +39,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig\n@@ -570,45 +570,20 @@ def __init__(self, config: Qwen2_5_VLConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block. In contrast to other models, Qwen2_5_VL has different position ids for the grids\n+        # In contrast to other models, Qwen2_5_VL has different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "b0a8745a9b728e2289849402ab381752ae1b0640",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -39,7 +39,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -185,47 +185,18 @@ def __init__(self, config: Qwen2MoeConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "824ad4df58d0695218217be6f83c63f0e1e4b661",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 33,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -112,45 +112,20 @@ def __init__(self, config: Qwen2VLConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block. In contrast to other models, Qwen2_VL has different position ids for the grids\n+        # In contrast to other models, Qwen2_VL has different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "5fec83d47888e44a0f3e0ffb2b067ce08e4712d3",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -330,47 +330,18 @@ def __init__(self, config: Qwen3Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "61e4a88049f656793795ad6a06a9fd5d4c512e84",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -40,7 +40,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -425,47 +425,18 @@ def __init__(self, config: Qwen3MoeConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "0d824c568cdcc63dbfde656a589b2b8a62758705",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -37,7 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -87,47 +87,18 @@ def __init__(self, config: StableLmConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "ba4ff8098bdd6962ebdb2c5ea7c6527cca97d7ca",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -40,7 +40,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -294,47 +294,18 @@ def __init__(self, config: Starcoder2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "fb3f9b481fc489bfd536a924909908658a0b0672",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 38,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -239,47 +239,18 @@ def __init__(\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (\n-                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n-            ).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n \n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n "
        },
        {
            "sha": "be964d8490e365e907e338351c553a4db880d221",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -549,6 +549,10 @@ def model_addition_debugger_context(*args, **kwargs):\n ROPE_INIT_FUNCTIONS = None\n \n \n+def dynamic_rope_update(*args, **kwargs):\n+    requires_backends(dynamic_rope_update, [\"torch\"])\n+\n+\n class AttentionInterface(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "b4119bb7b63aed848f3f8f9bb5dfe481447ebb4d",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3d157188f434655fe0919bad625088952a4dcf/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3d157188f434655fe0919bad625088952a4dcf/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=ad3d157188f434655fe0919bad625088952a4dcf",
            "patch": "@@ -1083,8 +1083,7 @@ def check_all_objects_are_documented():\n     undocumented_objs = [c for c in objects if c not in documented_objs and not ignore_undocumented(c)]\n     if len(undocumented_objs) > 0:\n         raise Exception(\n-            \"The following objects are in the public init so should be documented:\\n - \"\n-            + \"\\n - \".join(undocumented_objs)\n+            \"The following objects are in the public init, but not in the docs:\\n - \" + \"\\n - \".join(undocumented_objs)\n         )\n     check_model_type_doc_match()\n     check_public_method_exists(documented_methods_map)"
        }
    ],
    "stats": {
        "total": 2360,
        "additions": 527,
        "deletions": 1833
    }
}