{
    "author": "gabe-l-hart",
    "message": "fix(granitemoe*): Only create block_sparse_moe if num_local_experts > 0 (#42036)\n\n* fix(granitemoehybid): Only set self.block_sparse_moe if num_local_experts > 0\n\nBranch: GraniteMoeAsDenseFix\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\n\n* fix(granitemoehybrid): Regenerate modeling_granitemoehybrid.py\n\nBranch: GraniteMoeAsDenseFix\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\n\n* style: Fix import order\n\nBranch: GraniteMoeAsDenseFix\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\n\n* make fix-copies\n\n---------\n\nSigned-off-by: Gabe Goodhart <ghart@us.ibm.com>\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "a1afecaeb0fb0fc4d80142044b5d1815155ffdcf",
    "files": [
        {
            "sha": "c9e7245956f3bed7e6044e45c0779001dd2b11c3",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 49,
            "deletions": 47,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1afecaeb0fb0fc4d80142044b5d1815155ffdcf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1afecaeb0fb0fc4d80142044b5d1815155ffdcf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=a1afecaeb0fb0fc4d80142044b5d1815155ffdcf",
            "patch": "@@ -927,52 +927,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-class GraniteFlashAttentionKwargs(TypedDict, total=False):\n-    \"\"\"\n-    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n-    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n-\n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n-    \"\"\"\n-\n-    cu_seq_lens_q: torch.LongTensor\n-    cu_seq_lens_k: torch.LongTensor\n-    max_length_q: int\n-    max_length_k: int\n-    seq_idx: torch.IntTensor\n-\n-\n-@use_kernel_forward_from_hub(\"RMSNorm\")\n-class GraniteMoeHybridRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        GraniteMoeHybridRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n class GraniteMoeHybridParallelExperts(nn.Module):\n     def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n         \"\"\"\n@@ -1114,6 +1068,52 @@ def forward(self, layer_input):\n         return layer_output\n \n \n+class GraniteFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class GraniteMoeHybridRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        GraniteMoeHybridRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n class GraniteMoeHybridDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         super().__init__()\n@@ -1122,7 +1122,9 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.self_attn = None\n         self.input_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.block_sparse_moe = GraniteMoeHybridMoE(config)\n+\n+        # Allow non-MoE (dense)\n+        self.block_sparse_moe = GraniteMoeHybridMoE(config) if config.num_local_experts > 0 else None\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = GraniteMoeHybridMLP(config)\n         self.mamba = None"
        },
        {
            "sha": "1da0952d32a465043914f6fe6596772d68a9d23b",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1afecaeb0fb0fc4d80142044b5d1815155ffdcf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1afecaeb0fb0fc4d80142044b5d1815155ffdcf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=a1afecaeb0fb0fc4d80142044b5d1815155ffdcf",
            "patch": "@@ -37,6 +37,7 @@\n     GraniteMoeSharedForCausalLM,\n     GraniteMoeSharedMLP,\n     GraniteMoeSharedModel,\n+    GraniteMoeSharedMoE,\n     GraniteMoeSharedPreTrainedModel,\n     eager_attention_forward,\n )\n@@ -108,6 +109,10 @@ class GraniteMoeHybridRotaryEmbedding(Gemma2RotaryEmbedding):\n     pass\n \n \n+class GraniteMoeHybridMoE(GraniteMoeSharedMoE):\n+    pass\n+\n+\n class GraniteMoeHybridDecoderLayer(GraniteMoeSharedDecoderLayer):\n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n@@ -122,6 +127,9 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n             self.self_attn = GraniteMoeHybridAttention(config, layer_idx)\n         self.layer_type = config.layers_block_type[layer_idx]\n \n+        # Allow non-MoE (dense)\n+        self.block_sparse_moe = GraniteMoeHybridMoE(config) if config.num_local_experts > 0 else None\n+\n         # Accept 0 experts: skip MoE if num_local_experts == 0\n         self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n "
        }
    ],
    "stats": {
        "total": 104,
        "additions": 57,
        "deletions": 47
    }
}