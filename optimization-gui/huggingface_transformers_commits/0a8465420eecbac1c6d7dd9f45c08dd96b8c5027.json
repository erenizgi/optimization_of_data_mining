{
    "author": "kylesayrs",
    "message": "[Tests] Fix CompressedTensors tests (#42935)\n\nrefactor tests\n\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "0a8465420eecbac1c6d7dd9f45c08dd96b8c5027",
    "files": [
        {
            "sha": "6f6b7afd2f04a38827cfba827da39b72b2689ef4",
            "filename": "tests/quantization/compressed_tensors_integration/test_compressed_tensors.py",
            "status": "modified",
            "additions": 28,
            "deletions": 44,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a8465420eecbac1c6d7dd9f45c08dd96b8c5027/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a8465420eecbac1c6d7dd9f45c08dd96b8c5027/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py?ref=0a8465420eecbac1c6d7dd9f45c08dd96b8c5027",
            "patch": "@@ -1,15 +1,8 @@\n import gc\n import unittest\n-from unittest import skip\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, CompressedTensorsConfig\n-from transformers.testing_utils import (\n-    backend_empty_cache,\n-    require_compressed_tensors,\n-    require_deterministic_for_xpu,\n-    require_torch,\n-    torch_device,\n-)\n+from transformers.testing_utils import backend_empty_cache, require_compressed_tensors, require_torch, torch_device\n from transformers.utils import is_torch_available\n \n \n@@ -20,12 +13,12 @@\n @require_compressed_tensors\n @require_torch\n class CompressedTensorsTest(unittest.TestCase):\n-    tinyllama_w8a16 = \"nm-testing/tinyllama-w8a16-dense\"\n-    tinyllama_w4a16 = \"nm-testing/tinyllama-w4a16-compressed\"\n-    tinyllama_w8a8 = \"nm-testing/tinyllama-w8a8-compressed\"\n-    llama3_8b_fp8 = \"nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat\"\n+    tinyllama_w4a16 = \"nm-testing/TinyLlama-1.1B-Chat-v1.0-W4A16-e2e\"\n+    tinyllama_int8 = \"nm-testing/TinyLlama-1.1B-Chat-v1.0-W8A8-e2e\"\n+    tinyllama_fp8 = \"nm-testing/TinyLlama-1.1B-Chat-v1.0-FP8-e2e\"\n+    tinyllama_w8a16 = \"nm-testing/TinyLlama-1.1B-Chat-v1.0-W8A16-e2e\"\n \n-    prompt = \"Paris is the capital of which country?\"\n+    prompt = \"The capital of France is Paris, the capital of Germany is Berlin\"\n \n     def tearDown(self):\n         gc.collect()\n@@ -53,43 +46,30 @@ def test_config_to_from_dict(self):\n         self.assertIsInstance(config_from_dict.quantization_config, QuantizationConfig)\n         self.assertIsInstance(config_from_dict.sparsity_config, SparsityCompressionConfig)\n \n-    @skip(\"Test too flaky, depends on hardware also\")\n-    def test_tinyllama_w8a8(self):\n-        expected_out = [\n-            \"<s> Paris is the capital of which country?\\n\\n**A) 10** Paris is the capital of which country?\\n\\n**B) 11** Paris is the capital of which country?\\n\\n**C) 1\",\n-            \"<s> Paris is the capital of which country?\\n\\n** 10.** Which country is the capital of which country?\\n\\n** 11.** Which country is the capital of which country?\\n\\n** 12.\",  # XPU\n-        ]\n-        self._test_quantized_model(self.tinyllama_w8a8, expected_out)\n-\n     def test_tinyllama_w4a16(self):\n-        expected_out = [\n-            \"<s> Paris is the capital of which country?\\nAnswer: Paris is the capital of France.\\nQuestion: Which country is the capital of which city?\\nAnswer: The capital of the city of New York is New York.\\nQuestion: Which\"\n-        ]\n-        self._test_quantized_model(self.tinyllama_w4a16, expected_out)\n+        self._test_quantized_model(self.tinyllama_w4a16, 20.0)\n+\n+    def test_tinyllama_int8(self):\n+        self._test_quantized_model(self.tinyllama_int8, 30.0)\n+\n+    def test_tinyllama_fp8(self):\n+        self._test_quantized_model(self.tinyllama_fp8, 20.0)\n \n     def test_tinyllama_w8a16(self):\n-        expected_out = [\n-            \"<s> Paris is the capital of which country?\\nA. France\\nB. Germany\\nC. Spain\\nD. Italy\\nE. Switzerland\\nQ10. Which of the following is not a country in the European Union?\\nA.\"\n-        ]\n-        self._test_quantized_model(self.tinyllama_w8a16, expected_out)\n-\n-    def test_llama_8b_fp8(self):\n-        expected_out = [\n-            \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous bridge in Paris? Pont des Arts\\nWhat is the name of the famous opera? \",\n-            \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous bridge in Paris? Pont des Arts\\nWhat is the name of the famous opera\",  # XPU\n-        ]\n-        self._test_quantized_model(self.llama3_8b_fp8, expected_out)\n-\n-    @require_deterministic_for_xpu\n-    def _test_quantized_model(self, model_name: str, expected_output: list):\n-        \"\"\"Carry out generation\"\"\"\n+        self._test_quantized_model(self.tinyllama_w8a16, 20.0)\n+\n+    def _test_quantized_model(self, model_name: str, expected_perplexity: float):\n+        # load model\n         quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         device = quantized_model.device\n+\n+        # check config\n         self.assertIsNotNone(\n             quantized_model.config.quantization_config,\n             \"quantization_config should not be None\",\n         )\n+        # check scales\n         self.assertTrue(\n             any(\n                 key\n@@ -98,9 +78,13 @@ def _test_quantized_model(self, model_name: str, expected_output: list):\n             ),\n             \"quantized model should load a non-trivial scale into the state dict\",\n         )\n+\n+        # compute outputs with loss\n         inputs = tokenizer(self.prompt, return_tensors=\"pt\").to(device)\n-        generated_ids = quantized_model.generate(**inputs, max_length=50, do_sample=False)\n-        outputs = tokenizer.batch_decode(generated_ids)\n+        labels = inputs[\"input_ids\"]\n+        with torch.no_grad():\n+            outputs = quantized_model(**inputs, labels=labels)\n \n-        self.assertIsNotNone(outputs)\n-        self.assertIn(outputs[0], expected_output)\n+        # check perplexity\n+        perplexity = torch.exp(outputs.loss)\n+        self.assertLessEqual(perplexity, expected_perplexity)"
        }
    ],
    "stats": {
        "total": 72,
        "additions": 28,
        "deletions": 44
    }
}