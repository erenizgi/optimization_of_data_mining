{
    "author": "zucchini-nlp",
    "message": "[qwen-vl] Standardize config (#37268)\n\n* update\n\n* fix tests\n\n* fixup\n\n* update\n\n* skip this one\n\n* fixup\n\n* fix",
    "sha": "3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
    "files": [
        {
            "sha": "2fb2eadc53ebf881a203c1e68250263ca55ef6ff",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -232,10 +232,15 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2_5_VLConfig\n \n+## Qwen2_5_VLTextConfig\n+\n+[[autodoc]] Qwen2_5_VLTextConfig\n+\n ## Qwen2_5_VLProcessor\n \n [[autodoc]] Qwen2_5_VLProcessor\n \n+\n ## Qwen2_5_VLModel\n \n [[autodoc]] Qwen2_5_VLModel"
        },
        {
            "sha": "3d1845b6015058d01f9b49b509ecdf8b7f605c43",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -278,6 +278,10 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2VLConfig\n \n+## Qwen2VLTextConfig\n+\n+[[autodoc]] Qwen2VLTextConfig\n+\n ## Qwen2VLImageProcessor\n \n [[autodoc]] Qwen2VLImageProcessor"
        },
        {
            "sha": "a2a69e923cb1808ec1da892a849d62dba6740eeb",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -258,10 +258,12 @@\n         (\"qwen2\", \"Qwen2Config\"),\n         (\"qwen2_5_omni\", \"Qwen2_5OmniConfig\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLConfig\"),\n+        (\"qwen2_5_vl_text\", \"Qwen2_5_VLTextConfig\"),\n         (\"qwen2_audio\", \"Qwen2AudioConfig\"),\n         (\"qwen2_audio_encoder\", \"Qwen2AudioEncoderConfig\"),\n         (\"qwen2_moe\", \"Qwen2MoeConfig\"),\n         (\"qwen2_vl\", \"Qwen2VLConfig\"),\n+        (\"qwen2_vl_text\", \"Qwen2VLTextConfig\"),\n         (\"qwen3\", \"Qwen3Config\"),\n         (\"qwen3_moe\", \"Qwen3MoeConfig\"),\n         (\"rag\", \"RagConfig\"),\n@@ -625,10 +627,12 @@\n         (\"qwen2\", \"Qwen2\"),\n         (\"qwen2_5_omni\", \"Qwen2_5Omni\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VL\"),\n+        (\"qwen2_5_vl_text\", \"Qwen2_5_VL\"),\n         (\"qwen2_audio\", \"Qwen2Audio\"),\n         (\"qwen2_audio_encoder\", \"Qwen2AudioEncoder\"),\n         (\"qwen2_moe\", \"Qwen2MoE\"),\n         (\"qwen2_vl\", \"Qwen2VL\"),\n+        (\"qwen2_vl_text\", \"Qwen2VL\"),\n         (\"qwen3\", \"Qwen3\"),\n         (\"qwen3_moe\", \"Qwen3MoE\"),\n         (\"rag\", \"RAG\"),\n@@ -793,6 +797,8 @@\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),\n         (\"granitevision\", \"llava_next\"),\n+        (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n+        (\"qwen2_vl_text\", \"qwen2_vl\"),\n         (\"sam_vision_model\", \"sam\"),\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),"
        },
        {
            "sha": "7982ab8d981d355aef3193338a1385c13ef0c11b",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -234,9 +234,11 @@\n         (\"qdqbert\", \"QDQBertModel\"),\n         (\"qwen2\", \"Qwen2Model\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLModel\"),\n+        (\"qwen2_5_vl_text\", \"Qwen2_5_VLModel\"),\n         (\"qwen2_audio_encoder\", \"Qwen2AudioEncoder\"),\n         (\"qwen2_moe\", \"Qwen2MoeModel\"),\n         (\"qwen2_vl\", \"Qwen2VLModel\"),\n+        (\"qwen2_vl_text\", \"Qwen2VLModel\"),\n         (\"qwen3\", \"Qwen3Model\"),\n         (\"qwen3_moe\", \"Qwen3MoeModel\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaModel\"),"
        },
        {
            "sha": "2898354399b89a9b73597bf875a2a9191963dabd",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -1792,7 +1792,7 @@ def forward(\n \n \n class Qwen2_5OmniDecoderLayer(nn.Module):\n-    def __init__(self, config: Qwen2_5OmniConfig, layer_idx: int):\n+    def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n "
        },
        {
            "sha": "588ad214ebd2b16ec42c4cd1e1dd1a53a969caa7",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 70,
            "deletions": 16,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -67,17 +67,16 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class Qwen2_5_VLConfig(PretrainedConfig):\n+class Qwen2_5_VLTextConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`Qwen2_5_VLModel`]. It is used to instantiate a\n+    This is the configuration class to store the configuration of a [`Qwen2_5_VLTextModel`]. It is used to instantiate a\n     Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of\n     Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n \n-\n     Args:\n         vocab_size (`int`, *optional*, defaults to 152064):\n             Vocabulary size of the Qwen2_5_VL model. Defines the number of different tokens that can be represented by the\n@@ -120,8 +119,6 @@ class Qwen2_5_VLConfig(PretrainedConfig):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        vision_config (`Dict`, *optional*):\n-            The config for the visual encoder initialization.\n         rope_scaling (`Dict`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n@@ -161,20 +158,20 @@ class Qwen2_5_VLConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n \n     ```python\n-    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig\n+    >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig\n \n     >>> # Initializing a Qwen2_5_VL style configuration\n     >>> configuration = Qwen2_5_VLConfig()\n \n     >>> # Initializing a model from the Qwen2-VL-7B style configuration\n-    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)\n+    >>> model = Qwen2_5_VLTextModel(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"qwen2_5_vl\"\n-    sub_configs = {\"vision_config\": Qwen2_5_VLVisionConfig}\n+    model_type = \"qwen2_5_vl_text\"\n+    base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     # Default tensor parallel plan for base model `Qwen2_5_VL`\n     base_model_tp_plan = {\n@@ -211,15 +208,9 @@ def __init__(\n         sliding_window=4096,\n         max_window_layers=80,\n         attention_dropout=0.0,\n-        vision_config=None,\n         rope_scaling=None,\n         **kwargs,\n     ):\n-        if isinstance(vision_config, dict):\n-            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n-        elif vision_config is None:\n-            self.vision_config = self.sub_configs[\"vision_config\"]()\n-\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -257,4 +248,67 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-__all__ = [\"Qwen2_5_VLConfig\"]\n+class Qwen2_5_VLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen2_5_VLModel`]. It is used to instantiate a\n+    Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+\n+    ```python\n+    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig\n+\n+    >>> # Initializing a Qwen2_5_VL style configuration\n+    >>> configuration = Qwen2_5_VLConfig()\n+\n+    >>> # Initializing a model from the Qwen2-VL-7B style configuration\n+    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen2_5_vl\"\n+    sub_configs = {\"vision_config\": Qwen2_5_VLVisionConfig, \"text_config\": Qwen2_5_VLTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            # For BC use all kwargs to init `TextConfig`\n+            self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Qwen2_5_VLConfig\", \"Qwen2_5_VLTextConfig\"]"
        },
        {
            "sha": "8155a0d280b296b72dc4179fedac3f2b59312c09",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -48,7 +48,7 @@\n     logging,\n     replace_return_docstrings,\n )\n-from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig\n+from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n if is_flash_attn_available():\n@@ -390,7 +390,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n+        std = self.config.get_text_config().initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv3d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n@@ -566,7 +566,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n \n \n class Qwen2_5_VLRotaryEmbedding(nn.Module):\n-    def __init__(self, config: Qwen2_5_VLConfig, device=None):\n+    def __init__(self, config: Qwen2_5_VLTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n@@ -680,7 +680,7 @@ class Qwen2_5_VLAttention(nn.Module):\n     and \"Generating Long Sequences with Sparse Transformers\".\n     \"\"\"\n \n-    def __init__(self, config: Qwen2_5_VLConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -989,7 +989,7 @@ def forward(\n \n \n class Qwen2_5_VLDecoderLayer(nn.Module):\n-    def __init__(self, config: Qwen2_5_VLConfig, layer_idx: int):\n+    def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n@@ -1077,7 +1077,9 @@ def forward(\n     Qwen2_5_VL_START_DOCSTRING,\n )\n class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n-    def __init__(self, config: Qwen2_5_VLConfig):\n+    config_class = Qwen2_5_VLTextConfig\n+\n+    def __init__(self, config: Qwen2_5_VLTextConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n@@ -1497,9 +1499,11 @@ class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMi\n     def __init__(self, config):\n         super().__init__(config)\n         self.visual = Qwen2_5_VisionTransformerPretrainedModel._from_config(config.vision_config)\n-        self.model = Qwen2_5_VLModel(config)\n-        self.vocab_size = config.vocab_size\n-        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        text_config = config.get_text_config()\n+        self.model = Qwen2_5_VLModel._from_config(text_config)\n+        self.vocab_size = text_config.vocab_size\n+        self.lm_head = nn.Linear(text_config.hidden_size, text_config.vocab_size, bias=False)\n         self.rope_deltas = None  # cache rope_deltas here\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "f34c48bb5421db9c45cb1e2236474d71f0821948",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -28,7 +28,7 @@\n import torch.utils.checkpoint\n from torch.nn import CrossEntropyLoss\n \n-from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig\n+from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig\n from transformers.models.qwen2_vl.modeling_qwen2_vl import (\n     PatchEmbed,\n     PatchMerger,\n@@ -110,9 +110,13 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n+class Qwen2_5_VLTextConfig(Qwen2VLTextConfig):\n+    model_type = \"qwen2_5_vl_text\"\n+\n+\n class Qwen2_5_VLConfig(Qwen2VLConfig):\n     model_type = \"qwen2_5_vl\"\n-    sub_configs = {\"vision_config\": Qwen2_5_VLVisionConfig}\n+    sub_configs = {\"vision_config\": Qwen2_5_VLVisionConfig, \"text_config\": Qwen2_5_VLTextConfig}\n \n \n class Qwen2_5_VLMLP(nn.Module):\n@@ -227,7 +231,7 @@ def forward(\n \n class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel):\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n+        std = self.config.get_text_config().initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv3d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n@@ -971,6 +975,7 @@ def __call__(\n \n __all__ = [\n     \"Qwen2_5_VLConfig\",\n+    \"Qwen2_5_VLTextConfig\",\n     \"Qwen2_5_VLForConditionalGeneration\",\n     \"Qwen2_5_VLModel\",\n     \"Qwen2_5_VLPreTrainedModel\","
        },
        {
            "sha": "ee2ed40e4634518762d690068ef443f57442ddcb",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 70,
            "deletions": 16,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -56,17 +56,16 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class Qwen2VLConfig(PretrainedConfig):\n+class Qwen2VLTextConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`Qwen2VLModel`]. It is used to instantiate a\n+    This is the configuration class to store the configuration of a [`Qwen2VLTextModel`]. It is used to instantiate a\n     Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of\n     Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n \n-\n     Args:\n         vocab_size (`int`, *optional*, defaults to 152064):\n             Vocabulary size of the Qwen2VL model. Defines the number of different tokens that can be represented by the\n@@ -109,8 +108,6 @@ class Qwen2VLConfig(PretrainedConfig):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        vision_config (`Dict`, *optional*):\n-            The config for the visual encoder initialization.\n         rope_scaling (`Dict`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n@@ -150,20 +147,20 @@ class Qwen2VLConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n \n     ```python\n-    >>> from transformers import Qwen2VLForConditionalGeneration, Qwen2VLConfig\n+    >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig\n \n     >>> # Initializing a Qwen2VL style configuration\n     >>> configuration = Qwen2VLConfig()\n \n     >>> # Initializing a model from the Qwen2-VL-7B style configuration\n-    >>> model = Qwen2VLForConditionalGeneration(configuration)\n+    >>> model = Qwen2VLTextModel(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"qwen2_vl\"\n-    sub_configs = {\"vision_config\": Qwen2VLVisionConfig}\n+    model_type = \"qwen2_vl_text\"\n+    base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     # Default tensor parallel plan for base model `Qwen2VL`\n     base_model_tp_plan = {\n@@ -200,15 +197,9 @@ def __init__(\n         sliding_window=4096,\n         max_window_layers=80,\n         attention_dropout=0.0,\n-        vision_config=None,\n         rope_scaling=None,\n         **kwargs,\n     ):\n-        if isinstance(vision_config, dict):\n-            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n-        elif vision_config is None:\n-            self.vision_config = self.sub_configs[\"vision_config\"]()\n-\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -246,4 +237,67 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-__all__ = [\"Qwen2VLConfig\"]\n+class Qwen2VLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen2VLModel`]. It is used to instantiate a\n+    Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+\n+    ```python\n+    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig\n+\n+    >>> # Initializing a Qwen2_5_VL style configuration\n+    >>> configuration = Qwen2_5_VLConfig()\n+\n+    >>> # Initializing a model from the Qwen2-VL-7B style configuration\n+    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen2_vl\"\n+    sub_configs = {\"vision_config\": Qwen2VLVisionConfig, \"text_config\": Qwen2VLTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            # For BC use all kwargs to init `TextConfig`\n+            self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Qwen2VLConfig\", \"Qwen2VLTextConfig\"]"
        },
        {
            "sha": "ebc3740c357cadfd31423793482714fdbd11d694",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -44,7 +44,7 @@\n     logging,\n     replace_return_docstrings,\n )\n-from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLVisionConfig\n+from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig\n \n \n if is_flash_attn_available():\n@@ -101,7 +101,7 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n \n \n class Qwen2VLRotaryEmbedding(nn.Module):\n-    def __init__(self, config: Qwen2VLConfig, device=None):\n+    def __init__(self, config: Qwen2VLTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n@@ -494,7 +494,7 @@ class Qwen2VLAttention(nn.Module):\n     and \"Generating Long Sequences with Sparse Transformers\".\n     \"\"\"\n \n-    def __init__(self, config: Qwen2VLConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -803,7 +803,7 @@ def forward(\n \n \n class Qwen2VLDecoderLayer(nn.Module):\n-    def __init__(self, config: Qwen2VLConfig, layer_idx: int):\n+    def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n@@ -919,8 +919,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n \n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n+        std = self.config.get_text_config().initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv3d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n@@ -1029,7 +1028,9 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n     QWEN2VL_START_DOCSTRING,\n )\n class Qwen2VLModel(Qwen2VLPreTrainedModel):\n-    def __init__(self, config: Qwen2VLConfig):\n+    config_class = Qwen2VLTextConfig\n+\n+    def __init__(self, config: Qwen2VLTextConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n@@ -1410,9 +1411,11 @@ class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n     def __init__(self, config):\n         super().__init__(config)\n         self.visual = Qwen2VisionTransformerPretrainedModel._from_config(config.vision_config)\n-        self.model = Qwen2VLModel(config)\n-        self.vocab_size = config.vocab_size\n-        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        text_config = config.get_text_config()\n+        self.model = Qwen2VLModel._from_config(text_config)\n+        self.vocab_size = text_config.vocab_size\n+        self.lm_head = nn.Linear(text_config.hidden_size, text_config.vocab_size, bias=False)\n         self.rope_deltas = None  # cache rope_deltas here\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "a0579ce2029041adc9726ab857eb61b3fb900e9a",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -312,6 +312,10 @@ def test_generate_compile_fullgraph(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         super().test_prompt_lookup_decoding_matches_greedy_search()\n \n+    @unittest.skip(reason=\"The base class is LM only and cannot be init with XModelConfig`\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n \n @require_torch\n class Qwen2_5_VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "31cf74e7c97f8e37c74f7d13344b9e19c15df400",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -316,6 +316,10 @@ def test_model_is_small(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"The base class is LM only and cannot be init with XModelConfig`\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "a0b31c8cc021e45fa642e95b6d5daa18d07227b0",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc44eaaeee01b7f0d2d55c9991900b43cafe62d/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=3bc44eaaeee01b7f0d2d55c9991900b43cafe62d",
            "patch": "@@ -345,6 +345,7 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n \n     # common and important attributes, even if they do not always appear in the modeling files\n     attributes_to_allow = [\n+        \"initializer_range\",\n         \"bos_index\",\n         \"eos_index\",\n         \"pad_index\",\n@@ -355,6 +356,7 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"image_seq_length\",\n         \"video_seq_length\",\n         \"image_size\",\n+        \"text_config\",  # may appear as `get_text_config()`\n         \"use_cache\",\n         \"out_features\",\n         \"out_indices\","
        }
    ],
    "stats": {
        "total": 257,
        "additions": 202,
        "deletions": 55
    }
}