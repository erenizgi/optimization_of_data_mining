{
    "author": "gante",
    "message": "Generate: assistant should sample when the main model samples (#33534)",
    "sha": "77c5d59e0ee7d59fc12dbf83fbbfcc162a7491af",
    "files": [
        {
            "sha": "0b799dceb267c282e8d0bc02062b94f79fd24c86",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/77c5d59e0ee7d59fc12dbf83fbbfcc162a7491af/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77c5d59e0ee7d59fc12dbf83fbbfcc162a7491af/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=77c5d59e0ee7d59fc12dbf83fbbfcc162a7491af",
            "patch": "@@ -160,12 +160,6 @@ def __init__(\n         self.generation_config.output_scores = True\n         self.generation_config.assistant_confidence_threshold = self.assistant_confidence_threshold\n \n-        # Disable sampling -- this implementation of assisted generation/speculative decoding uses the assistant\n-        # greedily to maximize matches. Disables sampling-related flags to prevent warnings\n-        self.generation_config.do_sample = False\n-        for attr in (\"temperature\", \"top_p\", \"min_p\", \"typical_p\", \"top_k\", \"epsilon_cutoff\", \"eta_cutoff\"):\n-            setattr(self.generation_config, attr, None)\n-\n         # avoid unnecessary warnings that min_length is larger than max_new_tokens\n         # remove the `MinLengthLogitsProcessor` if exists (NOTE: no need to check for `MinNewTokensLogitsProcessor`)\n         self.main_model_min_length = self.generation_config.min_length"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 0,
        "deletions": 6
    }
}