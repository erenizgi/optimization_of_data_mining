{
    "author": "vasqu",
    "message": "[`RMSNorm`] Fix rms norm init for models that center around 1 (#40796)\n\n* fix\n\n* fixup inits\n\n* oops\n\n* fixup gemma\n\n* fixup modular order\n\n* how does this keep happen lol\n\n* vaultgemma is new i forgot\n\n* remove init check",
    "sha": "b4ba4e1da0c77710833a2a9ba86f729b42098e46",
    "files": [
        {
            "sha": "04d27b309a405010b0b797a45fbda03cc45a4e9b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -322,6 +322,13 @@ class GemmaPreTrainedModel(PreTrainedModel):\n         \"attentions\": GemmaAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        if \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n+\n \n @auto_docstring\n class GemmaModel(GemmaPreTrainedModel):"
        },
        {
            "sha": "94c3820de79c54bc29e4350797c0b0a83762ce2a",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -23,6 +23,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import TransformersKwargs, logging\n@@ -32,6 +33,8 @@\n     LlamaForTokenClassification,\n     LlamaMLP,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRotaryEmbedding,\n )\n from ..llama.tokenization_llama import LlamaTokenizer\n \n@@ -361,6 +364,19 @@ def __init__(self, config):\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n \n \n+class GemmaRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class GemmaPreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        if \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n+\n+\n class GemmaModel(LlamaModel):\n     def forward(\n         self,"
        },
        {
            "sha": "ec2f1521ef859c53673d7e71e751a3215e7a62d2",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 43,
            "deletions": 36,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -83,6 +83,42 @@ def forward(self, x):\n         return down_proj\n \n \n+class Gemma2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Gemma2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -299,42 +335,6 @@ def forward(\n         return outputs\n \n \n-class Gemma2RotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Gemma2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n @auto_docstring\n class Gemma2PreTrainedModel(PreTrainedModel):\n     config: Gemma2Config\n@@ -353,6 +353,13 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n         \"attentions\": Gemma2Attention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        if \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n+\n \n @auto_docstring\n class Gemma2Model(Gemma2PreTrainedModel):"
        },
        {
            "sha": "add7e6c0989b4f446e08426890f91f2111d63705",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -36,7 +36,9 @@\n     GemmaForTokenClassification,\n     GemmaMLP,\n     GemmaModel,\n+    GemmaPreTrainedModel,\n     GemmaRMSNorm,\n+    GemmaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n@@ -212,6 +214,10 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_activation]\n \n \n+class Gemma2RotaryEmbedding(GemmaRotaryEmbedding):\n+    pass\n+\n+\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -363,6 +369,10 @@ def forward(\n         return outputs\n \n \n+class Gemma2PreTrainedModel(GemmaPreTrainedModel):\n+    pass\n+\n+\n class Gemma2Model(GemmaModel):\n     def __init__(self, config: Gemma2Config):\n         super().__init__(config)"
        },
        {
            "sha": "4536ec7f69f7902f4099f652e51485a104f0c785",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -434,6 +434,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "8afbf566c06117c5a333c6534563908089e74e09",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -526,6 +526,9 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "21e5d4f9819c9c0ae2f88a6255a876b3db1dfba8",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -970,6 +970,9 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3NextGatedDeltaNet):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data.uniform_(0, 16).log_()\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif isinstance(module, Qwen3NextRMSNorm):\n+            module.weight.data.zero_()\n \n \n class Qwen3NextModel(Qwen3NextPreTrainedModel):"
        },
        {
            "sha": "9e92ecf312c3faad7a0c5bd354c4961b0b404f16",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -709,6 +709,9 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3NextGatedDeltaNet):\n             module.dt_bias.data.fill_(1.0)\n             module.A_log.data.uniform_(0, 16).log_()\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif isinstance(module, Qwen3NextRMSNorm):\n+            module.weight.data.zero_()\n \n \n class Qwen3NextModel(Qwen3NextPreTrainedModel):"
        },
        {
            "sha": "88364515459af62e3bdc74d41c9c1dfe97c88ba2",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -556,8 +556,9 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif isinstance(module, RecurrentGemmaRMSNorm):\n-            module.weight.data.fill_(1.0)\n+            module.weight.data.zero_()\n \n     def _setup_cache(self, config, batch, device, dtype):\n         layers = getattr(self, \"model\", self).layers"
        },
        {
            "sha": "b6be86e9cdd70b5640604b6496096d7419707fcf",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -611,6 +611,9 @@ def _init_weights(self, module):\n             if not self.config.tie_word_embeddings:\n                 scale = module.out_proj.weight.shape[0] ** -0.5\n                 module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n \n     def _shift_right(self, input_ids):\n         \"\"\""
        },
        {
            "sha": "d358a51d0e68eb3ca4f0355bf1903f6402233771",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -491,6 +491,9 @@ def _init_weights(self, module):\n             if not self.config.tie_word_embeddings:\n                 scale = module.out_proj.weight.shape[0] ** -0.5\n                 module.out_proj.weight.data.normal_(mean=0.0, std=std * scale)\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        elif \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n \n     def _shift_right(self, input_ids):\n         \"\"\""
        },
        {
            "sha": "eaad6c5335a41b980f3eb34697167839d0d23eed",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4ba4e1da0c77710833a2a9ba86f729b42098e46/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=b4ba4e1da0c77710833a2a9ba86f729b42098e46",
            "patch": "@@ -342,6 +342,13 @@ class VaultGemmaPreTrainedModel(PreTrainedModel):\n         \"attentions\": VaultGemmaAttention,\n     }\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n+        if \"RMSNorm\" in module.__class__.__name__:\n+            module.weight.data.zero_()\n+\n \n @auto_docstring\n class VaultGemmaModel(VaultGemmaPreTrainedModel):"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 103,
        "deletions": 37
    }
}