{
    "author": "Sai-Suraj-27",
    "message": "Tiny Cleanup - Removed duplicate class field definition's (#41293)\n\n* Removed duplicate-class-field-definition\n's using RUFF PIE794\n\n* Removed duplicate-class-field-definition\n's using RUFF PIE794\n\n* Ruff format.\n\n* Removed duplicate-class-field-definition\n\n* Added New ruff rule to detect duplicate class field defs\n\n* remove comment\n\n* order\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
    "files": [
        {
            "sha": "6e7217c2323d2dece11eb79fad6569668d84d4f9",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -33,7 +33,7 @@ line-length = 119\n ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\", \"UP009\", \"UP015\", \"UP031\", \"UP028\", \"UP004\", \"UP045\", \"UP007\", \"UP035\"]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\", \"PIE794\"]\n extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files."
        },
        {
            "sha": "d99160653557a450ea9bde593d88459a52a939d2",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -96,7 +96,6 @@ class DPTImageProcessorFast(BeitImageProcessorFast):\n     rescale_factor = 1 / 255\n     ensure_multiple_of = 1\n     keep_aspect_ratio = False\n-    do_reduce_labels = False\n     crop_size = None\n     do_center_crop = None\n     do_reduce_labels = None"
        },
        {
            "sha": "85530c0c01d14f39b8ee91a8a682d55b6ac79a35",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -404,10 +404,6 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "a3df19390892e38b66d9de912eb6c6a2fce5f335",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -49,7 +49,6 @@ class VoxtralPreTrainedModel(Qwen2AudioPreTrainedModel):\n     _supports_cache_class = True\n     _supports_attention_backend = True\n     _can_compile_fullgraph = True\n-    _supports_attention_backend = True\n     _no_split_modules = None\n \n "
        },
        {
            "sha": "dc818e542f21747a13d4b9b79a060639ffe3b92b",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -80,9 +80,6 @@ class TextToAudioPipeline(Pipeline):\n     See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-to-speech).\n     \"\"\"\n \n-    # Introducing the processor at load time for new behaviour\n-    _load_processor = True\n-\n     _pipeline_calls_generate = True\n     _load_processor = False\n     _load_image_processor = False"
        },
        {
            "sha": "bd4c90f0f41c8e14baf62f8c8575beed927e6142",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee5488440bcb8a45681b7cc04d0c73fb49ccf57a/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=ee5488440bcb8a45681b7cc04d0c73fb49ccf57a",
            "patch": "@@ -62,7 +62,6 @@ class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    model_tester_class = PersimmonModelTester\n \n     @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 1,
        "deletions": 11
    }
}