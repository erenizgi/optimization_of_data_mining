{
    "author": "cjfghk5697",
    "message": "Fixed VitDet for non-squre Images (#35969)\n\n* size tuple\n\n* delete original input_size\n\n* use zip\n\n* process the other case\n\n* Update src/transformers/models/vitdet/modeling_vitdet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* [VITDET] Test non-square image\n\n* [Fix] Make Quality\n\n* make fix style\n\n* Update src/transformers/models/vitdet/modeling_vitdet.py\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "9ebfda3263fcdc4e05fd87fad1aadc8a08294608",
    "files": [
        {
            "sha": "9585c295e18abef8d47e9b91b43a5848bf8ce16f",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ebfda3263fcdc4e05fd87fad1aadc8a08294608/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ebfda3263fcdc4e05fd87fad1aadc8a08294608/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=9ebfda3263fcdc4e05fd87fad1aadc8a08294608",
            "patch": "@@ -456,8 +456,14 @@ def __init__(\n         super().__init__()\n \n         dim = config.hidden_size\n-        input_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n \n+        image_size = config.image_size\n+        image_size = image_size if isinstance(image_size, (list, tuple)) else (image_size, image_size)\n+\n+        patch_size = config.patch_size\n+        patch_size = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n+\n+        input_size = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n         self.norm1 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = VitDetAttention(\n             config, input_size=input_size if window_size == 0 else (window_size, window_size)"
        },
        {
            "sha": "4b5ac0f3378cb2ff575d32d52ab6ade148bba0f3",
            "filename": "tests/models/vitdet/test_modeling_vitdet.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ebfda3263fcdc4e05fd87fad1aadc8a08294608/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ebfda3263fcdc4e05fd87fad1aadc8a08294608/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py?ref=9ebfda3263fcdc4e05fd87fad1aadc8a08294608",
            "patch": "@@ -290,6 +290,31 @@ def test_feed_forward_chunking(self):\n     def test_model_from_pretrained(self):\n         pass\n \n+    def test_non_square_image(self):\n+        non_square_image_size = (32, 40)\n+        patch_size = (2, 2)\n+        config = self.model_tester.get_config()\n+        config.image_size = non_square_image_size\n+        config.patch_size = patch_size\n+\n+        model = VitDetModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        batch_size = self.model_tester.batch_size\n+        # Create a dummy input tensor with non-square spatial dimensions.\n+        pixel_values = floats_tensor(\n+            [batch_size, config.num_channels, non_square_image_size[0], non_square_image_size[1]]\n+        )\n+\n+        result = model(pixel_values)\n+\n+        expected_height = non_square_image_size[0] / patch_size[0]\n+        expected_width = non_square_image_size[1] / patch_size[1]\n+        expected_shape = (batch_size, config.hidden_size, expected_height, expected_width)\n+\n+        self.assertEqual(result.last_hidden_state.shape, expected_shape)\n+\n \n @require_torch\n class VitDetBackboneTest(unittest.TestCase, BackboneTesterMixin):"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 32,
        "deletions": 1
    }
}