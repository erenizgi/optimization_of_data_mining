{
    "author": "voidism",
    "message": "fix MetaCLIP 2 wrong link & wrong model names in the docstrings (#40565)\n\n* fix MetaCLIP 2 wrong link & wrong model names in the documentation and docstrings\n\n* ruff reformatted\n\n* update files generated by modular\n\n* update meta_clip2 to metaclip_2 to match the original\n\n* _supports_flash_attn = False\n\n---------\n\nCo-authored-by: Yung-Sung Chuang <yungsung@meta.com>",
    "sha": "5e2e496149300977299a16160c8b75f5c2aefb9b",
    "files": [
        {
            "sha": "b9fbba090f0a8bed10140f7987a8523c729ef790",
            "filename": "docs/source/en/model_doc/metaclip_2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -32,7 +32,7 @@ MetaCLIP 2 is a replication of the original CLIP model trained on 300+ languages\n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/facebookresearch/MetaCLIP).\n \n-You can find all the MetaCLIP 2 checkpoints under the [Meta](https://huggingface.co/facebook?search_models=metaclip-2) organization.\n+You can find all the MetaCLIP 2 checkpoints under the [Meta](https://huggingface.co/facebook/models?search=metaclip-2) organization.\n \n > [!TIP]\n > Click on the MetaCLIP 2 models in the right sidebar for more examples of how to apply MetaCLIP 2 to different image and language tasks."
        },
        {
            "sha": "a0cec0f3c5b37c2142b65a480466e63714cdd493",
            "filename": "src/transformers/models/metaclip_2/configuration_metaclip_2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -14,18 +14,18 @@\n \n class MetaClip2TextConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MetaClip2TextModel`]. It is used to instantiate a METACLIP_2\n-    text encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n-    with the defaults will yield a similar configuration to that of the text encoder of the METACLIP_2\n-    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+    This is the configuration class to store the configuration of a [`MetaClip2TextModel`]. It is used to instantiate\n+    a MetaClip2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n-            Vocabulary size of the METACLIP_2 text model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`MetaClip2Model`].\n+            Vocabulary size of the MetaClip2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`MetaClip2TextModel`].\n         hidden_size (`int`, *optional*, defaults to 512):\n             Dimensionality of the encoder layers and the pooler layer.\n         intermediate_size (`int`, *optional*, defaults to 2048):\n@@ -63,10 +63,10 @@ class MetaClip2TextConfig(PretrainedConfig):\n     ```python\n     >>> from transformers import MetaClip2TextConfig, MetaClip2TextModel\n \n-    >>> # Initializing a MetaClip2TextConfig with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2TextConfig with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> configuration = MetaClip2TextConfig()\n \n-    >>> # Initializing a MetaClip2TextModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2TextModel (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> model = MetaClip2TextModel(configuration)\n \n     >>> # Accessing the model configuration\n@@ -115,10 +115,10 @@ def __init__(\n \n class MetaClip2VisionConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MetaClip2VisionModel`]. It is used to instantiate a\n-    METACLIP_2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the vision encoder of the METACLIP_2\n-    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+    This is the configuration class to store the configuration of a [`MetaClip2VisionModel`]. It is used to instantiate a MetaClip2\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the vision encoder of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n@@ -158,10 +158,10 @@ class MetaClip2VisionConfig(PretrainedConfig):\n     ```python\n     >>> from transformers import MetaClip2VisionConfig, MetaClip2VisionModel\n \n-    >>> # Initializing a MetaClip2VisionConfig with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2VisionConfig with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> configuration = MetaClip2VisionConfig()\n \n-    >>> # Initializing a MetaClip2VisionModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2VisionModel (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> model = MetaClip2VisionModel(configuration)\n \n     >>> # Accessing the model configuration\n@@ -207,10 +207,10 @@ def __init__(\n \n class MetaClip2Config(PretrainedConfig):\n     r\"\"\"\n-    [`MetaClip2Config`] is the configuration class to store the configuration of a [`MetaClip2Model`]. It is used to instantiate\n-    a METACLIP_2 model according to the specified arguments, defining the text model and vision model configs. Instantiating\n-    a configuration with the defaults will yield a similar configuration to that of the METACLIP_2\n-    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+    [`MetaClip2Config`] is the configuration class to store the configuration of a [`MetaClip2Model`]. It is used to\n+    instantiate a MetaClip2 model according to the specified arguments, defining the text model and vision model configs.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n@@ -223,7 +223,7 @@ class MetaClip2Config(PretrainedConfig):\n         projection_dim (`int`, *optional*, defaults to 512):\n             Dimensionality of text and vision projection layers.\n         logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n-            The initial value of the *logit_scale* parameter. Default is used as per the original METACLIP_2 implementation.\n+            The initial value of the *logit_scale* parameter. Default is used as per the original MetaClip2 implementation.\n         kwargs (*optional*):\n             Dictionary of keyword arguments.\n \n@@ -232,10 +232,10 @@ class MetaClip2Config(PretrainedConfig):\n     ```python\n     >>> from transformers import MetaClip2Config, MetaClip2Model\n \n-    >>> # Initializing a MetaClip2Config with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2Config with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> configuration = MetaClip2Config()\n \n-    >>> # Initializing a MetaClip2Model (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> # Initializing a MetaClip2Model (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n     >>> model = MetaClip2Model(configuration)\n \n     >>> # Accessing the model configuration"
        },
        {
            "sha": "c4a872979218d79bdf14429a4fef98ab7d6e88b5",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 185,
            "deletions": 17,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -547,6 +547,36 @@ def forward(\n     \"\"\"\n )\n class MetaClip2TextModel(MetaClip2PreTrainedModel):\n+    \"\"\"\n+    The text model from MetaClip2 without any head or projection on top.\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2TextConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import AutoTokenizer, MetaClip2TextModel\n+\n+    >>> model = MetaClip2TextModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> last_hidden_state = outputs.last_hidden_state\n+    >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n+    ```\"\"\"\n+\n     config: MetaClip2TextConfig\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n@@ -580,8 +610,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, MetaClip2TextModel\n \n-        >>> model = MetaClip2TextModel.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2TextModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n@@ -619,6 +649,36 @@ class MetaClip2TextModelOutput(ModelOutput):\n \n @auto_docstring\n class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n+    \"\"\"\n+    MetaClip2 text model with a projection layer on top (a linear layer on top of the pooled output).\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2TextConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n+\n+    >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> text_embeds = outputs.text_embeds\n+    ```\"\"\"\n+\n     config: MetaClip2TextConfig\n \n     _supports_flash_attn = False\n@@ -658,8 +718,8 @@ def forward(\n         >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n \n-        >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n@@ -748,6 +808,42 @@ def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n \n @auto_docstring\n class MetaClip2Model(MetaClip2PreTrainedModel):\n+    \"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2Config`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+    >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(\n+    ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+    ... )\n+\n+    >>> outputs = model(**inputs)\n+    >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+    >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+    ```\"\"\"\n+\n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n     _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n@@ -806,8 +902,8 @@ def get_text_features(\n         >>> import torch\n         >>> from transformers import AutoTokenizer, MetaClip2Model\n \n-        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n \n@@ -843,8 +939,8 @@ def get_image_features(\n         >>> from transformers import AutoProcessor, MetaClip2Model\n         >>> from transformers.image_utils import load_image\n \n-        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = load_image(url)\n@@ -877,8 +973,9 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> MetaClip2Output:\n         r\"\"\"\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n+        Args:\n+            return_loss (`bool`, *optional*):\n+                Whether or not to return the contrastive loss.\n \n         Examples:\n \n@@ -887,8 +984,8 @@ def forward(\n         >>> from transformers import AutoProcessor, MetaClip2Model\n         >>> from transformers.image_utils import load_image\n \n-        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = load_image(url)\n@@ -1008,6 +1105,42 @@ def forward(\n     \"\"\"\n )\n class MetaClip2VisionModel(MetaClip2PreTrainedModel):\n+    \"\"\"\n+    The vision model from MetaClip2 without any head or projection on top.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2VisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2VisionModel\n+\n+    >>> model = MetaClip2VisionModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> last_hidden_state = outputs.last_hidden_state\n+    >>> pooled_output = outputs.pooler_output  # pooled CLS states\n+    ```\"\"\"\n+\n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"MetaClip2EncoderLayer\"]\n@@ -1031,15 +1164,15 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n-        Example:\n+        Examples:\n \n         ```python\n         >>> from PIL import Image\n         >>> import requests\n         >>> from transformers import AutoProcessor, MetaClip2VisionModel\n \n-        >>> model = MetaClip2VisionModel.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2VisionModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n@@ -1079,6 +1212,41 @@ class MetaClip2VisionModelOutput(ModelOutput):\n \n @auto_docstring\n class MetaClip2VisionModelWithProjection(MetaClip2PreTrainedModel):\n+    \"\"\"\n+    MetaClip2 vision model with a projection layer on top (a linear layer on top of the pooled output).\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2VisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n+\n+    >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> image_embeds = outputs.image_embeds\n+    ```\"\"\"\n+\n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n \n@@ -1113,8 +1281,8 @@ def forward(\n         >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n         >>> from transformers.image_utils import load_image\n \n-        >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n-        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = load_image(url)"
        },
        {
            "sha": "2f6085519119378866d84223aab7ed108e183403",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 572,
            "deletions": 2,
            "changes": 574,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -28,15 +28,178 @@\n logger = logging.get_logger(__name__)\n \n \n+_CHECKPOINT_FOR_DOC = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+_CONFIG_FOR_DOC = \"MetaClip2Config\"\n+\n+\n class MetaClip2TextConfig(CLIPTextConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MetaClip2TextModel`]. It is used to instantiate\n+    a MetaClip2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 49408):\n+            Vocabulary size of the MetaClip2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`MetaClip2TextModel`].\n+        hidden_size (`int`, *optional*, defaults to 512):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 77):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 49406):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 49407):\n+            End of stream token id.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2TextConfig, MetaClip2TextModel\n+\n+    >>> # Initializing a MetaClip2TextConfig with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> configuration = MetaClip2TextConfig()\n+\n+    >>> # Initializing a MetaClip2TextModel (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> model = MetaClip2TextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n     pass\n \n \n class MetaClip2VisionConfig(CLIPVisionConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MetaClip2VisionModel`]. It is used to instantiate a MetaClip2\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the vision encoder of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 32):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2VisionConfig, MetaClip2VisionModel\n+\n+    >>> # Initializing a MetaClip2VisionConfig with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> configuration = MetaClip2VisionConfig()\n+\n+    >>> # Initializing a MetaClip2VisionModel (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> model = MetaClip2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n     pass\n \n \n class MetaClip2Config(CLIPConfig):\n+    r\"\"\"\n+    [`MetaClip2Config`] is the configuration class to store the configuration of a [`MetaClip2Model`]. It is used to\n+    instantiate a MetaClip2 model according to the specified arguments, defining the text model and vision model configs.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the MetaClip2\n+    [facebook/metaclip-2-worldwide-huge-quickgelu](https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`MetaClip2TextConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`MetaClip2VisionConfig`].\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n+            The initial value of the *logit_scale* parameter. Default is used as per the original MetaClip2 implementation.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2Config, MetaClip2Model\n+\n+    >>> # Initializing a MetaClip2Config with facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> configuration = MetaClip2Config()\n+\n+    >>> # Initializing a MetaClip2Model (with random weights) from the facebook/metaclip-2-worldwide-huge-quickgelu style configuration\n+    >>> model = MetaClip2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a MetaClip2Config from a MetaClip2TextConfig and a MetaClip2VisionConfig\n+    >>> from transformers import MetaClip2TextConfig, MetaClip2VisionConfig\n+\n+    >>> # Initializing a MetaClip2Text and MetaClip2Vision configuration\n+    >>> config_text = MetaClip2TextConfig()\n+    >>> config_vision = MetaClip2VisionConfig()\n+\n+    >>> config = MetaClip2Config.from_text_vision_configs(config_text, config_vision)\n+    ```\"\"\"\n+\n     pass\n \n \n@@ -175,14 +338,105 @@ def forward(\n \n \n class MetaClip2TextModel(CLIPTextModel):\n+    \"\"\"\n+    The text model from MetaClip2 without any head or projection on top.\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2TextConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import AutoTokenizer, MetaClip2TextModel\n+\n+    >>> model = MetaClip2TextModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> last_hidden_state = outputs.last_hidden_state\n+    >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n+    ```\"\"\"\n+\n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n         self.text_model = MetaClip2TextTransformer(config)\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2TextModel\n+\n+        >>> model = MetaClip2TextModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n \n class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n+    \"\"\"\n+    MetaClip2 text model with a projection layer on top (a linear layer on top of the pooled output).\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2TextConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n+\n+    >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> text_embeds = outputs.text_embeds\n+    ```\"\"\"\n+\n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n \n@@ -194,8 +448,74 @@ def __init__(self, config: MetaClip2TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n+\n+        >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> text_embeds = outputs.text_embeds\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n \n class MetaClip2Model(CLIPModel):\n+    \"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2Config`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+    >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(\n+    ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+    ... )\n+\n+    >>> outputs = model(**inputs)\n+    >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+    >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+    ```\"\"\"\n+\n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)\n \n@@ -219,13 +539,263 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        return_loss: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        r\"\"\"\n+        Args:\n+            return_loss (`bool`, *optional*):\n+                Whether or not to return the contrastive loss.\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(\n+        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+        ... )\n+\n+        >>> outputs = model(**inputs)\n+        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            return_loss=return_loss,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n+    def get_text_features(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Returns:\n+            text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n+            applying the projection layer to the pooled output of [`MetaClip2TextModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+        >>> text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n+        return super().get_text_features(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        r\"\"\"\n+        Returns:\n+            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n+            applying the projection layer to the pooled output of [`MetaClip2VisionModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> image_features = model.get_image_features(**inputs)\n+        ```\"\"\"\n+        return super().get_image_features(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n \n class MetaClip2VisionModel(CLIPVisionModel):\n-    pass\n+    \"\"\"\n+    The vision model from MetaClip2 without any head or projection on top.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2VisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2VisionModel\n+\n+    >>> model = MetaClip2VisionModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> last_hidden_state = outputs.last_hidden_state\n+    >>> pooled_output = outputs.pooler_output  # pooled CLS states\n+    ```\"\"\"\n+\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2VisionModel\n+\n+        >>> model = MetaClip2VisionModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n+        ```\"\"\"\n+        return super().forward(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n \n \n class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n-    pass\n+    \"\"\"\n+    MetaClip2 vision model with a projection layer on top (a linear layer on top of the pooled output).\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Args:\n+        config ([`MetaClip2VisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from PIL import Image\n+    >>> import requests\n+    >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n+\n+    >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+    >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+    >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+    >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+    >>> outputs = model(**inputs)\n+    >>> image_embeds = outputs.image_embeds\n+    ```\"\"\"\n+\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ):\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n+\n+        >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> image_embeds = outputs.image_embeds\n+        ```\"\"\"\n+        return super().forward(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n \n \n class MetaClip2ForImageClassification(CLIPForImageClassification):"
        }
    ],
    "stats": {
        "total": 820,
        "additions": 779,
        "deletions": 41
    }
}