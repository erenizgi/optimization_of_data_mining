{
    "author": "qubvel",
    "message": "OwlViT/Owlv2 post processing standardization (#34929)\n\n* Refactor owlvit post_process_object_detection + add text_labels\r\n\r\n* Fix copies in grounding dino\r\n\r\n* Sync with Owlv2 postprocessing\r\n\r\n* Add post_process_grounded_object_detection method to processor, deprecate post_process_object_detection\r\n\r\n* Add test cases\r\n\r\n* Move text_labels to processors only\r\n\r\n* [run-slow] owlvit owlv2\r\n\r\n* [run-slow] owlvit, owlv2\r\n\r\n* Update snippets\r\n\r\n* Update docs structure\r\n\r\n* Update deprecated objects for check_repo\r\n\r\n* Update docstring for post processing of image guided object detection",
    "sha": "94ae9a8da16b13cc28fa29201008c1c08119a2fc",
    "files": [
        {
            "sha": "696a1b03776aa209a8e5421deedc0546b0936388",
            "filename": "docs/source/en/model_doc/owlv2.md",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -50,20 +50,22 @@ OWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditio\n \n >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n >>> image = Image.open(requests.get(url, stream=True).raw)\n->>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n->>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n+>>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n+>>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n >>> outputs = model(**inputs)\n \n >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n->>> target_sizes = torch.Tensor([image.size[::-1]])\n->>> # Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\n->>> results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n->>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n->>> text = texts[i]\n->>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n->>> for box, score, label in zip(boxes, scores, labels):\n+>>> target_sizes = torch.tensor([(image.height, image.width)])\n+>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n+>>> results = processor.post_process_grounded_object_detection(\n+...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n+... )\n+>>> # Retrieve predictions for the first image for the corresponding text queries\n+>>> result = results[0]\n+>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n+>>> for box, score, text_label in zip(boxes, scores, text_labels):\n ...     box = [round(i, 2) for i in box.tolist()]\n-...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n+...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n Detected a photo of a cat with confidence 0.614 at location [341.67, 23.39, 642.32, 371.35]\n Detected a photo of a cat with confidence 0.665 at location [6.75, 51.96, 326.62, 473.13]\n ```\n@@ -103,6 +105,9 @@ Usage of OWLv2 is identical to [OWL-ViT](owlvit) with a new, updated image proce\n ## Owlv2Processor\n \n [[autodoc]] Owlv2Processor\n+    - __call__\n+    - post_process_grounded_object_detection\n+    - post_process_image_guided_detection\n \n ## Owlv2Model\n "
        },
        {
            "sha": "519648bbd8dcd45b15177c92518aa7ec76307a82",
            "filename": "docs/source/en/model_doc/owlvit.md",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -49,20 +49,22 @@ OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CL\n \n >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n >>> image = Image.open(requests.get(url, stream=True).raw)\n->>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n->>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n+>>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n+>>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n >>> outputs = model(**inputs)\n \n >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n->>> target_sizes = torch.Tensor([image.size[::-1]])\n+>>> target_sizes = torch.tensor([(image.height, image.width)])\n >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n->>> results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n->>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n->>> text = texts[i]\n->>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n->>> for box, score, label in zip(boxes, scores, labels):\n+>>> results = processor.post_process_grounded_object_detection(\n+...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n+... )\n+>>> # Retrieve predictions for the first image for the corresponding text queries\n+>>> result = results[0]\n+>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n+>>> for box, score, text_label in zip(boxes, scores, text_labels):\n ...     box = [round(i, 2) for i in box.tolist()]\n-...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n+...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]\n Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]\n ```\n@@ -91,16 +93,12 @@ A demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object de\n     - post_process_object_detection\n     - post_process_image_guided_detection\n \n-## OwlViTFeatureExtractor\n-\n-[[autodoc]] OwlViTFeatureExtractor\n-    - __call__\n-    - post_process\n-    - post_process_image_guided_detection\n-\n ## OwlViTProcessor\n \n [[autodoc]] OwlViTProcessor\n+    - __call__\n+    - post_process_grounded_object_detection\n+    - post_process_image_guided_detection\n \n ## OwlViTModel\n "
        },
        {
            "sha": "e3c99568cd24d7ac9218a468700ad0ecc2de410d",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 59,
            "deletions": 29,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -17,7 +17,7 @@\n import io\n import pathlib\n from collections import defaultdict\n-from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Callable, Dict, Iterable, List, Optional, Set, Tuple, Union\n \n import numpy as np\n \n@@ -77,6 +77,9 @@\n     import scipy.special\n     import scipy.stats\n \n+if TYPE_CHECKING:\n+    from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n+\n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n@@ -806,6 +809,35 @@ def compute_segments(\n     return segmentation, segments\n \n \n+# Copied from transformers.models.owlvit.image_processing_owlvit._scale_boxes\n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`List[Tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n class GroundingDinoImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Grounding DINO image processor.\n@@ -1533,7 +1565,10 @@ def preprocess(\n \n     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection with OwlViT->GroundingDino\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.1, target_sizes: Union[TensorType, List[Tuple]] = None\n+        self,\n+        outputs: \"GroundingDinoObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -1542,48 +1577,43 @@ def post_process_object_detection(\n         Args:\n             outputs ([`GroundingDinoObjectDetectionOutput`]):\n                 Raw outputs of the model.\n-            threshold (`float`, *optional*):\n+            threshold (`float`, *optional*, defaults to 0.1):\n                 Score threshold to keep object detection predictions.\n             target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n                 Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n                 `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n         \"\"\"\n-        # TODO: (amy) add support for other frameworks\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n \n-        if target_sizes is not None:\n-            if len(logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n \n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n \n         # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n+        batch_boxes = center_to_corners_format(batch_boxes)\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n-                img_h = torch.Tensor([i[0] for i in target_sizes])\n-                img_w = torch.Tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-\n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-            boxes = boxes * scale_fct[:, None, :]\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n \n         results = []\n-        for s, l, b in zip(scores, labels, boxes):\n-            score = s[s > threshold]\n-            label = l[s > threshold]\n-            box = b[s > threshold]\n-            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n \n         return results\n "
        },
        {
            "sha": "1dfdfbd1c21346455d17fa3b85f946c988b889d9",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 64,
            "deletions": 42,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Image processor class for OWLv2.\"\"\"\n \n import warnings\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n \n import numpy as np\n \n@@ -60,10 +60,43 @@\n if is_scipy_available():\n     from scipy import ndimage as ndi\n \n+if TYPE_CHECKING:\n+    from .modeling_owlv2 import Owlv2ObjectDetectionOutput\n \n logger = logging.get_logger(__name__)\n \n \n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`List[Tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    # for owlv2 image is padded to max size unlike owlvit, thats why we have to scale boxes to max size\n+    max_size = torch.max(image_height, image_width)\n+\n+    scale_factor = torch.stack([max_size, max_size, max_size, max_size], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n # Copied from transformers.models.owlvit.image_processing_owlvit._upcast\n def _upcast(t):\n     # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n@@ -466,62 +499,57 @@ def preprocess(\n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n+    # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection with OwlViT->Owlv2\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.1, target_sizes: Union[TensorType, List[Tuple]] = None\n+        self,\n+        outputs: \"Owlv2ObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n     ):\n         \"\"\"\n-        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n         bottom_right_x, bottom_right_y) format.\n \n         Args:\n-            outputs ([`OwlViTObjectDetectionOutput`]):\n+            outputs ([`Owlv2ObjectDetectionOutput`]):\n                 Raw outputs of the model.\n-            threshold (`float`, *optional*):\n+            threshold (`float`, *optional*, defaults to 0.1):\n                 Score threshold to keep object detection predictions.\n             target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n                 Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n                 `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n         \"\"\"\n-        # TODO: (amy) add support for other frameworks\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n \n-        if target_sizes is not None:\n-            if len(logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n \n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n \n         # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n+        batch_boxes = center_to_corners_format(batch_boxes)\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n-                img_h = torch.Tensor([i[0] for i in target_sizes])\n-                img_w = torch.Tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-\n-            # Rescale coordinates, image is padded to square for inference,\n-            # that is why we need to scale boxes to the max size\n-            size = torch.max(img_h, img_w)\n-            scale_fct = torch.stack([size, size, size, size], dim=1).to(boxes.device)\n-\n-            boxes = boxes * scale_fct[:, None, :]\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n \n         results = []\n-        for s, l, b in zip(scores, labels, boxes):\n-            score = s[s > threshold]\n-            label = l[s > threshold]\n-            box = b[s > threshold]\n-            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n \n         return results\n \n@@ -574,13 +602,7 @@ def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_thresh\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n-                img_h = torch.tensor([i[0] for i in target_sizes])\n-                img_w = torch.tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(target_boxes.device)\n-            target_boxes = target_boxes * scale_fct[:, None, :]\n+            target_boxes = _scale_boxes(target_boxes, target_sizes)\n \n         # Compute box display alphas based on prediction scores\n         results = []"
        },
        {
            "sha": "d69bcaa87f00d0619c96a626967e8d26ab830147",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 19,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -1749,33 +1749,30 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n         >>> import torch\n-        >>> from transformers import AutoProcessor, Owlv2ForObjectDetection\n \n-        >>> processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n+        >>> from transformers import Owlv2Processor, Owlv2ForObjectDetection\n+\n+        >>> processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n         >>> model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n-        >>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n-        >>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n-\n-        >>> # forward pass\n-        >>> with torch.no_grad():\n-        ...     outputs = model(**inputs)\n+        >>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n+        >>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n \n-        >>> target_sizes = torch.Tensor([image.size[::-1]])\n-        >>> # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n-        >>> results = processor.post_process_object_detection(\n-        ...     outputs=outputs, threshold=0.2, target_sizes=target_sizes\n+        >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n+        >>> target_sizes = torch.tensor([(image.height, image.width)])\n+        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n+        >>> results = processor.post_process_grounded_object_detection(\n+        ...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n         ... )\n-\n-        >>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n-        >>> text = texts[i]\n-        >>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n-\n-        >>> for box, score, label in zip(boxes, scores, labels):\n+        >>> # Retrieve predictions for the first image for the corresponding text queries\n+        >>> result = results[0]\n+        >>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n+        >>> for box, score, text_label in zip(boxes, scores, text_labels):\n         ...     box = [round(i, 2) for i in box.tolist()]\n-        ...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n+        ...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n         Detected a photo of a cat with confidence 0.614 at location [341.67, 23.39, 642.32, 371.35]\n         Detected a photo of a cat with confidence 0.665 at location [6.75, 51.96, 326.62, 473.13]\n         ```\"\"\""
        },
        {
            "sha": "b79ab626f7d20556af5e5c96fc36e97d84d22608",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 95,
            "deletions": 10,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -16,13 +16,18 @@\n Image/Text processor class for OWLv2\n \"\"\"\n \n-from typing import List\n+import warnings\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n import numpy as np\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import is_flax_available, is_tf_available, is_torch_available\n+from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_owlv2 import Owlv2ImageGuidedObjectDetectionOutput, Owlv2ObjectDetectionOutput\n \n \n class Owlv2Processor(ProcessorMixin):\n@@ -45,7 +50,7 @@ class Owlv2Processor(ProcessorMixin):\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OWLViT->OWLv2\n+    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OwlViT->Owlv2\n     def __call__(self, text=None, images=None, query_images=None, padding=\"max_length\", return_tensors=\"np\", **kwargs):\n         \"\"\"\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n@@ -157,21 +162,101 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_object_detection with OWLViT->OWLv2\n+    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_object_detection with OwlViT->Owlv2\n     def post_process_object_detection(self, *args, **kwargs):\n         \"\"\"\n-        This method forwards all its arguments to [`OwlViTImageProcessor.post_process_object_detection`]. Please refer\n+        This method forwards all its arguments to [`Owlv2ImageProcessor.post_process_object_detection`]. Please refer\n         to the docstring of this method for more information.\n         \"\"\"\n+        warnings.warn(\n+            \"`post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. \"\n+            \"Use `post_process_grounded_object_detection` instead.\",\n+            FutureWarning,\n+        )\n         return self.image_processor.post_process_object_detection(*args, **kwargs)\n \n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_image_guided_detection with OWLViT->OWLv2\n-    def post_process_image_guided_detection(self, *args, **kwargs):\n+    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_grounded_object_detection with OwlViT->Owlv2\n+    def post_process_grounded_object_detection(\n+        self,\n+        outputs: \"Owlv2ObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+        text_labels: Optional[List[List[str]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`Owlv2ObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            text_labels (`List[List[str]]`, *optional*):\n+                List of lists of text labels for each image in the batch. If unset, \"text_labels\" in output will be\n+                set to `None`.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+            - \"text_labels\": The text labels for each predicted bounding box on the image.\n         \"\"\"\n-        This method forwards all its arguments to [`OwlViTImageProcessor.post_process_one_shot_object_detection`].\n-        Please refer to the docstring of this method for more information.\n+        output = self.image_processor.post_process_object_detection(\n+            outputs=outputs, threshold=threshold, target_sizes=target_sizes\n+        )\n+\n+        if text_labels is not None and len(text_labels) != len(output):\n+            raise ValueError(\"Make sure that you pass in as many lists of text labels as images\")\n+\n+        # adding text labels to the output\n+        if text_labels is not None:\n+            for image_output, image_text_labels in zip(output, text_labels):\n+                object_text_labels = [image_text_labels[i] for i in image_output[\"labels\"]]\n+                image_output[\"text_labels\"] = object_text_labels\n+        else:\n+            for image_output in output:\n+                image_output[\"text_labels\"] = None\n+\n+        return output\n+\n+    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_image_guided_detection with OwlViT->Owlv2\n+    def post_process_image_guided_detection(\n+        self,\n+        outputs: \"Owlv2ImageGuidedObjectDetectionOutput\",\n+        threshold: float = 0.0,\n+        nms_threshold: float = 0.3,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the output of [`Owlv2ForObjectDetection.image_guided_detection`] into the format expected by the COCO\n+        api.\n+\n+        Args:\n+            outputs ([`Owlv2ImageGuidedObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Minimum confidence threshold to use to filter out predicted boxes.\n+            nms_threshold (`float`, *optional*, defaults to 0.3):\n+                IoU threshold for non-maximum suppression of overlapping boxes.\n+            target_sizes (`torch.Tensor`, *optional*):\n+                Tensor of shape (batch_size, 2) where each entry is the (height, width) of the corresponding image in\n+                the batch. If set, predicted normalized bounding boxes are rescaled to the target sizes. If left to\n+                None, predictions will not be unnormalized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+            - \"labels\": Set to `None`.\n         \"\"\"\n-        return self.image_processor.post_process_image_guided_detection(*args, **kwargs)\n+        return self.image_processor.post_process_image_guided_detection(\n+            outputs=outputs, threshold=threshold, nms_threshold=nms_threshold, target_sizes=target_sizes\n+        )\n \n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.batch_decode\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "59c64657346851420c502c2530985add92348439",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 59,
            "deletions": 37,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Image processor class for OwlViT\"\"\"\n \n import warnings\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n \n import numpy as np\n \n@@ -43,6 +43,9 @@\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_torch_available, logging\n \n \n+if TYPE_CHECKING:\n+    from .modeling_owlvit import OwlViTObjectDetectionOutput\n+\n if is_torch_available():\n     import torch\n \n@@ -58,6 +61,34 @@ def _upcast(t):\n         return t if t.dtype in (torch.int32, torch.int64) else t.int()\n \n \n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`List[Tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n def box_area(boxes):\n     \"\"\"\n     Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n@@ -459,7 +490,10 @@ def post_process(self, outputs, target_sizes):\n         return results\n \n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.1, target_sizes: Union[TensorType, List[Tuple]] = None\n+        self,\n+        outputs: \"OwlViTObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -468,52 +502,46 @@ def post_process_object_detection(\n         Args:\n             outputs ([`OwlViTObjectDetectionOutput`]):\n                 Raw outputs of the model.\n-            threshold (`float`, *optional*):\n+            threshold (`float`, *optional*, defaults to 0.1):\n                 Score threshold to keep object detection predictions.\n             target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n                 Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n                 `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n         \"\"\"\n-        # TODO: (amy) add support for other frameworks\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n \n-        if target_sizes is not None:\n-            if len(logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n \n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n \n         # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n+        batch_boxes = center_to_corners_format(batch_boxes)\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n-                img_h = torch.Tensor([i[0] for i in target_sizes])\n-                img_w = torch.Tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-\n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-            boxes = boxes * scale_fct[:, None, :]\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n \n         results = []\n-        for s, l, b in zip(scores, labels, boxes):\n-            score = s[s > threshold]\n-            label = l[s > threshold]\n-            box = b[s > threshold]\n-            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n \n         return results\n \n-    # TODO: (Amy) Make compatible with other frameworks\n     def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_threshold=0.3, target_sizes=None):\n         \"\"\"\n         Converts the output of [`OwlViTForObjectDetection.image_guided_detection`] into the format expected by the COCO\n@@ -562,13 +590,7 @@ def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_thresh\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n-                img_h = torch.tensor([i[0] for i in target_sizes])\n-                img_w = torch.tensor([i[1] for i in target_sizes])\n-            else:\n-                img_h, img_w = target_sizes.unbind(1)\n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(target_boxes.device)\n-            target_boxes = target_boxes * scale_fct[:, None, :]\n+            target_boxes = _scale_boxes(target_boxes, target_sizes)\n \n         # Compute box display alphas based on prediction scores\n         results = []"
        },
        {
            "sha": "d9c0e724098d1c53f6aa69b73acab900ba8b3853",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -1689,31 +1689,30 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n         >>> import torch\n-        >>> from transformers import AutoProcessor, OwlViTForObjectDetection\n \n-        >>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n+        >>> from transformers import OwlViTProcessor, OwlViTForObjectDetection\n+\n+        >>> processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n         >>> model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n-        >>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n-        >>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n+        >>> text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n+        >>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n         >>> outputs = model(**inputs)\n \n         >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n-        >>> target_sizes = torch.Tensor([image.size[::-1]])\n-        >>> # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n-        >>> results = processor.post_process_object_detection(\n-        ...     outputs=outputs, threshold=0.1, target_sizes=target_sizes\n+        >>> target_sizes = torch.tensor([(image.height, image.width)])\n+        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n+        >>> results = processor.post_process_grounded_object_detection(\n+        ...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n         ... )\n-\n-        >>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n-        >>> text = texts[i]\n-        >>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n-\n-        >>> for box, score, label in zip(boxes, scores, labels):\n+        >>> # Retrieve predictions for the first image for the corresponding text queries\n+        >>> result = results[0]\n+        >>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n+        >>> for box, score, text_label in zip(boxes, scores, text_labels):\n         ...     box = [round(i, 2) for i in box.tolist()]\n-        ...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n+        ...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n         Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]\n         Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]\n         ```\"\"\""
        },
        {
            "sha": "dd74da5546ed6080d8b9d1701a9c104795454e8b",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 89,
            "deletions": 6,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -17,13 +17,17 @@\n \"\"\"\n \n import warnings\n-from typing import List\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n import numpy as np\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import is_flax_available, is_tf_available, is_torch_available\n+from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_owlvit import OwlViTImageGuidedObjectDetectionOutput, OwlViTObjectDetectionOutput\n \n \n class OwlViTProcessor(ProcessorMixin):\n@@ -184,14 +188,93 @@ def post_process_object_detection(self, *args, **kwargs):\n         This method forwards all its arguments to [`OwlViTImageProcessor.post_process_object_detection`]. Please refer\n         to the docstring of this method for more information.\n         \"\"\"\n+        warnings.warn(\n+            \"`post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. \"\n+            \"Use `post_process_grounded_object_detection` instead.\",\n+            FutureWarning,\n+        )\n         return self.image_processor.post_process_object_detection(*args, **kwargs)\n \n-    def post_process_image_guided_detection(self, *args, **kwargs):\n+    def post_process_grounded_object_detection(\n+        self,\n+        outputs: \"OwlViTObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+        text_labels: Optional[List[List[str]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`OwlViTObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            text_labels (`List[List[str]]`, *optional*):\n+                List of lists of text labels for each image in the batch. If unset, \"text_labels\" in output will be\n+                set to `None`.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+            - \"text_labels\": The text labels for each predicted bounding box on the image.\n+        \"\"\"\n+        output = self.image_processor.post_process_object_detection(\n+            outputs=outputs, threshold=threshold, target_sizes=target_sizes\n+        )\n+\n+        if text_labels is not None and len(text_labels) != len(output):\n+            raise ValueError(\"Make sure that you pass in as many lists of text labels as images\")\n+\n+        # adding text labels to the output\n+        if text_labels is not None:\n+            for image_output, image_text_labels in zip(output, text_labels):\n+                object_text_labels = [image_text_labels[i] for i in image_output[\"labels\"]]\n+                image_output[\"text_labels\"] = object_text_labels\n+        else:\n+            for image_output in output:\n+                image_output[\"text_labels\"] = None\n+\n+        return output\n+\n+    def post_process_image_guided_detection(\n+        self,\n+        outputs: \"OwlViTImageGuidedObjectDetectionOutput\",\n+        threshold: float = 0.0,\n+        nms_threshold: float = 0.3,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+    ):\n         \"\"\"\n-        This method forwards all its arguments to [`OwlViTImageProcessor.post_process_one_shot_object_detection`].\n-        Please refer to the docstring of this method for more information.\n+        Converts the output of [`OwlViTForObjectDetection.image_guided_detection`] into the format expected by the COCO\n+        api.\n+\n+        Args:\n+            outputs ([`OwlViTImageGuidedObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Minimum confidence threshold to use to filter out predicted boxes.\n+            nms_threshold (`float`, *optional*, defaults to 0.3):\n+                IoU threshold for non-maximum suppression of overlapping boxes.\n+            target_sizes (`torch.Tensor`, *optional*):\n+                Tensor of shape (batch_size, 2) where each entry is the (height, width) of the corresponding image in\n+                the batch. If set, predicted normalized bounding boxes are rescaled to the target sizes. If left to\n+                None, predictions will not be unnormalized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+            - \"labels\": Set to `None`.\n         \"\"\"\n-        return self.image_processor.post_process_image_guided_detection(*args, **kwargs)\n+        return self.image_processor.post_process_image_guided_detection(\n+            outputs=outputs, threshold=threshold, nms_threshold=nms_threshold, target_sizes=target_sizes\n+        )\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "cab47c2b5eacda877b0a814ef79611274adeb3a1",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 3,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -974,8 +974,9 @@ def test_inference_object_detection(self):\n         processor = OwlViTProcessor.from_pretrained(model_name)\n \n         image = prepare_img()\n+        text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n         inputs = processor(\n-            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            text=text_labels,\n             images=image,\n             max_length=16,\n             padding=\"max_length\",\n@@ -991,11 +992,31 @@ def test_inference_object_detection(self):\n         expected_slice_logits = torch.tensor(\n             [[-21.413497, -21.612638], [-19.008193, -19.548841], [-20.958896, -21.382694]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits, atol=1e-4))\n+        resulted_slice_logits = outputs.logits[0, :3, :3]\n+        max_diff = torch.max(torch.abs(resulted_slice_logits - expected_slice_logits)).item()\n+        self.assertLess(max_diff, 3e-4)\n+\n         expected_slice_boxes = torch.tensor(\n             [[0.241309, 0.051896, 0.453267], [0.139474, 0.045701, 0.250660], [0.233022, 0.050479, 0.427671]],\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        resulted_slice_boxes = outputs.pred_boxes[0, :3, :3]\n+        max_diff = torch.max(torch.abs(resulted_slice_boxes - expected_slice_boxes)).item()\n+        self.assertLess(max_diff, 3e-4)\n+\n+        # test post-processing\n+        post_processed_output = processor.post_process_grounded_object_detection(outputs)\n+        self.assertIsNone(post_processed_output[0][\"text_labels\"])\n+\n+        post_processed_output_with_text_labels = processor.post_process_grounded_object_detection(\n+            outputs, text_labels=text_labels\n+        )\n+\n+        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].cpu().tolist()\n+        self.assertListEqual(objects_labels, [0, 0])\n+\n+        objects_text_labels = post_processed_output_with_text_labels[0][\"text_labels\"]\n+        self.assertIsNotNone(objects_text_labels)\n+        self.assertListEqual(objects_text_labels, [\"a photo of a cat\", \"a photo of a cat\"])\n \n     @slow\n     def test_inference_one_shot_object_detection(self):"
        },
        {
            "sha": "d207135a58e8876e75185e0c0fdb05b1b746fd4e",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -967,8 +967,9 @@ def test_inference_object_detection(self):\n         processor = OwlViTProcessor.from_pretrained(model_name)\n \n         image = prepare_img()\n+        text_labels = [[\"a photo of a cat\", \"a photo of a dog\"]]\n         inputs = processor(\n-            text=[[\"a photo of a cat\", \"a photo of a dog\"]],\n+            text=text_labels,\n             images=image,\n             max_length=16,\n             padding=\"max_length\",\n@@ -986,6 +987,21 @@ def test_inference_object_detection(self):\n         ).to(torch_device)\n         self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n \n+        # test post-processing\n+        post_processed_output = processor.post_process_grounded_object_detection(outputs)\n+        self.assertIsNone(post_processed_output[0][\"text_labels\"])\n+\n+        post_processed_output_with_text_labels = processor.post_process_grounded_object_detection(\n+            outputs, text_labels=text_labels\n+        )\n+\n+        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].cpu().tolist()\n+        self.assertListEqual(objects_labels, [0, 0])\n+\n+        objects_text_labels = post_processed_output_with_text_labels[0][\"text_labels\"]\n+        self.assertIsNotNone(objects_text_labels)\n+        self.assertListEqual(objects_text_labels, [\"a photo of a cat\", \"a photo of a cat\"])\n+\n     @slow\n     def test_inference_one_shot_object_detection(self):\n         model_name = \"google/owlvit-base-patch32\""
        },
        {
            "sha": "b4aed440622d46595903a9eb4df20bc738339eea",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/94ae9a8da16b13cc28fa29201008c1c08119a2fc/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94ae9a8da16b13cc28fa29201008c1c08119a2fc/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=94ae9a8da16b13cc28fa29201008c1c08119a2fc",
            "patch": "@@ -973,6 +973,7 @@ def find_all_documented_objects() -> List[str]:\n     \"xnli_processors\",\n     \"xnli_tasks_num_labels\",\n     \"TFTrainingArguments\",\n+    \"OwlViTFeatureExtractor\",\n ]\n \n # Exceptionally, some objects should not be documented after all rules passed."
        }
    ],
    "stats": {
        "total": 655,
        "additions": 467,
        "deletions": 188
    }
}