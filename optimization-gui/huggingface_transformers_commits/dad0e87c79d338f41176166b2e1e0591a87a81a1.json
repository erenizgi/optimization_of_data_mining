{
    "author": "anton-l",
    "message": "Add SmolLM3 (#38755)\n\n* init smollm3\n\n* integration tests\n\n* config quirks\n\n* docs stub\n\n* rests round 2\n\n* tests round 3\n\n* tests round 4\n\n* bring SWA back\n\n* config checker pls\n\n* final checkpoint\n\n* style and copies\n\n* Update src/transformers/models/smollm3/modular_smollm3.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/models/smollm3/modular_smollm3.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "dad0e87c79d338f41176166b2e1e0591a87a81a1",
    "files": [
        {
            "sha": "65038e7e24f4de657fe51d5dd9e7caf3ad2fb9bb",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -1053,6 +1053,8 @@\n         title: SigLIP\n       - local: model_doc/siglip2\n         title: SigLIP2\n+      - local: model_doc/smollm3\n+        title: SmolLM3\n       - local: model_doc/smolvlm\n         title: SmolVLM\n       - local: model_doc/speech-encoder-decoder"
        },
        {
            "sha": "3d1c297f927b6f1e56d72c01940e7fda6a7af895",
            "filename": "docs/source/en/model_doc/smollm3.md",
            "status": "added",
            "additions": 173,
            "deletions": 0,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,173 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# SmolLM3\n+\n+SmolLM3 is a fully open, compact language model designed for efficient deployment while maintaining strong performance. It uses a Transformer decoder architecture with Grouped Query Attention (GQA) to reduce the kv cache, and no RoPE, enabling improved performance on long-context tasks. It is trained using a multi-stage training approach on high-quality public datasets across web, code, and math domains. The model is multilingual and supports very large context lengths. The instruct variant is optimized for reasoning and tool use.\n+\n+> [!TIP]\n+> Click on the SmolLM3 models in the right sidebar for more examples of how to apply SmolLM3 to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line using the instruction-tuned models.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text-generation\",\n+    model=\"HuggingFaceTB/SmolLM3-3B\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=0\n+)\n+\n+messages = [\n+    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+    {\"role\": \"user\", \"content\": \"Tell me about yourself.\"},\n+]\n+outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n+print(outputs[0][\"generated_text\"][-1]['content'])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"HuggingFaceTB/SmolLM3-3B\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n+\n+prompt = \"Give me a short introduction to large language models.\"\n+messages = [\n+    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True\n+)\n+model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n+\n+generated_ids = model.generate(\n+    model_inputs.input_ids,\n+    cache_implementation=\"static\",\n+    max_new_tokens=512,\n+    do_sample=True,\n+    temperature=0.7,\n+    top_k=50,\n+    top_p=0.95\n+)\n+generated_ids = [\n+    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n+]\n+\n+response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+print(response)\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+# pip install -U flash-attn --no-build-isolation\n+transformers chat HuggingFaceTB/SmolLM3-3B --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 4-bits.\n+\n+```python\n+# pip install -U flash-attn --no-build-isolation\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_use_double_quant=True,\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"HuggingFaceTB/SmolLM3-3B\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config,\n+    attn_implementation=\"flash_attention_2\"\n+)\n+\n+inputs = tokenizer(\"Gravity is the force\", return_tensors=\"pt\").to(\"cuda\")\n+outputs = model.generate(**inputs, max_new_tokens=100)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n+```\n+\n+\n+## Notes\n+\n+- Ensure your Transformers library version is up-to-date. SmolLM3 requires Transformers>=4.53.0 for full support.\n+\n+## SmolLM3Config\n+\n+[[autodoc]] SmolLM3Config\n+\n+## SmolLM3Model\n+\n+[[autodoc]] SmolLM3Model\n+    - forward\n+\n+## SmolLM3ForCausalLM\n+\n+[[autodoc]] SmolLM3ForCausalLM\n+    - forward\n+\n+## SmolLM3ForSequenceClassification\n+\n+[[autodoc]] SmolLM3ForSequenceClassification\n+    - forward\n+\n+## SmolLM3ForTokenClassification\n+\n+[[autodoc]] SmolLM3ForTokenClassification\n+    - forward\n+\n+## SmolLM3ForQuestionAnswering\n+\n+[[autodoc]] SmolLM3ForQuestionAnswering\n+    - forward"
        },
        {
            "sha": "6e8a12351843746b1ab7ba846dad25d2b17a496b",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -315,6 +315,7 @@\n         (\"siglip\", \"SiglipConfig\"),\n         (\"siglip2\", \"Siglip2Config\"),\n         (\"siglip_vision_model\", \"SiglipVisionConfig\"),\n+        (\"smollm3\", \"SmolLM3Config\"),\n         (\"smolvlm\", \"SmolVLMConfig\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionConfig\"),\n         (\"speech-encoder-decoder\", \"SpeechEncoderDecoderConfig\"),\n@@ -705,6 +706,7 @@\n         (\"siglip2\", \"SigLIP2\"),\n         (\"siglip2_vision_model\", \"Siglip2VisionModel\"),\n         (\"siglip_vision_model\", \"SiglipVisionModel\"),\n+        (\"smollm3\", \"SmolLM3\"),\n         (\"smolvlm\", \"SmolVLM\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionTransformer\"),\n         (\"speech-encoder-decoder\", \"Speech Encoder decoder\"),"
        },
        {
            "sha": "b631e38828281ee85263e72e40a3819c5f4dbee9",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -295,6 +295,7 @@\n         (\"siglip\", \"SiglipModel\"),\n         (\"siglip2\", \"Siglip2Model\"),\n         (\"siglip_vision_model\", \"SiglipVisionModel\"),\n+        (\"smollm3\", \"SmolLM3Model\"),\n         (\"smolvlm\", \"SmolVLMModel\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionTransformer\"),\n         (\"speech_to_text\", \"Speech2TextModel\"),\n@@ -644,6 +645,7 @@\n         (\"roc_bert\", \"RoCBertForCausalLM\"),\n         (\"roformer\", \"RoFormerForCausalLM\"),\n         (\"rwkv\", \"RwkvForCausalLM\"),\n+        (\"smollm3\", \"SmolLM3ForCausalLM\"),\n         (\"speech_to_text_2\", \"Speech2Text2ForCausalLM\"),\n         (\"stablelm\", \"StableLmForCausalLM\"),\n         (\"starcoder2\", \"Starcoder2ForCausalLM\"),\n@@ -1158,6 +1160,7 @@\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForSequenceClassification\"),\n         (\"roc_bert\", \"RoCBertForSequenceClassification\"),\n         (\"roformer\", \"RoFormerForSequenceClassification\"),\n+        (\"smollm3\", \"SmolLM3ForSequenceClassification\"),\n         (\"squeezebert\", \"SqueezeBertForSequenceClassification\"),\n         (\"stablelm\", \"StableLmForSequenceClassification\"),\n         (\"starcoder2\", \"Starcoder2ForSequenceClassification\"),\n@@ -1244,6 +1247,7 @@\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForQuestionAnswering\"),\n         (\"roc_bert\", \"RoCBertForQuestionAnswering\"),\n         (\"roformer\", \"RoFormerForQuestionAnswering\"),\n+        (\"smollm3\", \"SmolLM3ForQuestionAnswering\"),\n         (\"splinter\", \"SplinterForQuestionAnswering\"),\n         (\"squeezebert\", \"SqueezeBertForQuestionAnswering\"),\n         (\"t5\", \"T5ForQuestionAnswering\"),\n@@ -1352,6 +1356,7 @@\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForTokenClassification\"),\n         (\"roc_bert\", \"RoCBertForTokenClassification\"),\n         (\"roformer\", \"RoFormerForTokenClassification\"),\n+        (\"smollm3\", \"SmolLM3ForTokenClassification\"),\n         (\"squeezebert\", \"SqueezeBertForTokenClassification\"),\n         (\"stablelm\", \"StableLmForTokenClassification\"),\n         (\"starcoder2\", \"Starcoder2ForTokenClassification\"),"
        },
        {
            "sha": "45c6ed1081e072adfad0f37e6a1458ba665eb805",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -172,6 +172,8 @@ class MimiEncoderOutput(ModelOutput):\n \n         If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n         have their past key value states given to this model).\n+    padding_cache (<fill_type>):\n+        <fill_docstring>\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "188d99ef78669b038c92223edb6b0caa4d2bb6be",
            "filename": "src/transformers/models/smollm3/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2F__init__.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_smollm3 import *\n+    from .modeling_smollm3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ff70e18b267655a56743d0e8c5942f14a88fe96e",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "added",
            "additions": 245,
            "deletions": 0,
            "changes": 245,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,245 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/smollm3/modular_smollm3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_smollm3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class SmolLM3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SmolLM3Model`]. It is used to instantiate a\n+    SmolLM3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the SmolLM3 3B.\n+    e.g. [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 128256):\n+            Vocabulary size of the SmolLM3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`SmolLM3Model`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 36):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `16`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 32768):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 128004):\n+            The id of the padding token.\n+        bos_token_id (`int`, *optional*, defaults to 128000):\n+            The id of the beginning of sentence token.\n+        eos_token_id (`int`, *optional*, defaults to 128001):\n+            The id of the end of sentence token.\n+        rope_theta (`float`, *optional*, defaults to 2000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        use_sliding_window (`bool`, *optional*, defaults to `False`):\n+            Whether to use sliding window attention.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention (SWA) window size. If not specified, will default to `None`.\n+        no_rope_layers (`List[int]`, *optional*):\n+            List with at least the same length as the number of layers in the model.\n+            A `1` at an index position indicates that the corresponding layer will use RoPE,\n+            while a `0` indicates that it's a NoPE layer.\n+        no_rope_layer_interval (`int`, *optional*, defaults to 4):\n+            If `no_rope_layers` is `None`, it will be created using a NoPE layer every\n+            `no_rope_layer_interval` layers.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Automatically computed based on sliding window and NoPE settings.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import SmolLM3Model, SmolLM3Config\n+\n+    >>> # Initializing a SmolLM3 style configuration\n+    >>> configuration = SmolLM3Config()\n+\n+    >>> # Initializing a model from the SmolLM3 style configuration\n+    >>> model = SmolLM3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"smollm3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=128256,\n+        hidden_size=2048,\n+        intermediate_size=11008,\n+        num_hidden_layers=36,\n+        num_attention_heads=16,\n+        num_key_value_heads=4,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=32768,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=128004,\n+        bos_token_id=128000,\n+        eos_token_id=128001,\n+        rope_theta=2000000.0,\n+        rope_scaling=None,\n+        use_sliding_window=False,\n+        sliding_window=None,\n+        no_rope_layers=None,\n+        no_rope_layer_interval=4,\n+        layer_types=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.use_sliding_window = use_sliding_window\n+        self.sliding_window = sliding_window\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        if no_rope_layers is None:\n+            self.no_rope_layers = [\n+                int((layer_idx + 1) % no_rope_layer_interval != 0) for layer_idx in range(num_hidden_layers)\n+            ]\n+        else:\n+            self.no_rope_layers = no_rope_layers\n+\n+        self.no_rope_layer_interval = no_rope_layer_interval\n+\n+        # Update layer_types based on sliding window and NoPE pattern\n+        if layer_types is None:\n+            layer_types = []\n+            for layer_idx in range(num_hidden_layers):\n+                has_rope = self.no_rope_layers[layer_idx]\n+                if use_sliding_window and sliding_window is not None and not has_rope:\n+                    layer_types.append(\"sliding_attention\")\n+                else:\n+                    layer_types.append(\"full_attention\")\n+\n+        self.layer_types = layer_types\n+        layer_type_validation(self.layer_types)\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+\n+__all__ = [\"SmolLM3Config\"]"
        },
        {
            "sha": "30b566be3e6adf8740578ecf7eca8035f4cd56f2",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "added",
            "additions": 845,
            "deletions": 0,
            "changes": 845,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,845 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/smollm3/modular_smollm3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_smollm3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutputWithPast,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n+from .configuration_smollm3 import SmolLM3Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class SmolLM3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: SmolLM3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+        self.use_rope = config.no_rope_layers[layer_idx]\n+        self.sliding_window = (\n+            config.sliding_window\n+            if config.use_sliding_window and config.layer_types[layer_idx] == \"sliding_attention\"\n+            else None\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if self.use_rope:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class SmolLM3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        SmolLM3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+@auto_docstring\n+class SmolLM3PreTrainedModel(PreTrainedModel):\n+    config_class = SmolLM3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"SmolLM3DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_3 = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, SmolLM3RMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+class SmolLM3MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class SmolLM3DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: SmolLM3Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = SmolLM3Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = SmolLM3MLP(config)\n+        self.input_layernorm = SmolLM3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = SmolLM3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class SmolLM3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: SmolLM3Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class SmolLM3Model(SmolLM3PreTrainedModel):\n+    def __init__(self, config: SmolLM3Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [SmolLM3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = SmolLM3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = SmolLM3RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+@auto_docstring\n+class SmolLM3ForCausalLM(SmolLM3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = SmolLM3Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, SmolLM3ForCausalLM\n+\n+        >>> model = SmolLM3ForCausalLM.from_pretrained(\"meta-smollm3/SmolLM3-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-smollm3/SmolLM3-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The SmolLM3 Model transformer with a sequence classification head on top (linear layer).\n+\n+    [`SmolLM3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n+    (e.g. GPT-2) do.\n+\n+    Since it does classification on the last token, it requires to know the position of the last token. If a\n+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n+    each row of the batch).\n+    \"\"\"\n+)\n+class SmolLM3ForSequenceClassification(SmolLM3PreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.model = SmolLM3Model(config)\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> SequenceClassifierOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        hidden_states = transformer_outputs.last_hidden_state\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+        else:\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutputWithPast(\n+            loss=loss,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class SmolLM3ForTokenClassification(SmolLM3PreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.model = SmolLM3Model(config)\n+        if getattr(config, \"classifier_dropout\", None) is not None:\n+            classifier_dropout = config.classifier_dropout\n+        elif getattr(config, \"hidden_dropout\", None) is not None:\n+            classifier_dropout = config.hidden_dropout\n+        else:\n+            classifier_dropout = 0.1\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> TokenClassifierOutput:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        sequence_output = outputs.last_hidden_state\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.score(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class SmolLM3ForQuestionAnswering(SmolLM3PreTrainedModel):\n+    base_model_prefix = \"transformer\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.transformer = SmolLM3Model(config)\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.transformer.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.transformer.embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> QuestionAnsweringModelOutput:\n+        outputs: BaseModelOutputWithPast = self.transformer(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        sequence_output = outputs.last_hidden_state\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        loss = None\n+        if start_positions is not None and end_positions is not None:\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"SmolLM3PreTrainedModel\",\n+    \"SmolLM3Model\",\n+    \"SmolLM3ForCausalLM\",\n+    \"SmolLM3ForSequenceClassification\",\n+    \"SmolLM3ForTokenClassification\",\n+    \"SmolLM3ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "290ab5ec695d8c7460898f3697389c7db4424c9c",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "added",
            "additions": 350,
            "deletions": 0,
            "changes": 350,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,350 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional\n+\n+import torch\n+\n+from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaForQuestionAnswering,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaPreTrainedModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..qwen2.modeling_qwen2 import Qwen2Model\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SmolLM3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SmolLM3Model`]. It is used to instantiate a\n+    SmolLM3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the SmolLM3 3B.\n+    e.g. [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 128256):\n+            Vocabulary size of the SmolLM3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`SmolLM3Model`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 36):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `16`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 32768):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 128004):\n+            The id of the padding token.\n+        bos_token_id (`int`, *optional*, defaults to 128000):\n+            The id of the beginning of sentence token.\n+        eos_token_id (`int`, *optional*, defaults to 128001):\n+            The id of the end of sentence token.\n+        rope_theta (`float`, *optional*, defaults to 2000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        use_sliding_window (`bool`, *optional*, defaults to `False`):\n+            Whether to use sliding window attention.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention (SWA) window size. If not specified, will default to `None`.\n+        no_rope_layers (`List[int]`, *optional*):\n+            List with at least the same length as the number of layers in the model.\n+            A `1` at an index position indicates that the corresponding layer will use RoPE,\n+            while a `0` indicates that it's a NoPE layer.\n+        no_rope_layer_interval (`int`, *optional*, defaults to 4):\n+            If `no_rope_layers` is `None`, it will be created using a NoPE layer every\n+            `no_rope_layer_interval` layers.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Automatically computed based on sliding window and NoPE settings.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import SmolLM3Model, SmolLM3Config\n+\n+    >>> # Initializing a SmolLM3 style configuration\n+    >>> configuration = SmolLM3Config()\n+\n+    >>> # Initializing a model from the SmolLM3 style configuration\n+    >>> model = SmolLM3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"smollm3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=128256,\n+        hidden_size=2048,\n+        intermediate_size=11008,\n+        num_hidden_layers=36,\n+        num_attention_heads=16,\n+        num_key_value_heads=4,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=32768,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=128004,\n+        bos_token_id=128000,\n+        eos_token_id=128001,\n+        rope_theta=2000000.0,\n+        rope_scaling=None,\n+        use_sliding_window=False,\n+        sliding_window=None,\n+        no_rope_layers=None,\n+        no_rope_layer_interval=4,\n+        layer_types=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.use_sliding_window = use_sliding_window\n+        self.sliding_window = sliding_window\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        if no_rope_layers is None:\n+            self.no_rope_layers = [\n+                int((layer_idx + 1) % no_rope_layer_interval != 0) for layer_idx in range(num_hidden_layers)\n+            ]\n+        else:\n+            self.no_rope_layers = no_rope_layers\n+\n+        self.no_rope_layer_interval = no_rope_layer_interval\n+\n+        # Update layer_types based on sliding window and NoPE pattern\n+        if layer_types is None:\n+            layer_types = []\n+            for layer_idx in range(num_hidden_layers):\n+                has_rope = self.no_rope_layers[layer_idx]\n+                if use_sliding_window and sliding_window is not None and not has_rope:\n+                    layer_types.append(\"sliding_attention\")\n+                else:\n+                    layer_types.append(\"full_attention\")\n+\n+        self.layer_types = layer_types\n+        layer_type_validation(self.layer_types)\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+\n+class SmolLM3Attention(LlamaAttention):\n+    def __init__(self, config: SmolLM3Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.use_rope = config.no_rope_layers[layer_idx]\n+        self.sliding_window = (\n+            config.sliding_window\n+            if config.use_sliding_window and config.layer_types[layer_idx] == \"sliding_attention\"\n+            else None\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if self.use_rope:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class SmolLM3PreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n+class SmolLM3Model(Qwen2Model):\n+    pass\n+\n+\n+class SmolLM3ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class SmolLM3ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class SmolLM3ForTokenClassification(LlamaForTokenClassification):\n+    pass\n+\n+\n+class SmolLM3ForQuestionAnswering(LlamaForQuestionAnswering):\n+    pass\n+\n+\n+__all__ = [\n+    \"SmolLM3Config\",\n+    \"SmolLM3PreTrainedModel\",\n+    \"SmolLM3Model\",\n+    \"SmolLM3ForCausalLM\",\n+    \"SmolLM3ForSequenceClassification\",\n+    \"SmolLM3ForTokenClassification\",\n+    \"SmolLM3ForQuestionAnswering\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/smollm3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/tests%2Fmodels%2Fsmollm3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/tests%2Fmodels%2Fsmollm3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2F__init__.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1"
        },
        {
            "sha": "7027716889f60c7804d09e18a33f6a0d09ffbe16",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "added",
            "additions": 227,
            "deletions": 0,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -0,0 +1,227 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SmolLM3 model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import pytest\n+from packaging import version\n+from parameterized import parameterized\n+\n+from transformers import AutoTokenizer, SmolLM3Config, is_torch_available\n+from transformers.generation.configuration_utils import GenerationConfig\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    is_flaky,\n+    require_bitsandbytes,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_torch_greater_or_equal\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        SmolLM3ForCausalLM,\n+        SmolLM3ForQuestionAnswering,\n+        SmolLM3ForSequenceClassification,\n+        SmolLM3ForTokenClassification,\n+        SmolLM3Model,\n+    )\n+\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+)\n+\n+\n+class SmolLM3ModelTester(CausalLMModelTester):\n+    config_class = SmolLM3Config\n+    if is_torch_available():\n+        base_model_class = SmolLM3Model\n+        causal_lm_class = SmolLM3ForCausalLM\n+        sequence_class = SmolLM3ForSequenceClassification\n+        token_class = SmolLM3ForTokenClassification\n+        question_answering_class = SmolLM3ForQuestionAnswering\n+\n+\n+@require_torch\n+class SmolLM3ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            SmolLM3Model,\n+            SmolLM3ForCausalLM,\n+            SmolLM3ForSequenceClassification,\n+            SmolLM3ForTokenClassification,\n+            SmolLM3ForQuestionAnswering,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = SmolLM3ModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": SmolLM3Model,\n+            \"text-classification\": SmolLM3ForSequenceClassification,\n+            \"token-classification\": SmolLM3ForTokenClassification,\n+            \"text-generation\": SmolLM3ForCausalLM,\n+            \"question-answering\": SmolLM3ForQuestionAnswering,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # flaky test_eager_matches_sdpa_inference_24_fp32_pad_left_output_attentions\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+\n+@require_torch\n+class SmolLM3IntegrationTest(unittest.TestCase):\n+    model_id = \"HuggingFaceTB/SmolLM3-3B\"\n+\n+    @slow\n+    def test_model_3b_logits(self):\n+        input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n+        model = SmolLM3ForCausalLM.from_pretrained(self.model_id, device_map=\"auto\")\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor([[9.3306, 8.1721, 6.4764, 7.6011, 11.1218, 7.5343, 7.1195, 8.0956]])\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # slicing logits[0, 0, 0:30]\n+        EXPECTED_SLICE = torch.tensor(\n+            [15.7759, 17.6274, 16.3404, 14.5543, 13.1366, 14.2475, 15.8710, 15.6753, 12.3856, 13.0386, 14.0792, 12.7253,\n+             13.9634, 12.1271, 12.4320, 16.0329, 17.3975, 17.1396, 17.8666, 17.0103, 17.2962, 16.8777, 16.7144, 16.3023,\n+             16.6084, 12.4649, 12.0723, 14.1148, 14.8239, 15.2733])  # fmt: skip\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @slow\n+    def test_model_3b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"Gravity is the force that pulls objects toward the center of the Earth. It is a force that is always present, even\"\"\"\n+        prompt = \"Gravity is the force\"\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n+        model = SmolLM3ForCausalLM.from_pretrained(self.model_id, device_map=\"auto\")\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @require_bitsandbytes\n+    @slow\n+    @require_flash_attn\n+    @pytest.mark.flash_attn_test\n+    def test_model_3b_long_prompt(self):\n+        EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n+        # An input with 4097 tokens that is above the size of the sliding window\n+        input_ids = [1] + [306, 338] * 2048\n+        model = SmolLM3ForCausalLM.from_pretrained(\n+            self.model_id,\n+            device_map=\"auto\",\n+            load_in_4bit=True,\n+            attn_implementation=\"flash_attention_2\",\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n+\n+        # Assisted generation\n+        assistant_model = model\n+        assistant_model.generation_config.num_assistant_tokens = 2\n+        assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"\n+        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n+\n+        del assistant_model\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @slow\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            self.model_id, pad_token=\"<|finetune_right_pad_id|>\", padding_side=\"right\"\n+        )\n+        EXPECTED_TEXT_COMPLETION = \"Gravity is the force that pulls objects toward the center of the Earth. It is a force that is always present, and\"\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = SmolLM3ForCausalLM.from_pretrained(\n+            self.model_id,\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompt = [\"Gravity is the force\"]\n+        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        strict = is_torch_greater_or_equal(\"2.7.0\")  # Due to https://github.com/pytorch/pytorch/issues/150994\n+        exported_program = convert_and_export_with_cache(model, strict=strict)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        },
        {
            "sha": "46c2bb1a9f5585c47316adba72314919a2ec707a",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dad0e87c79d338f41176166b2e1e0591a87a81a1/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dad0e87c79d338f41176166b2e1e0591a87a81a1/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=dad0e87c79d338f41176166b2e1e0591a87a81a1",
            "patch": "@@ -272,6 +272,7 @@\n         \"attention_chunk_size\",\n     ],\n     \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n+    \"SmolLM3Config\": [\"no_rope_layer_interval\"],\n }\n \n "
        }
    ],
    "stats": {
        "total": 1879,
        "additions": 1879,
        "deletions": 0
    }
}