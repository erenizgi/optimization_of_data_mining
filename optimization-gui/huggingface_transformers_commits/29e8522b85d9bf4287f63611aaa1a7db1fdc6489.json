{
    "author": "fjosw",
    "message": "[Fix] dots1 expert bias routing (#41663)",
    "sha": "29e8522b85d9bf4287f63611aaa1a7db1fdc6489",
    "files": [
        {
            "sha": "e10ddef71ef3792c9153a55a0a68d54658495291",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e8522b85d9bf4287f63611aaa1a7db1fdc6489/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e8522b85d9bf4287f63611aaa1a7db1fdc6489/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=29e8522b85d9bf4287f63611aaa1a7db1fdc6489",
            "patch": "@@ -369,9 +369,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()  # main diff with deepseekv3\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -381,7 +383,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        },
        {
            "sha": "291962ba32e4568d44d75e717623c87547a78c7d",
            "filename": "src/transformers/models/dots1/modular_dots1.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e8522b85d9bf4287f63611aaa1a7db1fdc6489/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e8522b85d9bf4287f63611aaa1a7db1fdc6489/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py?ref=29e8522b85d9bf4287f63611aaa1a7db1fdc6489",
            "patch": "@@ -61,9 +61,11 @@ class Dots1TopkRouter(DeepseekV3TopkRouter):\n class Dots1MoE(DeepseekV3MoE):\n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()  # main diff with deepseekv3\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -73,7 +75,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 10,
        "deletions": 6
    }
}