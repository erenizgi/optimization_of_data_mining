{
    "author": "NahieliV",
    "message": "add fast image processor nougat (#37661)\n\n* add fast image processor nougat\n\n* test fixes\n\n* docstring white space\n\n* last fixes\n\n* docstring_type\n\n* tolerance unit test\n\n* fix tolerance\n\n* fix rtol\n\n* remove traling white space\n\n* remove white space\n\n* note for tolerance unit test\n\n* fix tests\n\n* remove print\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
    "files": [
        {
            "sha": "accde09ffddf1e3337a1dbb7e6bb25acd5aacb05",
            "filename": "docs/source/en/model_doc/nougat.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -107,6 +107,11 @@ The model is identical to [Donut](donut) in terms of architecture.\n [[autodoc]] NougatImageProcessor\n     - preprocess\n \n+## NougatImageProcessorFast\n+\n+[[autodoc]] NougatImageProcessorFast\n+    - preprocess\n+\n ## NougatTokenizerFast\n \n [[autodoc]] NougatTokenizerFast"
        },
        {
            "sha": "4586627b9199d219ce2ebe6f9016748b80029b6f",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -126,7 +126,7 @@\n             (\"mobilevit\", (\"MobileViTImageProcessor\",)),\n             (\"mobilevitv2\", (\"MobileViTImageProcessor\",)),\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"nougat\", (\"NougatImageProcessor\",)),\n+            (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n             (\"oneformer\", (\"OneFormerImageProcessor\",)),\n             (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),"
        },
        {
            "sha": "6cd3208bfa203196d0c31d6b3a1037e4f5ba22f4",
            "filename": "src/transformers/models/nougat/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -19,6 +19,7 @@\n \n if TYPE_CHECKING:\n     from .image_processing_nougat import *\n+    from .image_processing_nougat_fast import *\n     from .processing_nougat import *\n     from .tokenization_nougat_fast import *\n else:"
        },
        {
            "sha": "827686a6066d38a792667d6a67c8d91028d3f938",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -169,6 +169,7 @@ def crop_margin(\n         min_val = data.min()\n         if max_val == min_val:\n             image = np.array(image)\n+            image = to_channel_dimension_format(image, input_data_format, ChannelDimension.LAST)\n             image = (\n                 to_channel_dimension_format(image, data_format, input_data_format)\n                 if data_format is not None"
        },
        {
            "sha": "29e1d6e21758d0dce0cee1f5b14eefaec8d43390",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "added",
            "additions": 327,
            "deletions": 0,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -0,0 +1,327 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Nougat.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_transforms import (\n+    get_resize_output_image_size,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class NougatFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    Args:\n+    do_crop_margin (`bool`, *optional*, defaults to `True`):\n+            Whether to crop the image margins.\n+    do_thumbnail (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image using thumbnail method.\n+    do_align_long_axis (`bool`, *optional*, defaults to `False`):\n+            Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the images to the largest image size in the batch.\n+    \"\"\"\n+\n+    do_crop_margin: Optional[bool]\n+    do_thumbnail: Optional[bool]\n+    do_align_long_axis: Optional[bool]\n+    do_pad: Optional[bool]\n+\n+\n+@auto_docstring\n+class NougatImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 896, \"width\": 672}\n+    do_resize: bool = (True,)\n+    do_normalize: bool = True\n+    do_thumbnail: bool = True\n+    do_align_long_axis: bool = False\n+    do_pad: bool = True\n+    do_rescale = True\n+    do_crop_margin: bool = True\n+    valid_kwargs = NougatFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[NougatFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[NougatFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def python_find_non_zero(\n+        self,\n+        image: \"torch.Tensor\",\n+    ):\n+        \"\"\"This is a reimplementation of a findNonZero function equivalent to cv2.\"\"\"\n+\n+        non_zero_indices = torch.nonzero(image, as_tuple=False)\n+        idxvec = non_zero_indices[:, [2, 1]]\n+        idxvec = idxvec.reshape(-1, 1, 2)\n+        return idxvec\n+\n+    def python_bounding_rect(self, coordinates):\n+        \"\"\"This is a reimplementation of a BoundingRect function equivalent to cv2.\"\"\"\n+\n+        min_values = torch.amin(coordinates, axis=(0, 1)).to(torch.int)\n+        max_values = torch.amax(coordinates, axis=(0, 1)).to(torch.int)\n+\n+        x_min, y_min = min_values[0], min_values[1]\n+        width = max_values[0] - x_min + 1\n+        height = max_values[1] - y_min + 1\n+        return x_min, y_min, width, height\n+\n+    def crop_margin(\n+        self,\n+        image: \"torch.Tensor\",\n+        gray_threshold: int = 200,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the\n+        threshold).\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                The image to be cropped.\n+            gray_threshold (`int`, *optional*, defaults to `200`)\n+                Value below which pixels are considered to be gray.\n+        \"\"\"\n+        data = F.rgb_to_grayscale(image, num_output_channels=1)\n+\n+        max_val = torch.max(data)\n+        min_val = torch.min(data)\n+\n+        if max_val == min_val:\n+            return image\n+        data = (data - min_val) / (max_val - min_val) * 255\n+        gray = data < gray_threshold\n+        coords = self.python_find_non_zero(gray)\n+        x_min, y_min, width, height = self.python_bounding_rect(coords)\n+        image = image[:, y_min : y_min + height, x_min : x_min + width]\n+\n+        return image\n+\n+    def align_long_axis(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Align the long axis of the image to the longest axis of the specified size.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                The image to be aligned.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to align the long axis to.\n+        Returns:\n+            `torch.Tensor`: The aligned image.\n+        \"\"\"\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = size.height, size.width\n+\n+        if (output_width < output_height and input_width > input_height) or (\n+            output_width > output_height and input_width < input_height\n+        ):\n+            image = torch.rot90(image, 3, dims=[1, 2])\n+\n+        return image\n+\n+    def thumbnail(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any\n+        corresponding dimension of the specified size.\n+\n+        Args:\n+            image (`torch.tensor`):\n+                The image to be resized.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to resize the image to.\n+        \"\"\"\n+\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = size.height, size.width\n+\n+        # We always resize to the smallest of either the input or output size.\n+        height = min(input_height, output_height)\n+        width = min(input_width, output_width)\n+\n+        if height == input_height and width == input_width:\n+            return image\n+\n+        if input_height > input_width:\n+            width = int(input_width * height / input_height)\n+        elif input_width > input_height:\n+            height = int(input_height * width / input_width)\n+\n+        new_size = (height, width)\n+\n+        return F.resize(image, new_size, interpolation=F.InterpolationMode.BICUBIC)\n+\n+    def pad_images(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads a batch of images to the specified size at the top, bottom, left and right.\n+\n+        Args:\n+            image (`torch.tensor`):\n+                The image to be padded.\n+            size (`Dict[str, int]`):\n+                The size `{\"height\": h, \"width\": w}` to pad the image to.\n+        \"\"\"\n+        input_height, input_width = image.shape[-2:]\n+        output_height, output_width = size.height, size.width\n+\n+        delta_width = output_width - input_width\n+        delta_height = output_height - input_height\n+\n+        pad_top = delta_height // 2\n+        pad_left = delta_width // 2\n+\n+        pad_bottom = delta_height - pad_top\n+        pad_right = delta_width - pad_left\n+\n+        padding = (pad_left, pad_top, pad_right, pad_bottom)\n+        return F.pad(image, padding)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BICUBIC`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BICUBIC\n+\n+        shortest_edge = min(size[\"height\"], size[\"width\"])\n+\n+        new_size = get_resize_output_image_size(\n+            image, size=shortest_edge, default_to_square=False, input_data_format=ChannelDimension.FIRST\n+        )\n+        return F.resize(image, new_size, interpolation=interpolation, antialias=antialias)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        do_align_long_axis: bool,\n+        do_thumbnail: bool,\n+        do_pad: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_crop_margin: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Crop images\n+        images = [self.crop_margin(image) for image in images]\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_align_long_axis:\n+                stacked_images = self.align_long_axis(image=stacked_images, size=size)\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size)\n+            if do_thumbnail:\n+                stacked_images = self.thumbnail(image=stacked_images, size=size)\n+            if do_pad:\n+                stacked_images = self.pad_images(image=stacked_images, size=size)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"NougatImageProcessorFast\"]"
        },
        {
            "sha": "6be868e39e901fe2a77f2bb0b611d7bba7b98590",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 164,
            "deletions": 46,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4336ecd1eaae778a24633dea6c62b3a90fb8afd1/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=4336ecd1eaae778a24633dea6c62b3a90fb8afd1",
            "patch": "@@ -16,10 +16,12 @@\n import unittest\n \n import numpy as np\n+import requests\n from huggingface_hub import hf_hub_download\n \n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import cached_property, is_torch_available, is_vision_available\n+from transformers.utils import cached_property, is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,6 +34,9 @@\n \n     from transformers import NougatImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import NougatImageProcessorFast\n+\n \n class NougatImageProcessingTester:\n     def __init__(\n@@ -68,6 +73,8 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n+        self.data_format = \"channels_first\"\n+        self.input_data_format = \"channels_first\"\n \n     def prepare_image_processor_dict(self):\n         return {\n@@ -112,6 +119,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class NougatImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = NougatImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = NougatImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -126,61 +134,106 @@ def image_processor(self):\n         return self.image_processing_class(**self.image_processor_dict)\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            kwargs = dict(self.image_processor_dict)\n+            kwargs.pop(\"size\", None)\n+            image_processor = self.image_processing_class(**kwargs, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_expected_output(self):\n         dummy_image = self.image_processor_tester.prepare_dummy_image()\n-        image_processor = self.image_processor\n-        inputs = image_processor(dummy_image, return_tensors=\"pt\")\n-        torch.testing.assert_close(inputs[\"pixel_values\"].mean(), torch.tensor(0.4906), rtol=1e-3, atol=1e-3)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            inputs = image_processor(dummy_image, return_tensors=\"pt\")\n+            torch.testing.assert_close(inputs[\"pixel_values\"].mean(), torch.tensor(0.4906), rtol=1e-3, atol=1e-3)\n \n     def test_crop_margin_all_white(self):\n-        image = np.uint8(np.ones((100, 100, 3)) * 255)\n-        image_processor = self.image_processor\n-        cropped_image = image_processor.crop_margin(image)\n-        self.assertTrue(np.array_equal(image, cropped_image))\n+        image = np.uint8(np.ones((3, 100, 100)) * 255)\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                cropped_image = image_processor.crop_margin(image)\n+                self.assertTrue(torch.equal(image, cropped_image))\n+            else:\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                cropped_image = image_processor.crop_margin(image)\n+                self.assertTrue(np.array_equal(image, cropped_image))\n \n     def test_crop_margin_centered_black_square(self):\n-        image = np.ones((100, 100, 3), dtype=np.uint8) * 255\n-        image[45:55, 45:55, :] = 0\n-        image_processor = self.image_processor\n-        cropped_image = image_processor.crop_margin(image)\n-        expected_cropped = image[45:55, 45:55, :]\n-        self.assertTrue(np.array_equal(expected_cropped, cropped_image))\n+        image = np.ones((3, 100, 100), dtype=np.uint8) * 255\n+        image[:, 45:55, 45:55] = 0\n+        expected_cropped = image[:, 45:55, 45:55]\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                expected_cropped = torch.from_numpy(expected_cropped)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                cropped_image = image_processor.crop_margin(image)\n+                self.assertTrue(torch.equal(expected_cropped, cropped_image))\n+            else:\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                cropped_image = image_processor.crop_margin(image)\n+                self.assertTrue(np.array_equal(expected_cropped, cropped_image))\n \n     def test_align_long_axis_no_rotation(self):\n-        image = np.uint8(np.ones((100, 200, 3)) * 255)\n-        image_processor = self.image_processor\n-        size = {\"height\": 200, \"width\": 300}\n-        aligned_image = image_processor.align_long_axis(image, size)\n-        self.assertEqual(image.shape, aligned_image.shape)\n+        image = np.uint8(np.ones((3, 100, 200)) * 255)\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                size = SizeDict(height=200, width=300)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                aligned_image = image_processor.align_long_axis(image, size)\n+                self.assertEqual(image.shape, aligned_image.shape)\n+            else:\n+                size = {\"height\": 200, \"width\": 300}\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                aligned_image = image_processor.align_long_axis(image, size)\n+                self.assertEqual(image.shape, aligned_image.shape)\n \n     def test_align_long_axis_with_rotation(self):\n-        image = np.uint8(np.ones((200, 100, 3)) * 255)\n-        image_processor = self.image_processor\n-        size = {\"height\": 300, \"width\": 200}\n-        aligned_image = image_processor.align_long_axis(image, size)\n-        self.assertEqual((200, 100, 3), aligned_image.shape)\n+        image = np.uint8(np.ones((3, 200, 100)) * 255)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                size = SizeDict(height=300, width=200)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                aligned_image = image_processor.align_long_axis(image, size)\n+                self.assertEqual(torch.Size([3, 200, 100]), aligned_image.shape)\n+            else:\n+                size = {\"height\": 300, \"width\": 200}\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                aligned_image = image_processor.align_long_axis(image, size)\n+                self.assertEqual((3, 200, 100), aligned_image.shape)\n \n     def test_align_long_axis_data_format(self):\n-        image = np.uint8(np.ones((100, 200, 3)) * 255)\n-        data_format = \"channels_first\"\n-        size = {\"height\": 200, \"width\": 300}\n-        image_processor = self.image_processor\n-        aligned_image = image_processor.align_long_axis(image, size, data_format=data_format)\n-        self.assertEqual((3, 100, 200), aligned_image.shape)\n+        image = np.uint8(np.ones((3, 100, 200)) * 255)\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                size = SizeDict(height=200, width=300)\n+                aligned_image = image_processor.align_long_axis(image, size)\n+                self.assertEqual(torch.Size([3, 100, 200]), aligned_image.shape)\n+            else:\n+                size = {\"height\": 200, \"width\": 300}\n+                data_format = \"channels_first\"\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                aligned_image = image_processor.align_long_axis(image, size, data_format)\n+                self.assertEqual((3, 100, 200), aligned_image.shape)\n \n     def prepare_dummy_np_image(self):\n         revision = \"ec57bf8c8b1653a209c13f6e9ee66b12df0fc2db\"\n@@ -191,12 +244,77 @@ def prepare_dummy_np_image(self):\n             revision=revision,\n         )\n         image = Image.open(filepath).convert(\"RGB\")\n-        return np.array(image)\n+        return np.array(image).transpose(2, 0, 1)\n \n     def test_crop_margin_equality_cv2_python(self):\n         image = self.prepare_dummy_np_image()\n-        image_processor = self.image_processor\n-        image_cropped_python = image_processor.crop_margin(image)\n-\n-        self.assertEqual(image_cropped_python.shape, (850, 685, 3))\n-        self.assertEqual(image_cropped_python.mean(), 237.43881150708458)\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessorFast:\n+                image = torch.from_numpy(image)\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                image_cropped_python = image_processor.crop_margin(image)\n+                self.assertEqual(image_cropped_python.shape, torch.Size([3, 850, 685]))\n+                self.assertAlmostEqual(image_cropped_python.float().mean().item(), 237.43881150708458, delta=0.001)\n+            else:\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+                image_cropped_python = image_processor.crop_margin(image)\n+                self.assertEqual(image_cropped_python.shape, (3, 850, 685))\n+                self.assertAlmostEqual(image_cropped_python.mean(), 237.43881150708458, delta=0.001)\n+\n+    def test_call_numpy_4_channels(self):\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == NougatImageProcessor:\n+                # Test that can process images which have an arbitrary number of channels\n+                # Initialize image_processing\n+                image_processor = image_processing_class(**self.image_processor_dict)\n+\n+                # create random numpy tensors\n+                self.image_processor_tester.num_channels = 4\n+                image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0],\n+                    return_tensors=\"pt\",\n+                    input_data_format=\"channels_last\",\n+                    image_mean=0,\n+                    image_std=1,\n+                ).pixel_values\n+                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n+                    [image_inputs[0]]\n+                )\n+                self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs,\n+                    return_tensors=\"pt\",\n+                    input_data_format=\"channels_last\",\n+                    image_mean=0,\n+                    image_std=1,\n+                ).pixel_values\n+                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+                self.assertEqual(\n+                    tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+                )\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        # Adding a larget than usual tolerance because the slow processor uses reducing_gap=2.0 during resizing.\n+        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=2e-1, rtol=0)\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-2\n+        )"
        }
    ],
    "stats": {
        "total": 546,
        "additions": 499,
        "deletions": 47
    }
}