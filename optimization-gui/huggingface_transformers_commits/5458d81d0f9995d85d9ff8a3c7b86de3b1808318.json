{
    "author": "ebezzam",
    "message": "Fix processor usage + add chat_template support to TTS pipeline, and shift common chat template logic to base class. (#42326)\n\n* Fix processor usage and add chat_template support to TTS pipeline.\n\n* Fallback to tokenizer for musicgen.\n\n* Fallback to tokenizer for musicgen.\n\n* Make style\n\n* style/quality after update?\n\n* FIx copied from\n\n* Smaller things.\n\n* Update src/transformers/pipelines/text_to_audio.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Shift common utilities to chat template utils.\n\n* Type nits\n\n* Remove tied weights.\n\n* Keep seamless error\n\n* Better audio output object.\n\n* Properly handle DIa and add test.\n\n* Shift chat template prep to base, test dia batch.\n\n* Backward compatibility for dicts passed to pipelines\n\n* Simplify postprocessing and tests.\n\n* Make quality/style\n\n* Remove chat from image text to text.\n\n* Only check first item, to not consume first item of generator inputs.\n\n* Nit\n\n* Simplify\n\n* Add checks for bark/musicgen to ensure output is audio.\n\n* Better var\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
    "files": [
        {
            "sha": "f44e6dd07cac2e0b8d49f9c9ae5477266baf06d9",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -651,8 +651,10 @@ def generate(\n         )  # size: 10048\n \n         # take the generated semantic tokens\n-        semantic_output = semantic_output[:, max_input_semantic_length + 1 :]\n-\n+        if kwargs.get(\"return_dict_in_generate\", False):\n+            semantic_output = semantic_output.sequences[:, max_input_semantic_length + 1 :]\n+        else:\n+            semantic_output = semantic_output[:, max_input_semantic_length + 1 :]\n         return semantic_output\n \n \n@@ -865,7 +867,10 @@ def generate(\n \n             input_coarse_len = input_coarse.shape[1]\n \n-            x_coarse = torch.hstack([x_coarse, output_coarse[:, input_coarse_len:]])\n+            if kwargs.get(\"return_dict_in_generate\", False):\n+                x_coarse = torch.hstack([x_coarse, output_coarse.sequences[:, input_coarse_len:]])\n+            else:\n+                x_coarse = torch.hstack([x_coarse, output_coarse[:, input_coarse_len:]])\n             total_generated_len = x_coarse.shape[1] - len_coarse_history\n \n             del output_coarse"
        },
        {
            "sha": "b51873aed8ee96310414ef84d0f42bf7d9852370",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -51,6 +51,7 @@\n     is_torch_xpu_available,\n     logging,\n )\n+from ..utils.chat_template_utils import Chat, is_valid_message\n \n \n GenericTensor = Union[list[\"GenericTensor\"], \"torch.Tensor\"]\n@@ -60,6 +61,7 @@\n     from torch.utils.data import DataLoader, Dataset\n \n     from ..modeling_utils import PreTrainedModel\n+    from .pt_utils import KeyDataset\n else:\n     Dataset = None\n \n@@ -1213,6 +1215,18 @@ def __call__(self, inputs, *args, num_workers=None, batch_size=None, **kwargs):\n         if args:\n             logger.warning(f\"Ignoring args : {args}\")\n \n+        # Detect if inputs are a chat-style input(s) and cast as `Chat` or list of `Chat`\n+        container_types = (list, tuple, types.GeneratorType)\n+        if is_torch_available():\n+            container_types = (*container_types, KeyDataset)\n+        if isinstance(inputs, container_types):\n+            if isinstance(inputs, types.GeneratorType):\n+                inputs = list(inputs)\n+            if is_valid_message(inputs[0]):\n+                inputs = Chat(inputs)\n+            elif isinstance(inputs[0], (list, tuple)) and all(chat and is_valid_message(chat[0]) for chat in inputs):\n+                inputs = [Chat(chat) for chat in inputs]\n+\n         if num_workers is None:\n             if self._num_workers is None:\n                 num_workers = 0"
        },
        {
            "sha": "7d0aa2d47f2dfc4bfc494e4ca9868c9251886a8c",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -25,6 +25,7 @@\n     logging,\n     requires_backends,\n )\n+from ..utils.chat_template_utils import Chat\n from .base import Pipeline, build_pipeline_init_args\n \n \n@@ -49,18 +50,6 @@ class ReturnType(enum.Enum):\n     FULL_TEXT = 2\n \n \n-class Chat:\n-    \"\"\"This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats\n-    to this format because the rest of the pipeline code tends to assume that lists of messages are\n-    actually a batch of samples rather than messages in the same conversation.\"\"\"\n-\n-    def __init__(self, messages: dict):\n-        for message in messages:\n-            if not (\"role\" in message and \"content\" in message):\n-                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n-        self.messages = messages\n-\n-\n @add_end_docstrings(build_pipeline_init_args(has_processor=True))\n class ImageTextToTextPipeline(Pipeline):\n     \"\"\""
        },
        {
            "sha": "dc1e2035393e5e2cb269353fab50ca919db66710",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 39,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -1,20 +1,16 @@\n import enum\n-import itertools\n-import types\n from typing import Any, overload\n \n from ..generation import GenerationConfig\n from ..utils import ModelOutput, add_end_docstrings, is_torch_available\n+from ..utils.chat_template_utils import Chat, ChatType\n from .base import Pipeline, build_pipeline_init_args\n \n \n if is_torch_available():\n     import torch\n \n     from ..models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n-    from .pt_utils import KeyDataset\n-\n-ChatType = list[dict[str, str]]\n \n \n class ReturnType(enum.Enum):\n@@ -23,18 +19,6 @@ class ReturnType(enum.Enum):\n     FULL_TEXT = 2\n \n \n-class Chat:\n-    \"\"\"This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats\n-    to this format because the rest of the pipeline code tends to assume that lists of messages are\n-    actually a batch of samples rather than messages in the same conversation.\"\"\"\n-\n-    def __init__(self, messages: dict):\n-        for message in messages:\n-            if not (\"role\" in message and \"content\" in message):\n-                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n-        self.messages = messages\n-\n-\n @add_end_docstrings(build_pipeline_init_args(has_tokenizer=True))\n class TextGenerationPipeline(Pipeline):\n     \"\"\"\n@@ -263,7 +247,7 @@ def __call__(self, text_inputs, **kwargs):\n         Complete the prompt(s) given as inputs.\n \n         Args:\n-            text_inputs (`str`, `list[str]`, list[dict[str, str]], or `list[list[dict[str, str]]]`):\n+            text_inputs (`str`, `list[str]`, `ChatType`, or `list[ChatType]`):\n                 One or several prompts (or one list of prompts) to complete. If strings or a list of string are\n                 passed, this pipeline will continue each prompt. Alternatively, a \"chat\", in the form of a list\n                 of dicts with \"role\" and \"content\" keys, can be passed, or a list of such chats. When chats are passed,\n@@ -308,27 +292,6 @@ def __call__(self, text_inputs, **kwargs):\n             - **generated_token_ids** (`torch.Tensor`, present when `return_tensors=True`) -- The token\n               ids of the generated text.\n         \"\"\"\n-        if isinstance(\n-            text_inputs,\n-            (list, tuple, types.GeneratorType, KeyDataset)\n-            if is_torch_available()\n-            else (list, tuple, types.GeneratorType),\n-        ):\n-            if isinstance(text_inputs, types.GeneratorType):\n-                text_inputs, _ = itertools.tee(text_inputs)\n-                text_inputs, first_item = (x for x in text_inputs), next(_)\n-            else:\n-                first_item = text_inputs[0]\n-            if isinstance(first_item, (list, tuple, dict)):\n-                # We have one or more prompts in list-of-dicts format, so this is chat mode\n-                if isinstance(first_item, dict):\n-                    return super().__call__(Chat(text_inputs), **kwargs)\n-                else:\n-                    chats = (Chat(chat) for chat in text_inputs)  # ðŸˆ ðŸˆ ðŸˆ\n-                    if isinstance(text_inputs, types.GeneratorType):\n-                        return super().__call__(chats, **kwargs)\n-                    else:\n-                        return super().__call__(list(chats), **kwargs)\n         return super().__call__(text_inputs, **kwargs)\n \n     def preprocess("
        },
        {
            "sha": "be7a9b9bc0c85dd1081d41764959243aa903dba5",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 69,
            "deletions": 38,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -11,10 +11,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.from typing import List, Union\n-from typing import Any, overload\n \n+from typing import Any, TypedDict, overload\n+\n+from ..audio_utils import AudioInput\n from ..generation import GenerationConfig\n from ..utils import is_torch_available\n+from ..utils.chat_template_utils import Chat, ChatType\n from .base import Pipeline\n \n \n@@ -27,6 +30,18 @@\n DEFAULT_VOCODER_ID = \"microsoft/speecht5_hifigan\"\n \n \n+class AudioOutput(TypedDict, total=False):\n+    \"\"\"\n+    audio (`AudioInput`):\n+        The generated audio waveform.\n+    sampling_rate (`int`):\n+        The sampling rate of the generated audio waveform.\n+    \"\"\"\n+\n+    audio: AudioInput\n+    sampling_rate: int\n+\n+\n class TextToAudioPipeline(Pipeline):\n     \"\"\"\n     Text-to-audio generation pipeline using any `AutoModelForTextToWaveform` or `AutoModelForTextToSpectrogram`. This\n@@ -81,7 +96,7 @@ class TextToAudioPipeline(Pipeline):\n     \"\"\"\n \n     _pipeline_calls_generate = True\n-    _load_processor = False\n+    _load_processor = None  # prioritize processors as some models require it\n     _load_image_processor = False\n     _load_feature_extractor = False\n     _load_tokenizer = True\n@@ -91,12 +106,9 @@ class TextToAudioPipeline(Pipeline):\n         max_new_tokens=256,\n     )\n \n-    def __init__(self, *args, vocoder=None, sampling_rate=None, no_processor=True, **kwargs):\n+    def __init__(self, *args, vocoder=None, sampling_rate=None, **kwargs):\n         super().__init__(*args, **kwargs)\n \n-        # Legacy behaviour just uses the tokenizer while new models use the processor as a whole at any given time\n-        self.no_processor = no_processor\n-\n         self.vocoder = None\n         if self.model.__class__ in MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING.values():\n             self.vocoder = (\n@@ -105,6 +117,10 @@ def __init__(self, *args, vocoder=None, sampling_rate=None, no_processor=True, *\n                 else vocoder\n             )\n \n+        if self.model.config.model_type in [\"musicgen\"]:\n+            # MusicGen expect to use the tokenizer\n+            self.processor = None\n+\n         self.sampling_rate = sampling_rate\n         if self.vocoder is not None:\n             self.sampling_rate = self.vocoder.config.sampling_rate\n@@ -127,7 +143,7 @@ def __init__(self, *args, vocoder=None, sampling_rate=None, no_processor=True, *\n                         self.sampling_rate = sampling_rate\n \n         # last fallback to get the sampling rate based on processor\n-        if self.sampling_rate is None and not self.no_processor and hasattr(self.processor, \"feature_extractor\"):\n+        if self.sampling_rate is None and self.processor is not None and hasattr(self.processor, \"feature_extractor\"):\n             self.sampling_rate = self.processor.feature_extractor.sampling_rate\n \n     def preprocess(self, text, **kwargs):\n@@ -141,16 +157,22 @@ def preprocess(self, text, **kwargs):\n                 \"add_special_tokens\": False,\n                 \"return_attention_mask\": True,\n                 \"return_token_type_ids\": False,\n-                \"padding\": \"max_length\",\n             }\n \n             # priority is given to kwargs\n             new_kwargs.update(kwargs)\n-\n             kwargs = new_kwargs\n \n-        preprocessor = self.tokenizer if self.no_processor else self.processor\n-        output = preprocessor(text, **kwargs, return_tensors=\"pt\")\n+        preprocessor = self.processor if self.processor is not None else self.tokenizer\n+        if isinstance(text, Chat):\n+            output = preprocessor.apply_chat_template(\n+                text.messages,\n+                tokenize=True,\n+                return_dict=True,\n+                **kwargs,\n+            )\n+        else:\n+            output = preprocessor(text, **kwargs, return_tensors=\"pt\")\n \n         return output\n \n@@ -171,6 +193,9 @@ def _forward(self, model_inputs, **kwargs):\n             # generate_kwargs get priority over forward_params\n             forward_params.update(generate_kwargs)\n \n+            # ensure dict output to facilitate postprocessing\n+            forward_params.update({\"return_dict_in_generate\": True})\n+\n             output = self.model.generate(**model_inputs, **forward_params)\n         else:\n             if len(generate_kwargs):\n@@ -188,18 +213,27 @@ def _forward(self, model_inputs, **kwargs):\n         return output\n \n     @overload\n-    def __call__(self, text_inputs: str, **forward_params: Any) -> dict[str, Any]: ...\n+    def __call__(self, text_inputs: str, **forward_params: Any) -> AudioOutput: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: list[str], **forward_params: Any) -> list[AudioOutput]: ...\n+\n+    @overload\n+    def __call__(self, text_inputs: ChatType, **forward_params: Any) -> AudioOutput: ...\n \n     @overload\n-    def __call__(self, text_inputs: list[str], **forward_params: Any) -> list[dict[str, Any]]: ...\n+    def __call__(self, text_inputs: list[ChatType], **forward_params: Any) -> list[AudioOutput]: ...\n \n-    def __call__(self, text_inputs: str | list[str], **forward_params) -> dict[str, Any] | list[dict[str, Any]]:\n+    def __call__(self, text_inputs, **forward_params):\n         \"\"\"\n         Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\n \n         Args:\n-            text_inputs (`str` or `list[str]`):\n-                The text(s) to generate.\n+            text_inputs (`str`, `list[str]`, `ChatType`, or `list[ChatType]`):\n+                One or several texts to generate. If strings or a list of string are passed, this pipeline will\n+                generate the corresponding text. Alternatively, a \"chat\", in the form of a list of dicts with \"role\"\n+                and \"content\" keys, can be passed, or a list of such chats. When chats are passed, the model's chat\n+                template will be used to format them before passing them to the model.\n             forward_params (`dict`, *optional*):\n                 Parameters passed to the model generation/forward method. `forward_params` are always passed to the\n                 underlying model.\n@@ -210,7 +244,7 @@ def __call__(self, text_inputs: str | list[str], **forward_params) -> dict[str,\n                 only passed to the underlying model if the latter is a generative model.\n \n         Return:\n-            A `dict` or a list of `dict`: The dictionaries have two keys:\n+            `AudioOutput` or a list of `AudioOutput`, which is a `TypedDict` with two keys:\n \n             - **audio** (`np.ndarray` of shape `(nb_channels, audio_length)`) -- The generated audio waveform.\n             - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\n@@ -241,29 +275,26 @@ def _sanitize_parameters(\n         return preprocess_params, params, postprocess_params\n \n     def postprocess(self, audio):\n-        output_dict = {}\n-\n-        if self.model.config.model_type == \"csm\":\n-            waveform_key = \"audio\"\n-        else:\n-            waveform_key = \"waveform\"\n-\n-        # We directly get the waveform\n-        if self.no_processor:\n-            if isinstance(audio, dict):\n-                waveform = audio[waveform_key]\n-            elif isinstance(audio, tuple):\n-                waveform = audio[0]\n+        needs_decoding = False\n+        if isinstance(audio, dict):\n+            if \"audio\" in audio:\n+                audio = audio[\"audio\"]\n             else:\n-                waveform = audio\n-        # Or we need to postprocess to get the waveform\n-        else:\n-            waveform = self.processor.decode(audio)\n+                needs_decoding = True\n+                audio = audio[\"sequences\"]\n+        elif isinstance(audio, tuple):\n+            audio = audio[0]\n+\n+        if needs_decoding and self.processor is not None:\n+            audio = self.processor.decode(audio)\n \n         if isinstance(audio, list):\n-            output_dict[\"audio\"] = [el.to(device=\"cpu\", dtype=torch.float).numpy() for el in waveform]\n+            audio = [el.to(device=\"cpu\", dtype=torch.float).numpy().squeeze() for el in audio]\n+            audio = audio if len(audio) > 1 else audio[0]\n         else:\n-            output_dict[\"audio\"] = waveform.to(device=\"cpu\", dtype=torch.float).numpy()\n-        output_dict[\"sampling_rate\"] = self.sampling_rate\n+            audio = audio.to(device=\"cpu\", dtype=torch.float).numpy().squeeze()\n \n-        return output_dict\n+        return AudioOutput(\n+            audio=audio,\n+            sampling_rate=self.sampling_rate,\n+        )"
        },
        {
            "sha": "ed3a6daee73e9b8ac150ebd523b31778cc6e6b86",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 2,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -53,6 +53,9 @@\n     from torch import Tensor\n \n \n+ChatType = list[dict[str, Any]]\n+\n+\n BASIC_TYPES = (int, float, str, bool, Any, type(None), ...)\n # Extracts the initial segment of the docstring, containing the function description\n description_re = re.compile(r\"^(.*?)[\\n\\s]*(Args:|Returns:|Raises:|\\Z)\", re.DOTALL)\n@@ -459,9 +462,9 @@ def strftime_now(format):\n \n \n def render_jinja_template(\n-    conversations: list[list[dict[str, str]]],\n+    conversations: list[ChatType],\n     tools: list[dict | Callable] | None = None,\n-    documents: list[dict[str, str]] | None = None,\n+    documents: ChatType | None = None,\n     chat_template: str | None = None,\n     return_assistant_tokens_mask: bool = False,\n     continue_final_message: bool = False,\n@@ -558,3 +561,26 @@ def render_jinja_template(\n         rendered.append(rendered_chat)\n \n     return rendered, all_generation_indices\n+\n+\n+def is_valid_message(message):\n+    \"\"\"\n+    Check that input is a valid message in a chat, namely a dict with \"role\" and \"content\" keys.\n+    \"\"\"\n+    if not isinstance(message, dict):\n+        return False\n+    if not (\"role\" in message and \"content\" in message):\n+        return False\n+    return True\n+\n+\n+class Chat:\n+    \"\"\"This class is intended to just be used internally for pipelines and not exposed to users. We convert chats\n+    to this format because the rest of the pipeline code tends to assume that lists of messages are\n+    actually a batch of samples rather than messages in the same conversation.\"\"\"\n+\n+    def __init__(self, messages: dict):\n+        for message in messages:\n+            if not is_valid_message(message):\n+                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n+        self.messages = messages"
        },
        {
            "sha": "e814b16eb19e7493495f83885d5ce3e2897530ee",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -220,8 +220,6 @@ def __getitem__(self, i):\n \n     @require_torch\n     def test_small_chat_model_with_iterator_pt(self):\n-        from transformers.pipelines.pt_utils import PipelineIterator\n-\n         text_generator = pipeline(\n             task=\"text-generation\",\n             model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\",\n@@ -253,7 +251,6 @@ def data():\n             yield from [chat1, chat2]\n \n         outputs = text_generator(data(), do_sample=False, max_new_tokens=10)\n-        assert isinstance(outputs, PipelineIterator)\n         outputs = list(outputs)\n         self.assertEqual(\n             outputs,"
        },
        {
            "sha": "19c7570e9e9d1faeba8cd8d503fe26e92e57a2f3",
            "filename": "tests/pipelines/test_pipelines_text_to_audio.py",
            "status": "modified",
            "additions": 56,
            "deletions": 6,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5458d81d0f9995d85d9ff8a3c7b86de3b1808318/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_to_audio.py?ref=5458d81d0f9995d85d9ff8a3c7b86de3b1808318",
            "patch": "@@ -45,9 +45,11 @@ def test_small_musicgen_pt(self):\n         music_generator = pipeline(\n             task=\"text-to-audio\", model=\"facebook/musicgen-small\", do_sample=False, max_new_tokens=5\n         )\n+        num_channels = 1  # model generates mono audio\n \n         outputs = music_generator(\"This is a test\")\n         self.assertEqual({\"audio\": ANY(np.ndarray), \"sampling_rate\": 32000}, outputs)\n+        self.assertEqual(len(outputs[\"audio\"].shape), num_channels)\n \n         # test two examples side-by-side\n         outputs = music_generator([\"This is a test\", \"This is a second test\"])\n@@ -88,6 +90,7 @@ def test_medium_seamless_m4t_pt(self):\n     @require_torch\n     def test_small_bark_pt(self):\n         speech_generator = pipeline(task=\"text-to-audio\", model=\"suno/bark-small\")\n+        num_channels = 1  # model generates mono audio\n \n         forward_params = {\n             # Using `do_sample=False` to force deterministic output\n@@ -100,6 +103,7 @@ def test_small_bark_pt(self):\n             {\"audio\": ANY(np.ndarray), \"sampling_rate\": 24000},\n             outputs,\n         )\n+        self.assertEqual(len(outputs[\"audio\"].shape), num_channels)\n \n         # test two examples side-by-side\n         outputs = speech_generator(\n@@ -151,7 +155,6 @@ def test_conversion_additional_tensor(self):\n             \"add_special_tokens\": False,\n             \"return_attention_mask\": True,\n             \"return_token_type_ids\": False,\n-            \"padding\": \"max_length\",\n         }\n         outputs = speech_generator(\n             \"This is a test\",\n@@ -247,22 +250,69 @@ def test_generative_model_kwargs(self):\n     @slow\n     @require_torch\n     def test_csm_model_pt(self):\n-        speech_generator = pipeline(task=\"text-to-audio\", model=\"sesame/csm-1b\")\n+        speech_generator = pipeline(task=\"text-to-audio\", model=\"sesame/csm-1b\", device=torch_device)\n+        generate_kwargs = {\"max_new_tokens\": 10, \"output_audio\": True}\n+        num_channels = 1  # model generates mono audio\n \n-        outputs = speech_generator(\"[0]This is a test\")\n+        outputs = speech_generator(\"[0]This is a test\", generate_kwargs=generate_kwargs)\n         self.assertEqual(outputs[\"sampling_rate\"], 24000)\n+        audio = outputs[\"audio\"]\n+        self.assertEqual(ANY(np.ndarray), audio)\n+        # ensure audio and not discrete codes\n+        self.assertEqual(len(audio.shape), num_channels)\n \n+        # test two examples side-by-side\n+        outputs = speech_generator([\"[0]This is a test\", \"[0]This is a second test\"], generate_kwargs=generate_kwargs)\n+        audio = [output[\"audio\"] for output in outputs]\n+        self.assertEqual([ANY(np.ndarray), ANY(np.ndarray)], audio)\n+        self.assertEqual(len(audio[0].shape), num_channels)\n+\n+        # test batching\n+        batch_size = 2\n+        outputs = speech_generator(\n+            [\"[0]This is a test\", \"[0]This is a second test\"], generate_kwargs=generate_kwargs, batch_size=batch_size\n+        )\n+        self.assertEqual(len(outputs), batch_size)\n+        audio = [output[\"audio\"] for output in outputs]\n+        self.assertEqual([ANY(np.ndarray), ANY(np.ndarray)], audio)\n+        self.assertEqual(len(outputs[0][\"audio\"].shape), num_channels)\n+\n+    @slow\n+    @require_torch\n+    def test_dia_model(self):\n+        speech_generator = pipeline(task=\"text-to-audio\", model=\"nari-labs/Dia-1.6B-0626\", device=torch_device)\n+        generate_kwargs = {\"max_new_tokens\": 20}\n+        num_channels = 1  # model generates mono audio\n+\n+        outputs = speech_generator(\n+            \"[S1] Dia is an open weights text to dialogue model.\", generate_kwargs=generate_kwargs\n+        )\n+        self.assertEqual(outputs[\"sampling_rate\"], 44100)\n         audio = outputs[\"audio\"]\n         self.assertEqual(ANY(np.ndarray), audio)\n+        # ensure audio (with one channel) and not discrete codes\n+        self.assertEqual(len(audio.shape), num_channels)\n \n         # test two examples side-by-side\n-        outputs = speech_generator([\"[0]This is a test\", \"[0]This is a second test\"])\n+        outputs = speech_generator(\n+            [\"[S1] Dia is an open weights text to dialogue model.\", \"[S2] This is a second example.\"],\n+            generate_kwargs=generate_kwargs,\n+        )\n         audio = [output[\"audio\"] for output in outputs]\n         self.assertEqual([ANY(np.ndarray), ANY(np.ndarray)], audio)\n+        self.assertEqual(len(audio[0].shape), num_channels)\n \n         # test batching\n-        outputs = speech_generator([\"[0]This is a test\", \"[0]This is a second test\"], batch_size=2)\n-        self.assertEqual(ANY(np.ndarray), outputs[0][\"audio\"])\n+        batch_size = 2\n+        outputs = speech_generator(\n+            [\"[S1] Dia is an open weights text to dialogue model.\", \"[S2] This is a second example.\"],\n+            generate_kwargs=generate_kwargs,\n+            batch_size=2,\n+        )\n+        self.assertEqual(len(outputs), batch_size)\n+        audio = [output[\"audio\"] for output in outputs]\n+        self.assertEqual([ANY(np.ndarray), ANY(np.ndarray)], audio)\n+        self.assertEqual(len(outputs[0][\"audio\"].shape), num_channels)\n \n     def get_test_pipeline(\n         self,"
        }
    ],
    "stats": {
        "total": 281,
        "additions": 178,
        "deletions": 103
    }
}