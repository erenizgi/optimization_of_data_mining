{
    "author": "Cyrilvallez",
    "message": "Force torch>=2.6 with torch.load to avoid vulnerability issue (#37785)\n\n* fix all main files\n\n* fix test files\n\n* oups forgot modular\n\n* add link\n\n* update message",
    "sha": "0cfbf9c95b91819ce924533d68f409334f2a1db2",
    "files": [
        {
            "sha": "5541e5927a3db14ec0a4b247e9fc1ce24ca3e60a",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -24,7 +24,7 @@\n from torch.utils.data import Dataset\n \n from ...tokenization_utils_base import PreTrainedTokenizerBase\n-from ...utils import logging\n+from ...utils import check_torch_load_is_safe, logging\n from ..processors.glue import glue_convert_examples_to_features, glue_output_modes, glue_processors\n from ..processors.utils import InputFeatures\n \n@@ -122,6 +122,7 @@ def __init__(\n         with FileLock(lock_path):\n             if os.path.exists(cached_features_file) and not args.overwrite_cache:\n                 start = time.time()\n+                check_torch_load_is_safe()\n                 self.features = torch.load(cached_features_file, weights_only=True)\n                 logger.info(\n                     f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start"
        },
        {
            "sha": "a84f83d8f848036c3b7d90f99ef6cb493b5cc198",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING\n from ...tokenization_utils import PreTrainedTokenizer\n-from ...utils import logging\n+from ...utils import check_torch_load_is_safe, logging\n from ..processors.squad import SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n \n \n@@ -148,6 +148,7 @@ def __init__(\n         with FileLock(lock_path):\n             if os.path.exists(cached_features_file) and not args.overwrite_cache:\n                 start = time.time()\n+                check_torch_load_is_safe()\n                 self.old_features = torch.load(cached_features_file, weights_only=True)\n \n                 # Legacy cache files have only features, while new cache files"
        },
        {
            "sha": "1639ce9094d6daa4b5a99220988bec441a116a77",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -27,7 +27,7 @@\n import transformers\n \n from . import is_safetensors_available, is_torch_available\n-from .utils import logging\n+from .utils import check_torch_load_is_safe, logging\n \n \n if is_torch_available():\n@@ -71,6 +71,7 @@ def load_pytorch_checkpoint_in_flax_state_dict(\n                 )\n                 raise\n \n+            check_torch_load_is_safe()\n             pt_state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n             logger.info(f\"PyTorch checkpoint contains {sum(t.numel() for t in pt_state_dict.values()):,} parameters.\")\n \n@@ -247,6 +248,7 @@ def convert_pytorch_sharded_state_dict_to_flax(shard_filenames, flax_model):\n     flax_state_dict = {}\n     for shard_file in shard_filenames:\n         # load using msgpack utils\n+        check_torch_load_is_safe()\n         pt_state_dict = torch.load(shard_file, weights_only=True)\n         weight_dtypes = {k: v.dtype for k, v in pt_state_dict.items()}\n         pt_state_dict = {"
        },
        {
            "sha": "51c21bb7fa4c2e70d7af2b529fa306911f846fef",
            "filename": "src/transformers/modeling_tf_pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -21,6 +21,7 @@\n \n from .utils import (\n     ExplicitEnum,\n+    check_torch_load_is_safe,\n     expand_dims,\n     is_numpy_array,\n     is_safetensors_available,\n@@ -198,6 +199,7 @@ def load_pytorch_checkpoint_in_tf2_model(\n         if pt_path.endswith(\".safetensors\"):\n             state_dict = safe_load_file(pt_path)\n         else:\n+            check_torch_load_is_safe()\n             state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n \n         pt_state_dict.update(state_dict)"
        },
        {
            "sha": "0ee4182f98cd564b8388da1bff67126fa2015818",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -94,6 +94,7 @@\n     ModelOutput,\n     PushToHubMixin,\n     cached_file,\n+    check_torch_load_is_safe,\n     copy_func,\n     download_url,\n     extract_commit_hash,\n@@ -445,7 +446,11 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n             error_message += f\"\\nMissing key(s): {str_unexpected_keys}.\"\n         raise RuntimeError(error_message)\n \n-    loader = safe_load_file if load_safe else partial(torch.load, map_location=\"cpu\", weights_only=True)\n+    if load_safe:\n+        loader = safe_load_file\n+    else:\n+        check_torch_load_is_safe()\n+        loader = partial(torch.load, map_location=\"cpu\", weights_only=True)\n \n     for shard_file in shard_files:\n         state_dict = loader(os.path.join(folder, shard_file))\n@@ -490,6 +495,7 @@ def load_state_dict(\n     \"\"\"\n     Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n     \"\"\"\n+    # Use safetensors if possible\n     if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n         with safe_open(checkpoint_file, framework=\"pt\") as f:\n             metadata = f.metadata()\n@@ -512,6 +518,9 @@ def load_state_dict(\n                     state_dict[k] = f.get_tensor(k)\n             return state_dict\n \n+    # Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\n+    if weights_only:\n+        check_torch_load_is_safe()\n     try:\n         if map_location is None:\n             if ("
        },
        {
            "sha": "d9a118f54d46225547568509aa862cad3ebb2ca3",
            "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -29,6 +29,7 @@\n from ....tokenization_utils import PreTrainedTokenizer\n from ....utils import (\n     cached_file,\n+    check_torch_load_is_safe,\n     is_sacremoses_available,\n     is_torch_available,\n     logging,\n@@ -222,6 +223,7 @@ def __init__(\n                             \"from a PyTorch pretrained vocabulary, \"\n                             \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n                         )\n+                    check_torch_load_is_safe()\n                     vocab_dict = torch.load(pretrained_vocab_file, weights_only=True)\n \n             if vocab_dict is not None:\n@@ -705,6 +707,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs,\n \n         # Instantiate tokenizer.\n         corpus = cls(*inputs, **kwargs)\n+        check_torch_load_is_safe()\n         corpus_dict = torch.load(resolved_corpus_file, weights_only=True)\n         for key, value in corpus_dict.items():\n             corpus.__dict__[key] = value\n@@ -784,6 +787,7 @@ def get_lm_corpus(datadir, dataset):\n     fn_pickle = os.path.join(datadir, \"cache.pkl\")\n     if os.path.exists(fn):\n         logger.info(\"Loading cached dataset...\")\n+        check_torch_load_is_safe()\n         corpus = torch.load(fn_pickle, weights_only=True)\n     elif os.path.exists(fn):\n         logger.info(\"Loading cached dataset from pickle...\")"
        },
        {
            "sha": "68e2404475ead381f9711268528c2bd9d6b5ab1a",
            "filename": "src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -26,6 +26,7 @@\n \n from transformers import AutoTokenizer, GPT2Config\n from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n+from transformers.utils import check_torch_load_is_safe\n \n \n def add_checkpointing_args(parser):\n@@ -275,6 +276,7 @@ def merge_transformers_sharded_states(path, num_checkpoints):\n     state_dict = {}\n     for i in range(1, num_checkpoints + 1):\n         checkpoint_path = os.path.join(path, f\"pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin\")\n+        check_torch_load_is_safe()\n         current_chunk = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n         state_dict.update(current_chunk)\n     return state_dict\n@@ -298,6 +300,7 @@ def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n             checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n             if os.path.isfile(checkpoint_path):\n                 break\n+        check_torch_load_is_safe()\n         state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n         tp_state_dicts.append(state_dict)\n     return tp_state_dicts\n@@ -338,6 +341,7 @@ def convert_checkpoint_from_megatron_to_transformers(args):\n             rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n             break\n     print(f\"Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}\")\n+    check_torch_load_is_safe()\n     state_dict = torch.load(rank0_checkpoint_path, map_location=\"cpu\", weights_only=True)\n     megatron_args = state_dict.get(\"args\", None)\n     if megatron_args is None:\n@@ -634,6 +638,7 @@ def convert_checkpoint_from_transformers_to_megatron(args):\n     sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith(\"pytorch_model\")]\n     if len(sub_dirs) == 1:\n         checkpoint_name = \"pytorch_model.bin\"\n+        check_torch_load_is_safe()\n         state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location=\"cpu\", weights_only=True)\n     else:\n         num_checkpoints = len(sub_dirs) - 1"
        },
        {
            "sha": "19c88c047a99318585e98d34b5a6819dd195eec8",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -41,6 +41,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    check_torch_load_is_safe,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n@@ -4391,7 +4392,8 @@ def enable_talker(self):\n         self.has_talker = True\n \n     def load_speakers(self, path):\n-        for key, value in torch.load(path).items():\n+        check_torch_load_is_safe()\n+        for key, value in torch.load(path, weights_only=True).items():\n             self.speaker_map[key] = value\n         logger.info(\"Speaker {} loaded\".format(list(self.speaker_map.keys())))\n "
        },
        {
            "sha": "2123be2903186358d915e321091bae2bcb17fbaa",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -49,6 +49,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    check_torch_load_is_safe,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n@@ -4078,7 +4079,8 @@ def enable_talker(self):\n         self.has_talker = True\n \n     def load_speakers(self, path):\n-        for key, value in torch.load(path).items():\n+        check_torch_load_is_safe()\n+        for key, value in torch.load(path, weights_only=True).items():\n             self.speaker_map[key] = value\n         logger.info(\"Speaker {} loaded\".format(list(self.speaker_map.keys())))\n "
        },
        {
            "sha": "f3708eaa2d5010429e73203564c94cdbfd85de66",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -45,6 +45,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     cached_file,\n+    check_torch_load_is_safe,\n     is_peft_available,\n     is_safetensors_available,\n     logging,\n@@ -1589,6 +1590,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                     cache_dir=cache_dir,\n                 )\n \n+                check_torch_load_is_safe()\n                 state_dict = torch.load(\n                     weight_path,\n                     map_location=\"cpu\",\n@@ -1600,6 +1602,9 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                 # to the original exception.\n                 raise\n \n+            except ValueError:\n+                raise\n+\n             except Exception:\n                 # For any other exception, we throw a generic error.\n                 raise EnvironmentError("
        },
        {
            "sha": "90957024bc655550bf78962604ec68dbed3541f1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -147,6 +147,7 @@\n     PushInProgress,\n     PushToHubMixin,\n     can_return_loss,\n+    check_torch_load_is_safe,\n     find_labels,\n     is_accelerate_available,\n     is_apex_available,\n@@ -2831,6 +2832,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                         logger.warning(\n                             \"Enabling FP16 and loading from smp < 1.10 checkpoint together is not supported.\"\n                         )\n+                    check_torch_load_is_safe()\n                     state_dict = torch.load(weights_file, map_location=\"cpu\", weights_only=True)\n                     # Required for smp to not auto-translate state_dict from hf to smp (is already smp).\n                     state_dict[\"_smp_is_partial\"] = False\n@@ -2850,6 +2852,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 if self.args.save_safetensors and os.path.isfile(safe_weights_file):\n                     state_dict = safetensors.torch.load_file(safe_weights_file, device=\"cpu\")\n                 else:\n+                    check_torch_load_is_safe()\n                     state_dict = torch.load(weights_file, map_location=\"cpu\", weights_only=True)\n \n                 # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n@@ -2944,6 +2947,7 @@ def _load_best_model(self):\n                     if self.args.save_safetensors and os.path.isfile(best_safe_model_path):\n                         state_dict = safetensors.torch.load_file(best_safe_model_path, device=\"cpu\")\n                     else:\n+                        check_torch_load_is_safe()\n                         state_dict = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n \n                     state_dict[\"_smp_is_partial\"] = False\n@@ -2999,6 +3003,7 @@ def _load_best_model(self):\n                     if self.args.save_safetensors and os.path.isfile(best_safe_model_path):\n                         state_dict = safetensors.torch.load_file(best_safe_model_path, device=\"cpu\")\n                     else:\n+                        check_torch_load_is_safe()\n                         state_dict = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n \n                     # If the model is on the GPU, it still works!\n@@ -3354,6 +3359,7 @@ def _load_optimizer_and_scheduler(self, checkpoint):\n             # deepspeed loads optimizer/lr_scheduler together with the model in deepspeed_init\n             if not isinstance(self.lr_scheduler, DeepSpeedSchedulerWrapper):\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n+                    check_torch_load_is_safe()\n                     self.lr_scheduler.load_state_dict(\n                         torch.load(os.path.join(checkpoint, SCHEDULER_NAME), weights_only=True)\n                     )\n@@ -3386,6 +3392,7 @@ def _load_optimizer_and_scheduler(self, checkpoint):\n             if is_torch_xla_available():\n                 # On TPU we have to take some extra precautions to properly load the states on the right device.\n                 if self.is_fsdp_xla_v1_enabled:\n+                    check_torch_load_is_safe()\n                     optimizer_state = torch.load(\n                         os.path.join(\n                             checkpoint, f\"rank{self.args.process_index}-of-{self.args.world_size}-{OPTIMIZER_NAME}\"\n@@ -3396,10 +3403,12 @@ def _load_optimizer_and_scheduler(self, checkpoint):\n                     # We only need `optimizer` when resuming from checkpoint\n                     optimizer_state = optimizer_state[\"optimizer\"]\n                 else:\n+                    check_torch_load_is_safe()\n                     optimizer_state = torch.load(\n                         os.path.join(checkpoint, OPTIMIZER_NAME), map_location=\"cpu\", weights_only=True\n                     )\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n+                    check_torch_load_is_safe()\n                     lr_scheduler_state = torch.load(\n                         os.path.join(checkpoint, SCHEDULER_NAME), map_location=\"cpu\", weights_only=True\n                     )\n@@ -3443,12 +3452,14 @@ def opt_load_hook(mod, opt):\n                             **_get_fsdp_ckpt_kwargs(),\n                         )\n                     else:\n+                        check_torch_load_is_safe()\n                         self.optimizer.load_state_dict(\n                             torch.load(\n                                 os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location, weights_only=True\n                             )\n                         )\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n+                    check_torch_load_is_safe()\n                     self.lr_scheduler.load_state_dict(\n                         torch.load(os.path.join(checkpoint, SCHEDULER_NAME), weights_only=True)\n                     )\n@@ -3486,6 +3497,7 @@ def _load_scaler(self, checkpoint):\n             # Load in scaler states\n             if is_torch_xla_available():\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n+                    check_torch_load_is_safe()\n                     scaler_state = torch.load(\n                         os.path.join(checkpoint, SCALER_NAME), map_location=\"cpu\", weights_only=True\n                     )\n@@ -3494,6 +3506,7 @@ def _load_scaler(self, checkpoint):\n                 self.accelerator.scaler.load_state_dict(scaler_state)\n             else:\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n+                    check_torch_load_is_safe()\n                     self.accelerator.scaler.load_state_dict(\n                         torch.load(os.path.join(checkpoint, SCALER_NAME), weights_only=True)\n                     )"
        },
        {
            "sha": "50b1a57c3b8303c6102821e93610ddb50cc26a6c",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -115,6 +115,7 @@\n     OptionalDependencyNotAvailable,\n     _LazyModule,\n     ccl_version,\n+    check_torch_load_is_safe,\n     direct_transformers_import,\n     get_torch_version,\n     is_accelerate_available,"
        },
        {
            "sha": "905781fa81a788fdf7d2370f9b442d3fbc5dcb89",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -1387,6 +1387,16 @@ def is_rich_available():\n     return _rich_available\n \n \n+def check_torch_load_is_safe():\n+    if not is_torch_greater_or_equal(\"2.6\"):\n+        raise ValueError(\n+            \"Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \"\n+            \"to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \"\n+            \"when loading files with safetensors.\"\n+            \"\\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\"\n+        )\n+\n+\n # docstyle-ignore\n AV_IMPORT_ERROR = \"\"\"\n {0} requires the PyAv library but it was not found in your environment. You can install it with:"
        },
        {
            "sha": "3c1435f33eed371b6beee190bb736dbce7d07aca",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import is_torch_available\n from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -414,6 +415,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n+    check_torch_load_is_safe()\n     batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n "
        },
        {
            "sha": "cb197631975ed6466c2ce3773fcddbcb77cac094",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import is_torch_available\n from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -475,6 +476,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n+    check_torch_load_is_safe()\n     batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n "
        },
        {
            "sha": "669a23e109c1cbebc5f24b5a8d20c8717eb6251f",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -33,6 +33,7 @@\n     slow,\n     torch_device,\n )\n+from transformers.utils import check_torch_load_is_safe\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -366,6 +367,7 @@ def test_small_model_integration_test(self):\n             filename=\"llava_1_6_input_ids.pt\",\n             repo_type=\"dataset\",\n         )\n+        check_torch_load_is_safe()\n         original_input_ids = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n         # replace -200 by image_token_index (since we use token ID = 32000 for the image token)\n         # remove image token indices because HF impl expands image tokens `image_seq_length` times\n@@ -378,6 +380,7 @@ def test_small_model_integration_test(self):\n             filename=\"llava_1_6_pixel_values.pt\",\n             repo_type=\"dataset\",\n         )\n+        check_torch_load_is_safe()\n         original_pixel_values = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n         assert torch.allclose(original_pixel_values, inputs.pixel_values.half())\n "
        },
        {
            "sha": "79791e7151380b68abc9490f8696e2952f2cea35",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -412,7 +412,6 @@ def test_logits(self):\n         # verify that prompt without BOS token is identical to Metaseq -> add_special_tokens=False\n         inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n         logits = model(inputs.input_ids, attention_mask=inputs.attention_mask)[0].mean(dim=-1)\n-        # logits_meta = torch.load(self.path_logits_meta)\n         logits_meta = torch.Tensor(\n             [\n                 [1.3851, -13.8923, -10.5229, -10.7533, -0.2309, -10.2384, -0.5365, -9.0947, -5.1670],"
        },
        {
            "sha": "5c5ff13153376e2d8b8cd356e188d3776a64f301",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -27,6 +27,7 @@\n from transformers import is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -451,6 +452,7 @@ def test_model_get_set_embeddings(self):\n def prepare_batch(repo_id=\"ibm/patchtsmixer-etth1-test-data\", file=\"pretrain_batch.pt\"):\n     # TODO: Make repo public\n     file = hf_hub_download(repo_id=repo_id, filename=file, repo_type=\"dataset\")\n+    check_torch_load_is_safe()\n     batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n "
        },
        {
            "sha": "8b64da124e5a201a685e6af871642a9cbddaf2f7",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -23,6 +23,7 @@\n from transformers import is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -302,6 +303,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(repo_id=\"hf-internal-testing/etth1-hourly-batch\", file=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=repo_id, filename=file, repo_type=\"dataset\")\n+    check_torch_load_is_safe()\n     batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n "
        },
        {
            "sha": "42a663e744ed98f8b03ab9e099538a4e5e7462fe",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import is_torch_available\n from transformers.testing_utils import is_flaky, require_torch, slow, torch_device\n+from transformers.utils import check_torch_load_is_safe\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -480,6 +481,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n+    check_torch_load_is_safe()\n     batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n "
        },
        {
            "sha": "1b8b08d14b60d114491fcb7504d455cf49f08e53",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -32,7 +32,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_available, is_vision_available\n+from transformers.utils import cached_property, check_torch_load_is_safe, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -455,6 +455,7 @@ def test_inference_for_pretraining(self):\n \n         # add boolean mask, indicating which patches to mask\n         local_path = hf_hub_download(repo_id=\"hf-internal-testing/bool-masked-pos\", filename=\"bool_masked_pos.pt\")\n+        check_torch_load_is_safe()\n         inputs[\"bool_masked_pos\"] = torch.load(local_path, weights_only=True)\n \n         # forward pass"
        },
        {
            "sha": "4a9132fcf6a7b8afdccf85428a3dbda9bf5986e5",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -38,7 +38,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_available\n+from transformers.utils import check_torch_load_is_safe, is_torch_available\n \n \n if is_torch_available():\n@@ -552,6 +552,7 @@ def test_peft_add_adapter_with_state_dict(self):\n \n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n \n+                check_torch_load_is_safe()\n                 dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 model.load_adapter(adapter_state_dict=dummy_state_dict, peft_config=peft_config)\n@@ -577,6 +578,7 @@ def test_peft_add_adapter_with_state_dict_low_cpu_mem_usage(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                check_torch_load_is_safe()\n                 dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # this should always work\n@@ -645,6 +647,7 @@ def test_peft_from_pretrained_unexpected_keys_warning(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                check_torch_load_is_safe()\n                 dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # add unexpected key\n@@ -672,6 +675,7 @@ def test_peft_from_pretrained_missing_keys_warning(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                check_torch_load_is_safe()\n                 dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # remove a key so that we have missing keys"
        },
        {
            "sha": "d9ee30f7caee3dd034dc07255f5a4d26a29177b9",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -113,6 +113,7 @@\n     SAFE_WEIGHTS_NAME,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n+    check_torch_load_is_safe,\n     is_accelerate_available,\n     is_apex_available,\n     is_bitsandbytes_available,\n@@ -646,6 +647,7 @@ def check_best_model_has_been_loaded(\n         else:\n             best_model = RegressionModel()\n             if not safe_weights:\n+                check_torch_load_is_safe()\n                 state_dict = torch.load(os.path.join(checkpoint, WEIGHTS_NAME), weights_only=True)\n             else:\n                 state_dict = safetensors.torch.load_file(os.path.join(checkpoint, SAFE_WEIGHTS_NAME))\n@@ -678,6 +680,7 @@ def convert_to_sharded_checkpoint(self, folder, save_safe=True, load_safe=True):\n             loader = safetensors.torch.load_file\n             weights_file = os.path.join(folder, SAFE_WEIGHTS_NAME)\n         else:\n+            check_torch_load_is_safe()\n             loader = torch.load\n             weights_file = os.path.join(folder, WEIGHTS_NAME)\n "
        },
        {
            "sha": "77d87dc3546d5ac68242fe41319d3f764049763c",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0cfbf9c95b91819ce924533d68f409334f2a1db2/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=0cfbf9c95b91819ce924533d68f409334f2a1db2",
            "patch": "@@ -74,6 +74,7 @@\n     SAFE_WEIGHTS_NAME,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n+    check_torch_load_is_safe,\n )\n from transformers.utils.import_utils import (\n     is_flash_attn_2_available,\n@@ -739,6 +740,7 @@ def test_checkpoint_sharding_local_bin(self):\n                     # Note: pickle adds some junk so the weight of the file can end up being slightly bigger than\n                     # the size asked for (since we count parameters)\n                     if size >= max_size_int + 50000:\n+                        check_torch_load_is_safe()\n                         state_dict = torch.load(shard_file, weights_only=True)\n                         self.assertEqual(len(state_dict), 1)\n "
        }
    ],
    "stats": {
        "total": 97,
        "additions": 88,
        "deletions": 9
    }
}