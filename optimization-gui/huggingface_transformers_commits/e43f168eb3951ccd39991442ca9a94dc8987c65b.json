{
    "author": "keetrap",
    "message": "Add Fast LeViT Processor (#37154)\n\n* Add Fast LeViT Processor\n\n* Update levit.md\n\n* Update src/transformers/models/levit/image_processing_levit_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* ruff check\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "e43f168eb3951ccd39991442ca9a94dc8987c65b",
    "files": [
        {
            "sha": "f794f7902f02349bfaf95c8b5c9f39e09e0389e2",
            "filename": "docs/source/en/model_doc/levit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e43f168eb3951ccd39991442ca9a94dc8987c65b/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e43f168eb3951ccd39991442ca9a94dc8987c65b/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md?ref=e43f168eb3951ccd39991442ca9a94dc8987c65b",
            "patch": "@@ -94,6 +94,11 @@ If you're interested in submitting a resource to be included here, please feel f\n   [[autodoc]] LevitImageProcessor\n     - preprocess\n \n+## LevitImageProcessorFast\n+\n+  [[autodoc]] LevitImageProcessorFast\n+    - preprocess\n+\n ## LevitModel\n \n [[autodoc]] LevitModel"
        },
        {
            "sha": "6387baa20cd72596226adb4538bf97110a9c6c84",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e43f168eb3951ccd39991442ca9a94dc8987c65b",
            "patch": "@@ -104,7 +104,7 @@\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n-            (\"levit\", (\"LevitImageProcessor\",)),\n+            (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),"
        },
        {
            "sha": "d3ae097b66a7f23a97cc24eda5ae80051bcd475c",
            "filename": "src/transformers/models/levit/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Flevit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Flevit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2F__init__.py?ref=e43f168eb3951ccd39991442ca9a94dc8987c65b",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_levit import *\n     from .feature_extraction_levit import *\n     from .image_processing_levit import *\n+    from .image_processing_levit_fast import *\n     from .modeling_levit import *\n else:\n     import sys"
        },
        {
            "sha": "87b0d0ba3e19cf98b6045bf75b226e3039b62b56",
            "filename": "src/transformers/models/levit/image_processing_levit_fast.py",
            "status": "added",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e43f168eb3951ccd39991442ca9a94dc8987c65b/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py?ref=e43f168eb3951ccd39991442ca9a94dc8987c65b",
            "patch": "@@ -0,0 +1,101 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for LeViT.\"\"\"\n+\n+from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast, SizeDict\n+from ...image_transforms import (\n+    ChannelDimension,\n+    get_resize_output_image_size,\n+)\n+from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n+from ...utils import add_start_docstrings, is_torch_available, is_torchvision_available, is_torchvision_v2_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Levit image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class LevitImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = None\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image.\n+\n+        If size is a dict with keys \"width\" and \"height\", the image will be resized to `(size[\"height\"],\n+        size[\"width\"])`.\n+\n+        If size is a dict with key \"shortest_edge\", the shortest edge value `c` is rescaled to `int(c * (256/224))`.\n+        The smaller edge of the image will be matched to this value i.e, if height > width, then image will be rescaled\n+        to `(size[\"shortest_egde\"] * height / width, size[\"shortest_egde\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the output image after resizing. If size is a dict with keys \"width\" and \"height\", the image\n+                will be resized to (height, width). If size is a dict with key \"shortest_edge\", the shortest edge value\n+                `c` is rescaled to int(`c` * (256/224)). The smaller edge of the image will be matched to this value\n+                i.e, if height > width, then image will be rescaled to (size * height / width, size).\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BICUBIC`):\n+                Resampling filter to use when resiizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BICUBIC\n+        if size.shortest_edge:\n+            shortest_edge = int((256 / 224) * size[\"shortest_edge\"])\n+            new_size = get_resize_output_image_size(\n+                image, size=shortest_edge, default_to_square=False, input_data_format=ChannelDimension.FIRST\n+            )\n+        elif size.height and size.width:\n+            new_size = (size.height, size.width)\n+        else:\n+            raise ValueError(\n+                f\"Size dict must have keys 'height' and 'width' or 'shortest_edge'. Got {size.keys()} {size.keys()}.\"\n+            )\n+        return F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"LevitImageProcessorFast\"]"
        },
        {
            "sha": "beb3c77c1521453de17cf7f10f57e44c0359731f",
            "filename": "tests/models/levit/test_image_processing_levit.py",
            "status": "modified",
            "additions": 21,
            "deletions": 15,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e43f168eb3951ccd39991442ca9a94dc8987c65b/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e43f168eb3951ccd39991442ca9a94dc8987c65b/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py?ref=e43f168eb3951ccd39991442ca9a94dc8987c65b",
            "patch": "@@ -16,14 +16,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import LevitImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import LevitImageProcessorFast\n+\n \n class LevitImageProcessingTester:\n     def __init__(\n@@ -88,6 +91,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class LevitImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = LevitImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = LevitImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -98,19 +102,21 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n-\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 129,
        "deletions": 16
    }
}