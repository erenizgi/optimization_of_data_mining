{
    "author": "LysandreJik",
    "message": "Streaming should be handled at the request-level rather than at the istance level (#41444)\n\n* Streaming should be handled at the request-level rather than at the instance level\n\n* Add tests\n\n* Require torch GPU",
    "sha": "17c31a98ac7c728615eb934220c1a443be69a7c5",
    "files": [
        {
            "sha": "e9e4ed6dbc5e8a227a98269720bdcdccce94ff55",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/17c31a98ac7c728615eb934220c1a443be69a7c5/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17c31a98ac7c728615eb934220c1a443be69a7c5/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=17c31a98ac7c728615eb934220c1a443be69a7c5",
            "patch": "@@ -149,7 +149,6 @@ def __init__(\n         model_device: torch.device,\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n-        streaming: bool = False,\n         manual_eviction: bool = False,\n         slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n     ) -> None:\n@@ -165,7 +164,6 @@ def __init__(\n             model_device: Device for model inputs/outputs\n             model_dtype: Data type for model inputs/outputs\n             scheduler: The [`Scheduler`] to use\n-            streaming: Whether to stream tokens as they're generated\n             manual_eviction: Whether to manually evict blocks from the cache\n             slice_inputs: Whether to slice the inputs to the model\n         \"\"\"\n@@ -178,7 +176,6 @@ def __init__(\n         self.model_device = model_device\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n-        self.streaming = streaming\n         self.manual_eviction = manual_eviction\n         self.slice_inputs = slice_inputs\n \n@@ -519,7 +516,7 @@ def _sync(self):\n     @traced\n     def _maybe_send_output(self, state: RequestState, token: int):\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n-        if self.streaming:\n+        if state.streaming:\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n@@ -596,7 +593,6 @@ def __init__(\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n         max_queue_size=0,\n-        streaming: bool = True,\n         slice_inputs: bool = True,\n     ):\n         \"\"\"\n@@ -606,7 +602,6 @@ def __init__(\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n-            streaming: Whether to stream tokens as they are generated\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -625,7 +620,6 @@ def __init__(\n         self.input_queue = queue.Queue(maxsize=max_queue_size)\n         self.output_queue = queue.Queue()\n         self.stop_event = threading.Event()\n-        self.streaming = streaming\n         self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n         self._generation_thread = None\n         self._request_counter = 0\n@@ -690,7 +684,11 @@ def join(self, timeout: Optional[float] = None):\n                 self._generation_thread = None\n \n     def add_request(\n-        self, input_ids: list[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n+        self,\n+        input_ids: list[int],\n+        request_id: Optional[str] = None,\n+        max_new_tokens: Optional[int] = None,\n+        streaming: bool = False,\n     ) -> str:\n         \"\"\"Add a new generation request to the queue.\n \n@@ -716,6 +714,7 @@ def add_request(\n             full_prompt_ids=list(input_ids),\n             max_new_tokens=max_new_tokens,\n             eos_token_id=self.generation_config.eos_token_id,\n+            streaming=streaming,\n         )\n \n         # Use block=True with timeout to handle backpressure if queue is full\n@@ -872,7 +871,6 @@ def _run_generation_loop(self):\n                 self.model.device,\n                 self.model.dtype,\n                 scheduler(paged_attention_cache, self.manual_eviction),\n-                self.streaming,\n                 self.manual_eviction,\n                 slice_inputs=self.slice_inputs,\n             )\n@@ -956,15 +954,13 @@ def init_continuous_batching(\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        streaming: bool = False,\n         slice_inputs: bool = True,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n             generation_config: Custom generation configuration\n             max_queue_size: Maximum size of the input request queue\n-            streaming: Whether to stream tokens as they are generated\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -986,7 +982,6 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            streaming=streaming,\n             slice_inputs=slice_inputs,\n         )\n "
        },
        {
            "sha": "2f6373d75d226fa08b54fc5e91f24226338f27e5",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17c31a98ac7c728615eb934220c1a443be69a7c5/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17c31a98ac7c728615eb934220c1a443be69a7c5/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=17c31a98ac7c728615eb934220c1a443be69a7c5",
            "patch": "@@ -102,6 +102,7 @@ class RequestState:\n                                 SPLIT_PENDING_REMAINDER, DECODING, FINISHED, FAILED\n         max_new_tokens (int): The maximum number of new tokens to generate.\n         eos_token_id (int): The ID of the end-of-sequence token.\n+        streaming (bool): Whether to stream tokens as they're generated\n         created_time (float): The time the request was created.\n         error (Optional[str]): Any error message associated with the request. When None, has had no error yet.\n     \"\"\"\n@@ -117,6 +118,7 @@ class RequestState:\n     _status: RequestStatus = RequestStatus.PENDING  # Status of the request, hidden behind a property\n     max_new_tokens: int = 20  # Maximum number of new tokens to generate\n     eos_token_id: int = -1  # ID of the end-of-sequence token\n+    streaming: bool = False  # Whether to stream tokens as they're generated\n     created_time: float = field(default_factory=time.time)  # Time the request was created\n     error: Optional[str] = None  # Error message if the request failed\n     lifespan: tuple[float, float] = (-1, -1)  # (time request was no longer pending, time request finished)"
        },
        {
            "sha": "473912735a012dac7cc895e2f6fe2c3de2ad94bb",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 97,
            "deletions": 1,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/17c31a98ac7c728615eb934220c1a443be69a7c5/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17c31a98ac7c728615eb934220c1a443be69a7c5/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=17c31a98ac7c728615eb934220c1a443be69a7c5",
            "patch": "@@ -18,7 +18,7 @@\n import torch\n from parameterized import parameterized\n \n-from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import Expectations, require_kernels, require_torch_gpu, slow\n@@ -337,6 +337,102 @@ def test_attn_implementation(self) -> None:\n         manager = model.init_continuous_batching()\n         assert \"paged|eager\" == manager.model.config._attn_implementation\n \n+    @require_torch_gpu\n+    def test_streaming_request(self) -> None:\n+        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n+        max_new_tokens = 3\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+\n+        manager = model.init_continuous_batching()\n+        manager.logit_processor = LogitsProcessorList()\n+        manager.start()\n+\n+        messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n+\n+        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\n+            model.device\n+        )[0]\n+\n+        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=True)\n+\n+        # In streaming mode, the total number of generated tokens is incremented by 1 on each iteration\n+        chunk_1 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_1.generated_tokens), 1)\n+\n+        chunk_2 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_2.generated_tokens), 2)\n+\n+        chunk_3 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_3.generated_tokens), 3)\n+\n+        manager.stop(block=True)\n+\n+    @require_torch_gpu\n+    def test_non_streaming_request(self) -> None:\n+        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n+        max_new_tokens = 3\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+\n+        manager = model.init_continuous_batching()\n+        manager.logit_processor = LogitsProcessorList()\n+        manager.start()\n+\n+        messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n+\n+        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\n+            model.device\n+        )[0]\n+\n+        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=False)\n+\n+        chunk = next(manager.request_id_iter(request_id))\n+\n+        # In non-streaming mode, the total number of generated tokens is equal to the max new tokens\n+        self.assertEqual(len(chunk.generated_tokens), max_new_tokens)\n+\n+        manager.stop(block=True)\n+\n+    @require_torch_gpu\n+    def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n+        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n+        max_new_tokens = 3\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+\n+        manager = model.init_continuous_batching()\n+        manager.logit_processor = LogitsProcessorList()\n+        manager.start()\n+\n+        messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n+\n+        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\n+            model.device\n+        )[0]\n+\n+        # Non-streaming request\n+        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=False)\n+        chunk = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk.generated_tokens), max_new_tokens)\n+\n+        # Streaming request works afterward\n+        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=True)\n+\n+        chunk_1 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_1.generated_tokens), 1)\n+\n+        chunk_2 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_2.generated_tokens), 2)\n+\n+        chunk_3 = next(manager.request_id_iter(request_id))\n+        self.assertEqual(len(chunk_3.generated_tokens), 3)\n+\n+        manager.stop(block=True)\n+\n \n # FIXME: the gemma test seem broken, there is a message about cuda graphs and the sdpa and flash expecteations are\n # inverted on CUDA. On AMD they do fine."
        }
    ],
    "stats": {
        "total": 119,
        "additions": 106,
        "deletions": 13
    }
}