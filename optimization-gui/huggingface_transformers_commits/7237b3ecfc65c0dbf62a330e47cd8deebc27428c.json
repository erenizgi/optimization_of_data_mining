{
    "author": "muellerzr",
    "message": "Fix FSDP no longer working (#35212)\n\nFix FSDP failing",
    "sha": "7237b3ecfc65c0dbf62a330e47cd8deebc27428c",
    "files": [
        {
            "sha": "b1a95b43ada98cbcebbc69e6a51dc9b7eac9c1a9",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7237b3ecfc65c0dbf62a330e47cd8deebc27428c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7237b3ecfc65c0dbf62a330e47cd8deebc27428c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7237b3ecfc65c0dbf62a330e47cd8deebc27428c",
            "patch": "@@ -2251,7 +2251,7 @@ def _inner_training_loop(\n             else:\n                 debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n \n-        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled\n+        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n \n         # We need to reset the scheduler, as its parameters may be different on subsequent calls\n         if self._created_lr_scheduler:\n@@ -2304,12 +2304,13 @@ def _inner_training_loop(\n             # In case of auto_find_batch_size=True\n             # Remove FSDP wrapping from sub-models.\n             self.model = unwrap_model(self.model, recursive=True)\n-            # configure fsdp plugin for qlora if any\n-            self._fsdp_qlora_plugin_updates()\n \n         if delay_optimizer_creation:\n             if use_accelerator_prepare:\n-                self.model = self.accelerator.prepare(self.model)\n+                # configure fsdp plugin for qlora if any\n+                self._fsdp_qlora_plugin_updates()\n+                if self.accelerator.mixed_precision != \"fp8\":\n+                    self.model = self.accelerator.prepare(self.model)\n             self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n \n         # prepare using `accelerator` prepare\n@@ -4187,7 +4188,7 @@ def evaluation_loop(\n             start_time = time.time()\n             model = (\n                 self.accelerator.prepare(model)\n-                if self.is_deepspeed_enabled or self.is_fsdp_enabled\n+                if self.is_deepspeed_enabled or (self.is_fsdp_enabled and self.accelerator.mixed_precision != \"fp8\")\n                 else self.accelerator.prepare_model(model, evaluation_mode=True)\n             )\n             self.model_preparation_time = round(time.time() - start_time, 4)"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 6,
        "deletions": 5
    }
}