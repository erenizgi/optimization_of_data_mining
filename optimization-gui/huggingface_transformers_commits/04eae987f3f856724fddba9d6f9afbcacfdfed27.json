{
    "author": "ydshieh",
    "message": "Fix flaky `test_beam_search_low_memory` (#35611)\n\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "04eae987f3f856724fddba9d6f9afbcacfdfed27",
    "files": [
        {
            "sha": "510f3fe1a926046d8ef75af68362e2761ef8a0e7",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/04eae987f3f856724fddba9d6f9afbcacfdfed27/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04eae987f3f856724fddba9d6f9afbcacfdfed27/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=04eae987f3f856724fddba9d6f9afbcacfdfed27",
            "patch": "@@ -204,6 +204,8 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n                 \"vision_start_token_id\",\n             ]:\n                 token_index = getattr(config, key, None)\n+                if token_index is None and hasattr(self, \"model_tester\"):\n+                    token_index = getattr(self.model_tester, key, None)\n                 if token_index is not None and token_index < config.get_text_config().vocab_size:\n                     logits_processor_kwargs[\"bad_words_ids\"].append([token_index])\n \n@@ -1077,14 +1079,20 @@ def test_beam_search_low_memory(self):\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n+            set_model_tester_for_less_flaky_test(self)\n+\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            set_config_for_less_flaky_test(config)\n             # batch_size=1 is ok, but batch_size>1 will cause non-identical output\n \n             config.use_cache = True\n             config.is_decoder = True\n \n             # test output equality of low versus high memory\n             model = model_class(config).to(torch_device).eval()\n+            set_model_for_less_flaky_test(model)\n+\n+            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n \n             low_output = model.generate(\n                 **inputs_dict,\n@@ -1093,6 +1101,10 @@ def test_beam_search_low_memory(self):\n                 early_stopping=True,\n                 low_memory=True,\n                 use_cache=True,\n+                output_scores=True,\n+                output_logits=True,\n+                return_dict_in_generate=True,\n+                **logits_processor_kwargs,\n             )\n \n             high_output = model.generate(\n@@ -1102,8 +1114,13 @@ def test_beam_search_low_memory(self):\n                 early_stopping=True,\n                 low_memory=False,\n                 use_cache=True,\n+                output_scores=True,\n+                output_logits=True,\n+                return_dict_in_generate=True,\n+                **logits_processor_kwargs,\n             )\n-            self.assertListEqual(low_output.tolist(), high_output.tolist())\n+            # The two outputs must match and their shape must be as expected\n+            self._check_similar_generate_outputs(low_output, high_output)\n \n     @pytest.mark.generate\n     @parameterized.expand([(\"random\",), (\"same\",)])"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 18,
        "deletions": 1
    }
}