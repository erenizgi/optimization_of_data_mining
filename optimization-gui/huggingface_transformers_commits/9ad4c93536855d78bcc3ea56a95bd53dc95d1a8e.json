{
    "author": "aymeric-roucher",
    "message": "Add Aria (#34157)\n\n* Add Aria\r\n---------\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
    "files": [
        {
            "sha": "6e325e499f342dd42c48c8dac01c299106d1a18a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -810,6 +810,8 @@\n         title: ALIGN\n       - local: model_doc/altclip\n         title: AltCLIP\n+      - local: model_doc/aria\n+        title: Aria\n       - local: model_doc/blip\n         title: BLIP\n       - local: model_doc/blip-2"
        },
        {
            "sha": "181ec8b10fb1fc9f9d0524b48f41e002456dc95d",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -62,6 +62,8 @@ Flax), PyTorch, and/or TensorFlow.\n |                        [ALBERT](model_doc/albert)                        |       ‚úÖ        |         ‚úÖ         |      ‚úÖ      |\n |                         [ALIGN](model_doc/align)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                       [AltCLIP](model_doc/altclip)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                          [Aria](model_doc/aria)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                     [AriaText](model_doc/aria_text)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n | [Audio Spectrogram Transformer](model_doc/audio-spectrogram-transformer) |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                    [Autoformer](model_doc/autoformer)                    |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                          [Bark](model_doc/bark)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n@@ -172,6 +174,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                       [IDEFICS](model_doc/idefics)                       |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                      [Idefics2](model_doc/idefics2)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                      [Idefics3](model_doc/idefics3)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|          [Idefics3VisionTransformer](model_doc/idefics3_vision)          |       ‚ùå        |         ‚ùå         |      ‚ùå      |\n |                      [ImageGPT](model_doc/imagegpt)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                      [Informer](model_doc/informer)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                  [InstructBLIP](model_doc/instructblip)                  |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "9ff7a6687aa939e7d3d9a4bf6c37d93ad7ef3558",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "added",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,106 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Aria\n+\n+## Overview\n+\n+The Aria model was proposed in [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://huggingface.co/papers/2410.05993) by Li et al. from the Rhymes.AI team.\n+\n+Aria is an open multimodal-native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. It has a Mixture-of-Experts architecture, with respectively 3.9B and 3.5B activated parameters per visual token and text token. \n+\n+The abstract from the paper is the following:\n+\n+*Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.*\n+\n+This model was contributed by [m-ric](https://huggingface.co/m-ric).\n+The original code can be found [here](https://github.com/rhymes-ai/Aria).\n+\n+## Usage tips\n+\n+Here's how to use the model for vision tasks:\n+```python\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import AriaProcessor, AriaForConditionalGeneration\n+\n+model_id_or_path = \"rhymes-ai/Aria\"\n+\n+model = AriaForConditionalGeneration.from_pretrained(\n+    model_id_or_path, device_map=\"auto\"\n+)\n+\n+processor = AriaProcessor.from_pretrained(model_id_or_path)\n+\n+image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"text\": \"what is the image?\", \"type\": \"text\"},\n+        ],\n+    }\n+]\n+\n+text = processor.apply_chat_template(messages, add_generation_prompt=True)\n+inputs = processor(text=text, images=image, return_tensors=\"pt\")\n+inputs.to(model.device)\n+\n+output = model.generate(\n+    **inputs,\n+    max_new_tokens=15,\n+    stop_strings=[\"<|im_end|>\"],\n+    tokenizer=processor.tokenizer,\n+    do_sample=True,\n+    temperature=0.9,\n+)\n+output_ids = output[0][inputs[\"input_ids\"].shape[1]:]\n+response = processor.decode(output_ids, skip_special_tokens=True)\n+```\n+\n+\n+## AriaImageProcessor\n+\n+[[autodoc]] AriaImageProcessor\n+\n+## AriaProcessor\n+\n+[[autodoc]] AriaProcessor\n+\n+## AriaTextConfig\n+\n+[[autodoc]] AriaTextConfig\n+\n+## AriaConfig\n+\n+[[autodoc]] AriaConfig\n+\n+## AriaTextModel\n+\n+[[autodoc]] AriaTextModel\n+\n+## AriaTextForCausalLM\n+\n+[[autodoc]] AriaTextForCausalLM\n+\n+## AriaForConditionalGeneration\n+\n+[[autodoc]] AriaForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "cf7c043e9289019db2fe04b0907140014a76f2f2",
            "filename": "docs/source/en/model_doc/idefics3.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -51,6 +51,13 @@ This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts)\n \n [[autodoc]] Idefics3Config\n \n+## Idefics3VisionConfig\n+\n+[[autodoc]] Idefics3VisionConfig\n+\n+## Idefics3VisionTransformer\n+\n+[[autodoc]] Idefics3VisionTransformer\n \n ## Idefics3Model\n "
        },
        {
            "sha": "ab5e1c47a448f308c6501a2061573a5ae468a0c8",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -37,6 +37,7 @@ FlashAttention-2 is experimental and may change considerably in future versions.\n 2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them\n \n FlashAttention-2 is currently supported for the following architectures:\n+* [Aria](https://huggingface.co/docs/transformers/model_doc/aria#transformers.AriaForConditionalGeneration)\n * [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n * [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)\n@@ -216,6 +217,7 @@ PyTorch's [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.o\n \n For now, Transformers supports SDPA inference and training for the following architectures:\n * [Albert](https://huggingface.co/docs/transformers/model_doc/albert#transformers.AlbertModel)\n+* [Aria](https://huggingface.co/docs/transformers/model_doc/aria#transformers.AriaForConditionalGeneration)\n * [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n * [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)"
        },
        {
            "sha": "2eaec8f1def96e9d6547ddb5a867fffcb48662a2",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -170,6 +170,11 @@\n         \"AltCLIPTextConfig\",\n         \"AltCLIPVisionConfig\",\n     ],\n+    \"models.aria\": [\n+        \"AriaConfig\",\n+        \"AriaProcessor\",\n+        \"AriaTextConfig\",\n+    ],\n     \"models.audio_spectrogram_transformer\": [\n         \"ASTConfig\",\n         \"ASTFeatureExtractor\",\n@@ -1176,6 +1181,7 @@\n     _import_structure[\"image_processing_base\"] = [\"ImageProcessingMixin\"]\n     _import_structure[\"image_processing_utils\"] = [\"BaseImageProcessor\"]\n     _import_structure[\"image_utils\"] = [\"ImageFeatureExtractionMixin\"]\n+    _import_structure[\"models.aria\"].extend([\"AriaImageProcessor\"])\n     _import_structure[\"models.beit\"].extend([\"BeitFeatureExtractor\", \"BeitImageProcessor\"])\n     _import_structure[\"models.bit\"].extend([\"BitImageProcessor\"])\n     _import_structure[\"models.blip\"].extend([\"BlipImageProcessor\"])\n@@ -1406,6 +1412,15 @@\n             \"AltCLIPVisionModel\",\n         ]\n     )\n+    _import_structure[\"models.aria\"].extend(\n+        [\n+            \"AriaForConditionalGeneration\",\n+            \"AriaPreTrainedModel\",\n+            \"AriaTextForCausalLM\",\n+            \"AriaTextModel\",\n+            \"AriaTextPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.audio_spectrogram_transformer\"].extend(\n         [\n             \"ASTForAudioClassification\",\n@@ -2461,6 +2476,8 @@\n             \"Idefics3Model\",\n             \"Idefics3PreTrainedModel\",\n             \"Idefics3Processor\",\n+            \"Idefics3VisionConfig\",\n+            \"Idefics3VisionTransformer\",\n         ]\n     )\n     _import_structure[\"models.ijepa\"].extend(\n@@ -5033,6 +5050,11 @@\n         AltCLIPTextConfig,\n         AltCLIPVisionConfig,\n     )\n+    from .models.aria import (\n+        AriaConfig,\n+        AriaProcessor,\n+        AriaTextConfig,\n+    )\n     from .models.audio_spectrogram_transformer import (\n         ASTConfig,\n         ASTFeatureExtractor,\n@@ -6096,6 +6118,7 @@\n         from .image_processing_base import ImageProcessingMixin\n         from .image_processing_utils import BaseImageProcessor\n         from .image_utils import ImageFeatureExtractionMixin\n+        from .models.aria import AriaImageProcessor\n         from .models.beit import BeitFeatureExtractor, BeitImageProcessor\n         from .models.bit import BitImageProcessor\n         from .models.blip import BlipImageProcessor\n@@ -6325,6 +6348,13 @@\n             AltCLIPTextModel,\n             AltCLIPVisionModel,\n         )\n+        from .models.aria import (\n+            AriaForConditionalGeneration,\n+            AriaPreTrainedModel,\n+            AriaTextForCausalLM,\n+            AriaTextModel,\n+            AriaTextPreTrainedModel,\n+        )\n         from .models.audio_spectrogram_transformer import (\n             ASTForAudioClassification,\n             ASTModel,\n@@ -7189,6 +7219,8 @@\n             Idefics3Model,\n             Idefics3PreTrainedModel,\n             Idefics3Processor,\n+            Idefics3VisionConfig,\n+            Idefics3VisionTransformer,\n         )\n         from .models.ijepa import (\n             IJepaForImageClassification,"
        },
        {
            "sha": "89c57cb913fec21f25cccc4da117d73f46227fd3",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -1465,6 +1465,7 @@ def _prepare_generated_length(\n         elif (\n             model_input_name == \"inputs_embeds\"\n             and input_ids_length != inputs_tensor.shape[1]\n+            and input_ids_length != 0\n             and not self.config.is_encoder_decoder\n         ):\n             generation_config.max_length -= inputs_tensor.shape[1]"
        },
        {
            "sha": "116b71c81ad9dffa1a3854f23559c42497caad12",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -16,6 +16,7 @@\n     albert,\n     align,\n     altclip,\n+    aria,\n     audio_spectrogram_transformer,\n     auto,\n     autoformer,"
        },
        {
            "sha": "f73301321527c185cfab149b171a38f5fd4f7852",
            "filename": "src/transformers/models/aria/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2F__init__.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,30 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_aria import *\n+    from .image_processing_aria import *\n+    from .modeling_aria import *\n+    from .processing_aria import *\n+\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ff34d59f5dfe1ae4777faad62664bf618e3fd1c4",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "added",
            "additions": 299,
            "deletions": 0,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,299 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/aria/modular_aria.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aria.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Dict\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class AriaTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This class handles the configuration for the text component of the Aria model.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n+    [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n+    This class extends the LlamaConfig to include additional parameters specific to the Mixture of Experts (MoE) architecture.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`LlamaModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            The size of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n+            Llama 2 up to 4096, CodeLlama up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 2):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n+            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n+            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_heads\n+        moe_num_experts (`int`, *optional*, defaults to 8):\n+            The number of experts in the MoE layer.\n+        moe_topk (`int`, *optional*, defaults to 2):\n+            The number of top experts to route to for each token.\n+        moe_num_shared_experts (`int`, *optional*, defaults to 2):\n+            The number of shared experts.\n+    \"\"\"\n+\n+    model_type = \"aria_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `AriaTextModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=4096,\n+        intermediate_size: int = 4096,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=2,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        head_dim=None,\n+        moe_num_experts: int = 8,\n+        moe_topk: int = 2,\n+        moe_num_shared_experts: int = 2,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+        self.moe_num_experts = moe_num_experts\n+        self.moe_topk = moe_topk\n+        self.moe_num_shared_experts = moe_num_shared_experts\n+\n+\n+class AriaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This class handles the configuration for both vision and text components of the Aria model,\n+    as well as additional parameters for image token handling and projector mapping.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n+    [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`AriaVisionConfig` or `dict`, *optional*):\n+            Configuration for the vision component.\n+        vision_feature_layer (`int`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature.\n+        text_config (`AriaTextConfig` or `dict`, *optional*):\n+            Configuration for the text component.\n+        projector_patch_to_query_dict (`dict`, *optional*):\n+            Mapping of patch sizes to query dimensions.\n+        image_token_index (`int`, *optional*, defaults to 9):\n+            Index used to represent image tokens.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated normal initializer for initializing all weight matrices.\n+\n+    Attributes:\n+        model_type (`str`):\n+            Type of the model, set to `\"aria\"`.\n+        image_token_index (`int`):\n+            Index used to represent image tokens.\n+        projector_patch_to_query_dict (`dict`):\n+            Mapping of patch sizes to query dimensions.\n+        vision_config (`AriaVisionConfig`):\n+            Configuration for the vision component.\n+        text_config (`AriaTextConfig`):\n+            Configuration for the text component.\n+    \"\"\"\n+\n+    model_type = \"aria\"\n+    sub_configs = {\"text_config\": AriaTextConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        vision_feature_layer: int = -1,\n+        text_config: AriaTextConfig = None,\n+        projector_patch_to_query_dict: Dict = None,\n+        image_token_index: int = 9,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        self.image_token_index = image_token_index\n+\n+        # Convert the keys and values of projector_patch_to_query_dict to integers\n+        # This ensures consistency even if they were provided as strings\n+        if projector_patch_to_query_dict is None:\n+            projector_patch_to_query_dict = {\n+                1225: 128,\n+                4900: 256,\n+            }\n+        self.projector_patch_to_query_dict = {int(k): int(v) for k, v in projector_patch_to_query_dict.items()}\n+        self.max_value_projector_patch_to_query_dict = max(self.projector_patch_to_query_dict.values())\n+        self.vision_feature_layer = vision_feature_layer\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = \"idefics3_vision\"\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"idefics3_vision\"]()\n+\n+        self.vision_config = vision_config\n+        self.initializer_range = initializer_range\n+\n+        if isinstance(text_config, dict) and \"model_type\" in text_config:\n+            text_config = AriaTextConfig(**text_config)\n+        elif text_config is None:\n+            text_config = AriaTextConfig()\n+\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"AriaConfig\", \"AriaTextConfig\"]"
        },
        {
            "sha": "dcc9e4d139767229b945eb961a6ec9171885ae7c",
            "filename": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "status": "added",
            "additions": 162,
            "deletions": 0,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,162 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import glob\n+\n+import torch\n+from huggingface_hub import snapshot_download\n+from safetensors import safe_open\n+\n+from transformers import (\n+    AddedToken,\n+    AriaForConditionalGeneration,\n+    AriaProcessor,\n+    AutoConfig,\n+    AutoTokenizer,\n+)\n+\n+\n+EPILOG_TXT = \"\"\"Example:\n+    python transformers/src/transformers/models/aria/convert_aria_weights_to_hf.py --text_model_id rhymes-ai/Aria --vision_model_id rhymes-ai/Aria --output_hub_path m-ric/Aria_hf_2 --old_state_dict_id rhymes-ai/Aria\n+\n+Example for creating the old state dict file with Python:\n+\n+    import torch\n+    from aria.model.language_model.aria_llama import AriaTextForCausalLM\n+\n+    # load model\n+    kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n+    model = AriaTextForCausalLM.from_pretrained(\"rhymes-ai/Aria\", low_cpu_mem_usage=True, **kwargs)\n+\n+    # load vision tower\n+    model.get_vision_tower().load_model()\n+\n+    # Save state dict\n+    torch.save(model.state_dict(), \"tmp/hf_models/aria/model_state_dict.bin\")\n+\"\"\"\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"vision_tower.vision_model\": \"vision_tower\",\n+    \"ln_ffn\": \"layer_norm\",\n+    \"ffn\": \"feed_forward\",\n+    \"ln_kv\": \"layer_norm_kv\",\n+}\n+\n+\n+def load_original_state_dict(model_id):\n+    directory_path = snapshot_download(repo_id=model_id, allow_patterns=[\"*.safetensors\"])\n+\n+    original_state_dict = {}\n+    for path in glob.glob(f\"{directory_path}/*\"):\n+        if path.endswith(\".safetensors\"):\n+            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n+                for key in f.keys():\n+                    original_state_dict[key] = f.get_tensor(key)\n+\n+    return original_state_dict\n+\n+\n+def convert_state_dict_to_hf(state_dict):\n+    new_state_dict = {}\n+    for key, value in state_dict.items():\n+        if key.endswith(\".inv_freq\"):\n+            continue\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        new_state_dict[key] = value\n+    new_state_dict[\"vision_tower.post_layernorm.weight\"] = torch.zeros((1152,))\n+    new_state_dict[\"vision_tower.post_layernorm.bias\"] = torch.zeros((1152,))\n+\n+    return new_state_dict\n+\n+\n+def convert_aria_llama_to_hf(text_model_id, vision_model_id, output_hub_path, old_state_dict_id):\n+    torch.set_default_dtype(torch.float16)\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        text_model_id,\n+        extra_special_tokens={\n+            \"image_token\": \"<|img|>\",\n+            \"pad_token\": \"<pad>\",\n+        },\n+    )\n+    tokenizer.add_tokens(AddedToken(\"<|img|>\", special=True, normalized=False), special_tokens=True)\n+    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n+    tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}{% elif message['content'] is iterable %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<fim_prefix><|img|><fim_suffix>{% endif %}{% endfor %}{% endif %}<|im_end|>\\n{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n+\n+    processor = AriaProcessor.from_pretrained(\n+        text_model_id,\n+        tokenizer=tokenizer,\n+    )\n+\n+    config = AutoConfig.from_pretrained(text_model_id)\n+    config.vision_config.hidden_size = 1152\n+    config.vision_config.attention_heads = 16\n+    config.pad_token_id = 2\n+    config.image_token_index = 9\n+    config.intermediate_size = config.moe_intermediate_size\n+    config.auto_map = {\n+        \"AutoConfig\": \"modeling_aria.AriaConfig\",\n+        \"AutoModelForCausalLM\": \"modeling_aria.AriaForConditionalGeneration\",\n+    }\n+\n+    with torch.device(\"meta\"):\n+        model = AriaForConditionalGeneration(config)\n+\n+    state_dict = load_original_state_dict(old_state_dict_id)\n+\n+    state_dict = convert_state_dict_to_hf(state_dict)\n+    model.load_state_dict(state_dict, strict=False, assign=True)\n+\n+    # print(\"Saving models\")\n+    # model.save_pretrained(\"local_aria\", safe_serialization=False)\n+    # processor.save_pretrained(\"local_aria\")\n+    print(\"Pushing to hub\")\n+    model.push_to_hub(output_hub_path, create_pr=True)\n+    processor.push_to_hub(output_hub_path, create_pr=True)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(\n+        epilog=EPILOG_TXT,\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+    parser.add_argument(\n+        \"--text_model_id\",\n+        default=\"rhymes-ai/Aria\",\n+        help=\"Hub location of the text model\",\n+    )\n+    parser.add_argument(\n+        \"--vision_model_id\",\n+        default=\"rhymes-ai/Aria\",\n+        help=\"Hub location of the vision model\",\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        default=\"rhymes-ai/Aria\",\n+        help=\"Location on the hub of the converted model\",\n+    )\n+    parser.add_argument(\n+        \"--old_state_dict_id\",\n+        default=\"rhymes-ai/Aria\",\n+        help=\"Location on the hub of the raw state dict of the original model. The filename needs to be `model_state_dict.bin`\",\n+    )\n+    args = parser.parse_args()\n+    convert_aria_llama_to_hf(args.text_model_id, args.vision_model_id, args.output_hub_path, args.old_state_dict_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "7b00665aa2859ddbe611b3d8ba2fa0bf14f01046",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "added",
            "additions": 504,
            "deletions": 0,
            "changes": 504,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,504 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/aria/modular_aria.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aria.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Iterable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, select_best_resolution\n+from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType\n+\n+\n+def make_batched_images(images) -> List[List[ImageInput]]:\n+    \"\"\"\n+    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n+\n+    Args:\n+        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n+            The input image.\n+\n+    Returns:\n+        list: A list of images.\n+    \"\"\"\n+    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n+        return [img for img_list in images for img in img_list]\n+\n+    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n+        return images\n+\n+    elif is_valid_image(images):\n+        return [images]\n+\n+    raise ValueError(f\"Could not make batched video from {images}\")\n+\n+\n+def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n+    \"\"\"\n+    Divides an image into patches of a specified size.\n+\n+    Args:\n+        image (`np.array`):\n+            The input image.\n+        patch_size (`int`):\n+            The size of each patch.\n+        input_data_format (`ChannelDimension` or `str`):\n+            The channel dimension format of the input image.\n+\n+    Returns:\n+        list: A list of np.array representing the patches.\n+    \"\"\"\n+    patches = []\n+    height, width = get_image_size(image, channel_dim=input_data_format)\n+    for i in range(0, height, patch_size):\n+        for j in range(0, width, patch_size):\n+            if input_data_format == ChannelDimension.LAST:\n+                patch = image[i : i + patch_size, j : j + patch_size]\n+            else:\n+                patch = image[:, i : i + patch_size, j : j + patch_size]\n+            patches.append(patch)\n+\n+    return patches\n+\n+\n+def _get_patch_output_size(image, target_resolution, input_data_format):\n+    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n+    target_height, target_width = target_resolution\n+\n+    scale_w = target_width / original_width\n+    scale_h = target_height / original_height\n+\n+    if scale_w < scale_h:\n+        new_width = target_width\n+        new_height = min(math.ceil(original_height * scale_w), target_height)\n+    else:\n+        new_height = target_height\n+        new_width = min(math.ceil(original_width * scale_h), target_width)\n+\n+    return new_height, new_width\n+\n+\n+class AriaImageProcessor(BaseImageProcessor):\n+    \"\"\"\n+    A vision processor for the Aria model that handles image preprocessing.\n+    Initialize the AriaImageProcessor.\n+\n+    Args:\n+        image_mean (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+            Mean values for normalization.\n+        image_std (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+            Standard deviation values for normalization.\n+        max_image_size (`int`, *optional*, defaults to 980):\n+            Maximum image size.\n+        min_image_size (`int`, *optional*, defaults to 336):\n+            Minimum image size.\n+        split_resolutions (`list`, *optional*, defaults to a list of optimal,resolutions as tuples):\n+            The optimal resolutions for splitting the image.\n+        split_image (`bool`, *optional*, defaults to `False`):\n+            Whether to split the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        resample (PILImageResampling, *optional*, defaults to `BICUBIC`):\n+            The resampling filter to use if resizing the image.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        image_mean: List[float] = None,\n+        image_std: List[float] = None,\n+        max_image_size: int = 980,\n+        min_image_size: int = 336,\n+        split_resolutions: Optional[List[Tuple[int, int]]] = None,\n+        split_image: Optional[bool] = False,\n+        do_convert_rgb: Optional[bool] = True,\n+        do_normalize: Optional[bool] = True,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if image_mean is None:\n+            image_mean = [0.5, 0.5, 0.5]\n+        if image_std is None:\n+            image_std = [0.5, 0.5, 0.5]\n+        self.max_image_size = max_image_size\n+        self.min_image_size = min_image_size\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.split_image = split_image\n+        if split_resolutions is None:\n+            split_resolutions = [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (2, 4), (2, 3), (2, 2), (2, 1), (3, 1), (3, 2), (4, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)]  # fmt: skip\n+            split_resolutions = [(el[0] * 490, el[1] * 490) for el in split_resolutions]\n+        self.split_resolutions = split_resolutions\n+        self.do_convert_rgb = do_convert_rgb\n+        self.do_normalize = do_normalize\n+        self.resample = resample\n+\n+    def preprocess(\n+        self,\n+        images: Union[ImageInput, List[ImageInput]],\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        max_image_size: Optional[int] = None,\n+        min_image_size: Optional[int] = None,\n+        split_image: Optional[bool] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        do_normalize: Optional[bool] = None,\n+        resample: PILImageResampling = None,\n+        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Process a list of images.\n+\n+        Args:\n+            images (ImageInput or list of ImageInput):\n+                The input image or a list of images.\n+            image_mean (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+                Mean values for normalization.\n+            image_std (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+                Standard deviation values for normalization.\n+            max_image_size (`int`, *optional*, defaults to `self.max_image_size` (980)):\n+                Maximum image size.\n+            min_image_size (`int`, *optional*, defaults to `self.min_image_size` (336)):\n+                Minimum image size.\n+            split_image (`bool`, *optional*, defaults to `self.split_image` (False)):\n+                Whether to split the image.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb` (True)):\n+                Whether to convert the image to RGB.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize` (True)):\n+                Whether to normalize the image.\n+            resample (PILImageResampling, *optional*, defaults to `self.resample` (BICUBIC)):\n+                The resampling filter to use if resizing the image.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to \"pt\"):\n+                The type of tensor to return.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`:\n+                        image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`:\n+                        image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`:\n+                        image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`:\n+                        image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            BatchFeature:\n+                A BatchFeature object containing:\n+                - 'pixel_values':\n+                    Tensor of processed image pixel values.\n+                - 'pixel_mask':\n+                    Boolean pixel mask. This mask is a 2D tensor of shape (max_image_size, max_image_size) where:\n+                    - True (1) values indicate pixels that belong to the original resized image.\n+                    - False (0) values indicate pixels that are part of the padding.\n+                  The mask helps distinguish between actual image content and padded areas in subsequent processing steps.\n+                - 'num_crops':\n+                    The maximum number of crops across all images.\n+        \"\"\"\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        max_image_size = max_image_size if max_image_size is not None else self.max_image_size\n+        min_image_size = min_image_size if min_image_size is not None else self.min_image_size\n+        split_image = split_image if split_image is not None else self.split_image\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        resample = resample if resample is not None else self.resample\n+\n+        if max_image_size not in [490, 980]:\n+            raise ValueError(\"max_image_size must be either 490 or 980\")\n+\n+        images = make_batched_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        pixel_values = []\n+        pixel_masks = []\n+        num_crops = None\n+\n+        for image in images:\n+            if split_image:\n+                crop_images = self.get_image_patches(\n+                    image,\n+                    self.split_resolutions,\n+                    max_image_size,\n+                    resample,\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+            else:\n+                crop_images = [image]\n+            if num_crops is None or len(crop_images) > num_crops:\n+                num_crops = len(crop_images)\n+\n+            for crop_image in crop_images:\n+                # At this point the scale is the rescaling factor that would bring the image to max_size in its larger dimension\n+                h, w = get_image_size(crop_image)\n+                scale = max_image_size / max(h, w)\n+                if w >= h:\n+                    new_size = (max(int(h * scale), min_image_size), max_image_size)  # h, w\n+                else:\n+                    new_size = (max_image_size, max(int(w * scale), min_image_size))  # h, w\n+\n+                crop_image_resized = resize(\n+                    crop_image,\n+                    new_size,\n+                    resample=resample,\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+\n+                padding_bottom, padding_right = max_image_size - new_size[0], max_image_size - new_size[1]\n+                crop_image_padded = pad(\n+                    crop_image_resized,\n+                    ((0, padding_bottom), (0, padding_right)),\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+\n+                # Create a pixel mask\n+                pixel_mask = np.zeros((max_image_size, max_image_size), dtype=bool)\n+                pixel_mask[: new_size[0], : new_size[1]] = 1\n+                pixel_masks.append(pixel_mask)\n+\n+                if do_normalize:\n+                    crop_image_padded = self.normalize(\n+                        crop_image_padded / 255.0,\n+                        self.image_mean,\n+                        self.image_std,\n+                        data_format=input_data_format,\n+                        input_data_format=input_data_format,\n+                    )\n+                    crop_image_padded = (\n+                        to_channel_dimension_format(crop_image_padded, data_format, input_data_format)\n+                        if data_format is not None\n+                        else crop_image_padded\n+                    )\n+\n+                pixel_values.append(crop_image_padded)\n+        return BatchFeature(\n+            data={\n+                \"pixel_values\": np.stack(pixel_values, axis=0),\n+                \"pixel_mask\": np.stack(pixel_masks, axis=0),\n+                \"num_crops\": num_crops,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def _resize_for_patching(\n+        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Resizes an image to a target resolution while maintaining aspect ratio.\n+\n+        Args:\n+            image (np.array):\n+                The input image.\n+            target_resolution (tuple):\n+                The target resolution (height, width) of the image.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            np.array: The resized and padded image.\n+        \"\"\"\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        # Resize the image\n+        resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n+\n+        return resized_image\n+\n+    def _pad_for_patching(\n+        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Pad an image to a target resolution while maintaining aspect ratio.\n+        \"\"\"\n+        target_height, target_width = target_resolution\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        paste_x = (target_width - new_width) // 2\n+        paste_y = (target_height - new_height) // 2\n+\n+        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+\n+        return padded_image\n+\n+    def pad(\n+        self,\n+        image: np.ndarray,\n+        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n+        mode: PaddingMode = PaddingMode.CONSTANT,\n+        constant_values: Union[float, Iterable[float]] = 0.0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n+        dimension of in the (`num_patches`) dimension. In the second case an iterable if tuples is expected\n+        as input.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+                Padding to apply to the edges of the height, width axes. Can be one of three formats:\n+                - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n+                - `((before, after),)` yields same before and after pad for height and width.\n+                - `(pad,)` or int is a shortcut for before = after = pad width for all axes.\n+            mode (`PaddingMode`):\n+                The padding mode to use. Can be one of:\n+                    - `\"constant\"`: pads with a constant value.\n+                    - `\"reflect\"`: pads with the reflection of the vector mirrored on the first and last values of the\n+                    vector along each axis.\n+                    - `\"replicate\"`: pads with the replication of the last value on the edge of the array along each axis.\n+                    - `\"symmetric\"`: pads with the reflection of the vector mirrored along the edge of the array.\n+            constant_values (`float` or `Iterable[float]`, *optional*):\n+                The value to use for the padding if `mode` is `\"constant\"`.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+\n+        \"\"\"\n+\n+        # call the general `pad` if padding on `height/width`, otherwise it's the `num_patched` dim\n+        if isinstance(padding, int) or len(padding) != 4:\n+            return pad(image, padding, mode, constant_values, data_format, input_data_format)\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        padding_mode_mapping = {\n+            PaddingMode.CONSTANT: \"constant\",\n+            PaddingMode.REFLECT: \"reflect\",\n+            PaddingMode.REPLICATE: \"edge\",\n+            PaddingMode.SYMMETRIC: \"symmetric\",\n+        }\n+        image = np.pad(image, padding, mode=padding_mode_mapping[mode], constant_values=constant_values)\n+        image = (\n+            to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image\n+        )\n+        return image\n+\n+    def get_image_patches(\n+        self,\n+        image: np.array,\n+        grid_pinpoints: List[Tuple[int, int]],\n+        patch_size: int,\n+        resample: PILImageResampling,\n+        data_format: ChannelDimension,\n+        input_data_format: ChannelDimension,\n+    ) -> List[np.array]:\n+        \"\"\"\n+        Process an image with variable resolutions by dividing it into patches.\n+\n+        Args:\n+            image (`np.array`):\n+                The input image to be processed.\n+            grid_pinpoints (List[Tuple[int, int]]):\n+                A list of possible resolutions as tuples.\n+            patch_size (`int`):\n+                Size of the patches to divide the image into.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            data_format (`ChannelDimension` or `str`):\n+                The channel dimension format for the output image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            `List[np.array]`: A list of NumPy arrays containing the processed image patches.\n+        \"\"\"\n+        if not isinstance(grid_pinpoints, list):\n+            raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")\n+\n+        possible_resolutions = grid_pinpoints\n+\n+        image_size = get_image_size(image, channel_dim=input_data_format)\n+        best_resolution = select_best_resolution(image_size, possible_resolutions)\n+        resized_image = self._resize_for_patching(\n+            image, best_resolution, resample=resample, input_data_format=input_data_format\n+        )\n+        padded_image = self._pad_for_patching(resized_image, best_resolution, input_data_format=input_data_format)\n+\n+        patches = divide_to_patches(padded_image, patch_size=patch_size, input_data_format=input_data_format)\n+\n+        # make sure that all patches are in the input data format\n+        patches = [\n+            to_channel_dimension_format(patch, channel_dim=data_format, input_channel_dim=input_data_format)\n+            for patch in patches\n+        ]\n+        return patches\n+\n+\n+__all__ = [\"AriaImageProcessor\"]"
        },
        {
            "sha": "1b4e4087b1a49d36718b1adc016f6f7d2b76831e",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "added",
            "additions": 1920,
            "deletions": 0,
            "changes": 1920,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,1920 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/aria/modular_aria.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aria.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.import_utils import is_torch_available\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_aria import AriaConfig, AriaTextConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+logger = logging.get_logger(__name__)\n+_CONFIG_FOR_DOC = \"AriaTextConfig\"\n+\n+\n+class AriaTextRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        AriaTextRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class AriaProjectorMLP(nn.Module):\n+    \"\"\"\n+    Feed-Forward Network module for the Aria Projector.\n+\n+    Args:\n+        in_features (`int`):\n+            Input embedding dimension.\n+        hidden_features (`int`):\n+            Hidden dimension of the feed-forward network.\n+        output_dim (`int`):\n+            Output dimension.\n+    \"\"\"\n+\n+    def __init__(self, in_features, hidden_features, output_dim):\n+        super().__init__()\n+        self.linear_in = nn.Linear(in_features, hidden_features, bias=False)\n+        self.linear_out = nn.Linear(hidden_features, output_dim, bias=False)\n+        self.act = ACT2FN[\"gelu_new\"]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.act(self.linear_in(hidden_states))\n+        hidden_states = self.linear_out(hidden_states)\n+        return hidden_states\n+\n+\n+class AriaCrossAttention(nn.Module):\n+    \"\"\"\n+    Aria Cross-Attention module.\n+\n+    Args:\n+        config (`AriaConfig`):\n+            The configuration to use.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaConfig, dropout_rate: float = 0):\n+        super().__init__()\n+        hidden_size = config.vision_config.hidden_size\n+        num_heads = config.vision_config.num_attention_heads\n+        self.num_heads = num_heads\n+        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+\n+        # Original code here: https://github.com/rhymes-ai/Aria/blob/719ff4e52b727443cba3793b0e27fe64e0244fe1/aria/model/projector.py#L48\n+        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n+        self.linear = nn.Linear(hidden_size, hidden_size)\n+        self.dropout = nn.Dropout(dropout_rate)\n+\n+        self.layer_norm = nn.LayerNorm(hidden_size)\n+        self.layer_norm_kv = nn.LayerNorm(hidden_size)\n+\n+    def forward(self, key_value_states, hidden_states, attn_mask=None):\n+        \"\"\"\n+        Forward pass of the AriaCrossAttention module.\n+\n+        Args:\n+            key_value_states (`torch.Tensor`):\n+                Input tensor for key and value.\n+            hidden_states (`torch.Tensor`):\n+                Input tensor for query.\n+            attn_mask (`torch.Tensor`, *optional*, defaults to None):\n+                Attention mask.\n+\n+        Returns:\n+            torch.Tensor:\n+                Output tensor after cross-attention.\n+        \"\"\"\n+        query = self.q_proj(self.layer_norm(hidden_states))\n+\n+        key_value_states = self.layer_norm_kv(key_value_states)\n+        key = self.k_proj(key_value_states)\n+        value = self.v_proj(key_value_states)\n+\n+        attn_output, _ = self.multihead_attn(query, key, value, attn_mask=attn_mask)\n+\n+        attn_output = self.dropout(self.linear(attn_output))\n+\n+        return attn_output\n+\n+\n+class AriaProjector(nn.Module):\n+    \"\"\"\n+    Aria Projector module.\n+\n+    This module projects vision features into the language model's embedding space, enabling interaction between vision and language components.\n+\n+    Args:\n+        config (`AriaConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: AriaConfig,\n+    ):\n+        super().__init__()\n+\n+        self.patch_to_query_dict = config.projector_patch_to_query_dict\n+        self.in_features = config.vision_config.hidden_size\n+        self.num_heads = config.vision_config.num_attention_heads\n+        self.kv_dim = config.vision_config.hidden_size\n+        self.hidden_features = config.text_config.hidden_size\n+        self.output_dim = config.text_config.hidden_size\n+\n+        self.query = nn.Parameter(torch.zeros(config.max_value_projector_patch_to_query_dict, self.in_features))\n+\n+        self.cross_attn = AriaCrossAttention(config)\n+\n+        self.layer_norm = nn.LayerNorm(self.in_features)\n+        self.feed_forward = AriaProjectorMLP(self.in_features, self.hidden_features, self.output_dim)\n+\n+    def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n+        \"\"\"\n+        Forward pass of the Projector module.\n+\n+        Args:\n+            key_value_states (`torch.Tensor`):\n+                Input tensor of shape (batch_size, num_patches, kv_dim).\n+            attn_mask (`torch.Tensor`, *optional*, default is None):\n+                Attention mask.\n+\n+        Returns:\n+            `torch.Tensor`: Output tensor of shape (batch_size, query_number, output_dim).\n+        \"\"\"\n+        batch_size, num_patches = key_value_states.shape[0], key_value_states.shape[1]\n+\n+        if num_patches not in self.patch_to_query_dict.keys():\n+            raise KeyError(\n+                f\"Number of patches {num_patches} not found in patch_to_query_dict amongst possible values {self.patch_to_query_dict.keys()}.\"\n+            )\n+        query_num = self.patch_to_query_dict[num_patches]\n+\n+        queries = self.query[:query_num].unsqueeze(0).repeat(batch_size, 1, 1)\n+\n+        if attn_mask is not None:\n+            attn_mask = attn_mask.repeat_interleave(self.num_heads, 0)\n+            attn_mask = attn_mask.unsqueeze(1).expand(-1, queries.size(1), -1)\n+\n+        attention_out = self.cross_attn(key_value_states, queries, attn_mask=attn_mask)\n+\n+        out = self.feed_forward(self.layer_norm(attention_out))\n+\n+        return out\n+\n+\n+class AriaSharedExpertsMLP(nn.Module):\n+    \"\"\"\n+    Shared Expert MLP for shared experts.\n+\n+    Unlike routed experts, shared experts process all tokens without routing.\n+    This class reconfigures the intermediate size in comparison to the LlamaMLP.\n+\n+    Args:\n+        config (`AriaTextConfig`): Configuration object for the Aria language model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size * config.moe_num_shared_experts\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n+    \"\"\"\n+    Compute the matrix multiplication (GEMM) for each expert sequentially. This approach is computationally inefficient, especially when dealing with a large number of experts.\n+\n+    Args:\n+        token_states (torch.Tensor): Input tensor of shape (num_tokens, in_features).\n+        expert_weights (torch.Tensor): Weight tensor of shape (num_experts, in_features, out_features).\n+        tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n+\n+    Returns:\n+        torch.Tensor: Output tensor of shape (num_tokens, out_features).\n+    \"\"\"\n+    num_tokens = token_states.shape[0]\n+    out_features = expert_weights.shape[-1]\n+    output = torch.zeros(num_tokens, out_features, dtype=token_states.dtype, device=token_states.device)\n+\n+    cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=0)\n+    # Insert zero at the begining for offset index's convenience\n+    zero_tensor = torch.zeros(1, dtype=torch.long, device=cumsum_num_tokens.device)\n+    cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n+\n+    for expert_num in range(expert_weights.shape[0]):\n+        start = cumsum_num_tokens[expert_num]\n+        end = cumsum_num_tokens[expert_num + 1]\n+        tokens = token_states[start:end]\n+\n+        out = torch.matmul(tokens, expert_weights[expert_num])\n+        output[start:end] = out\n+    return output\n+\n+\n+class AriaGroupedExpertsGemm(nn.Module):\n+    \"\"\"\n+    Grouped GEMM (General Matrix Multiplication) module for efficient expert computation.\n+    This module utilizes the grouped_gemm library (https://github.com/fanshiqing/grouped_gemm)\n+    for optimized performance. If the grouped_gemm library is not installed, it gracefully\n+    falls back to a sequential GEMM implementation, which may be slower but ensures\n+    functionality.\n+\n+    Args:\n+        in_features (`int`):\n+            Number of input features.\n+        out_features (`int`):\n+            Number of output features.\n+        groups (`int`):\n+            Number of expert groups.\n+    \"\"\"\n+\n+    def __init__(self, in_features, out_features, groups):\n+        super().__init__()\n+        self.in_features = in_features\n+        self.out_features = out_features\n+        self.groups = groups\n+        self.weight = nn.Parameter(torch.empty(groups, in_features, out_features))\n+\n+    def forward(self, input, tokens_per_expert):\n+        \"\"\"\n+        Perform grouped matrix multiplication.\n+\n+        Args:\n+            input (`torch.Tensor`):\n+                Input tensor of shape (num_tokens, in_features).\n+            tokens_per_expert (`torch.Tensor`):\n+                Number of tokens assigned to each expert.\n+\n+        Returns:\n+            torch.Tensor: Output tensor of shape (num_tokens, out_features).\n+        \"\"\"\n+        return sequential_experts_gemm(\n+            input,\n+            self.weight,\n+            tokens_per_expert.cpu(),\n+        )\n+\n+\n+class AriaGroupedExpertsMLP(nn.Module):\n+    \"\"\"\n+    Grouped MLP module for Mixture of Experts.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.fc1 = AriaGroupedExpertsGemm(config.hidden_size, config.intermediate_size * 2, config.moe_num_experts)\n+        self.fc2 = AriaGroupedExpertsGemm(config.intermediate_size, config.hidden_size, config.moe_num_experts)\n+\n+    def forward(self, permuted_tokens, tokens_per_expert):\n+        \"\"\"\n+        Forward pass of the Grouped MLP.\n+\n+        Args:\n+            permuted_tokens (torch.Tensor): Permuted input tokens.\n+            tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n+\n+        Returns:\n+            torch.Tensor: Output tensor after passing through the MLP.\n+        \"\"\"\n+        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n+        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n+        fc1_output = nn.functional.silu(projection) * gate\n+        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n+        return fc2_output\n+\n+\n+# Token permutation adapted from https://github.com/NVIDIA/Megatron-LM/blob/54f1f78529cbc2b9cddad313e7f9d96ac0420a27/megatron/core/transformer/moe/token_dispatcher.py#L291-L587\n+class AriaTextMoELayer(nn.Module):\n+    \"\"\"\n+    Aria Text Mixture of Experts (MoE) Layer.\n+\n+    This layer applies a gating mechanism to route input tokens to different experts.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the text component of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__()\n+\n+        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n+        self.experts = AriaGroupedExpertsMLP(config)\n+        self.shared_experts = AriaSharedExpertsMLP(config)\n+        self.config = config\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Forward pass of the MoE Layer.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input tensor of shape (batch_size, sequence_length, hidden_size).\n+\n+        Returns:\n+            torch.Tensor: Output tensor after passing through the MoE layer.\n+\n+        Process:\n+        1. Route tokens to experts using the router.\n+        2. Permute tokens based on routing decisions.\n+        3. Process tokens through experts.\n+        4. Unpermute and combine expert outputs.\n+        5. Add shared expert output to the final result.\n+        \"\"\"\n+        original_shape = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n+\n+        # Top K Routing\n+        logits = self.router(hidden_states)\n+        top_logits, top_indices = torch.topk(logits, k=self.config.moe_topk, dim=1)\n+        scores = nn.functional.softmax(top_logits, dim=-1)\n+\n+        original_dtype = top_indices.dtype\n+\n+        tokens_per_expert = torch.histc(\n+            top_indices.flatten().to(torch.float32),\n+            bins=self.config.moe_num_experts,\n+            min=0,\n+            max=self.config.moe_num_experts - 1,\n+        ).to(original_dtype)\n+        indices = top_indices\n+\n+        # Token permutation\n+        flatten_indices = indices.view(-1)\n+        sorted_indices = torch.argsort(flatten_indices)\n+        permuted_tokens = hidden_states.index_select(0, sorted_indices // self.config.moe_topk)\n+\n+        # Process through experts\n+        expert_output = self.experts(permuted_tokens, tokens_per_expert)\n+\n+        # Token unpermutation\n+        unpermuted_tokens = torch.zeros(\n+            (scores.shape[0] * self.config.moe_topk, expert_output.size(1)),\n+            dtype=expert_output.dtype,\n+            device=expert_output.device,\n+        )\n+        unpermuted_tokens.index_copy_(0, sorted_indices, expert_output)\n+        unpermuted_tokens = unpermuted_tokens.view(-1, self.config.moe_topk, expert_output.size(1))\n+\n+        output = (unpermuted_tokens * scores.unsqueeze(-1)).sum(dim=1).view(original_shape)\n+\n+        # Add shared expert output\n+        shared_expert_output = self.shared_experts(hidden_states.view(original_shape))\n+        return output + shared_expert_output\n+\n+\n+class AriaTextRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[AriaTextConfig] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`AriaTextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class AriaTextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: AriaTextConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n+        self.rotary_emb = AriaTextRotaryEmbedding(config=self.config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class AriaTextFlashAttention2(AriaTextAttention):\n+    \"\"\"\n+    AriaText flash attention module. This module inherits from `AriaTextAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (AriaTextRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class AriaTextSdpaAttention(AriaTextAttention):\n+    \"\"\"\n+    AriaText attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `AriaTextAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from AriaTextAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"AriaTextModel is using AriaTextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+ARIA_TEXT_ATTENTION_CLASSES = {\n+    \"eager\": AriaTextAttention,\n+    \"flash_attention_2\": AriaTextFlashAttention2,\n+    \"sdpa\": AriaTextSdpaAttention,\n+}\n+\n+\n+class AriaTextDecoderLayer(nn.Module):\n+    \"\"\"\n+    Aria Text Decoder Layer.\n+\n+    This class defines a single decoder layer in the language model, incorporating self-attention and Mixture of Experts (MoE) feed-forward network.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the text component of the model.\n+        layer_idx (`int`):\n+            Index of the layer.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = ARIA_TEXT_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.mlp = AriaTextMoELayer(config)\n+        self.input_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+class AriaTextPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n+    \"\"\"\n+\n+    config_class = AriaConfig\n+    base_model_prefix = \"model\"\n+    _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaGroupedExpertsGemm):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.Conv2d):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+ARIA_TEXT_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`AriaTextConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Aria Model outputting raw hidden-states without any specific head on top.\",\n+    ARIA_TEXT_START_DOCSTRING,\n+)\n+class AriaPreTrainedModel(PreTrainedModel):\n+    config_class = AriaTextConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"AriaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaProjector):\n+            nn.init.trunc_normal_(module.query, std=std)\n+\n+\n+ARIA_TEXT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare AriaText Model outputting raw hidden-states without any specific head on top.\",\n+    ARIA_TEXT_START_DOCSTRING,\n+)\n+class AriaTextModel(AriaTextPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`AriaTextDecoderLayer`]\n+\n+    Args:\n+        config: AriaTextConfig\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [AriaTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = AriaTextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n+    \"\"\"\n+    Aria model for causal language modeling tasks.\n+\n+    This class extends `LlamaForCausalLM` to incorporate the Mixture of Experts (MoE) approach,\n+    allowing for more efficient and scalable language modeling.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    config_class = AriaTextConfig\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__(config)\n+        self.model = AriaTextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AriaTextForCausalLM\n+\n+        >>> model = AriaTextForCausalLM.from_pretrained(\"meta-aria_text/AriaText-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-aria_text/AriaText-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@dataclass\n+class AriaCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Aria causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+ARIA_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor`, *optional*):\n+            Input token IDs.\n+        pixel_values (`torch.FloatTensor`, *optional*):\n+            Pixel values of the images.\n+        pixel_mask (`torch.LongTensor`, *optional*):\n+            Mask for the pixel values.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask.\n+        position_ids (`torch.LongTensor`, *optional*):\n+            Position IDs.\n+        past_key_values (`List[torch.FloatTensor]`, *optional*):\n+            Past key values for efficient processing.\n+        inputs_embeds (`torch.FloatTensor`, *optional*):\n+            Input embeddings.\n+        labels (`torch.LongTensor`, *optional*):\n+            Labels for computing the language modeling loss.\n+        use_cache (`bool`, *optional*):\n+            Whether to use the model's cache mechanism.\n+        output_attentions (`bool`, *optional*):\n+            Whether to output attention weights.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether to output hidden states.\n+        return_dict (`bool`, *optional*):\n+            Whether to return a `ModelOutput` object.\n+        num_logits_to_keep (`int`, *optional*, defaults to 0):\n+            Calculate logits for the last `num_logits_to_keep` tokens, or all `input_ids` if `0`.\n+        cache_position (`torch.LongTensor`, *optional*):\n+            Cache positions.\n+        **loss_kwargs:\n+            Additional keyword arguments for loss calculation.\n+\"\"\"\n+\n+ARIA_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config (`AriaConfig`):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Aria model for conditional generation tasks.\n+\n+    This model combines a vision tower, a multi-modal projector, and a language model\n+    to perform tasks that involve both image and text inputs.\"\"\",\n+    ARIA_START_DOCSTRING,\n+)\n+class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n+    config_class = AriaConfig\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+\n+    def __init__(self, config: AriaConfig):\n+        super().__init__(config)\n+\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+        self.multi_modal_projector = AriaProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n+        self.post_init()\n+\n+    def _create_patch_attention_mask(self, pixel_mask):\n+        if pixel_mask is None:\n+            return None\n+\n+        patches_subgrid = pixel_mask.unfold(\n+            dimension=1,\n+            size=self.vision_tower.config.patch_size,\n+            step=self.vision_tower.config.patch_size,\n+        )\n+        patches_subgrid = patches_subgrid.unfold(\n+            dimension=2,\n+            size=self.vision_tower.config.patch_size,\n+            step=self.vision_tower.config.patch_size,\n+        )\n+        return (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def tie_weights(self):\n+        return self.language_model.tie_weights()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: torch.FloatTensor = None,\n+        vision_feature_layer: int = -1,\n+    ):\n+        patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n+        image_outputs = self.vision_tower(\n+            pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True\n+        )\n+        image_attn_mask = None\n+        if patch_attention_mask is not None:\n+            flattened_mask = patch_attention_mask.flatten(1)\n+            image_attn_mask = torch.logical_not(flattened_mask)\n+\n+        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n+        return image_features\n+\n+    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **loss_kwargs,\n+    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`).\n+                Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n+                computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import requests\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from io import BytesIO\n+\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n+        >>> image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n+        >>> image2 = load_image(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n+        >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"Rhymes-AI/Aria\")\n+        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+\n+        >>> # Create inputs\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"What can we see in this image?\"},\n+        ...         ]\n+        ...     },\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"In which city is that bridge located?\"},\n+        ...         ]\n+        ...     }\n+        ... ]\n+\n+        >>> prompts = [processor.apply_chat_template([message], add_generation_prompt=True) for message in messages]\n+        >>> images = [[image1, image2], [image3]]\n+        >>> inputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(model.device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=256)\n+        >>> generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        >>> print(generated_texts[0])\n+        Assistant: There are buildings, trees, lights, and water visible in this image.\n+\n+        >>> print(generated_texts[1])\n+        Assistant: The bridge is in San Francisco.\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # 2. Merge text and images\n+        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                image_embeds = input_ids == self.config.image_token_index\n+                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                pixel_mask=pixel_mask,\n+                vision_feature_layer=self.config.vision_feature_layer,\n+            )\n+            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n+            n_image_features = n_images * n_features_per_image\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return AriaCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        pixel_mask=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_mask\"] = pixel_mask\n+\n+        return model_inputs\n+\n+\n+__all__ = [\n+    \"AriaForConditionalGeneration\",\n+    \"AriaPreTrainedModel\",\n+    \"AriaTextPreTrainedModel\",\n+    \"AriaTextModel\",\n+    \"AriaTextForCausalLM\",\n+]"
        },
        {
            "sha": "78c6e08bdfd0e5350d727664c897c1bcec5ecdfd",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "added",
            "additions": 1598,
            "deletions": 0,
            "changes": 1598,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,1598 @@\n+# coding=utf-8\n+# Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Dict, Iterable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...activations import ACT2FN\n+from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, select_best_resolution\n+from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.import_utils import is_torch_available\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer\n+from ..llama.configuration_llama import LlamaConfig\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaMLP,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+)\n+from ..llava.modeling_llava import LlavaCausalLMOutputWithPast\n+from ..llava_next.image_processing_llava_next import divide_to_patches, make_batched_images\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n+    \"\"\"\n+    Compute the matrix multiplication (GEMM) for each expert sequentially. This approach is computationally inefficient, especially when dealing with a large number of experts.\n+\n+    Args:\n+        token_states (torch.Tensor): Input tensor of shape (num_tokens, in_features).\n+        expert_weights (torch.Tensor): Weight tensor of shape (num_experts, in_features, out_features).\n+        tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n+\n+    Returns:\n+        torch.Tensor: Output tensor of shape (num_tokens, out_features).\n+    \"\"\"\n+    num_tokens = token_states.shape[0]\n+    out_features = expert_weights.shape[-1]\n+    output = torch.zeros(num_tokens, out_features, dtype=token_states.dtype, device=token_states.device)\n+\n+    cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=0)\n+    # Insert zero at the begining for offset index's convenience\n+    zero_tensor = torch.zeros(1, dtype=torch.long, device=cumsum_num_tokens.device)\n+    cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n+\n+    for expert_num in range(expert_weights.shape[0]):\n+        start = cumsum_num_tokens[expert_num]\n+        end = cumsum_num_tokens[expert_num + 1]\n+        tokens = token_states[start:end]\n+\n+        out = torch.matmul(tokens, expert_weights[expert_num])\n+        output[start:end] = out\n+    return output\n+\n+\n+class AriaTextConfig(LlamaConfig):\n+    r\"\"\"\n+    This class handles the configuration for the text component of the Aria model.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n+    [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n+    This class extends the LlamaConfig to include additional parameters specific to the Mixture of Experts (MoE) architecture.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`LlamaModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            The size of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n+            Llama 2 up to 4096, CodeLlama up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 2):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n+            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n+            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_heads\n+        moe_num_experts (`int`, *optional*, defaults to 8):\n+            The number of experts in the MoE layer.\n+        moe_topk (`int`, *optional*, defaults to 2):\n+            The number of top experts to route to for each token.\n+        moe_num_shared_experts (`int`, *optional*, defaults to 2):\n+            The number of shared experts.\n+    \"\"\"\n+\n+    model_type = \"aria_text\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        intermediate_size: int = 4096,\n+        moe_num_experts: int = 8,\n+        moe_topk: int = 2,\n+        moe_num_shared_experts: int = 2,\n+        pad_token_id=2,\n+        **super_kwargs,\n+    ):\n+        super().__init__(pad_token_id=pad_token_id, **super_kwargs)\n+        self.intermediate_size = intermediate_size\n+        self.moe_num_experts = moe_num_experts\n+        self.moe_topk = moe_topk\n+        self.moe_num_shared_experts = moe_num_shared_experts\n+\n+\n+class AriaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This class handles the configuration for both vision and text components of the Aria model,\n+    as well as additional parameters for image token handling and projector mapping.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n+    [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`AriaVisionConfig` or `dict`, *optional*):\n+            Configuration for the vision component.\n+        vision_feature_layer (`int`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature.\n+        text_config (`AriaTextConfig` or `dict`, *optional*):\n+            Configuration for the text component.\n+        projector_patch_to_query_dict (`dict`, *optional*):\n+            Mapping of patch sizes to query dimensions.\n+        image_token_index (`int`, *optional*, defaults to 9):\n+            Index used to represent image tokens.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated normal initializer for initializing all weight matrices.\n+\n+    Attributes:\n+        model_type (`str`):\n+            Type of the model, set to `\"aria\"`.\n+        image_token_index (`int`):\n+            Index used to represent image tokens.\n+        projector_patch_to_query_dict (`dict`):\n+            Mapping of patch sizes to query dimensions.\n+        vision_config (`AriaVisionConfig`):\n+            Configuration for the vision component.\n+        text_config (`AriaTextConfig`):\n+            Configuration for the text component.\n+    \"\"\"\n+\n+    model_type = \"aria\"\n+    sub_configs = {\"text_config\": AriaTextConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        vision_feature_layer: int = -1,\n+        text_config: AriaTextConfig = None,\n+        projector_patch_to_query_dict: Dict = None,\n+        image_token_index: int = 9,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        self.image_token_index = image_token_index\n+\n+        # Convert the keys and values of projector_patch_to_query_dict to integers\n+        # This ensures consistency even if they were provided as strings\n+        if projector_patch_to_query_dict is None:\n+            projector_patch_to_query_dict = {\n+                1225: 128,\n+                4900: 256,\n+            }\n+        self.projector_patch_to_query_dict = {int(k): int(v) for k, v in projector_patch_to_query_dict.items()}\n+        self.max_value_projector_patch_to_query_dict = max(self.projector_patch_to_query_dict.values())\n+        self.vision_feature_layer = vision_feature_layer\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = \"idefics3_vision\"\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"idefics3_vision\"]()\n+\n+        self.vision_config = vision_config\n+        self.initializer_range = initializer_range\n+\n+        if isinstance(text_config, dict) and \"model_type\" in text_config:\n+            text_config = AriaTextConfig(**text_config)\n+        elif text_config is None:\n+            text_config = AriaTextConfig()\n+\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+class AriaTextRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class AriaProjectorMLP(nn.Module):\n+    \"\"\"\n+    Feed-Forward Network module for the Aria Projector.\n+\n+    Args:\n+        in_features (`int`):\n+            Input embedding dimension.\n+        hidden_features (`int`):\n+            Hidden dimension of the feed-forward network.\n+        output_dim (`int`):\n+            Output dimension.\n+    \"\"\"\n+\n+    def __init__(self, in_features, hidden_features, output_dim):\n+        super().__init__()\n+        self.linear_in = nn.Linear(in_features, hidden_features, bias=False)\n+        self.linear_out = nn.Linear(hidden_features, output_dim, bias=False)\n+        self.act = ACT2FN[\"gelu_new\"]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.act(self.linear_in(hidden_states))\n+        hidden_states = self.linear_out(hidden_states)\n+        return hidden_states\n+\n+\n+class AriaCrossAttention(nn.Module):\n+    \"\"\"\n+    Aria Cross-Attention module.\n+\n+    Args:\n+        config (`AriaConfig`):\n+            The configuration to use.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaConfig, dropout_rate: float = 0):\n+        super().__init__()\n+        hidden_size = config.vision_config.hidden_size\n+        num_heads = config.vision_config.num_attention_heads\n+        self.num_heads = num_heads\n+        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n+\n+        # Original code here: https://github.com/rhymes-ai/Aria/blob/719ff4e52b727443cba3793b0e27fe64e0244fe1/aria/model/projector.py#L48\n+        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n+        self.linear = nn.Linear(hidden_size, hidden_size)\n+        self.dropout = nn.Dropout(dropout_rate)\n+\n+        self.layer_norm = nn.LayerNorm(hidden_size)\n+        self.layer_norm_kv = nn.LayerNorm(hidden_size)\n+\n+    def forward(self, key_value_states, hidden_states, attn_mask=None):\n+        \"\"\"\n+        Forward pass of the AriaCrossAttention module.\n+\n+        Args:\n+            key_value_states (`torch.Tensor`):\n+                Input tensor for key and value.\n+            hidden_states (`torch.Tensor`):\n+                Input tensor for query.\n+            attn_mask (`torch.Tensor`, *optional*, defaults to None):\n+                Attention mask.\n+\n+        Returns:\n+            torch.Tensor:\n+                Output tensor after cross-attention.\n+        \"\"\"\n+        query = self.q_proj(self.layer_norm(hidden_states))\n+\n+        key_value_states = self.layer_norm_kv(key_value_states)\n+        key = self.k_proj(key_value_states)\n+        value = self.v_proj(key_value_states)\n+\n+        attn_output, _ = self.multihead_attn(query, key, value, attn_mask=attn_mask)\n+\n+        attn_output = self.dropout(self.linear(attn_output))\n+\n+        return attn_output\n+\n+\n+class AriaProjector(nn.Module):\n+    \"\"\"\n+    Aria Projector module.\n+\n+    This module projects vision features into the language model's embedding space, enabling interaction between vision and language components.\n+\n+    Args:\n+        config (`AriaConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: AriaConfig,\n+    ):\n+        super().__init__()\n+\n+        self.patch_to_query_dict = config.projector_patch_to_query_dict\n+        self.in_features = config.vision_config.hidden_size\n+        self.num_heads = config.vision_config.num_attention_heads\n+        self.kv_dim = config.vision_config.hidden_size\n+        self.hidden_features = config.text_config.hidden_size\n+        self.output_dim = config.text_config.hidden_size\n+\n+        self.query = nn.Parameter(torch.zeros(config.max_value_projector_patch_to_query_dict, self.in_features))\n+\n+        self.cross_attn = AriaCrossAttention(config)\n+\n+        self.layer_norm = nn.LayerNorm(self.in_features)\n+        self.feed_forward = AriaProjectorMLP(self.in_features, self.hidden_features, self.output_dim)\n+\n+    def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n+        \"\"\"\n+        Forward pass of the Projector module.\n+\n+        Args:\n+            key_value_states (`torch.Tensor`):\n+                Input tensor of shape (batch_size, num_patches, kv_dim).\n+            attn_mask (`torch.Tensor`, *optional*, default is None):\n+                Attention mask.\n+\n+        Returns:\n+            `torch.Tensor`: Output tensor of shape (batch_size, query_number, output_dim).\n+        \"\"\"\n+        batch_size, num_patches = key_value_states.shape[0], key_value_states.shape[1]\n+\n+        if num_patches not in self.patch_to_query_dict.keys():\n+            raise KeyError(\n+                f\"Number of patches {num_patches} not found in patch_to_query_dict amongst possible values {self.patch_to_query_dict.keys()}.\"\n+            )\n+        query_num = self.patch_to_query_dict[num_patches]\n+\n+        queries = self.query[:query_num].unsqueeze(0).repeat(batch_size, 1, 1)\n+\n+        if attn_mask is not None:\n+            attn_mask = attn_mask.repeat_interleave(self.num_heads, 0)\n+            attn_mask = attn_mask.unsqueeze(1).expand(-1, queries.size(1), -1)\n+\n+        attention_out = self.cross_attn(key_value_states, queries, attn_mask=attn_mask)\n+\n+        out = self.feed_forward(self.layer_norm(attention_out))\n+\n+        return out\n+\n+\n+def _get_patch_output_size(image, target_resolution, input_data_format):\n+    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n+    target_height, target_width = target_resolution\n+\n+    scale_w = target_width / original_width\n+    scale_h = target_height / original_height\n+\n+    if scale_w < scale_h:\n+        new_width = target_width\n+        new_height = min(math.ceil(original_height * scale_w), target_height)\n+    else:\n+        new_height = target_height\n+        new_width = min(math.ceil(original_width * scale_h), target_width)\n+\n+    return new_height, new_width\n+\n+\n+class AriaImageProcessor(BaseImageProcessor):\n+    \"\"\"\n+    A vision processor for the Aria model that handles image preprocessing.\n+    Initialize the AriaImageProcessor.\n+\n+    Args:\n+        image_mean (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+            Mean values for normalization.\n+        image_std (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+            Standard deviation values for normalization.\n+        max_image_size (`int`, *optional*, defaults to 980):\n+            Maximum image size.\n+        min_image_size (`int`, *optional*, defaults to 336):\n+            Minimum image size.\n+        split_resolutions (`list`, *optional*, defaults to a list of optimal,resolutions as tuples):\n+            The optimal resolutions for splitting the image.\n+        split_image (`bool`, *optional*, defaults to `False`):\n+            Whether to split the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        resample (PILImageResampling, *optional*, defaults to `BICUBIC`):\n+            The resampling filter to use if resizing the image.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        image_mean: List[float] = None,\n+        image_std: List[float] = None,\n+        max_image_size: int = 980,\n+        min_image_size: int = 336,\n+        split_resolutions: Optional[List[Tuple[int, int]]] = None,\n+        split_image: Optional[bool] = False,\n+        do_convert_rgb: Optional[bool] = True,\n+        do_normalize: Optional[bool] = True,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if image_mean is None:\n+            image_mean = [0.5, 0.5, 0.5]\n+        if image_std is None:\n+            image_std = [0.5, 0.5, 0.5]\n+        self.max_image_size = max_image_size\n+        self.min_image_size = min_image_size\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.split_image = split_image\n+        if split_resolutions is None:\n+            split_resolutions = [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (2, 4), (2, 3), (2, 2), (2, 1), (3, 1), (3, 2), (4, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)]  # fmt: skip\n+            split_resolutions = [(el[0] * 490, el[1] * 490) for el in split_resolutions]\n+        self.split_resolutions = split_resolutions\n+        self.do_convert_rgb = do_convert_rgb\n+        self.do_normalize = do_normalize\n+        self.resample = resample\n+\n+    def preprocess(\n+        self,\n+        images: Union[ImageInput, List[ImageInput]],\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        max_image_size: Optional[int] = None,\n+        min_image_size: Optional[int] = None,\n+        split_image: Optional[bool] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        do_normalize: Optional[bool] = None,\n+        resample: PILImageResampling = None,\n+        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Process a list of images.\n+\n+        Args:\n+            images (ImageInput or list of ImageInput):\n+                The input image or a list of images.\n+            image_mean (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+                Mean values for normalization.\n+            image_std (`list`, *optional*, defaults to [0.5, 0.5, 0.5]):\n+                Standard deviation values for normalization.\n+            max_image_size (`int`, *optional*, defaults to `self.max_image_size` (980)):\n+                Maximum image size.\n+            min_image_size (`int`, *optional*, defaults to `self.min_image_size` (336)):\n+                Minimum image size.\n+            split_image (`bool`, *optional*, defaults to `self.split_image` (False)):\n+                Whether to split the image.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb` (True)):\n+                Whether to convert the image to RGB.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize` (True)):\n+                Whether to normalize the image.\n+            resample (PILImageResampling, *optional*, defaults to `self.resample` (BICUBIC)):\n+                The resampling filter to use if resizing the image.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to \"pt\"):\n+                The type of tensor to return.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`:\n+                        image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`:\n+                        image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`:\n+                        image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`:\n+                        image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            BatchFeature:\n+                A BatchFeature object containing:\n+                - 'pixel_values':\n+                    Tensor of processed image pixel values.\n+                - 'pixel_mask':\n+                    Boolean pixel mask. This mask is a 2D tensor of shape (max_image_size, max_image_size) where:\n+                    - True (1) values indicate pixels that belong to the original resized image.\n+                    - False (0) values indicate pixels that are part of the padding.\n+                  The mask helps distinguish between actual image content and padded areas in subsequent processing steps.\n+                - 'num_crops':\n+                    The maximum number of crops across all images.\n+        \"\"\"\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        max_image_size = max_image_size if max_image_size is not None else self.max_image_size\n+        min_image_size = min_image_size if min_image_size is not None else self.min_image_size\n+        split_image = split_image if split_image is not None else self.split_image\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        resample = resample if resample is not None else self.resample\n+\n+        if max_image_size not in [490, 980]:\n+            raise ValueError(\"max_image_size must be either 490 or 980\")\n+\n+        images = make_batched_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        pixel_values = []\n+        pixel_masks = []\n+        num_crops = None\n+\n+        for image in images:\n+            if split_image:\n+                crop_images = self.get_image_patches(\n+                    image,\n+                    self.split_resolutions,\n+                    max_image_size,\n+                    resample,\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+            else:\n+                crop_images = [image]\n+            if num_crops is None or len(crop_images) > num_crops:\n+                num_crops = len(crop_images)\n+\n+            for crop_image in crop_images:\n+                # At this point the scale is the rescaling factor that would bring the image to max_size in its larger dimension\n+                h, w = get_image_size(crop_image)\n+                scale = max_image_size / max(h, w)\n+                if w >= h:\n+                    new_size = (max(int(h * scale), min_image_size), max_image_size)  # h, w\n+                else:\n+                    new_size = (max_image_size, max(int(w * scale), min_image_size))  # h, w\n+\n+                crop_image_resized = resize(\n+                    crop_image,\n+                    new_size,\n+                    resample=resample,\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+\n+                padding_bottom, padding_right = max_image_size - new_size[0], max_image_size - new_size[1]\n+                crop_image_padded = pad(\n+                    crop_image_resized,\n+                    ((0, padding_bottom), (0, padding_right)),\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+\n+                # Create a pixel mask\n+                pixel_mask = np.zeros((max_image_size, max_image_size), dtype=bool)\n+                pixel_mask[: new_size[0], : new_size[1]] = 1\n+                pixel_masks.append(pixel_mask)\n+\n+                if do_normalize:\n+                    crop_image_padded = self.normalize(\n+                        crop_image_padded / 255.0,\n+                        self.image_mean,\n+                        self.image_std,\n+                        data_format=input_data_format,\n+                        input_data_format=input_data_format,\n+                    )\n+                    crop_image_padded = (\n+                        to_channel_dimension_format(crop_image_padded, data_format, input_data_format)\n+                        if data_format is not None\n+                        else crop_image_padded\n+                    )\n+\n+                pixel_values.append(crop_image_padded)\n+        return BatchFeature(\n+            data={\n+                \"pixel_values\": np.stack(pixel_values, axis=0),\n+                \"pixel_mask\": np.stack(pixel_masks, axis=0),\n+                \"num_crops\": num_crops,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def _resize_for_patching(\n+        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Resizes an image to a target resolution while maintaining aspect ratio.\n+\n+        Args:\n+            image (np.array):\n+                The input image.\n+            target_resolution (tuple):\n+                The target resolution (height, width) of the image.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            np.array: The resized and padded image.\n+        \"\"\"\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        # Resize the image\n+        resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n+\n+        return resized_image\n+\n+    def _pad_for_patching(\n+        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Pad an image to a target resolution while maintaining aspect ratio.\n+        \"\"\"\n+        target_height, target_width = target_resolution\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        paste_x = (target_width - new_width) // 2\n+        paste_y = (target_height - new_height) // 2\n+\n+        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+\n+        return padded_image\n+\n+    def pad(\n+        self,\n+        image: np.ndarray,\n+        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n+        mode: PaddingMode = PaddingMode.CONSTANT,\n+        constant_values: Union[float, Iterable[float]] = 0.0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n+        dimension of in the (`num_patches`) dimension. In the second case an iterable if tuples is expected\n+        as input.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+                Padding to apply to the edges of the height, width axes. Can be one of three formats:\n+                - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n+                - `((before, after),)` yields same before and after pad for height and width.\n+                - `(pad,)` or int is a shortcut for before = after = pad width for all axes.\n+            mode (`PaddingMode`):\n+                The padding mode to use. Can be one of:\n+                    - `\"constant\"`: pads with a constant value.\n+                    - `\"reflect\"`: pads with the reflection of the vector mirrored on the first and last values of the\n+                    vector along each axis.\n+                    - `\"replicate\"`: pads with the replication of the last value on the edge of the array along each axis.\n+                    - `\"symmetric\"`: pads with the reflection of the vector mirrored along the edge of the array.\n+            constant_values (`float` or `Iterable[float]`, *optional*):\n+                The value to use for the padding if `mode` is `\"constant\"`.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+\n+        \"\"\"\n+\n+        # call the general `pad` if padding on `height/width`, otherwise it's the `num_patched` dim\n+        if isinstance(padding, int) or len(padding) != 4:\n+            return pad(image, padding, mode, constant_values, data_format, input_data_format)\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        padding_mode_mapping = {\n+            PaddingMode.CONSTANT: \"constant\",\n+            PaddingMode.REFLECT: \"reflect\",\n+            PaddingMode.REPLICATE: \"edge\",\n+            PaddingMode.SYMMETRIC: \"symmetric\",\n+        }\n+        image = np.pad(image, padding, mode=padding_mode_mapping[mode], constant_values=constant_values)\n+        image = (\n+            to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image\n+        )\n+        return image\n+\n+    def get_image_patches(\n+        self,\n+        image: np.array,\n+        grid_pinpoints: List[Tuple[int, int]],\n+        patch_size: int,\n+        resample: PILImageResampling,\n+        data_format: ChannelDimension,\n+        input_data_format: ChannelDimension,\n+    ) -> List[np.array]:\n+        \"\"\"\n+        Process an image with variable resolutions by dividing it into patches.\n+\n+        Args:\n+            image (`np.array`):\n+                The input image to be processed.\n+            grid_pinpoints (List[Tuple[int, int]]):\n+                A list of possible resolutions as tuples.\n+            patch_size (`int`):\n+                Size of the patches to divide the image into.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            data_format (`ChannelDimension` or `str`):\n+                The channel dimension format for the output image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            `List[np.array]`: A list of NumPy arrays containing the processed image patches.\n+        \"\"\"\n+        if not isinstance(grid_pinpoints, list):\n+            raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")\n+\n+        possible_resolutions = grid_pinpoints\n+\n+        image_size = get_image_size(image, channel_dim=input_data_format)\n+        best_resolution = select_best_resolution(image_size, possible_resolutions)\n+        resized_image = self._resize_for_patching(\n+            image, best_resolution, resample=resample, input_data_format=input_data_format\n+        )\n+        padded_image = self._pad_for_patching(resized_image, best_resolution, input_data_format=input_data_format)\n+\n+        patches = divide_to_patches(padded_image, patch_size=patch_size, input_data_format=input_data_format)\n+\n+        # make sure that all patches are in the input data format\n+        patches = [\n+            to_channel_dimension_format(patch, channel_dim=data_format, input_channel_dim=input_data_format)\n+            for patch in patches\n+        ]\n+        return patches\n+\n+\n+class AriaProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"max_image_size\": 980,\n+            \"split_image\": False,\n+        },\n+        \"return_tensors\": TensorType.PYTORCH,\n+    }\n+\n+\n+class AriaProcessor(ProcessorMixin):\n+    \"\"\"\n+    AriaProcessor is a processor for the Aria model which wraps the Aria image preprocessor and the LLama slow tokenizer.\n+\n+    Args:\n+        image_processor (`AriaImageProcessor`, *optional*):\n+            The AriaImageProcessor to use for image preprocessing.\n+        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n+            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n+        size_conversion (`Dict`, *optional*):\n+            A dictionary indicating size conversions for images.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"size_conversion\"]\n+    image_processor_class = \"AriaImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer: Union[AutoTokenizer, str] = None,\n+        chat_template: Optional[str] = None,\n+        size_conversion: Optional[Dict[Union[float, int], int]] = None,\n+    ):\n+        if size_conversion is None:\n+            size_conversion = {490: 128, 980: 256}\n+        self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n+\n+        if tokenizer is not None and tokenizer.pad_token is None:\n+            tokenizer.pad_token = tokenizer.unk_token\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n+        images: Optional[ImageInput] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[AriaProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s).\n+\n+        Args:\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+            `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_mask** -- Pixel mask to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            AriaProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+        if images is not None:\n+            image_inputs = self.image_processor(\n+                images,\n+                **output_kwargs[\"images_kwargs\"],\n+            )\n+            # expand the image_token according to the num_crops and tokens per image\n+            tokens_per_image = self.size_conversion[image_inputs.pixel_values.shape[2]]\n+            prompt_strings = []\n+            num_crops = image_inputs.pop(\"num_crops\") * tokens_per_image\n+            for sample in text:\n+                sample = sample.replace(self.tokenizer.image_token, self.tokenizer.image_token * num_crops)\n+                prompt_strings.append(sample)\n+\n+        else:\n+            image_inputs = {}\n+            prompt_strings = text\n+\n+        text_inputs = self.tokenizer(\n+            prompt_strings,\n+            **output_kwargs[\"text_kwargs\"],\n+        )\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+class AriaSharedExpertsMLP(LlamaMLP):\n+    \"\"\"\n+    Shared Expert MLP for shared experts.\n+\n+    Unlike routed experts, shared experts process all tokens without routing.\n+    This class reconfigures the intermediate size in comparison to the LlamaMLP.\n+\n+    Args:\n+        config (`AriaTextConfig`): Configuration object for the Aria language model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__(self)\n+        self.intermediate_size = config.intermediate_size * config.moe_num_shared_experts\n+\n+\n+class AriaGroupedExpertsGemm(nn.Module):\n+    \"\"\"\n+    Grouped GEMM (General Matrix Multiplication) module for efficient expert computation.\n+    This module utilizes the grouped_gemm library (https://github.com/fanshiqing/grouped_gemm)\n+    for optimized performance. If the grouped_gemm library is not installed, it gracefully\n+    falls back to a sequential GEMM implementation, which may be slower but ensures\n+    functionality.\n+\n+    Args:\n+        in_features (`int`):\n+            Number of input features.\n+        out_features (`int`):\n+            Number of output features.\n+        groups (`int`):\n+            Number of expert groups.\n+    \"\"\"\n+\n+    def __init__(self, in_features, out_features, groups):\n+        super().__init__()\n+        self.in_features = in_features\n+        self.out_features = out_features\n+        self.groups = groups\n+        self.weight = nn.Parameter(torch.empty(groups, in_features, out_features))\n+\n+    def forward(self, input, tokens_per_expert):\n+        \"\"\"\n+        Perform grouped matrix multiplication.\n+\n+        Args:\n+            input (`torch.Tensor`):\n+                Input tensor of shape (num_tokens, in_features).\n+            tokens_per_expert (`torch.Tensor`):\n+                Number of tokens assigned to each expert.\n+\n+        Returns:\n+            torch.Tensor: Output tensor of shape (num_tokens, out_features).\n+        \"\"\"\n+        return sequential_experts_gemm(\n+            input,\n+            self.weight,\n+            tokens_per_expert.cpu(),\n+        )\n+\n+\n+class AriaGroupedExpertsMLP(nn.Module):\n+    \"\"\"\n+    Grouped MLP module for Mixture of Experts.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.fc1 = AriaGroupedExpertsGemm(config.hidden_size, config.intermediate_size * 2, config.moe_num_experts)\n+        self.fc2 = AriaGroupedExpertsGemm(config.intermediate_size, config.hidden_size, config.moe_num_experts)\n+\n+    def forward(self, permuted_tokens, tokens_per_expert):\n+        \"\"\"\n+        Forward pass of the Grouped MLP.\n+\n+        Args:\n+            permuted_tokens (torch.Tensor): Permuted input tokens.\n+            tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n+\n+        Returns:\n+            torch.Tensor: Output tensor after passing through the MLP.\n+        \"\"\"\n+        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n+        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n+        fc1_output = nn.functional.silu(projection) * gate\n+        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n+        return fc2_output\n+\n+\n+# Token permutation adapted from https://github.com/NVIDIA/Megatron-LM/blob/54f1f78529cbc2b9cddad313e7f9d96ac0420a27/megatron/core/transformer/moe/token_dispatcher.py#L291-L587\n+class AriaTextMoELayer(nn.Module):\n+    \"\"\"\n+    Aria Text Mixture of Experts (MoE) Layer.\n+\n+    This layer applies a gating mechanism to route input tokens to different experts.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the text component of the model.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__()\n+\n+        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n+        self.experts = AriaGroupedExpertsMLP(config)\n+        self.shared_experts = AriaSharedExpertsMLP(config)\n+        self.config = config\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Forward pass of the MoE Layer.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input tensor of shape (batch_size, sequence_length, hidden_size).\n+\n+        Returns:\n+            torch.Tensor: Output tensor after passing through the MoE layer.\n+\n+        Process:\n+        1. Route tokens to experts using the router.\n+        2. Permute tokens based on routing decisions.\n+        3. Process tokens through experts.\n+        4. Unpermute and combine expert outputs.\n+        5. Add shared expert output to the final result.\n+        \"\"\"\n+        original_shape = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n+\n+        # Top K Routing\n+        logits = self.router(hidden_states)\n+        top_logits, top_indices = torch.topk(logits, k=self.config.moe_topk, dim=1)\n+        scores = nn.functional.softmax(top_logits, dim=-1)\n+\n+        original_dtype = top_indices.dtype\n+\n+        tokens_per_expert = torch.histc(\n+            top_indices.flatten().to(torch.float32),\n+            bins=self.config.moe_num_experts,\n+            min=0,\n+            max=self.config.moe_num_experts - 1,\n+        ).to(original_dtype)\n+        indices = top_indices\n+\n+        # Token permutation\n+        flatten_indices = indices.view(-1)\n+        sorted_indices = torch.argsort(flatten_indices)\n+        permuted_tokens = hidden_states.index_select(0, sorted_indices // self.config.moe_topk)\n+\n+        # Process through experts\n+        expert_output = self.experts(permuted_tokens, tokens_per_expert)\n+\n+        # Token unpermutation\n+        unpermuted_tokens = torch.zeros(\n+            (scores.shape[0] * self.config.moe_topk, expert_output.size(1)),\n+            dtype=expert_output.dtype,\n+            device=expert_output.device,\n+        )\n+        unpermuted_tokens.index_copy_(0, sorted_indices, expert_output)\n+        unpermuted_tokens = unpermuted_tokens.view(-1, self.config.moe_topk, expert_output.size(1))\n+\n+        output = (unpermuted_tokens * scores.unsqueeze(-1)).sum(dim=1).view(original_shape)\n+\n+        # Add shared expert output\n+        shared_expert_output = self.shared_experts(hidden_states.view(original_shape))\n+        return output + shared_expert_output\n+\n+\n+class AriaTextDecoderLayer(LlamaDecoderLayer):\n+    \"\"\"\n+    Aria Text Decoder Layer.\n+\n+    This class defines a single decoder layer in the language model, incorporating self-attention and Mixture of Experts (MoE) feed-forward network.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the text component of the model.\n+        layer_idx (`int`):\n+            Index of the layer.\n+    \"\"\"\n+\n+    def __init__(self, config: AriaTextConfig, layer_idx: int):\n+        super().__init__(self)\n+        self.mlp = AriaTextMoELayer(config)\n+\n+\n+class AriaTextPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n+    \"\"\"\n+\n+    config_class = AriaConfig\n+    base_model_prefix = \"model\"\n+    _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaGroupedExpertsGemm):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.Conv2d):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+class AriaPreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, AriaProjector):\n+            nn.init.trunc_normal_(module.query, std=std)\n+\n+\n+class AriaTextModel(LlamaModel):\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [AriaTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+\n+class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):\n+    \"\"\"\n+    Aria model for causal language modeling tasks.\n+\n+    This class extends `LlamaForCausalLM` to incorporate the Mixture of Experts (MoE) approach,\n+    allowing for more efficient and scalable language modeling.\n+\n+    Args:\n+        config (`AriaTextConfig`):\n+            Configuration object for the model.\n+    \"\"\"\n+\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    config_class = AriaTextConfig\n+\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__(config)\n+        self.model = AriaTextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+class AriaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+ARIA_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor`, *optional*):\n+            Input token IDs.\n+        pixel_values (`torch.FloatTensor`, *optional*):\n+            Pixel values of the images.\n+        pixel_mask (`torch.LongTensor`, *optional*):\n+            Mask for the pixel values.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask.\n+        position_ids (`torch.LongTensor`, *optional*):\n+            Position IDs.\n+        past_key_values (`List[torch.FloatTensor]`, *optional*):\n+            Past key values for efficient processing.\n+        inputs_embeds (`torch.FloatTensor`, *optional*):\n+            Input embeddings.\n+        labels (`torch.LongTensor`, *optional*):\n+            Labels for computing the language modeling loss.\n+        use_cache (`bool`, *optional*):\n+            Whether to use the model's cache mechanism.\n+        output_attentions (`bool`, *optional*):\n+            Whether to output attention weights.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether to output hidden states.\n+        return_dict (`bool`, *optional*):\n+            Whether to return a `ModelOutput` object.\n+        num_logits_to_keep (`int`, *optional*, defaults to 0):\n+            Calculate logits for the last `num_logits_to_keep` tokens, or all `input_ids` if `0`.\n+        cache_position (`torch.LongTensor`, *optional*):\n+            Cache positions.\n+        **loss_kwargs:\n+            Additional keyword arguments for loss calculation.\n+\"\"\"\n+\n+ARIA_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config (`AriaConfig`):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Aria model for conditional generation tasks.\n+\n+    This model combines a vision tower, a multi-modal projector, and a language model\n+    to perform tasks that involve both image and text inputs.\"\"\",\n+    ARIA_START_DOCSTRING,\n+)\n+class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n+    config_class = AriaConfig\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+\n+    def __init__(self, config: AriaConfig):\n+        super().__init__(config)\n+\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+        self.multi_modal_projector = AriaProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n+        self.post_init()\n+\n+    def _create_patch_attention_mask(self, pixel_mask):\n+        if pixel_mask is None:\n+            return None\n+\n+        patches_subgrid = pixel_mask.unfold(\n+            dimension=1,\n+            size=self.vision_tower.config.patch_size,\n+            step=self.vision_tower.config.patch_size,\n+        )\n+        patches_subgrid = patches_subgrid.unfold(\n+            dimension=2,\n+            size=self.vision_tower.config.patch_size,\n+            step=self.vision_tower.config.patch_size,\n+        )\n+        return (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def tie_weights(self):\n+        return self.language_model.tie_weights()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: torch.FloatTensor = None,\n+        vision_feature_layer: int = -1,\n+    ):\n+        patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n+        image_outputs = self.vision_tower(\n+            pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True\n+        )\n+        image_attn_mask = None\n+        if patch_attention_mask is not None:\n+            flattened_mask = patch_attention_mask.flatten(1)\n+            image_attn_mask = torch.logical_not(flattened_mask)\n+\n+        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n+        return image_features\n+\n+    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **loss_kwargs,\n+    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`).\n+                Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n+                computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import requests\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from io import BytesIO\n+\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n+        >>> image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n+        >>> image2 = load_image(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n+        >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"Rhymes-AI/Aria\")\n+        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+\n+        >>> # Create inputs\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"What can we see in this image?\"},\n+        ...         ]\n+        ...     },\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\"},\n+        ...             {\"type\": \"text\", \"text\": \"In which city is that bridge located?\"},\n+        ...         ]\n+        ...     }\n+        ... ]\n+\n+        >>> prompts = [processor.apply_chat_template([message], add_generation_prompt=True) for message in messages]\n+        >>> images = [[image1, image2], [image3]]\n+        >>> inputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(model.device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=256)\n+        >>> generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        >>> print(generated_texts[0])\n+        Assistant: There are buildings, trees, lights, and water visible in this image.\n+\n+        >>> print(generated_texts[1])\n+        Assistant: The bridge is in San Francisco.\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # 2. Merge text and images\n+        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                image_embeds = input_ids == self.config.image_token_index\n+                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                pixel_mask=pixel_mask,\n+                vision_feature_layer=self.config.vision_feature_layer,\n+            )\n+            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n+            n_image_features = n_images * n_features_per_image\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return AriaCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        pixel_mask=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_mask\"] = pixel_mask\n+\n+        return model_inputs\n+\n+\n+__all__ = [\n+    \"AriaConfig\",\n+    \"AriaTextConfig\",\n+    \"AriaImageProcessor\",\n+    \"AriaProcessor\",\n+    \"AriaForConditionalGeneration\",\n+    \"AriaPreTrainedModel\",\n+    \"AriaTextPreTrainedModel\",\n+    \"AriaTextModel\",\n+    \"AriaTextForCausalLM\",\n+]"
        },
        {
            "sha": "2cfbd72a00206105202e7b867e23fcddbd43c751",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,164 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/aria/modular_aria.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aria.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Dict, List, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...utils import TensorType\n+from ..auto import AutoTokenizer\n+\n+\n+class AriaProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"max_image_size\": 980,\n+            \"split_image\": False,\n+        },\n+        \"return_tensors\": TensorType.PYTORCH,\n+    }\n+\n+\n+class AriaProcessor(ProcessorMixin):\n+    \"\"\"\n+    AriaProcessor is a processor for the Aria model which wraps the Aria image preprocessor and the LLama slow tokenizer.\n+\n+    Args:\n+        image_processor (`AriaImageProcessor`, *optional*):\n+            The AriaImageProcessor to use for image preprocessing.\n+        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n+            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n+        size_conversion (`Dict`, *optional*):\n+            A dictionary indicating size conversions for images.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"size_conversion\"]\n+    image_processor_class = \"AriaImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer: Union[AutoTokenizer, str] = None,\n+        chat_template: Optional[str] = None,\n+        size_conversion: Optional[Dict[Union[float, int], int]] = None,\n+    ):\n+        if size_conversion is None:\n+            size_conversion = {490: 128, 980: 256}\n+        self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n+\n+        if tokenizer is not None and tokenizer.pad_token is None:\n+            tokenizer.pad_token = tokenizer.unk_token\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n+        images: Optional[ImageInput] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[AriaProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s).\n+\n+        Args:\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+            `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_mask** -- Pixel mask to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            AriaProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+        if images is not None:\n+            image_inputs = self.image_processor(\n+                images,\n+                **output_kwargs[\"images_kwargs\"],\n+            )\n+            # expand the image_token according to the num_crops and tokens per image\n+            tokens_per_image = self.size_conversion[image_inputs.pixel_values.shape[2]]\n+            prompt_strings = []\n+            num_crops = image_inputs.pop(\"num_crops\") * tokens_per_image\n+            for sample in text:\n+                sample = sample.replace(self.tokenizer.image_token, self.tokenizer.image_token * num_crops)\n+                prompt_strings.append(sample)\n+\n+        else:\n+            image_inputs = {}\n+            prompt_strings = text\n+\n+        text_inputs = self.tokenizer(\n+            prompt_strings,\n+            **output_kwargs[\"text_kwargs\"],\n+        )\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"AriaProcessor\"]"
        },
        {
            "sha": "cc3a7d5baaeb49b58b1be8f7027b37a27e40184a",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -35,6 +35,8 @@\n         (\"albert\", \"AlbertConfig\"),\n         (\"align\", \"AlignConfig\"),\n         (\"altclip\", \"AltCLIPConfig\"),\n+        (\"aria\", \"AriaConfig\"),\n+        (\"aria_text\", \"AriaTextConfig\"),\n         (\"audio-spectrogram-transformer\", \"ASTConfig\"),\n         (\"autoformer\", \"AutoformerConfig\"),\n         (\"bark\", \"BarkConfig\"),\n@@ -135,6 +137,7 @@\n         (\"idefics\", \"IdeficsConfig\"),\n         (\"idefics2\", \"Idefics2Config\"),\n         (\"idefics3\", \"Idefics3Config\"),\n+        (\"idefics3_vision\", \"Idefics3VisionConfig\"),\n         (\"ijepa\", \"IJepaConfig\"),\n         (\"imagegpt\", \"ImageGPTConfig\"),\n         (\"informer\", \"InformerConfig\"),\n@@ -327,6 +330,8 @@\n         (\"albert\", \"ALBERT\"),\n         (\"align\", \"ALIGN\"),\n         (\"altclip\", \"AltCLIP\"),\n+        (\"aria\", \"Aria\"),\n+        (\"aria_text\", \"AriaText\"),\n         (\"audio-spectrogram-transformer\", \"Audio Spectrogram Transformer\"),\n         (\"autoformer\", \"Autoformer\"),\n         (\"bark\", \"Bark\"),\n@@ -441,6 +446,7 @@\n         (\"idefics\", \"IDEFICS\"),\n         (\"idefics2\", \"Idefics2\"),\n         (\"idefics3\", \"Idefics3\"),\n+        (\"idefics3_vision\", \"Idefics3VisionTransformer\"),\n         (\"ijepa\", \"I-JEPA\"),\n         (\"imagegpt\", \"ImageGPT\"),\n         (\"informer\", \"Informer\"),\n@@ -687,6 +693,8 @@\n         (\"clip_vision_model\", \"clip\"),\n         (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n         (\"clip_text_model\", \"clip\"),\n+        (\"aria_text\", \"aria\"),\n+        (\"idefics3_vision\", \"idefics3\"),\n         (\"siglip_vision_model\", \"siglip\"),\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),"
        },
        {
            "sha": "a699314f8589281e59dd1469288b96869018d81d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -54,6 +54,7 @@\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n             (\"align\", (\"EfficientNetImageProcessor\",)),\n+            (\"aria\", (\"AriaImageProcessor\")),\n             (\"beit\", (\"BeitImageProcessor\",)),\n             (\"bit\", (\"BitImageProcessor\",)),\n             (\"blip\", (\"BlipImageProcessor\",)),"
        },
        {
            "sha": "e8e4814e6a0f6a25829bb3153d7348c40e78fb28",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -35,6 +35,8 @@\n         (\"albert\", \"AlbertModel\"),\n         (\"align\", \"AlignModel\"),\n         (\"altclip\", \"AltCLIPModel\"),\n+        (\"aria\", \"AriaForConditionalGeneration\"),\n+        (\"aria_text\", \"AriaTextModel\"),\n         (\"audio-spectrogram-transformer\", \"ASTModel\"),\n         (\"autoformer\", \"AutoformerModel\"),\n         (\"bark\", \"BarkModel\"),\n@@ -132,6 +134,7 @@\n         (\"idefics\", \"IdeficsModel\"),\n         (\"idefics2\", \"Idefics2Model\"),\n         (\"idefics3\", \"Idefics3Model\"),\n+        (\"idefics3_vision\", \"Idefics3VisionTransformer\"),\n         (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"informer\", \"InformerModel\"),\n@@ -464,6 +467,7 @@\n MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Causal LM mapping\n+        (\"aria_text\", \"AriaTextForCausalLM\"),\n         (\"bart\", \"BartForCausalLM\"),\n         (\"bert\", \"BertLMHeadModel\"),\n         (\"bert-generation\", \"BertGenerationDecoder\"),\n@@ -768,6 +772,7 @@\n \n MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES = OrderedDict(\n     [\n+        (\"aria\", \"AriaForConditionalGeneration\"),\n         (\"blip\", \"BlipForConditionalGeneration\"),\n         (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n         (\"chameleon\", \"ChameleonForConditionalGeneration\"),"
        },
        {
            "sha": "3e475b1be211fa83a1001128d0dac2b0bfd343c5",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -47,6 +47,7 @@\n     [\n         (\"align\", \"AlignProcessor\"),\n         (\"altclip\", \"AltCLIPProcessor\"),\n+        (\"aria\", \"AriaProcessor\"),\n         (\"bark\", \"BarkProcessor\"),\n         (\"blip\", \"BlipProcessor\"),\n         (\"blip-2\", \"Blip2Processor\"),"
        },
        {
            "sha": "3cc181ac87adc4f2cbcf31ef965e4604b8b545c5",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -68,6 +68,7 @@\n                 ),\n             ),\n             (\"align\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"aria\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"bark\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"bart\", (\"BartTokenizer\", \"BartTokenizerFast\")),\n             ("
        },
        {
            "sha": "cec07ca6f5e2d3d6563558567021be045c7b9440",
            "filename": "src/transformers/models/idefics3/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -16,7 +16,7 @@\n from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n \n \n-_import_structure = {\"configuration_idefics3\": [\"Idefics3Config\"]}\n+_import_structure = {\"configuration_idefics3\": [\"Idefics3Config\", \"Idefics3VisionConfig\"]}\n \n \n try:\n@@ -38,11 +38,12 @@\n         \"Idefics3ForConditionalGeneration\",\n         \"Idefics3PreTrainedModel\",\n         \"Idefics3Model\",\n+        \"Idefics3VisionTransformer\",\n     ]\n     _import_structure[\"processing_idefics3\"] = [\"Idefics3Processor\"]\n \n if TYPE_CHECKING:\n-    from .configuration_idefics3 import Idefics3Config\n+    from .configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n \n     try:\n         if not is_vision_available():\n@@ -62,6 +63,7 @@\n             Idefics3ForConditionalGeneration,\n             Idefics3Model,\n             Idefics3PreTrainedModel,\n+            Idefics3VisionTransformer,\n         )\n         from .processing_idefics3 import Idefics3Processor\n "
        },
        {
            "sha": "8747939a62ce2a300400448d3ed90df95da7b6f4",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -685,6 +685,41 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class AriaForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class AriaPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class AriaTextForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class AriaTextModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class AriaTextPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class ASTForAudioClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -4978,6 +5013,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Idefics3VisionConfig(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Idefics3VisionTransformer(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class IJepaForImageClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "3ebda4404aae9c49586476a0a99198c9087078a7",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -23,6 +23,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class AriaImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class BeitFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "76ab793e3a36c0c35865d5f567116b9bd14988e9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -1727,6 +1727,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             num_hidden_layers = text_config.num_hidden_layers\n \n             inputs_embeds = model.get_input_embeddings()(input_ids)\n+            max_cache_len += inputs_embeds.shape[1]\n             outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n \n             # we should get `max_length` in shape, not `max_length - embeds_length`"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/aria/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2F__init__.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e"
        },
        {
            "sha": "8a0f84d34eefed157bd502e81d9a5bce42cbc9ea",
            "filename": "tests/models/aria/test_image_processing_aria.py",
            "status": "added",
            "additions": 268,
            "deletions": 0,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,268 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import PILImageResampling\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AriaImageProcessor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class AriaImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        num_images=1,\n+        min_resolution=30,\n+        max_resolution=40,\n+        size=None,\n+        max_image_size=980,\n+        min_image_size=336,\n+        split_resolutions=None,\n+        split_image=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_convert_rgb=True,\n+        resample=PILImageResampling.BICUBIC,\n+    ):\n+        super().__init__()\n+        self.size = size if size is not None else {\"longest_edge\": max_resolution}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.num_images = num_images\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.resample = resample\n+        self.max_image_size = max_image_size\n+        self.min_image_size = min_image_size\n+        self.split_resolutions = split_resolutions if split_resolutions is not None else [[980, 980]]\n+        self.split_image = split_image\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"max_image_size\": self.max_image_size,\n+            \"min_image_size\": self.min_image_size,\n+            \"split_resolutions\": self.split_resolutions,\n+            \"split_image\": self.split_image,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"do_normalize\": self.do_normalize,\n+            \"resample\": self.resample,\n+        }\n+\n+    def get_expected_values(self, image_inputs, batched=False):\n+        \"\"\"\n+        This function computes the expected height and width when providing images to AriaImageProcessor,\n+        assuming do_resize is set to True. The expected size in that case the max image size.\n+        \"\"\"\n+        return self.max_image_size, self.max_image_size\n+\n+    def expected_output_image_shape(self, images):\n+        height, width = self.get_expected_values(images, batched=True)\n+        return self.num_channels, height, width\n+\n+    def prepare_image_inputs(\n+        self,\n+        batch_size=None,\n+        min_resolution=None,\n+        max_resolution=None,\n+        num_channels=None,\n+        num_images=None,\n+        size_divisor=None,\n+        equal_resolution=False,\n+        numpify=False,\n+        torchify=False,\n+    ):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+\n+        One can specify whether the images are of the same resolution or not.\n+        \"\"\"\n+        assert not (numpify and torchify), \"You cannot specify both numpy and PyTorch tensors at the same time\"\n+\n+        batch_size = batch_size if batch_size is not None else self.batch_size\n+        min_resolution = min_resolution if min_resolution is not None else self.min_resolution\n+        max_resolution = max_resolution if max_resolution is not None else self.max_resolution\n+        num_channels = num_channels if num_channels is not None else self.num_channels\n+        num_images = num_images if num_images is not None else self.num_images\n+\n+        images_list = []\n+        for i in range(batch_size):\n+            images = []\n+            for j in range(num_images):\n+                if equal_resolution:\n+                    width = height = max_resolution\n+                else:\n+                    # To avoid getting image width/height 0\n+                    if size_divisor is not None:\n+                        # If `size_divisor` is defined, the image needs to have width/size >= `size_divisor`\n+                        min_resolution = max(size_divisor, min_resolution)\n+                    width, height = np.random.choice(np.arange(min_resolution, max_resolution), 2)\n+                images.append(np.random.randint(255, size=(num_channels, width, height), dtype=np.uint8))\n+            images_list.append(images)\n+\n+        if not numpify and not torchify:\n+            # PIL expects the channel dimension as last dimension\n+            images_list = [[Image.fromarray(np.moveaxis(image, 0, -1)) for image in images] for images in images_list]\n+\n+        if torchify:\n+            images_list = [[torch.from_numpy(image) for image in images] for images in images_list]\n+\n+        if numpify:\n+            # Numpy images are typically in channels last format\n+            images_list = [[image.transpose(1, 2, 0) for image in images] for images in images_list]\n+\n+        return images_list\n+\n+\n+@require_torch\n+@require_vision\n+class AriaImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = AriaImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = AriaImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(image_processing, \"max_image_size\"))\n+        self.assertTrue(hasattr(image_processing, \"min_image_size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"split_image\"))\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for sample_images in image_inputs:\n+                for image in sample_images:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_numpy_4_channels(self):\n+        # Aria always processes images as RGB, so it always returns images with 3 channels\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processor_dict = self.image_processor_dict\n+            image_processing = self.image_processing_class(**image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+            for sample_images in image_inputs:\n+                for image in sample_images:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for images in image_inputs:\n+                for image in images:\n+                    self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for images in image_inputs:\n+                for image in images:\n+                    self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )"
        },
        {
            "sha": "d3458530ac349ef02ec37fa92f8f8cef8db22771",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "added",
            "additions": 669,
            "deletions": 0,
            "changes": 669,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,669 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Aria model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    AriaConfig,\n+    AriaForConditionalGeneration,\n+    AriaTextConfig,\n+    AutoProcessor,\n+    AutoTokenizer,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.models.idefics3 import Idefics3VisionConfig\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_gpu,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+else:\n+    is_torch_greater_or_equal_than_2_0 = False\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class AriaVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        image_token_index=9,\n+        projector_hidden_act=\"gelu\",\n+        seq_length=7,\n+        vision_feature_select_strategy=\"default\",\n+        vision_feature_layer=-1,\n+        text_config=AriaTextConfig(\n+            seq_length=7,\n+            is_training=True,\n+            use_input_mask=True,\n+            use_token_type_ids=False,\n+            use_labels=True,\n+            hidden_act=\"gelu\",\n+            hidden_dropout_prob=0.1,\n+            attention_probs_dropout_prob=0.1,\n+            type_vocab_size=16,\n+            type_sequence_label_size=2,\n+            initializer_range=0.02,\n+            num_labels=3,\n+            num_choices=4,\n+            pad_token_id=1,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            max_position_embeddings=60,\n+            model_type=\"aria_moe_lm\",\n+            moe_intermediate_size=4,\n+            moe_num_experts=4,\n+            moe_topk=2,\n+            num_attention_heads=20,\n+            num_experts_per_tok=3,\n+            num_hidden_layers=2,\n+            num_key_value_heads=20,\n+            rope_theta=5000000,\n+            vocab_size=99,\n+            eos_token_id=2,\n+            head_dim=2,\n+        ),\n+        is_training=True,\n+        vision_config=Idefics3VisionConfig(\n+            image_size=358,\n+            patch_size=10,\n+            num_channels=3,\n+            is_training=True,\n+            hidden_size=32,\n+            projection_dim=20,\n+            num_hidden_layers=2,\n+            num_attention_heads=16,\n+            intermediate_size=10,\n+            dropout=0.1,\n+            attention_dropout=0.1,\n+            initializer_range=0.02,\n+        ),\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.image_token_index = image_token_index\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.pad_token_id = text_config.pad_token_id\n+        self.eos_token_id = text_config.eos_token_id\n+        self.num_hidden_layers = text_config.num_hidden_layers\n+        self.vocab_size = text_config.vocab_size\n+        self.hidden_size = text_config.hidden_size\n+        self.num_attention_heads = text_config.num_attention_heads\n+        self.is_training = is_training\n+\n+        self.batch_size = 10\n+        self.num_channels = 3\n+        self.image_size = 358\n+        self.num_image_tokens = 128\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+    def get_config(self):\n+        return AriaConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            ignore_index=self.ignore_index,\n+            image_token_index=self.image_token_index,\n+            projector_hidden_act=self.projector_hidden_act,\n+            vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            vision_feature_layer=self.vision_feature_layer,\n+            eos_token_id=self.eos_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.vision_config.num_channels,\n+                self.vision_config.image_size,\n+                self.vision_config.image_size,\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_aria_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n+        model = AriaForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                pixel_values=pixel_values.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `AriaForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (AriaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (AriaForConditionalGeneration,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = AriaVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=AriaConfig, has_text_modality=False)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"\")\n+    def test_new_cache_format_0(self):\n+        pass\n+\n+    @unittest.skip(reason=\"\")\n+    def test_new_cache_format_1(self):\n+        pass\n+\n+    @unittest.skip(reason=\"\")\n+    def test_new_cache_format_2(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Unstable test\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Unstable test\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Unsupported\")\n+    def test_generate_from_inputs_embeds_0_greedy(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Unsupported\")\n+    def test_generate_from_inputs_embeds_1_beam_search(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Unsupported\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+\n+@require_torch\n+class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n+\n+        prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n+        image_file = \"https://aria-vl.github.io/static/images/view.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n+        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+        EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_llama_single(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model_id = \"rhymes-ai/Aria\"\n+\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? ASSISTANT:\"\n+        image_file = \"https://aria-vl.github.io/static/images/view.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n+\n+        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n+        EXPECTED_DECODED_TEXT = \"USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Finally, be respectful of the environment and other visitors, and follow any posted rules or guidelines for the area.\"  # fmt: skip\n+\n+        self.assertEqual(\n+            processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_llama_batched(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model_id = \"rhymes-ai/Aria\"\n+\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        prompts = [\n+            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT:\",\n+            \"USER: <image>\\nWhat is this? ASSISTANT:\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, you', 'USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat is located on']  # fmt: skip\n+\n+        self.assertEqual(\n+            processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_batch(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n+        # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n+        prompts = [\n+            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n+            \"USER: <image>\\nWhat is this?\\nASSISTANT:\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring.',\n+            'USER:  \\nWhat is this?\\nASSISTANT: Cats'\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_llama_batched_regression(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model_id = \"rhymes-ai/Aria\"\n+\n+        # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True, attn_implementation=\"eager\")\n+        processor = AutoProcessor.from_pretrained(model_id, pad_token=\"<pad>\")\n+\n+        prompts = [\n+            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n+            \"USER: <image>\\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER: <image>\\nAnd this?\\nASSISTANT:\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://aria-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=[image1, image2, image1], text=prompts, return_tensors=\"pt\", padding=True)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, which appears to be a dock or pier extending over a body of water', 'USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat sleeping on a bed.']  # fmt: skip\n+\n+        self.assertEqual(\n+            processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_torch\n+    @require_vision\n+    def test_batched_generation(self):\n+        model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n+\n+        processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n+\n+        prompt1 = \"<image>\\n<image>\\nUSER: What's the the difference of two images?\\nASSISTANT:\"\n+        prompt2 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n+        prompt3 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n+        url1 = \"https://images.unsplash.com/photo-1552053831-71594a27632d?q=80&w=3062&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n+        url2 = \"https://images.unsplash.com/photo-1617258683320-61900b281ced?q=80&w=3087&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n+        image1 = Image.open(requests.get(url1, stream=True).raw)\n+        image2 = Image.open(requests.get(url2, stream=True).raw)\n+\n+        inputs = processor(\n+            images=[image1, image2, image1, image2],\n+            text=[prompt1, prompt2, prompt3],\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n+\n+        model = model.eval()\n+\n+        EXPECTED_OUTPUT = [\n+            \"\\n \\nUSER: What's the the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n+            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n+            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n+        ]\n+\n+        generate_ids = model.generate(**inputs, max_new_tokens=20)\n+        outputs = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n+        self.assertEqual(outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_aria_index_error_bug(self):\n+        # This is a reproducer of https://github.com/huggingface/transformers/pull/28032 and makes sure it does not happen anymore\n+        # Please refer to that PR, or specifically https://github.com/huggingface/transformers/pull/28032#issuecomment-1860650043 for\n+        # more details\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # Simulate a super long prompt\n+        user_prompt = \"Describe the image:?\\n\" * 200\n+        prompt = f\"USER: <image>\\n{user_prompt}ASSISTANT:\"\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n+\n+        # Make sure that `generate` works\n+        _ = model.generate(**inputs, max_new_tokens=20)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_aria_merge_inputs_error_bug(self):\n+        # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+\n+        # Simulate some user inputs\n+        pixel_values = torch.randn(\n+            (1, 3, 336, 336),\n+            dtype=torch.float,\n+            device=torch_device,\n+        )\n+        input_ids = torch.tensor(\n+            [\n+                [32001, 32001, 1, 15043, 7084, 32000, 29871, 13, 7900],\n+            ],\n+            dtype=torch.long,\n+            device=torch_device,\n+        )\n+        attention_mask = torch.tensor(\n+            [[0, 0, 1, 1, 1, 1, 1, 1, 1]],\n+            dtype=torch.long,\n+            device=torch_device,\n+        )\n+\n+        # Make sure that the loss is properly computed\n+        loss = model(\n+            pixel_values=pixel_values,\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            labels=input_ids,\n+        ).loss\n+        loss.backward()\n+\n+    def test_tokenizer_integration(self):\n+        model_id = \"rhymes-ai/Aria\"\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\n+            model_id, bos_token=\"<|startoftext|>\", eos_token=\"<|endoftext|>\", use_fast=False\n+        )\n+        slow_tokenizer.add_tokens(\"<image>\", True)\n+\n+        fast_tokenizer = AutoTokenizer.from_pretrained(\n+            model_id,\n+            bos_token=\"<|startoftext|>\",\n+            eos_token=\"<|endoftext|>\",\n+            from_slow=True,\n+            legacy=False,\n+        )\n+        fast_tokenizer.add_tokens(\"<image>\", True)\n+\n+        prompt = \"<|startoftext|><|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\"\n+        EXPECTED_OUTPUT = ['<|startoftext|>', '<', '|', 'im', '_', 'start', '|', '>', 'system', '\\n', 'Answer', '‚ñÅthe', '‚ñÅquestions', '.<', '|', 'im', '_', 'end', '|', '><', '|', 'im', '_', 'start', '|', '>', 'user', '\\n', '<image>', '\\n', 'What', '‚ñÅis', '‚ñÅshown', '‚ñÅin', '‚ñÅthis', '‚ñÅimage', '?', '<', '|', 'im', '_', 'end', '|', '>']  # fmt: skip\n+        self.assertEqual(slow_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n+        self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_generation_no_images(self):\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # Prepare inputs with no images\n+        inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n+\n+        # Make sure that `generate` works\n+        _ = model.generate(**inputs, max_new_tokens=20)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_generation_siglip_backbone(self):\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"float16\", device_map=torch_device)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # check processing with expansion of inputs (w/o expansion should work with any backbone)\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = processor(\n+            text=\"<|im_start|>user\\n<image>\\nWhat are these?<|im_end|>\\n<|im_start|>assistant\",\n+            images=raw_image,\n+            return_tensors=\"pt\",\n+        ).to(torch_device, torch.float16)\n+\n+        # Make sure that `generate` works\n+        output = model.generate(**inputs, max_new_tokens=30)\n+\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n+        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_expansion_in_processing(self):\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        prompt = \"USER: <image>\\nDescribe the image:\\nASSISTANT:\"\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+\n+        # check processing with expansion of inputs\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+        inputs_expanded = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 593)\n+\n+        # check processing without expansion of inputs (legacy behavior)\n+        processor.vision_feature_select_strategy = None\n+        processor.patch_size = None\n+        inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        self.assertTrue(inputs.input_ids.shape[-1] == 18)\n+\n+        # generate exactly 20 tokens\n+        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n+        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n+\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_pixtral(self):\n+        model_id = \"rhymes-ai/Aria\"\n+        model = AriaForConditionalGeneration.from_pretrained(model_id)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        IMG_URLS = [\n+            Image.open(requests.get(\"https://picsum.photos/id/237/400/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/231/200/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/27/500/500\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/17/150/600\", stream=True).raw),\n+        ]\n+        PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n+\n+        # image = Image.open(requests.get(url, stream=True).raw)\n+        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\n+        generate_ids = model.generate(**inputs, max_new_tokens=500)\n+        ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        # fmt: off\n+        EXPECTED_GENERATION = \"\"\"\n+Describe the images.\n+Sure, let's break down each image description:\n+\n+1. **Image 1:**\n+   - **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n+   - **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n+\n+2. **Image 2:**\n+   - **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n+   - **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n+\n+3. **Image 3:**\n+   - **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n+   - **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n+\n+4. **Image 4:**\n+   - **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n+   - **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\n+\n+Each image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n+\"\"\"\n+        # fmt: on\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(ouptut, EXPECTED_GENERATION)"
        },
        {
            "sha": "7e23d861c775c080f439e01bead35078f0ca8ce2",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "added",
            "additions": 391,
            "deletions": 0,
            "changes": 391,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -0,0 +1,391 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+from io import BytesIO\n+from typing import Optional\n+\n+import numpy as np\n+import requests\n+\n+from transformers import AriaProcessor\n+from transformers.models.auto.processing_auto import AutoProcessor\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+@require_torch\n+@require_vision\n+class AriaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = AriaProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor = AriaProcessor.from_pretrained(\"m-ric/Aria_hf_2\", image_seq_len=2)\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image1 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+                ).content\n+            )\n+        )\n+        cls.image2 = Image.open(\n+            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n+        )\n+        cls.image3 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n+                ).content\n+            )\n+        )\n+        cls.bos_token = \"<|im_start|>\"\n+        cls.eos_token = \"<|im_end|>\"\n+\n+        cls.image_token = processor.tokenizer.image_token\n+        cls.fake_image_token = \"o\"\n+        cls.global_img_token = \"<|img|>\"\n+\n+        cls.bos_token_id = processor.tokenizer.convert_tokens_to_ids(cls.bos_token)\n+        cls.eos_token_id = processor.tokenizer.convert_tokens_to_ids(cls.eos_token)\n+\n+        cls.image_token_id = processor.tokenizer.convert_tokens_to_ids(cls.image_token)\n+        cls.fake_image_token_id = processor.tokenizer.convert_tokens_to_ids(cls.fake_image_token)\n+        cls.global_img_tokens_id = processor.tokenizer(cls.global_img_token, add_special_tokens=False)[\"input_ids\"]\n+        cls.padding_token_id = processor.tokenizer.pad_token_id\n+        cls.image_seq_len = 256\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname)\n+\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\n+            \"image_processor\", do_rescale=True, rescale_factor=1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(**processor_components)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+\n+    def test_process_interleaved_images_prompts_image_splitting(self):\n+        processor = self.get_processor()\n+        processor.image_processor.split_image = True\n+\n+        # Test that a single image is processed correctly\n+        inputs = processor(images=self.image1, text=\"Ok<|img|>\", images_kwargs={\"split_image\": True})\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (2, 3, 980, 980))\n+        self.assertEqual(np.array(inputs[\"pixel_mask\"]).shape, (2, 980, 980))\n+\n+    def test_process_interleaved_images_prompts_no_image_splitting(self):\n+        processor = self.get_processor()\n+        processor.image_processor.split_image = False\n+\n+        # Test that a single image is processed correctly\n+        inputs = processor(images=self.image1, text=\"Ok<|img|>\")\n+        image1_expected_size = (980, 980)\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (1, 3, *image1_expected_size))\n+        self.assertEqual(np.array(inputs[\"pixel_mask\"]).shape, (1, *image1_expected_size))\n+        # fmt: on\n+\n+        # Test a single sample with image and text\n+        image_str = \"<|img|>\"\n+        text_str = \"In this image, we see\"\n+        text = image_str + text_str\n+        inputs = processor(text=text, images=self.image1)\n+\n+        # fmt: off\n+        tokenized_sentence = processor.tokenizer(text_str, add_special_tokens=False)\n+\n+        expected_input_ids = [[self.image_token_id] * self.image_seq_len + tokenized_sentence[\"input_ids\"]]\n+        # self.assertEqual(len(inputs[\"input_ids\"]), len(expected_input_ids))\n+\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids[0])])\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (1, 3, *image1_expected_size))\n+        self.assertEqual(np.array(inputs[\"pixel_mask\"]).shape, (1, *image1_expected_size))\n+        # fmt: on\n+\n+        # Test that batch is correctly processed\n+        image_str = \"<|img|>\"\n+        text_str_1 = \"In this image, we see\"\n+        text_str_2 = \"In this image, we see\"\n+\n+        text = [\n+            image_str + text_str_1,\n+            image_str + image_str + text_str_2,\n+        ]\n+        images = [[self.image1], [self.image2, self.image3]]\n+\n+        inputs = processor(text=text, images=images, padding=True)\n+\n+        # fmt: off\n+        tokenized_sentence_1 = processor.tokenizer(text_str_1, add_special_tokens=False)\n+        tokenized_sentence_2 = processor.tokenizer(text_str_2, add_special_tokens=False)\n+\n+        image_tokens = [self.image_token_id] * self.image_seq_len\n+        expected_input_ids_1 = image_tokens + tokenized_sentence_1[\"input_ids\"]\n+        expected_input_ids_2 = 2 * image_tokens + tokenized_sentence_2[\"input_ids\"]\n+\n+        # Pad the first input to match the second input\n+        pad_len = len(expected_input_ids_2) - len(expected_input_ids_1)\n+\n+        expected_attention_mask = [[0] * pad_len + [1] * len(expected_input_ids_1), [1] * (len(expected_input_ids_2))]\n+\n+        self.assertEqual(\n+            inputs[\"attention_mask\"],\n+            expected_attention_mask\n+        )\n+        self.assertEqual(np.array(inputs['pixel_values']).shape, (3, 3, 980, 980))\n+        self.assertEqual(np.array(inputs['pixel_mask']).shape, (3, 980, 980))\n+        # fmt: on\n+\n+    def test_non_nested_images_with_batched_text(self):\n+        processor = self.get_processor()\n+        processor.image_processor.do_image_splitting = False\n+\n+        image_str = \"<|img|>\"\n+        text_str_1 = \"In this image, we see\"\n+        text_str_2 = \"In this image, we see\"\n+\n+        text = [\n+            image_str + text_str_1,\n+            image_str + image_str + text_str_2,\n+        ]\n+        images = [self.image1, self.image2, self.image3]\n+\n+        inputs = processor(text=text, images=images, padding=True)\n+\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (3, 3, 980, 980))\n+        self.assertEqual(np.array(inputs[\"pixel_mask\"]).shape, (3, 980, 980))\n+\n+    def test_apply_chat_template(self):\n+        # Message contains content which a mix of lists with images and image urls and string\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What do these images show?\"},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"image\"},\n+                    \"What do these images show?\",\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"The first image shows the statue of Liberty in New York. The second image picture depicts Idefix, the dog of Obelix in Asterix and Obelix.\",\n+                    }\n+                ],\n+            },\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"And who is that?\"}]},\n+        ]\n+        processor = self.get_processor()\n+        # Make short sequence length to test that the fake tokens are added correctly\n+        rendered = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        print(rendered)\n+\n+        expected_rendered = \"\"\"<|im_start|>user\n+What do these images show?<fim_prefix><|img|><fim_suffix><fim_prefix><|img|><fim_suffix><|im_end|>\n+<|im_start|>assistant\n+The first image shows the statue of Liberty in New York. The second image picture depicts Idefix, the dog of Obelix in Asterix and Obelix.<|im_end|>\n+<|im_start|>user\n+And who is that?<|im_end|>\n+<|im_start|>assistant\n+\"\"\"\n+        self.assertEqual(rendered, expected_rendered)\n+\n+    # Override as AriaProcessor needs image tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <|img|>\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer <|img|>\"]\n+        return [\"lower newer <|img|>\", \"<|img|> upper older longer string\"] + [\"<|img|> lower newer\"] * (\n+            batch_size - 2\n+        )\n+\n+    # Override tests as inputs_ids padded dimension is the second one but not the last one\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=30)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            common_kwargs={\"return_tensors\": \"pt\"},\n+            images_kwargs={\"max_image_size\": 980},\n+            text_kwargs={\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"max_image_size\": 980},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n+\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"longest\",\n+            max_length=76,\n+            truncation=True,\n+            max_image_size=980,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[1], 3)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            max_image_size=980,\n+            padding=\"max_length\",\n+            max_length=120,\n+            truncation=\"longest_first\",\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)"
        },
        {
            "sha": "1c81c08fd845b1e0946c01977108fcc3fbc4987f",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -381,7 +381,7 @@ def check_config_attributes_being_used(config_class):\n \n \n def check_config_attributes():\n-    \"\"\"Check the arguments in `__init__` of all configuration classes are used in  python files\"\"\"\n+    \"\"\"Check the arguments in `__init__` of all configuration classes are used in python files\"\"\"\n     configs_with_unused_attributes = {}\n     for _config_class in list(CONFIG_MAPPING.values()):\n         # Skip deprecated models"
        },
        {
            "sha": "a63ca59690f748fdfbeeb84e4a82851d9f9c0426",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -865,9 +865,10 @@ def match_docstring_with_signature(obj: Any) -> Optional[Tuple[str, str]]:\n \n     # We went too far by one (perhaps more if there are a lot of new lines)\n     idx -= 1\n-    while len(obj_doc_lines[idx].strip()) == 0:\n-        arguments[current_arg] = arguments[current_arg][:-1]\n-        idx -= 1\n+    if current_arg:\n+        while len(obj_doc_lines[idx].strip()) == 0:\n+            arguments[current_arg] = arguments[current_arg][:-1]\n+            idx -= 1\n     # And we went too far by one again.\n     idx += 1\n "
        },
        {
            "sha": "3dbe59f192293ab8b18f7f9cf66d282c33abf0cd",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -85,6 +85,8 @@\n     \"Idefics2PerceiverResampler\",\n     \"Idefics2VisionTransformer\",\n     \"Idefics3VisionTransformer\",\n+    \"AriaTextForCausalLM\",\n+    \"AriaTextModel\",\n ]\n \n # Update this list for models that are not tested with a comment explaining the reason it should not be."
        },
        {
            "sha": "e8d117cd2af08feefc4dcf8d04b49a173bbc42a4",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=9ad4c93536855d78bcc3ea56a95bd53dc95d1a8e",
            "patch": "@@ -1678,7 +1678,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"src/transformers/models/gemma/modular_gemma.py\"],\n+        default=[\"src/transformers/models/aria/modular_aria.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        }
    ],
    "stats": {
        "total": 6251,
        "additions": 6244,
        "deletions": 7
    }
}