{
    "author": "MekkCyber",
    "message": "Cleaning hub kernels  (#41477)\n\n* disable kernel mapping\n\n* cleaning\n\n* revert\n\n* fix style",
    "sha": "d99069195b11824e35fdffa0319baea266b8a885",
    "files": [
        {
            "sha": "d7d7ffc41a7cb015bc0abc9551729d1c7e5d2584",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=d99069195b11824e35fdffa0319baea266b8a885",
            "patch": "@@ -27,7 +27,7 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n-from ...integrations.hub_kernels import load_and_register_kernel\n+from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n@@ -616,7 +616,7 @@ def __init__(\n             if attn_implementation not in ALL_ATTENTION_FUNCTIONS._global_mapping:  # when its a kernel\n                 from ...integrations.flash_paged import paged_attention_forward\n \n-                load_and_register_kernel(attn_implementation, paged_attention_forward)\n+                load_and_register_attn_kernel(attn_implementation, paged_attention_forward)\n \n             model.config._attn_implementation = attn_implementation\n         self.model = model.eval()"
        },
        {
            "sha": "bbbb8bbf64448111c4a7ae32b2f514499a5bca01",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=d99069195b11824e35fdffa0319baea266b8a885",
            "patch": "@@ -42,7 +42,6 @@\n         },\n         \"Llama4TextMoe\": {\n             \"cuda\": LayerRepository(\n-                # Move to kernels-community/moe once we release.\n                 repo_id=\"kernels-community/moe\",\n                 layer_name=\"Llama4TextMoe\",\n             )\n@@ -51,13 +50,11 @@\n             \"cuda\": LayerRepository(\n                 repo_id=\"kernels-community/liger_kernels\",\n                 layer_name=\"LigerRMSNorm\",\n-                # revision=\"pure-layer-test\",\n             ),\n             \"rocm\": {\n                 Mode.INFERENCE: LayerRepository(\n                     repo_id=\"kernels-community/liger_kernels\",\n                     layer_name=\"LigerRMSNorm\",\n-                    # revision=\"pure-layer-test\",\n                 )\n             },\n         },\n@@ -169,7 +166,7 @@ def is_kernel(attn_implementation: Optional[str]) -> bool:\n     )\n \n \n-def load_and_register_kernel(attn_implementation: str, attention_wrapper: Optional[Callable] = None) -> None:\n+def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: Optional[Callable] = None) -> None:\n     \"\"\"\n     Load and register the kernel associated to `attn_implementation`.\n "
        },
        {
            "sha": "201ea3eff305015fc0d8edd18973b9dab9819e0d",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=d99069195b11824e35fdffa0319baea266b8a885",
            "patch": "@@ -129,7 +129,7 @@ def lazy_import_flash_attention(implementation: Optional[str], force_import: Opt\n     Lazily import flash attention and return the respective functions + flags.\n \n     NOTE: For fullgraph, this needs to be called before compile, while no fullgraph can\n-    work without preloading. See `load_and_register_kernel` in `integrations.hub_kernels`.\n+    work without preloading. See `load_and_register_attn_kernel` in `integrations.hub_kernels`.\n     \"\"\"\n     global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n     if force_import or any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):"
        },
        {
            "sha": "aa55c5f5b051ea77b0b16b18f708741ca0c441af",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d99069195b11824e35fdffa0319baea266b8a885/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d99069195b11824e35fdffa0319baea266b8a885",
            "patch": "@@ -56,7 +56,7 @@\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flash_paged import paged_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n-from .integrations.hub_kernels import is_kernel, load_and_register_kernel\n+from .integrations.hub_kernels import is_kernel, load_and_register_attn_kernel\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n@@ -2553,7 +2553,7 @@ def _check_and_adjust_attn_implementation(\n \n         if is_kernel(applicable_attn_implementation):\n             try:\n-                load_and_register_kernel(applicable_attn_implementation)\n+                load_and_register_attn_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n                 if attn_implementation.startswith(\"flash_attention\"):\n                     logger.warning_once("
        }
    ],
    "stats": {
        "total": 15,
        "additions": 6,
        "deletions": 9
    }
}