{
    "author": "BakerBunker",
    "message": "ðŸš¨ Fix Inconsistant `input_feature` length and `attention_mask` length in `WhisperFeatureExtractor` (#39221)\n\n* Update feature_extraction_whisper.py\n\n* Reformat\n\n* Add feature extractor shape test\n\n* reformat\n\n* fix omni\n\n* fix new failing whisper test\n\n* Update src/transformers/models/whisper/feature_extraction_whisper.py\n\n* make style\n\n* revert omni test changes\n\n* add comment\n\n---------\n\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "a72e5a4b9daaef13c251d3313094eebbb2e10acc",
    "files": [
        {
            "sha": "bf548ac8408fb795bd85a0a981f7405ae8b9e047",
            "filename": "src/transformers/models/whisper/feature_extraction_whisper.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72e5a4b9daaef13c251d3313094eebbb2e10acc/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72e5a4b9daaef13c251d3313094eebbb2e10acc/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py?ref=a72e5a4b9daaef13c251d3313094eebbb2e10acc",
            "patch": "@@ -326,7 +326,14 @@ def __call__(\n \n         if return_attention_mask:\n             # rescale from sample (48000) to feature (3000)\n-            padded_inputs[\"attention_mask\"] = padded_inputs[\"attention_mask\"][:, :: self.hop_length]\n+            rescaled_attention_mask = padded_inputs[\"attention_mask\"][:, :: self.hop_length]\n+\n+            # The STFT computation produces L//hop_length + 1 frames, but we skip the last frame (see `_torch_extract_fbank_features`).\n+            # This means we need to trim the rescaled attention mask to match the actual number of frames (L//hop_length) when the input length\n+            # is not perfectly divisible by the hop length.\n+            if padded_inputs[\"attention_mask\"].shape[1] % self.hop_length != 0:\n+                rescaled_attention_mask = rescaled_attention_mask[:, :-1]\n+            padded_inputs[\"attention_mask\"] = rescaled_attention_mask\n \n         if return_token_timestamps is not None:\n             logger.warning_once("
        },
        {
            "sha": "aa359fafb8eeaa55f61b6e74194804686730ef7f",
            "filename": "tests/models/whisper/test_feature_extraction_whisper.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72e5a4b9daaef13c251d3313094eebbb2e10acc/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72e5a4b9daaef13c251d3313094eebbb2e10acc/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py?ref=a72e5a4b9daaef13c251d3313094eebbb2e10acc",
            "patch": "@@ -237,6 +237,39 @@ def test_dither(self):\n         self.assertTrue(np.abs(diff).mean() <= 1e-4)\n         self.assertTrue(np.abs(diff).max() <= 5e-3)\n \n+    def test_feature_shape(self):\n+        feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n+        hop_length = feature_extractor.hop_length\n+        test_inputs = np.random.randn(16000)\n+\n+        self.assertTrue(\n+            feature_extractor(\n+                [test_inputs[: hop_length * 5 + 1]],\n+                return_attention_mask=True,\n+                padding=False,\n+                return_tensors=\"np\",\n+            ).attention_mask.shape[-1]\n+            == 5\n+        )\n+        self.assertTrue(\n+            feature_extractor(\n+                [test_inputs[: hop_length * 5]],\n+                return_attention_mask=True,\n+                padding=False,\n+                return_tensors=\"np\",\n+            ).attention_mask.shape[-1]\n+            == 5\n+        )\n+        self.assertTrue(\n+            feature_extractor(\n+                [test_inputs[: hop_length * 5 - 1]],\n+                return_attention_mask=True,\n+                padding=False,\n+                return_tensors=\"np\",\n+            ).attention_mask.shape[-1]\n+            == 4\n+        )\n+\n     @require_torch\n     def test_double_precision_pad(self):\n         import torch"
        },
        {
            "sha": "061c92c2e55a9b7ce3414214b22500db5546801b",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72e5a4b9daaef13c251d3313094eebbb2e10acc/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72e5a4b9daaef13c251d3313094eebbb2e10acc/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=a72e5a4b9daaef13c251d3313094eebbb2e10acc",
            "patch": "@@ -2158,7 +2158,7 @@ def test_tiny_token_timestamp_generation_longform(self):\n             torch.tensor([44.7000, 44.8600, 44.9400, 45.1400, 45.1400, 45.2800, 45.6200, 45.9000, 46.2600, 47.1600, 47.4800, 47.7400, 48.1000, 48.2800, 48.4000, 48.6200, 48.8400, 49.0400, 49.2800, 49.4800, 49.6600, 49.9400, 50.5400, 50.5400]),\n             torch.tensor([50.5400, 50.6600, 50.8800, 51.2400, 51.7200, 52.8400, 52.9600]),\n             torch.tensor([52.9600, 53.0400, 53.2600, 53.4200, 53.5800, 53.9200, 54.1200, 54.7200, 54.9400, 55.2600, 55.6200, 55.9800, 56.5600, 56.8000, 56.9200, 57.3600, 57.9200, 58.1600, 58.5200, 58.6400, 58.8200, 59.4200, 59.4200]),\n-            torch.tensor([58.6800, 59.1400, 59.5400, 59.9200, 60.1400, 60.3800, 60.8400, 61.6000, 62.2400, 62.3800, 62.4400])\n+            torch.tensor([58.6800, 59.1400, 59.5400, 59.9200, 60.1400, 60.3800, 60.8400, 61.6000, 62.2400, 62.4200, 62.4200])\n         ]\n         # fmt: on\n "
        }
    ],
    "stats": {
        "total": 44,
        "additions": 42,
        "deletions": 2
    }
}