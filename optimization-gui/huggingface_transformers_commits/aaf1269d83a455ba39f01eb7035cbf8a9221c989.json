{
    "author": "cyyever",
    "message": "Remove unnecessary Optional typing (#41198)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "aaf1269d83a455ba39f01eb7035cbf8a9221c989",
    "files": [
        {
            "sha": "231a78f38a0a435d1432cfccdfd0757dd87cb88a",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -321,9 +321,7 @@ def mel_to_hertz(mels: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Unio\n     return freq\n \n \n-def hertz_to_octave(\n-    freq: Union[float, np.ndarray], tuning: Optional[float] = 0.0, bins_per_octave: Optional[int] = 12\n-):\n+def hertz_to_octave(freq: Union[float, np.ndarray], tuning: float = 0.0, bins_per_octave: int = 12):\n     \"\"\"\n     Convert frequency from hertz to fractional octave numbers.\n     Adapted from *librosa*."
        },
        {
            "sha": "35b615c490133d06858b2924586e1bd0656f03d0",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -790,7 +790,7 @@ def early_initialization(\n         for layer in self.layers:\n             layer.lazy_initialization(fake_keys_tensor)\n \n-    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+    def get_seq_length(self, layer_idx: int = 0) -> int:\n         \"\"\"Returns the sequence length of the cache for the given layer.\"\"\"\n         if layer_idx >= len(self.layers):\n             return 0\n@@ -1286,7 +1286,7 @@ def from_legacy_cache(\n                     cache.is_updated[layer_idx] = True\n         return cache\n \n-    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+    def get_seq_length(self, layer_idx: int = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         return self.self_attention_cache.get_seq_length(layer_idx)\n "
        },
        {
            "sha": "8510a02c803abcb0d8580bce473167fd09e8d2fb",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -331,7 +331,7 @@ def finalize(\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n         eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: Optional[int] = 0,\n+        decoder_prompt_len: int = 0,\n     ) -> tuple[torch.LongTensor]:\n         batch_size = len(self._beam_hyps) // self.num_beam_groups\n \n@@ -804,7 +804,7 @@ def finalize(\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n         eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: Optional[int] = 0,\n+        decoder_prompt_len: int = 0,\n     ) -> tuple[torch.LongTensor]:\n         batch_size = len(self._beam_hyps)\n \n@@ -965,7 +965,7 @@ def add(\n             else:\n                 self.worst_score = min(score, self.worst_score)\n \n-    def is_done(self, best_sum_logprobs: float, cur_len: int, decoder_prompt_len: Optional[int] = 0) -> bool:\n+    def is_done(self, best_sum_logprobs: float, cur_len: int, decoder_prompt_len: int = 0) -> bool:\n         \"\"\"\n         If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n         one in the heap, then we are done with this sentence."
        },
        {
            "sha": "d8f843bd59aebf825ae8df601a04803e9d72717f",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -2289,7 +2289,7 @@ def __init__(\n         model,\n         unconditional_ids: Optional[torch.LongTensor] = None,\n         unconditional_attention_mask: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = True,\n+        use_cache: bool = True,\n     ):\n         self.guidance_scale = guidance_scale\n         self.model = model"
        },
        {
            "sha": "fbc60f7db5edf4bc603e333297ad32d6786d9e70",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -248,7 +248,7 @@ def pad(\n         pad_size: SizeDict = None,\n         fill_value: Optional[int] = 0,\n         padding_mode: Optional[str] = \"constant\",\n-        return_mask: Optional[bool] = False,\n+        return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n         **kwargs,\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "49191a518c8e9cf5983f83bc4f6a74254478af19",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -192,8 +192,8 @@ def preprocess(\n         points_per_batch=64,\n         crops_n_layers: int = 0,\n         crop_overlap_ratio: float = 512 / 1500,\n-        points_per_crop: Optional[int] = 32,\n-        crop_n_points_downscale_factor: Optional[int] = 1,\n+        points_per_crop: int = 32,\n+        crop_n_points_downscale_factor: int = 1,\n         timeout: Optional[float] = None,\n     ):\n         image = load_image(image, timeout=timeout)"
        },
        {
            "sha": "068ff81fd3cdf3c75299fdcad80e5ccae5d927d3",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -1285,7 +1285,7 @@ class AcceleratorConfig:\n         },\n     )\n \n-    non_blocking: Optional[bool] = field(\n+    non_blocking: bool = field(\n         default=False,\n         metadata={\n             \"help\": \"Whether to use non-blocking CUDA calls to help minimize synchronization during \""
        },
        {
            "sha": "b39ed65251b228c7541a8f6c07f45a6757687dac",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf1269d83a455ba39f01eb7035cbf8a9221c989/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=aaf1269d83a455ba39f01eb7035cbf8a9221c989",
            "patch": "@@ -804,7 +804,7 @@ class OutputRecorder:\n     \"\"\"\n \n     target_class: \"type[torch.nn.Module]\"\n-    index: Optional[int] = 0\n+    index: int = 0\n     layer_name: Optional[str] = None\n     class_name: Optional[str] = None\n "
        }
    ],
    "stats": {
        "total": 26,
        "additions": 12,
        "deletions": 14
    }
}