{
    "author": "gante",
    "message": "[XGLM] tag tests as slow (#36592)\n\nthese tests should be slow",
    "sha": "5275ef6f3d4a1a78a25e958496cde48fd0257dc2",
    "files": [
        {
            "sha": "7ddab81a26117b735f84c8bd60dc058d37aa5e19",
            "filename": "tests/models/xglm/test_modeling_tf_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5275ef6f3d4a1a78a25e958496cde48fd0257dc2/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5275ef6f3d4a1a78a25e958496cde48fd0257dc2/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py?ref=5275ef6f3d4a1a78a25e958496cde48fd0257dc2",
            "patch": "@@ -239,6 +239,7 @@ def test_batch_generation(self):\n         self.assertListEqual(expected_output_sentence, batch_out_sentence)\n         self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])\n \n+    @slow\n     def test_loss_with_padding(self):\n         tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n         model = TFXGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")"
        },
        {
            "sha": "88f3c13497eb05a348ae0a1d001902ba8b19d88f",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5275ef6f3d4a1a78a25e958496cde48fd0257dc2/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5275ef6f3d4a1a78a25e958496cde48fd0257dc2/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=5275ef6f3d4a1a78a25e958496cde48fd0257dc2",
            "patch": "@@ -449,6 +449,7 @@ def test_batched_nan_fp16(self):\n                 torch.isnan(outputs.logits[0]).any().item()\n             )  # the first logits could contain NaNs if it fails\n \n+    @slow\n     def test_loss_with_padding(self):\n         tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n         model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}