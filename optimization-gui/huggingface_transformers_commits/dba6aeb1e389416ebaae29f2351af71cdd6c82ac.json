{
    "author": "yao-matrix",
    "message": "fix continuous batching issues, extend ut cases to xpu (#41830)\n\n* extend conrinuous batching cases to xpu\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "dba6aeb1e389416ebaae29f2351af71cdd6c82ac",
    "files": [
        {
            "sha": "ba9e198055c5a3cd97a5dd7f66a9c7245e279aea",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=dba6aeb1e389416ebaae29f2351af71cdd6c82ac",
            "patch": "@@ -19,6 +19,7 @@\n \n import torch\n \n+from ...utils import is_torch_xpu_available\n from ...utils.logging import logging\n from ...utils.metrics import traced\n \n@@ -35,6 +36,13 @@ def get_device_and_memory_breakdown() -> tuple[torch.device, int, int, int]:\n         total_memory = torch.cuda.get_device_properties(device).total_memory\n         reserved_memory = torch.cuda.memory_reserved(device)\n         allocated_memory = torch.cuda.memory_allocated(device)\n+    elif is_torch_xpu_available():\n+        device = torch.device(\"xpu\")\n+        torch.xpu.empty_cache()\n+        torch.xpu.synchronize()\n+        total_memory = torch.xpu.get_device_properties(device).total_memory\n+        reserved_memory = torch.xpu.memory_reserved(device)\n+        allocated_memory = torch.xpu.memory_allocated(device)\n     elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n         device = torch.device(\"mps\")\n         # MPS memory reporting (PyTorch 2.0+)"
        },
        {
            "sha": "6552e068aaaf340dfb207ff00c69dd8fa846871e",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=dba6aeb1e389416ebaae29f2351af71cdd6c82ac",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_torch_available, is_torch_xpu_available, logging\n \n \n if is_torch_available():\n@@ -114,6 +114,9 @@ def convert_moe_packed_tensors(\n     if not blocks.is_cuda and torch.cuda.is_available():\n         blocks = blocks.cuda()\n         scales = scales.cuda()\n+    elif (blocks.device.type != \"xpu\") and is_torch_xpu_available():\n+        blocks = blocks.to(\"xpu\")\n+        scales = scales.to(\"xpu\")\n \n     scales = scales.to(torch.int32) - 127  # TODO that's because 128=2**7\n \n@@ -351,6 +354,8 @@ def dequantize(module, param_name, param_value, target_device, dq_param_name, **\n                 dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                 if target_device == \"cpu\" and torch.cuda.is_available():\n                     torch.cuda.empty_cache()\n+                elif target_device == \"cpu\" and is_torch_xpu_available():\n+                    torch.xpu.empty_cache()\n                 setattr(module, proj, torch.nn.Parameter(dequantized.to(target_device)))\n                 delattr(module, blocks_attr)\n                 delattr(module, scales_attr)\n@@ -395,7 +400,7 @@ def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, trito\n         else:\n             blocks = blocks.reshape(local_experts, -1, module.intermediate_size // 2)\n         if getattr(target_device, \"type\", target_device) == \"cpu\":\n-            target_device = \"cuda\"\n+            target_device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n         blocks = blocks.to(target_device).contiguous()\n         scales = scales.to(target_device).contiguous()\n         with on_device(target_device):"
        },
        {
            "sha": "1393623793fce5a5b048d6dadea2810a08753a26",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 46,
            "deletions": 19,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dba6aeb1e389416ebaae29f2351af71cdd6c82ac/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=dba6aeb1e389416ebaae29f2351af71cdd6c82ac",
            "patch": "@@ -20,7 +20,15 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n-from transformers.testing_utils import Expectations, require_kernels, require_read_token, require_torch_gpu, slow\n+from transformers.testing_utils import (\n+    Expectations,\n+    require_kernels,\n+    require_read_token,\n+    require_torch_accelerator,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n \n \n ALLOW_EXPECTED_OUTPUTS = True  # this is a debug flag when you want to measure deviation between CB and non-CB gen\n@@ -148,7 +156,7 @@ def _continuous_batching_parity(\n \n         # Generation with continuous batching\n         model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=attn_implementation, dtype=\"auto\")\n-        model = model.cuda().eval()\n+        model = model.to(torch_device).eval()\n         model.generation_config.max_new_tokens = 40\n         model.generation_config.do_sample = False\n         model.generation_config.use_cuda_graph = False\n@@ -169,14 +177,14 @@ def _continuous_batching_parity(\n         model = AutoModelForCausalLM.from_pretrained(\n             model_id, attn_implementation=non_cb_attn_implementation, dtype=\"auto\"\n         )\n-        model = model.cuda().eval()\n+        model = model.to(torch_device).eval()\n         model.generation_config.max_new_tokens = 40\n         model.generation_config.do_sample = False\n         model.generation_config.use_cuda_graph = False\n \n         for request_id, request in cb_outputs.items():\n             # Generate without continuous batching\n-            input_ids = torch.tensor([request.prompt_ids]).cuda()\n+            input_ids = torch.tensor([request.prompt_ids]).to(torch_device)\n             attention_mask = torch.ones_like(input_ids)\n             outputs = model.generate(\n                 input_ids, attention_mask=attention_mask, generation_config=model.generation_config\n@@ -208,8 +216,8 @@ def _continuous_batching_parity(\n                     )\n \n     # Eager tests\n+    @require_torch_accelerator\n     @require_read_token\n-    @require_torch_gpu\n     @slow\n     def test_continuous_batching_parity_llama_eager(self) -> None:\n         expected_outputs = Expectations({\n@@ -219,11 +227,15 @@ def test_continuous_batching_parity_llama_eager(self) -> None:\n             (\"cuda\", (9, 0)): {\n                 \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5. The total number of bolts is 4.5. The total\",\n                 \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The answer is not 3.5 bolts of blue fiber and 1.5 bolts of white fiber. The answer'\",\n+                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|eager\", expected_outputs)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_continuous_batching_parity_gemma_eager(self) -> None:\n         expected_outputs = Expectations({\n@@ -233,53 +245,68 @@ def test_continuous_batching_parity_gemma_eager(self) -> None:\n             (\"cuda\", (9, 0)): {\n                 \"req_0\": \"\\n\\n**$12**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 13\",\n                 \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \"\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_0\": \"\\n\\n**$12**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 13\",\n+                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \",\n+                \"req_2\": \"\\n\\n**$100,000**\\n\\n**Explanation:**\\n\\nHere's how to calculate the profit:\\n\\n1. **Calculate the total cost:** $80,00\",\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|eager\", expected_outputs)\n \n     # FIXME: set expected_outputs\n-    # @require_torch_gpu\n+    # @require_torch_accelerator\n     # @slow\n     # def test_continuous_batching_parity_qwen_eager(self) -> None:\n     #     expected_outputs = {}\n     #     self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"paged|eager\", expected_outputs)\n \n     # FIXME: OOMs\n-    # @require_torch_gpu\n+    # @require_torch_accelerator\n     # @slow\n     # def test_continuous_batching_parity_gpt_oss_eager(self) -> None:\n     #     expected_outputs = Expectations({\n     #         (\"cuda\", (9, 0)): {\n     #             \"req_1\": \" 2.5 bolts. The question: \\\"What is the name of the puzzle that involves a robe taking 2 bolts of blue fiber and half that much white fiber?\\\" The answer: \\\"The\",\n     #             \"req_2\": \" 50%.\\\"\\n\\nWe need to parse: He buys a house for $80,000. He puts in $50,000 in repairs. This increased the value of the house by 150%.\"\n-    #         }\n+    #         },\n+    #         (\"xpu\", None): {\n+    #             \"req_1\": \" 2.5 bolts. The question: \\\"What is the name of the puzzle that involves a robe taking 2 bolts of blue fiber and half that much white fiber?\\\" The answer: \\\"The\",\n+    #             \"req_2\": \" 50%.\\\"\\n\\nWe need to parse: He buys a house for $80,000. He puts in $50,000 in repairs. This increased the value of the house by 150%.\"\n+    #         },\n     #     }).get_expectation()  # fmt: skip\n     #     self._continuous_batching_parity(\"openai/gpt-oss-20b\", \"paged|eager\", expected_outputs)\n \n     # SDPA tests\n     @require_read_token\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_continuous_batching_parity_llama_sdpa(self) -> None:\n         expected_outputs = Expectations({\n             (\"rocm\", (9, 4)): {\n                 \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|sdpa\", expected_outputs)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_continuous_batching_parity_gemma_sdpa(self) -> None:\n         expected_outputs = Expectations({\n             (\"cuda\", (9, 0)): {\n                 \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n-            }\n+            },\n+            (\"xpu\", None): {\n+                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n+            },\n         }).get_expectation()  # fmt: skip\n         self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|sdpa\", expected_outputs)\n \n     # FIXME: set expected_outputs\n-    # @require_torch_gpu\n+    # @require_torch_accelerator\n     # @slow\n     # def test_continuous_batching_parity_qwen_sdpa(self) -> None:\n     #     expected_outputs = {}\n@@ -333,7 +360,7 @@ def test_attn_implementation(self) -> None:\n         manager = model.init_continuous_batching()\n         assert \"paged|eager\" == manager.model.config._attn_implementation\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_streaming_request(self) -> None:\n         model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n         max_new_tokens = 3\n@@ -365,7 +392,7 @@ def test_streaming_request(self) -> None:\n \n         manager.stop(block=True)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_non_streaming_request(self) -> None:\n         model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n         max_new_tokens = 3\n@@ -392,7 +419,7 @@ def test_non_streaming_request(self) -> None:\n \n         manager.stop(block=True)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n         model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n         max_new_tokens = 3"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 61,
        "deletions": 21
    }
}