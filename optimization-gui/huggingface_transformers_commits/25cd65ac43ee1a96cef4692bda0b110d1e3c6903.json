{
    "author": "pcuenca",
    "message": "Random serve fixes (#39176)\n\n* Fix index out of bounds exception on wrong kv reuse\n\n* Prevent loading same model twice\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "25cd65ac43ee1a96cef4692bda0b110d1e3c6903",
    "files": [
        {
            "sha": "d8f61603692df38d3baf3bb82972bc43ecaaf54c",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/25cd65ac43ee1a96cef4692bda0b110d1e3c6903/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25cd65ac43ee1a96cef4692bda0b110d1e3c6903/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=25cd65ac43ee1a96cef4692bda0b110d1e3c6903",
            "patch": "@@ -347,7 +347,7 @@ def _serve(req: \"ChatCompletionInput\"):\n             if not req.stream:\n                 return {\"error\": \"Only streaming mode is supported.\"}\n \n-            update_model = req.model != self.loaded_model\n+            update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n \n             if update_model:\n                 self.model, self.tokenizer = self.load_model_and_tokenizer(req.model, self.args)\n@@ -402,7 +402,7 @@ def is_continuation(self, req: \"ChatCompletionInput\") -> bool:\n         if self.last_messages is None:\n             req_continues_last_messages = False\n         # The new request has fewer rounds of conversation: this is a new request\n-        elif len(self.last_messages) > len(req.messages):\n+        elif len(self.last_messages) >= len(req.messages):\n             req_continues_last_messages = False\n         # Otherwise, check that the last messages are a subset of the new request\n         else:\n@@ -417,7 +417,7 @@ def is_continuation(self, req: \"ChatCompletionInput\") -> bool:\n     def generate(self, app):\n         @app.post(\"/v1/chat/completions\")\n         def _serve(req: \"ChatCompletionInput\"):\n-            update_model = req.model != self.loaded_model\n+            update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n \n             if update_model:\n                 self.model, self.tokenizer = self.load_model_and_tokenizer(req.model, self.args)\n@@ -585,6 +585,11 @@ def get_quantization_config(model_args: ServeArguments) -> Optional[\"BitsAndByte\n \n         return quantization_config\n \n+    def canonicalized_model_name(self, model_id: str) -> str:\n+        if \"@\" in model_id:\n+            return model_id\n+        return f\"{model_id}@main\"\n+\n     def load_model_and_tokenizer(\n         self, model_id_and_revision: str, args: ServeArguments\n     ) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n@@ -621,9 +626,9 @@ def load_model_and_tokenizer(\n         if getattr(model, \"hf_device_map\", None) is None:\n             model = model.to(args.device)\n \n-        self.loaded_model = model_id_and_revision\n+        self.loaded_model = f\"{model_id}@{revision}\"\n \n-        logger.warning(f\"Loaded model {model_id_and_revision}\")\n+        logger.warning(f\"Loaded model {self.loaded_model}\")\n         return model, tokenizer\n \n "
        }
    ],
    "stats": {
        "total": 15,
        "additions": 10,
        "deletions": 5
    }
}