{
    "author": "itazap",
    "message": "ðŸš¨ rm already deprecated pad_to_max_length arg (#37617)\n\n* rm already deprecated padding max length\n\n* truncate_strategy AS AN ARG is already deprecated for a few years\n\n* fix\n\n* rm test_padding_to_max_length\n\n* rm pad_to_max_length=True in other tests\n\n* rm from common\n\n* missed fnet",
    "sha": "c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
    "files": [
        {
            "sha": "6635481ff91ab9c60bfe4e506e0f5c7c49c8a598",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 37,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -2759,11 +2759,8 @@ def _get_padding_truncation_strategies(\n         self, padding=False, truncation=None, max_length=None, pad_to_multiple_of=None, verbose=True, **kwargs\n     ):\n         \"\"\"\n-        Find the correct padding/truncation strategy with backward compatibility for old arguments (truncation_strategy\n-        and pad_to_max_length) and behaviors.\n+        Find the correct padding/truncation strategy\n         \"\"\"\n-        old_truncation_strategy = kwargs.pop(\"truncation_strategy\", \"do_not_truncate\")\n-        old_pad_to_max_length = kwargs.pop(\"pad_to_max_length\", False)\n \n         # Backward compatibility for previous behavior, maybe we should deprecate it:\n         # If you only set max_length, it activates truncation for max_length\n@@ -2781,21 +2778,7 @@ def _get_padding_truncation_strategies(\n             truncation = \"longest_first\"\n \n         # Get padding strategy\n-        if padding is False and old_pad_to_max_length:\n-            if verbose:\n-                warnings.warn(\n-                    \"The `pad_to_max_length` argument is deprecated and will be removed in a future version, \"\n-                    \"use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or \"\n-                    \"use `padding='max_length'` to pad to a max length. In this case, you can give a specific \"\n-                    \"length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the \"\n-                    \"maximal input size of the model (e.g. 512 for Bert).\",\n-                    FutureWarning,\n-                )\n-            if max_length is None:\n-                padding_strategy = PaddingStrategy.LONGEST\n-            else:\n-                padding_strategy = PaddingStrategy.MAX_LENGTH\n-        elif padding is not False:\n+        if padding is not False:\n             if padding is True:\n                 if verbose:\n                     if max_length is not None and (\n@@ -2805,8 +2788,6 @@ def _get_padding_truncation_strategies(\n                             \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n                             \"To pad to max length, use `padding='max_length'`.\"\n                         )\n-                    if old_pad_to_max_length is not False:\n-                        warnings.warn(\"Though `pad_to_max_length` = `True`, it is ignored because `padding`=`True`.\")\n                 padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch\n             elif not isinstance(padding, PaddingStrategy):\n                 padding_strategy = PaddingStrategy(padding)\n@@ -2816,21 +2797,7 @@ def _get_padding_truncation_strategies(\n             padding_strategy = PaddingStrategy.DO_NOT_PAD\n \n         # Get truncation strategy\n-        if truncation is None and old_truncation_strategy != \"do_not_truncate\":\n-            if verbose:\n-                warnings.warn(\n-                    \"The `truncation_strategy` argument is deprecated and will be removed in a future version, use\"\n-                    \" `truncation=True` to truncate examples to a max length. You can give a specific length with\"\n-                    \" `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input\"\n-                    \" size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific\"\n-                    \" truncation strategy selected among `truncation='only_first'` (will only truncate the first\"\n-                    \" sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the\"\n-                    \" pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence\"\n-                    \" in the pairs).\",\n-                    FutureWarning,\n-                )\n-            truncation_strategy = TruncationStrategy(old_truncation_strategy)\n-        elif truncation is not False and truncation is not None:\n+        if truncation is not False and truncation is not None:\n             if truncation is True:\n                 truncation_strategy = (\n                     TruncationStrategy.LONGEST_FIRST\n@@ -3146,7 +3113,6 @@ def encode_plus(\n                 method).\n         \"\"\"\n \n-        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n             padding=padding,\n             truncation=truncation,"
        },
        {
            "sha": "f07756f731f30e72acaa278d8405ad26c3dc6ba0",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -1074,10 +1074,10 @@ def _add_eos_to_examples(example):\n \n             def _convert_to_features(example_batch):\n                 input_encodings = tokenizer.batch_encode_plus(\n-                    example_batch[\"input_text\"], pad_to_max_length=True, max_length=512, truncation=True\n+                    example_batch[\"input_text\"], padding=\"max_length\", max_length=512, truncation=True\n                 )\n                 target_encodings = tokenizer.batch_encode_plus(\n-                    example_batch[\"target_text\"], pad_to_max_length=True, max_length=16, truncation=True\n+                    example_batch[\"target_text\"], padding=\"max_length\", max_length=16, truncation=True\n                 )\n \n                 encodings = {"
        },
        {
            "sha": "6df93374137b6c6cc5b6c1970b4bbbc14740ece9",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -829,7 +829,6 @@ def test_bert2bert_summarization(self):\n         input_dict = tokenizer(\n             [ARTICLE_SIGMA, ARTICLE_AMERICA],\n             padding=\"max_length\",\n-            pad_to_max_length=True,\n             max_length=512,\n             return_tensors=\"pt\",\n         )"
        },
        {
            "sha": "3efb764e18fda44635b146472e35d392e9324ef7",
            "filename": "tests/models/fnet/test_tokenization_fnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -205,9 +205,6 @@ def test_padding(self, max_length=50):\n                 pad_token_id = tokenizer_p.pad_token_id\n \n                 # Encode - Simple input\n-                input_r = tokenizer_r.encode(\"This is a simple input\", max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(\"This is a simple input\", max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(\"This is a simple input\", max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(\"This is a simple input\", max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -217,13 +214,6 @@ def test_padding(self, max_length=50):\n                 self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n \n                 # Encode - Pair input\n-                input_r = tokenizer_r.encode(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(\n                     \"This is a simple input\", \"This is a pair\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -236,14 +226,6 @@ def test_padding(self, max_length=50):\n                 self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n \n                 # Encode_plus - Simple input\n-                input_r = tokenizer_r.encode_plus(\n-                    \"This is a simple input\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    \"This is a simple input\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.encode_plus(\n                     \"This is a simple input\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -259,14 +241,6 @@ def test_padding(self, max_length=50):\n                 )\n \n                 # Encode_plus - Pair input\n-                input_r = tokenizer_r.encode_plus(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.encode_plus(\n                     \"This is a simple input\", \"This is a pair\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -282,18 +256,6 @@ def test_padding(self, max_length=50):\n                 )\n \n                 # Batch_encode_plus - Simple input\n-                input_r = tokenizer_r.batch_encode_plus(\n-                    [\"This is a simple input 1\", \"This is a simple input 2\"],\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    [\"This is a simple input 1\", \"This is a simple input 2\"],\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     [\"This is a simple input 1\", \"This is a simple input 2\"],\n                     max_length=max_length,"
        },
        {
            "sha": "6eb8abf0b506056c1057f1f1a72a00e6589b364d",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -566,41 +566,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes()\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, words)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    words, boxes=boxes, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(words, boxes=boxes, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -612,9 +577,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -625,13 +587,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -641,10 +596,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n@@ -660,14 +611,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     question, words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -686,20 +629,6 @@ def test_padding(self, max_length=50):\n                 # Batch_encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes_batch()\n \n-                input_r = tokenizer_r.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     words,\n                     boxes=boxes,"
        },
        {
            "sha": "6e5f1ee11a76d5d0b77f31e565f5462bcf5bafcb",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -460,41 +460,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes()\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, words)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    words, boxes=boxes, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(words, boxes=boxes, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -506,9 +471,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -519,13 +481,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -535,10 +490,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n@@ -554,14 +505,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     question, words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -580,20 +523,6 @@ def test_padding(self, max_length=50):\n                 # Batch_encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes_batch()\n \n-                input_r = tokenizer_r.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     words,\n                     boxes=boxes,"
        },
        {
            "sha": "056726f00474b4dca39f7785b45f634a985c7c92",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -497,41 +497,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes()\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, words)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    words, boxes=boxes, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(words, boxes=boxes, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -543,9 +508,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -556,13 +518,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(question, words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -572,10 +527,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode_plus(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n@@ -591,14 +542,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     question, words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -617,20 +560,6 @@ def test_padding(self, max_length=50):\n                 # Batch_encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes_batch()\n \n-                input_r = tokenizer_r.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     words,\n                     boxes=boxes,"
        },
        {
            "sha": "3cdbd4acf835fafe64403acc3784a1ea033deccf",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -382,41 +382,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                nodes, xpaths = self.get_nodes_and_xpaths()\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, nodes)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -428,9 +393,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Simple input\n                 nodes, xpaths = self.get_nodes_and_xpaths()\n-                input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -441,13 +403,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Pair input\n                 question, nodes, xpaths = self.get_question_nodes_and_xpaths()\n-                input_r = tokenizer_r.encode(\n-                    question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(\n                     question, nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -461,10 +416,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Simple input\n                 nodes, xpaths = self.get_nodes_and_xpaths()\n-                input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n@@ -480,14 +431,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Pair input\n                 question, nodes, xpaths = self.get_question_nodes_and_xpaths()\n-                input_r = tokenizer_r.encode_plus(\n-                    question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     question, nodes, xpaths=xpaths, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -506,20 +449,6 @@ def test_padding(self, max_length=50):\n                 # Batch_encode_plus - Simple input\n                 nodes, xpaths = self.get_nodes_and_xpaths_batch()\n \n-                input_r = tokenizer_r.batch_encode_plus(\n-                    nodes,\n-                    xpaths=xpaths,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    nodes,\n-                    xpaths=xpaths,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     nodes,\n                     xpaths=xpaths,"
        },
        {
            "sha": "25ae9528111ae4e4fa7da1803d57b2ddc404b7d4",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -657,42 +657,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                table = self.get_table(tokenizer)\n-                sequence = \"Sequence\"\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, sequence)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(table, sequence)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    table, sequence, max_length=sequence_length + padding_size, padding=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(table, sequence)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(table, sequence, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_call(self):\n         # Tests that all call wrap to encode_plus and batch_encode_plus\n         tokenizers = self.get_tokenizers(do_lower_case=False)"
        },
        {
            "sha": "581cee32e21a2eacecf36344eed037ea9d223900",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 75,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -417,41 +417,6 @@ def test_number_of_added_tokens(self):\n                         tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences)\n                     )\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                words, boxes = self.get_words_and_boxes()\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, words)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode_boxes(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode_boxes(\n-                    words, boxes=boxes, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                assert sequence_length + padding_size == padded_sequence_length\n-                assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode_boxes(words, boxes=boxes)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode_boxes(words, boxes=boxes, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                assert sequence_length == padded_sequence_right_length\n-                assert encoded_sequence == padded_sequence_right\n-\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n@@ -463,9 +428,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode_boxes(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode_boxes(words, boxes=boxes, max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode_boxes(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode_boxes(words, boxes=boxes, max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -476,13 +438,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode_boxes(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_boxes(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode_boxes(\n                     question, words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -496,14 +451,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus_boxes(\n-                    words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus_boxes(\n-                    words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus_boxes(\n                     words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -523,14 +470,6 @@ def test_padding(self, max_length=50):\n \n                 # Encode_plus - Pair input\n                 question, words, boxes = self.get_question_words_and_boxes()\n-                input_r = tokenizer_r.encode_plus_boxes(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus_boxes(\n-                    question, words, boxes=boxes, max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus_boxes(\n                     question, words, boxes=boxes, max_length=max_length, padding=\"max_length\"\n                 )\n@@ -549,20 +488,6 @@ def test_padding(self, max_length=50):\n                 # Batch_encode_plus - Simple input\n                 words, boxes = self.get_words_and_boxes_batch()\n \n-                input_r = tokenizer_r.batch_encode_plus_boxes(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus_boxes(\n-                    words,\n-                    boxes=boxes,\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus_boxes(\n                     words,\n                     boxes=boxes,"
        },
        {
            "sha": "b1749f281e6fa2bedce5e3bcd1ea3ef48e787227",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=c80f65265b75b7c9c8135fd1d99be0c1dd4dcc61",
            "patch": "@@ -2475,41 +2475,6 @@ def test_right_and_left_truncation(self):\n                 self.assertEqual(sequence_length, truncated_sequence_left_length)\n                 self.assertEqual(encoded_sequence, truncated_sequence_left)\n \n-    def test_padding_to_max_length(self):\n-        \"\"\"We keep this test for backward compatibility but it should be remove when `pad_to_max_length` is deprecated.\"\"\"\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                sequence = \"Sequence\"\n-                padding_size = 10\n-\n-                # check correct behaviour if no pad_token_id exists and add it eventually\n-                self._check_no_pad_token_padding(tokenizer, sequence)\n-\n-                padding_idx = tokenizer.pad_token_id\n-\n-                # Check that it correctly pads when a maximum length is specified along with the padding flag set to True\n-                tokenizer.padding_side = \"right\"\n-                encoded_sequence = tokenizer.encode(sequence)\n-                sequence_length = len(encoded_sequence)\n-                # FIXME: the next line should be padding(max_length) to avoid warning\n-                padded_sequence = tokenizer.encode(\n-                    sequence, max_length=sequence_length + padding_size, pad_to_max_length=True\n-                )\n-                padded_sequence_length = len(padded_sequence)\n-                self.assertEqual(sequence_length + padding_size, padded_sequence_length)\n-                self.assertEqual(encoded_sequence + [padding_idx] * padding_size, padded_sequence)\n-\n-                # Check that nothing is done when a maximum length is not specified\n-                encoded_sequence = tokenizer.encode(sequence)\n-                sequence_length = len(encoded_sequence)\n-\n-                tokenizer.padding_side = \"right\"\n-                padded_sequence_right = tokenizer.encode(sequence, pad_to_max_length=True)\n-                padded_sequence_right_length = len(padded_sequence_right)\n-                self.assertEqual(sequence_length, padded_sequence_right_length)\n-                self.assertEqual(encoded_sequence, padded_sequence_right)\n-\n     def test_padding_to_multiple_of(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:\n@@ -3900,9 +3865,6 @@ def test_padding(self, max_length=50):\n                 pad_token_id = tokenizer_p.pad_token_id\n \n                 # Encode - Simple input\n-                input_r = tokenizer_r.encode(\"This is a simple input\", max_length=max_length, pad_to_max_length=True)\n-                input_p = tokenizer_p.encode(\"This is a simple input\", max_length=max_length, pad_to_max_length=True)\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(\"This is a simple input\", max_length=max_length, padding=\"max_length\")\n                 input_p = tokenizer_p.encode(\"This is a simple input\", max_length=max_length, padding=\"max_length\")\n                 self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n@@ -3912,13 +3874,6 @@ def test_padding(self, max_length=50):\n                 self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n \n                 # Encode - Pair input\n-                input_r = tokenizer_r.encode(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n                 input_r = tokenizer_r.encode(\n                     \"This is a simple input\", \"This is a pair\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -3931,14 +3886,6 @@ def test_padding(self, max_length=50):\n                 self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n \n                 # Encode_plus - Simple input\n-                input_r = tokenizer_r.encode_plus(\n-                    \"This is a simple input\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    \"This is a simple input\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     \"This is a simple input\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -3957,14 +3904,6 @@ def test_padding(self, max_length=50):\n                 self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n \n                 # Encode_plus - Pair input\n-                input_r = tokenizer_r.encode_plus(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                input_p = tokenizer_p.encode_plus(\n-                    \"This is a simple input\", \"This is a pair\", max_length=max_length, pad_to_max_length=True\n-                )\n-                self.assert_padded_input_match(input_r[\"input_ids\"], input_p[\"input_ids\"], max_length, pad_token_id)\n-                self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n                 input_r = tokenizer_r.encode_plus(\n                     \"This is a simple input\", \"This is a pair\", max_length=max_length, padding=\"max_length\"\n                 )\n@@ -3981,18 +3920,6 @@ def test_padding(self, max_length=50):\n                 self.assertSequenceEqual(input_r[\"attention_mask\"], input_p[\"attention_mask\"])\n \n                 # Batch_encode_plus - Simple input\n-                input_r = tokenizer_r.batch_encode_plus(\n-                    [\"This is a simple input 1\", \"This is a simple input 2\"],\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                input_p = tokenizer_p.batch_encode_plus(\n-                    [\"This is a simple input 1\", \"This is a simple input 2\"],\n-                    max_length=max_length,\n-                    pad_to_max_length=True,\n-                )\n-                self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n-\n                 input_r = tokenizer_r.batch_encode_plus(\n                     [\"This is a simple input 1\", \"This is a simple input 2\"],\n                     max_length=max_length,"
        }
    ],
    "stats": {
        "total": 551,
        "additions": 5,
        "deletions": 546
    }
}