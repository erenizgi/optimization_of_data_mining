{
    "author": "faaany",
    "message": "[docs] use device-agnostic API instead of hard-coded cuda (#35048)\n\nreplace cuda",
    "sha": "329f5dbf97a5cb2473914c88c05aa3dcb242e19a",
    "files": [
        {
            "sha": "13df87c4d8225469429f6e65366a263a9e684b40",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/329f5dbf97a5cb2473914c88c05aa3dcb242e19a/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/329f5dbf97a5cb2473914c88c05aa3dcb242e19a/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=329f5dbf97a5cb2473914c88c05aa3dcb242e19a",
            "patch": "@@ -63,7 +63,7 @@ model.generation_config.cache_implementation = \"static\"\n \n model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n \n outputs = model.generate(**input_ids)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -93,7 +93,7 @@ model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\n \n model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n prompt_length = input_ids.input_ids.shape[1]\n model.generation_config.max_new_tokens = 16\n \n@@ -126,14 +126,15 @@ If you want to go further down a level, the [`StaticCache`] object can also be p\n from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n from transformers.testing_utils import CaptureLogger\n import torch\n+from accelerate.test_utils.testing import get_backend\n \n prompts = [\n     \"Simply put, the theory of relativity states that \",\n     \"My favorite all time favorite condiment is ketchup.\",\n ]\n \n NUM_TOKENS_TO_GENERATE = 40\n-torch_device = \"cuda\"\n+torch_device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n \n tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\n model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\")\n@@ -205,7 +206,7 @@ model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\n \n model.generate = torch.compile(model.generate, mode=\"reduce-overhead\", fullgraph=True)\n input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n \n outputs = model.generate(**input_ids)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -241,8 +242,9 @@ Enable speculative decoding by loading an assistant model and passing it to the\n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n import torch\n+from accelerate.test_utils.testing import get_backend\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -262,8 +264,9 @@ For speculative sampling decoding, add the `do_sample` and `temperature` paramet\n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n import torch\n+from accelerate.test_utils.testing import get_backend\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -290,8 +293,9 @@ To enable prompt lookup decoding, specify the number of tokens that should be ov\n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n import torch\n+from accelerate.test_utils.testing import get_backend\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n@@ -311,8 +315,9 @@ For prompt lookup decoding with sampling, add the `do_sample` and `temperature`\n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n import torch\n+from accelerate.test_utils.testing import get_backend\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 13,
        "deletions": 8
    }
}