{
    "author": "MekkCyber",
    "message": "[kernels] rm yoso kernel (#41495)\n\n* disable kernel mapping\n\n* rm kernel\n\n* delete files\n\n* style\n\n* typo",
    "sha": "3585737746e5c73a37b6d43f429ca6f56f1e3da5",
    "files": [
        {
            "sha": "e5085c88dd3ea9a12eec264a8c48946bf2b80b23",
            "filename": "src/transformers/kernels/yoso/common.h",
            "status": "removed",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon.h?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,10 +0,0 @@\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-#define ceil_divide(a, b) ((a)/(b)+((a)%(b)!=0))\n-#define select(cond, a, b) ((cond)?(a):(b))\n-#define PI 3.141592\n-#define EPSILON 1e-8\n-#define MAX_VAL 1e12\n-#define MIN_VAL -1e12\n-#define EMPTY_VALUE -1"
        },
        {
            "sha": "97030870649a2fdac58cb26cf966e8f5c8cc7909",
            "filename": "src/transformers/kernels/yoso/common_cuda.h",
            "status": "removed",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda.h?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,9 +0,0 @@\n-\n-#define MAX_THREADS_PER_BLOCK 1024\n-#define OPTIMAL_THREADS_PER_BLOCK 256\n-#define WARP_SIZE 32\n-#define MAX_NUM_BLOCK_X 2147483647\n-#define MAX_NUM_BLOCK_Y 65535\n-#define MAX_NUM_BLOCK_Z 65535\n-#define MAX_SHARED_MEM_PER_BLOCK 48000\n-#define FULL_MASK 0xffffffff"
        },
        {
            "sha": "6674f93afdc25ab35c5d83881d00028bcf2989fc",
            "filename": "src/transformers/kernels/yoso/common_cuda_device.h",
            "status": "removed",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda_device.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda_device.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Fcommon_cuda_device.h?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,79 +0,0 @@\n-\n-#include \"common.h\"\n-\n-template<typename T>\n-__device__ int set_insert(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    T prev = atomicCAS(&set[slot], EMPTY_VALUE, value);\n-    if (prev == EMPTY_VALUE || prev == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ int set_lookup(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    if (set[slot] == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ void init_buffer(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void copy_data(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void init_buffer_nonblocking(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-}\n-\n-template<typename T>\n-__device__ void copy_data_nonblocking(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-}"
        },
        {
            "sha": "c6b13e6cb5f53c9c62e51d2c399a14d14dab7037",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 588,
            "changes": 588,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.cu?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,588 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation.cu\n-\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <vector>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_vector.size(0);\n-  int num_query = query_vector.size(1);\n-  int num_key = key_vector.size(1);\n-  int vector_dim = query_vector.size(2);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  int num_part = max(1, ceil_divide(num_hash_f, num_hash_per_part));\n-\n-  at::Tensor Dmat = 2 * at::randint(0, 2, {batch_size, 3, num_part, vector_dim}, query_mask.options()) - 1;\n-  at::Tensor query_hash_code = at::zeros({batch_size, num_query, num_hash_f}, query_mask.options());\n-  at::Tensor key_hash_code = at::zeros({batch_size, num_key, num_hash_f}, key_mask.options());\n-\n-  int *query_mask_ptr = query_mask.data_ptr<int>();\n-  float *query_vector_ptr = query_vector.data_ptr<float>();\n-  int *key_mask_ptr = key_mask.data_ptr<int>();\n-  float *key_vector_ptr = key_vector.data_ptr<float>();\n-\n-  int *Dmat_ptr = Dmat.data_ptr<int>();\n-\n-  int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-  int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-\n-  if (use_cuda) {\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_query, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_vector_ptr,\n-        Dmat_ptr,\n-        query_hash_code_ptr,\n-        batch_size,\n-        num_query,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_key, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        key_mask_ptr,\n-        key_vector_ptr,\n-        Dmat_ptr,\n-        key_hash_code_ptr,\n-        batch_size,\n-        num_key,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-  }\n-\n-  return {query_hash_code, key_hash_code};\n-\n-}\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-\n-  at::Tensor hashtable_value = at::empty({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-\n-      cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-      lsh_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        value_ptr,\n-        hashtable_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key,\n-        value_dim,\n-        value_offset\n-      );\n-\n-      lsh_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        hashtable_value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query,\n-        value_dim,\n-        value_offset\n-      );\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor hashtable_value = at::zeros({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-      for (int weight_idx = 0; weight_idx < weight_dim; weight_idx++) {\n-\n-        cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-        lsh_weighted_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-          key_mask_ptr,\n-          key_hash_code_ptr,\n-          key_weight_ptr,\n-          value_ptr,\n-          hashtable_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_key,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-\n-        lsh_weighted_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-          query_mask_ptr,\n-          query_hash_code_ptr,\n-          query_weight_ptr,\n-          hashtable_value_ptr,\n-          cumulation_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_query,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-      }\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor key_sorted_idxes = at::zeros({batch_size, num_hash_f, num_key}, query_hash_code.options());\n-  at::Tensor query_info = at::zeros({batch_size, num_query, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *key_sorted_idxes_ptr = key_sorted_idxes.data_ptr<int>();\n-    int *query_info_ptr = query_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_query, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver2_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_info_ptr,\n-        key_sorted_idxes_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + value_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver3_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, batch_size);\n-      int shared_mem = (weight_dim + value_dim + 2 * num_hash_f) * sizeof(float);\n-      lsh_weighted_cumulation_ver4_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}"
        },
        {
            "sha": "dd48de0ed159f49ee3afe93b12aaae719fe87688",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.h",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation.h?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,71 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);"
        },
        {
            "sha": "22944e97044659f896451936c6253d5aadd7a769",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 825,
            "changes": 825,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,825 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation_cuda.cu\n-\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda_device.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <stdio.h>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-inline __device__ void fast_hadamard_transform(float *vector_buffer, int vector_dim, int dim_idx) {\n-  int stride = vector_dim / 2;\n-  while (stride > (WARP_SIZE / 2)) {\n-    __syncthreads();\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    float val1 = vector_buffer[dim_idx];\n-    float val2 = vector_buffer[dim_idx + sign * stride];\n-    __syncthreads();\n-    vector_buffer[dim_idx] = float(sign) * val1 + val2;\n-    stride = stride / 2;\n-  }\n-\n-  float val = vector_buffer[dim_idx];\n-  #pragma unroll\n-  for (stride = (WARP_SIZE / 2); stride > 0; stride = stride / 2) {\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    val = float(sign) * val + __shfl_xor_sync(FULL_MASK, val, stride);\n-  }\n-  vector_buffer[dim_idx] = val;\n-}\n-\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [batch_size, 3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int vector_idx = blockIdx.y;\n-  int part_idx = blockIdx.x;\n-\n-  int dim_idx = threadIdx.x;\n-\n-  int batch_idx__vector_idx = batch_idx * num_vector + vector_idx;\n-  if (mask[batch_idx__vector_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *vector_buffer = buffer;\n-\n-  vector_buffer[dim_idx] = vector[batch_idx__vector_idx * vector_dim + dim_idx];\n-\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 0) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 1) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 2) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  if (hash_code_len == 8 || hash_code_len == 16) {\n-    int code = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    for (int offset = 1; offset < hash_code_len; offset = offset * 2) {\n-      code += __shfl_xor_sync(FULL_MASK, code, offset);\n-    }\n-    if (dim_idx % hash_code_len == 0) {\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx / hash_code_len;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  } else {\n-    vector_buffer[dim_idx] = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    __syncthreads();\n-    if (dim_idx < num_hash_per_part) {\n-      int code = 0;\n-      for (int i = 0; i < hash_code_len; i++) {\n-        code += vector_buffer[dim_idx * hash_code_len + i];\n-      }\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  }\n-}\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  atomicAdd(&count_sort_table[(batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code], 1);\n-\n-}\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int hash_f_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.x;\n-  int thread_id = threadIdx.x;\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  extern __shared__ float buffer[];\n-  int *table_buffer = (int*)buffer;\n-\n-  if (thread_id == 0) {\n-    table_buffer[0] = 0;\n-  }\n-  copy_data<int>(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], &table_buffer[1], hashtable_capacity - 1, num_threads, thread_id);\n-\n-  for (int table_idx_start = 0; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + num_threads) {\n-    int thread_value = table_buffer[table_idx_start + thread_id];\n-    int next_thread_value = 0;\n-    for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-      next_thread_value = __shfl_up_sync(FULL_MASK, thread_value, offset);\n-      if (thread_id % WARP_SIZE >= offset) {\n-        thread_value = thread_value + next_thread_value;\n-      }\n-    }\n-    table_buffer[table_idx_start + thread_id] = thread_value;\n-  }\n-  __syncthreads();\n-\n-  if (hashtable_capacity > WARP_SIZE) {\n-    if (thread_id < WARP_SIZE) {\n-      for (int table_idx_start = WARP_SIZE; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + WARP_SIZE) {\n-        table_buffer[table_idx_start + thread_id] += table_buffer[table_idx_start - 1];\n-      }\n-    }\n-  }\n-\n-  copy_data<int>(table_buffer, &count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], hashtable_capacity, num_threads, thread_id);\n-\n-}\n-\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  int sort_idx = atomicAdd(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity + hash_code], 1);\n-  key_sorted_idxes[batch_idx__hash_f_idx * num_key + sort_idx] = key_idx;\n-\n-}\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_idx];\n-  int batch_idx__hash_f_idx__hash_code = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code;\n-\n-  int key_offset = select(hash_code == 0, 0, count_sort_table[batch_idx__hash_f_idx__hash_code - 1]);\n-  int key_count = count_sort_table[batch_idx__hash_f_idx__hash_code] - key_offset;\n-\n-  query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx] = key_offset;\n-  query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx] = key_count;\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int query_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int key_offset = query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx];\n-  int key_count = query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (key_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (key_count == 1) {\n-    if (warp_idx == 0) {\n-      int key_idx = key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset];\n-      int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    int *key_idxes_buffer = (int*)&buffer[weight_dim];\n-\n-    copy_data_nonblocking<float>(&query_weight[batch_idx__query_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-\n-    while (key_count > 0) {\n-      int work_size = min(WARP_SIZE, key_count);\n-      copy_data_nonblocking<int>(&key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset], key_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < key_count) {\n-          int key_idx = key_idxes_buffer[work_idx];\n-          int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      key_count = key_count - work_size;\n-      key_offset = key_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int query_offset = key_info[batch_idx__key_idx * 2 * num_hash_f + hash_f_idx];\n-  int query_count = key_info[(batch_idx__key_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (query_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (query_count == 1) {\n-    if (warp_idx == 0) {\n-      int query_idx = query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset];\n-      int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    float *value_buffer = &buffer[weight_dim];\n-    int *query_idxes_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-    copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-    copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-\n-    while (query_count > 0) {\n-      int work_size = min(WARP_SIZE, query_count);\n-      copy_data_nonblocking<int>(&query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset], query_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < query_count) {\n-          int query_idx = query_idxes_buffer[work_idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      query_count = query_count - work_size;\n-      query_offset = query_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *weight_buffer = buffer;\n-  float *value_buffer = &buffer[weight_dim];\n-  int *key_info_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-  copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-  copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-  copy_data_nonblocking<int>(&key_info[batch_idx__key_idx * 2 * num_hash_f], key_info_buffer, 2 * num_hash_f, num_threads, thread_id);\n-\n-  int *query_offset_buffer = key_info_buffer;\n-  int *query_count_buffer = &key_info_buffer[num_hash_f];\n-\n-  const int hashtable_size = 1024 + OPTIMAL_THREADS_PER_BLOCK;\n-  __shared__ int hashtable_query[hashtable_size];\n-  __shared__ int hashtable_count[hashtable_size];\n-  __shared__ int inserted_query[hashtable_size];\n-  __shared__ int query_counter[1];\n-\n-  int hash_f_idx_base = 0;\n-\n-  while (true) {\n-\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, hashtable_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, hashtable_count, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, inserted_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, query_counter, 1, num_threads, thread_id);\n-    __syncthreads();\n-\n-    while (hash_f_idx_base < num_hash_f) {\n-\n-      int hash_f_idx = hash_f_idx_base + warp_idx;\n-      int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-      int stop_flag = 0;\n-\n-      int query_offset = query_offset_buffer[hash_f_idx];\n-      int query_count = query_count_buffer[hash_f_idx];\n-\n-      while (query_count > 0) {\n-\n-        int work_size = min(query_count, WARP_SIZE);\n-\n-        // try inserting query to set and check whether the query is new\n-        int found_new_query = 0;\n-        int query_idx = -1;\n-        if (warp_thread_idx < work_size) {\n-          query_idx = query_sorted_idxes[batch_idx__hash_f_idx * num_query + query_offset + warp_thread_idx];\n-          int slot = set_insert<int>(hashtable_query, hashtable_size, query_idx);\n-          if (slot >= 0) {\n-            found_new_query = atomicAdd(&hashtable_count[slot], 1) == 0;\n-          }\n-        }\n-\n-        // compute cumulative offset\n-        int position_offset = found_new_query;\n-        int next_position_offset = 0;\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          next_position_offset = __shfl_up_sync(FULL_MASK, position_offset, offset);\n-          if (thread_id % WARP_SIZE >= offset) {\n-            position_offset = position_offset + next_position_offset;\n-          }\n-        }\n-\n-        // get the inserted query list end index\n-        int inserted_query_base = 0;\n-        if (thread_id % WARP_SIZE == WARP_SIZE - 1) {\n-          inserted_query_base = atomicAdd(query_counter, position_offset);\n-        }\n-        inserted_query_base = __shfl_sync(FULL_MASK, inserted_query_base, WARP_SIZE - 1);\n-\n-        // insert new queries to list\n-        int insert_idx = inserted_query_base + position_offset - 1;\n-        if (found_new_query) {\n-          inserted_query[insert_idx] = query_idx;\n-        }\n-\n-        // remove inserted queries from list\n-        query_offset_buffer[hash_f_idx] += work_size;\n-        query_count_buffer[hash_f_idx] -= work_size;\n-        query_offset += work_size;\n-        query_count -= work_size;\n-\n-        // if list is almost full, stop inserting\n-        if (inserted_query_base + OPTIMAL_THREADS_PER_BLOCK > hashtable_size) {\n-          stop_flag = 1;\n-          break;\n-        }\n-\n-      }\n-\n-      if (stop_flag) {\n-        break;\n-      }\n-\n-      hash_f_idx_base = hash_f_idx_base + num_warps;\n-\n-    }\n-\n-    __syncthreads();\n-\n-    int num_distinct_query = query_counter[0];\n-\n-    if (num_distinct_query > 0) {\n-      for (int idx_base = 0; idx_base < num_distinct_query; idx_base = idx_base + num_warps) {\n-        int idx = idx_base + warp_idx;\n-        if (idx < num_distinct_query) {\n-          int query_idx = inserted_query[idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-\n-          int slot = set_lookup<int>(hashtable_query, hashtable_size, query_idx);\n-          int duplicate_count = hashtable_count[slot];\n-\n-          float weight = 0;\n-          for (int weight_idx_base = 0; weight_idx_base < weight_dim; weight_idx_base = weight_idx_base + WARP_SIZE) {\n-            int weight_dim_idx = weight_idx_base + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-\n-          weight = (float)duplicate_count * weight / float(num_hash_f);\n-\n-          for (int value_idx_base = 0; value_idx_base < value_dim; value_idx_base = value_idx_base + WARP_SIZE) {\n-            int value_dim_idx = value_idx_base + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-    } else {\n-\n-      // all computation is completed if num_distinct_query == 0\n-      break;\n-\n-    }\n-\n-    __syncthreads();\n-\n-  }\n-\n-}"
        },
        {
            "sha": "b2adc0f735358d0fcb6a056e7d19ba745977e129",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.h",
            "status": "removed",
            "additions": 0,
            "deletions": 157,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.h?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,157 +0,0 @@\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-);\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-);\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);"
        },
        {
            "sha": "e150a2be604b28f600ab345a8cc9e97819cca416",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_torch.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 128,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_torch.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_torch.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_torch.cpp?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -1,128 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"common_cuda.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda,\n-  int version\n-) {\n-  return fast_hash_ver1_kernel(\n-    query_mask,\n-    query_vector,\n-    key_mask,\n-    key_vector,\n-    num_hash_f,\n-    hash_code_len,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  return lsh_cumulation_ver1_kernel(\n-    query_mask,\n-    query_hash_code,\n-    key_mask,\n-    key_hash_code,\n-    value,\n-    hashtable_capacity,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_weighted_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor query_weight,       // [batch_size, num_query, weight_dim]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor key_weight,         // [batch_size, num_key, weight_dim]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  if (version == 1) {\n-    return lsh_weighted_cumulation_ver1_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 2) {\n-    return lsh_weighted_cumulation_ver2_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 3) {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 4) {\n-    return lsh_weighted_cumulation_ver4_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  }\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"fast_hash\", &fast_hash, \"Fast Hash (CUDA)\");\n-  m.def(\"lsh_cumulation\", &lsh_cumulation, \"LSH Cumulation (CUDA)\");\n-  m.def(\"lsh_weighted_cumulation\", &lsh_weighted_cumulation, \"LSH Weighted Cumulation (CUDA)\");\n-}"
        },
        {
            "sha": "ac79fe54b4c40f366076a4b30509218d3d3eacda",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3585737746e5c73a37b6d43f429ca6f56f1e3da5/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3585737746e5c73a37b6d43f429ca6f56f1e3da5/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=3585737746e5c73a37b6d43f429ca6f56f1e3da5",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch YOSO model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -36,6 +35,7 @@\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -51,17 +51,12 @@\n \n def load_cuda_kernels():\n     global lsh_cumulation\n-    from torch.utils.cpp_extension import load\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    def append_root(files):\n-        src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"yoso\"\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"fast_lsh_cumulation_torch.cpp\", \"fast_lsh_cumulation.cu\", \"fast_lsh_cumulation_cuda.cu\"])\n-\n-    load(\"fast_lsh_cumulation\", src_files, verbose=True)\n-\n-    import fast_lsh_cumulation as lsh_cumulation\n+    yoso = get_kernel(\"kernels-community/yoso\")\n+    lsh_cumulation = yoso.lsh_cumulation\n \n \n def to_contiguous(input_tensors):"
        }
    ],
    "stats": {
        "total": 1884,
        "additions": 6,
        "deletions": 1878
    }
}