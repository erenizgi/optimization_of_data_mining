{
    "author": "lambertwjh",
    "message": "Add glm4.5&&glm4.5V doc (#40095)\n\n* Docs: GLM-4-MoE & GLM-4V-MoE pages\n\n* Docs: polish GLM-4V-MoE intro, remove placeholders; pin image\n\n* Docs\n\n---------\n\nCo-authored-by: wujiahan <lambert@gmail.com>",
    "sha": "f6b6e177198264bf5d24ac4889cdc15ed9429fc9",
    "files": [
        {
            "sha": "9ec764ec34487d087666d3262b563b505ca7151e",
            "filename": "docs/source/en/model_doc/glm4_moe.md",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6b6e177198264bf5d24ac4889cdc15ed9429fc9/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6b6e177198264bf5d24ac4889cdc15ed9429fc9/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md?ref=f6b6e177198264bf5d24ac4889cdc15ed9429fc9",
            "patch": "@@ -18,7 +18,21 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-This will update After model release.\n+The [**GLM-4.5**](https://arxiv.org/abs/2508.06471) series models are foundation models designed for intelligent agents, MoE variants are documented here as Glm4Moe.\n+\n+GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n+\n+Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n+\n+We have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n+\n+As demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of **63.2**, in the **3rd** place among all the proprietary and open-source models. Notably, GLM-4.5-Air delivers competitive results at **59.8** while maintaining superior efficiency.\n+\n+![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png)\n+\n+For more eval results, show cases, and technical details, please visit our [technical report](https://arxiv.org/abs/2508.06471) or [technical blog](https://z.ai/blog/glm-4.5).\n+\n+The model code, tool parser and reasoning parser can be found in the implementation of [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4_moe), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/glm4_moe_mtp.py) and [SGLang](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/glm4_moe.py).\n \n ## Glm4MoeConfig\n "
        },
        {
            "sha": "e951b02fecb74fc1179625c7ced023bfeeba5e3a",
            "filename": "docs/source/en/model_doc/glm4v_moe.md",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6b6e177198264bf5d24ac4889cdc15ed9429fc9/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6b6e177198264bf5d24ac4889cdc15ed9429fc9/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md?ref=f6b6e177198264bf5d24ac4889cdc15ed9429fc9",
            "patch": "@@ -25,20 +25,22 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Glm4vMoe model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n-<INSERT SHORT SUMMARY HERE>\n+Vision-language models (VLMs) have become a key cornerstone of intelligent systems. As real-world AI tasks grow increasingly complex, VLMs urgently need to enhance reasoning capabilities beyond basic multimodal perception — improving accuracy, comprehensiveness, and intelligence — to enable complex problem solving, long-context understanding, and multimodal agents.\n \n-The abstract from the paper is the following:\n+Through our open-source work, we aim to explore the technological frontier together with the community while empowering more developers to create exciting and innovative applications.\n \n-*<INSERT PAPER ABSTRACT HERE>*\n+[GLM-4.5V](https://github.com/zai-org/GLM-V) is based on ZhipuAI’s next-generation flagship text foundation model GLM-4.5-Air (106B parameters, 12B active).  It continues the technical approach of [GLM-4.1V-Thinking](https://arxiv.org/abs/2507.01006), achieving SOTA performance among models of the same scale on 42 public vision-language benchmarks.  It covers common tasks such as image, video, and document understanding, as well as GUI agent operations.\n \n-Tips:\n+![bench_45](https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/bench_45v.jpeg)\n \n-<INSERT TIPS ABOUT MODEL HERE>\n-\n-This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).\n-The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n+Beyond benchmark performance, GLM-4.5V focuses on real-world usability. Through efficient hybrid training, it can handle diverse types of visual content, enabling full-spectrum vision reasoning, including:\n+- **Image reasoning** (scene understanding, complex multi-image analysis, spatial recognition)\n+- **Video understanding** (long video segmentation and event recognition)\n+- **GUI tasks** (screen reading, icon recognition, desktop operation assistance)\n+- **Complex chart & long document parsing** (research report analysis, information extraction)\n+- **Grounding** (precise visual element localization)\n \n+The model also introduces a **Thinking Mode** switch, allowing users to balance between quick responses and deep reasoning. This switch works the same as in the `GLM-4.5` language model.\n \n ## Glm4vMoeConfig\n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 26,
        "deletions": 10
    }
}