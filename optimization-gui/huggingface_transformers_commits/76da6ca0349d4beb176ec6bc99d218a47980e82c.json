{
    "author": "gante",
    "message": "Pipeline: simple API for assisted generation (#34504)\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "76da6ca0349d4beb176ec6bc99d218a47980e82c",
    "files": [
        {
            "sha": "4c4c19a3d6628d1efc6018a7db63a50e2b4ab464",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -441,6 +441,28 @@ To enable assisted decoding, set the `assistant_model` argument with a model.\n ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n ```\n \n+<Tip>\n+\n+If you're using a `pipeline` object, all you need to do is to pass the assistant checkpoint under `assistant_model`\n+\n+```python\n+>>> from transformers import pipeline\n+>>> import torch\n+\n+>>> pipe = pipeline(\n+...     \"text-generation\",\n+...     model=\"meta-llama/Llama-3.1-8B\",\n+...     assistant_model=\"meta-llama/Llama-3.2-1B\",  # This extra line is all that's needed, also works with UAD\n+...     torch_dtype=torch.bfloat16\n+>>> )\n+>>> pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n+>>> pipe_output[0][\"generated_text\"]\n+'Once upon a time, 3D printing was a niche technology that was only'\n+```\n+\n+</Tip>\n+\n+\n When using assisted decoding with sampling methods, you can use the `temperature` argument to control the randomness,\n just like in multinomial sampling. However, in assisted decoding, reducing the temperature may help improve the latency.\n "
        },
        {
            "sha": "134666d45a3a9e572df0503281af7272a3afaf9d",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -347,7 +347,6 @@ def generate(\n             eos_token_id = generation_config.eos_token_id\n             if isinstance(eos_token_id, list):\n                 eos_token_id = eos_token_id[0]\n-            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n             generation_config.pad_token_id = eos_token_id\n \n         if generation_config.decoder_start_token_id is None and self.config.is_encoder_decoder:"
        },
        {
            "sha": "dd1f819fe548aa589ac11c8d3e3ef3338d3d208c",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -773,7 +773,6 @@ def generate(\n             eos_token_id = generation_config.eos_token_id\n             if isinstance(eos_token_id, list):\n                 eos_token_id = eos_token_id[0]\n-            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n             generation_config.pad_token_id = eos_token_id\n \n         use_xla = not tf.executing_eagerly()"
        },
        {
            "sha": "66a9c49ea5f3516053fa9d5835109dcd53e3ff1a",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -348,6 +348,12 @@ def _sanitize_parameters(\n                 raise ValueError(\"Only Whisper can return language for now.\")\n             postprocess_params[\"return_language\"] = return_language\n \n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n         return preprocess_params, forward_params, postprocess_params\n \n     def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):"
        },
        {
            "sha": "a24e9c3f69787849de363dc501666d511e84ee13",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 64,
            "deletions": 3,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -33,7 +33,7 @@\n from ..feature_extraction_utils import PreTrainedFeatureExtractor\n from ..image_processing_utils import BaseImageProcessor\n from ..modelcard import ModelCard\n-from ..models.auto.configuration_auto import AutoConfig\n+from ..models.auto import AutoConfig, AutoTokenizer\n from ..processing_utils import ProcessorMixin\n from ..tokenization_utils import PreTrainedTokenizer\n from ..utils import (\n@@ -425,6 +425,62 @@ def get_default_model_and_revision(\n     return default_models[framework]\n \n \n+def load_assistant_model(\n+    model: \"PreTrainedModel\",\n+    assistant_model: Optional[Union[str, \"PreTrainedModel\"]],\n+    assistant_tokenizer: Optional[PreTrainedTokenizer],\n+) -> Tuple[Optional[\"PreTrainedModel\"], Optional[PreTrainedTokenizer]]:\n+    \"\"\"\n+    Prepares the assistant model and the assistant tokenizer for a pipeline whose model that can call `generate`.\n+\n+    Args:\n+        model ([`PreTrainedModel`]):\n+            The main model that will be used by the pipeline to make predictions.\n+        assistant_model (`str` or [`PreTrainedModel`], *optional*):\n+            The assistant model that will be used by the pipeline to make predictions.\n+        assistant_tokenizer ([`PreTrainedTokenizer`], *optional*):\n+            The assistant tokenizer that will be used by the pipeline to encode data for the model.\n+\n+    Returns:\n+        Tuple: The loaded assistant model and (optionally) the loaded tokenizer.\n+    \"\"\"\n+    if not model.can_generate() or assistant_model is None:\n+        return None, None\n+\n+    if not isinstance(model, PreTrainedModel):\n+        raise ValueError(\n+            \"Assisted generation, triggered by the `assistant_model` argument, is only available for \"\n+            \"`PreTrainedModel` model instances. For instance, TF or JAX models are not supported.\"\n+        )\n+\n+    # If the model is passed as a string, load the model and the corresponding tokenizer\n+    if isinstance(assistant_model, str):\n+        assistant_config = AutoConfig.from_pretrained(assistant_model)\n+        _, loaded_assistant_model = infer_framework_load_model(assistant_model, config=assistant_config)\n+        loaded_assistant_model = loaded_assistant_model.to(device=model.device, dtype=model.dtype)\n+        loaded_assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_model)\n+    else:\n+        loaded_assistant_model = assistant_model\n+        loaded_assistant_tokenizer = assistant_tokenizer\n+\n+    # Finally, let's check the tokenizers: if the two models have different tokenizers, we need to keep the assistant\n+    # tokenizer\n+    same_vocab_size = model.config.vocab_size == loaded_assistant_model.config.vocab_size\n+    same_special_tokens = all(\n+        getattr(model.config, token) == getattr(loaded_assistant_model.config, token)\n+        for token in (\"eos_token_id\", \"pad_token_id\", \"bos_token_id\")\n+    )\n+    if same_vocab_size and same_special_tokens:\n+        loaded_assistant_tokenizer = None\n+    elif loaded_assistant_tokenizer is None:\n+        raise ValueError(\n+            \"The assistant model has a different tokenizer than the main model. You should pass the assistant \"\n+            \"tokenizer.\"\n+        )\n+\n+    return loaded_assistant_model, loaded_assistant_tokenizer\n+\n+\n class PipelineException(Exception):\n     \"\"\"\n     Raised by a [`Pipeline`] when handling __call__.\n@@ -925,8 +981,13 @@ def __init__(\n         ):\n             self.model.to(self.device)\n \n-        # If the model can generate, create a local generation config. This is done to avoid side-effects on the model\n-        # as we apply local tweaks to the generation config.\n+        # If the model can generate:\n+        # 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\n+        # tweaks to the generation config.\n+        # 2 - load the assistant model if it is passed.\n+        self.assistant_model, self.assistant_tokenizer = load_assistant_model(\n+            self.model, kwargs.pop(\"assistant_model\", None), kwargs.pop(\"assistant_tokenizer\", None)\n+        )\n         if self.model.can_generate():\n             self.prefix = self.model.config.prefix if hasattr(self.model.config, \"prefix\") else None\n             self.generation_config = copy.deepcopy(self.model.generation_config)"
        },
        {
            "sha": "c176d841e29fa6c6bb8c6867562f985d181c7138",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -189,7 +189,14 @@ def _sanitize_parameters(\n         if handle_impossible_answer is not None:\n             postprocess_params[\"handle_impossible_answer\"] = handle_impossible_answer\n \n-        return preprocess_params, {}, postprocess_params\n+        forward_params = {}\n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n+        return preprocess_params, forward_params, postprocess_params\n \n     def __call__(\n         self,"
        },
        {
            "sha": "32a3ec218dac305f93d8e41959200a78c590c8df",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -92,6 +92,12 @@ def _sanitize_parameters(self, max_new_tokens=None, generate_kwargs=None, prompt\n                 )\n             forward_params.update(generate_kwargs)\n \n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n         return preprocess_params, forward_params, {}\n \n     def __call__(self, inputs: Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]] = None, **kwargs):"
        },
        {
            "sha": "10ea7170fed40cbc6f14c8b712741ce570fbf3f7",
            "filename": "src/transformers/pipelines/table_question_answering.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftable_question_answering.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -358,6 +358,13 @@ def _sanitize_parameters(self, sequential=None, padding=None, truncation=None, *\n         forward_params = {}\n         if sequential is not None:\n             forward_params[\"sequential\"] = sequential\n+\n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n         return preprocess_params, forward_params, {}\n \n     def preprocess(self, pipeline_input, sequential=None, padding=True, truncation=None):"
        },
        {
            "sha": "9bc7544550286ecb2ad2108d7dffb142cc123877",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -106,6 +106,12 @@ def _sanitize_parameters(\n                 )\n             generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n \n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n         return preprocess_params, forward_params, postprocess_params\n \n     def check_inputs(self, input_length: int, min_length: int, max_length: int):"
        },
        {
            "sha": "c0f14663ffdf5876d1aa4612cf54432974049606",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -1,7 +1,6 @@\n import enum\n import itertools\n import types\n-import warnings\n from typing import Dict\n \n from ..utils import add_end_docstrings, is_tf_available, is_torch_available\n@@ -194,12 +193,13 @@ def _sanitize_parameters(\n \n         if stop_sequence is not None:\n             stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n-            if len(stop_sequence_ids) > 1:\n-                warnings.warn(\n-                    \"Stopping on a multiple token sequence is not yet supported on transformers. The first token of\"\n-                    \" the stop sequence will be used as the stop sequence string in the interim.\"\n-                )\n-            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n+            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids\n+\n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n \n         return preprocess_params, forward_params, postprocess_params\n "
        },
        {
            "sha": "b7beca586d21957b2eb3ec2dbb7daa2c49453970",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -148,10 +148,9 @@ def _forward(self, model_inputs, **kwargs):\n         else:\n             if len(generate_kwargs):\n                 raise ValueError(\n-                    f\"\"\"You're using the `TextToAudioPipeline` with a forward-only model, but `generate_kwargs` is non empty.\n-                                 For forward-only TTA models, please use `forward_params` instead of of\n-                                 `generate_kwargs`. For reference, here are the `generate_kwargs` used here:\n-                                 {generate_kwargs.keys()}\"\"\"\n+                    \"You're using the `TextToAudioPipeline` with a forward-only model, but `generate_kwargs` is non \"\n+                    \"empty. For forward-only TTA models, please use `forward_params` instead of `generate_kwargs`. \"\n+                    f\"For reference, the `generate_kwargs` used here are: {generate_kwargs.keys()}\"\n                 )\n             output = self.model(**model_inputs, **forward_params)[0]\n \n@@ -191,6 +190,12 @@ def _sanitize_parameters(\n         forward_params=None,\n         generate_kwargs=None,\n     ):\n+        if self.assistant_model is not None:\n+            generate_kwargs[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            generate_kwargs[\"tokenizer\"] = self.tokenizer\n+            generate_kwargs[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n         params = {\n             \"forward_params\": forward_params if forward_params else {},\n             \"generate_kwargs\": generate_kwargs if generate_kwargs else {},"
        },
        {
            "sha": "6d600c9eaf50bc99f6810b0c2836b154cd62ed51",
            "filename": "src/transformers/pipelines/visual_question_answering.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -66,7 +66,15 @@ def _sanitize_parameters(self, top_k=None, padding=None, truncation=None, timeou\n             preprocess_params[\"timeout\"] = timeout\n         if top_k is not None:\n             postprocess_params[\"top_k\"] = top_k\n-        return preprocess_params, {}, postprocess_params\n+\n+        forward_params = {}\n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+\n+        return preprocess_params, forward_params, postprocess_params\n \n     def __call__(\n         self,"
        },
        {
            "sha": "da57a002c4f5bc370b7716c0bf82c16584209759",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -1933,6 +1933,20 @@ def test_slow_unfinished_sequence(self):\n             },\n         )\n \n+    @require_torch\n+    def test_pipeline_assisted_generation(self):\n+        \"\"\"Tests that we can run assisted generation in the pipeline\"\"\"\n+        model = \"openai/whisper-tiny\"\n+        pipe = pipeline(\"automatic-speech-recognition\", model=model, assistant_model=model)\n+\n+        # We can run the pipeline\n+        prompt = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")[\"audio\"]\n+        _ = pipe(prompt)\n+\n+        # It is running assisted generation under the hood (e.g. flags incompatible with assisted gen will crash)\n+        with self.assertRaises(ValueError):\n+            _ = pipe(prompt, generate_kwargs={\"num_beams\": 2})\n+\n \n def require_ffmpeg(test_case):\n     \"\"\""
        },
        {
            "sha": "d5014586b331c123625991298a66a99807a35c0f",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/76da6ca0349d4beb176ec6bc99d218a47980e82c/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=76da6ca0349d4beb176ec6bc99d218a47980e82c",
            "patch": "@@ -653,3 +653,17 @@ def test_pipeline_length_setting_warning(self):\n         with CaptureLogger(logger) as cl:\n             _ = text_generator(prompt, max_length=10)\n         self.assertNotIn(logger_msg, cl.out)\n+\n+    @require_torch\n+    def test_pipeline_assisted_generation(self):\n+        \"\"\"Tests that we can run assisted generation in the pipeline\"\"\"\n+        model = \"hf-internal-testing/tiny-random-MistralForCausalLM\"\n+        pipe = pipeline(\"text-generation\", model=model, assistant_model=model)\n+\n+        # We can run the pipeline\n+        prompt = \"Hello world\"\n+        _ = pipe(prompt)\n+\n+        # It is running assisted generation under the hood (e.g. flags incompatible with assisted gen will crash)\n+        with self.assertRaises(ValueError):\n+            _ = pipe(prompt, generate_kwargs={\"num_beams\": 2})"
        }
    ],
    "stats": {
        "total": 190,
        "additions": 172,
        "deletions": 18
    }
}