{
    "author": "simonreise",
    "message": "Add fast version of `convert_segmentation_map_to_binary_masks` to EoMT (#43073)\n\n* Add fast version of convert_segmentation_map_to_binary_masks\n\n* Reformat\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "98578fc62260130652be4725a5597ad257bfe800",
    "files": [
        {
            "sha": "1b595077ae8b79e20e8c74a929122a5e42f26217",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/98578fc62260130652be4725a5597ad257bfe800/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98578fc62260130652be4725a5597ad257bfe800/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=98578fc62260130652be4725a5597ad257bfe800",
            "patch": "@@ -44,12 +44,43 @@\n from .image_processing_eomt import (\n     EomtImageProcessorKwargs,\n     compute_segments,\n-    convert_segmentation_map_to_binary_masks,\n     get_size_with_aspect_ratio,\n     remove_low_and_no_objects,\n )\n \n \n+# Adapted from transformers.models.maskformer.image_processing_maskformer_fast.convert_segmentation_map_to_binary_masks_fast\n+def convert_segmentation_map_to_binary_masks_fast(\n+    segmentation_map: \"torch.Tensor\",\n+    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+    ignore_index: Optional[int] = None,\n+):\n+    if ignore_index is not None:\n+        segmentation_map = torch.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n+\n+    all_labels = torch.unique(segmentation_map)\n+\n+    if ignore_index is not None:\n+        all_labels = all_labels[all_labels != ignore_index]  # drop background label if applicable\n+\n+    binary_masks = [(segmentation_map == i) for i in all_labels]\n+    if binary_masks:\n+        binary_masks = torch.stack(binary_masks, dim=0)\n+    else:\n+        binary_masks = torch.zeros((0, *segmentation_map.shape), device=segmentation_map.device)\n+\n+    # Convert instance ids to class ids\n+    if instance_id_to_semantic_id is not None:\n+        labels = torch.zeros(all_labels.shape[0], device=segmentation_map.device)\n+\n+        for i, label in enumerate(all_labels):\n+            class_id = instance_id_to_semantic_id[(label.item() + 1 if ignore_index is not None else label.item())]\n+            labels[i] = class_id - 1 if ignore_index is not None else class_id\n+    else:\n+        labels = all_labels\n+    return binary_masks.float(), labels.long()\n+\n+\n def get_target_size(size_dict: dict[str, int]) -> tuple[int, int]:\n     \"\"\"Returns the height and width from a size dict.\"\"\"\n     target_height = size_dict[\"shortest_edge\"]\n@@ -194,14 +225,14 @@ def _preprocess_image_like_inputs(\n                 else:\n                     instance_id = instance_id_to_semantic_id\n                 # Use instance2class_id mapping per image\n-                masks, classes = convert_segmentation_map_to_binary_masks(\n+                masks, classes = convert_segmentation_map_to_binary_masks_fast(\n                     segmentation_map,\n                     instance_id,\n                     ignore_index=ignore_index,\n                 )\n \n-                mask_labels.append(torch.from_numpy(masks))\n-                class_labels.append(torch.from_numpy(classes))\n+                mask_labels.append(masks)\n+                class_labels.append(classes)\n \n             # we cannot batch them since they don't share a common class size\n             outputs[\"mask_labels\"] = mask_labels"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 35,
        "deletions": 4
    }
}