{
    "author": "Cyrilvallez",
    "message": "Fix image token mask in Gemma3 (#38295)\n\nfix mask",
    "sha": "b5ececb900f562a1cf507efb4310984bb7bc3205",
    "files": [
        {
            "sha": "08740173009d1e937e5acb6c6f5b9b89933cd913",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5ececb900f562a1cf507efb4310984bb7bc3205/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5ececb900f562a1cf507efb4310984bb7bc3205/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=b5ececb900f562a1cf507efb4310984bb7bc3205",
            "patch": "@@ -782,7 +782,7 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Optional[Callable]:\n+def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_per_image: int) -> Optional[Callable]:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n     not start and end indices.\n@@ -792,8 +792,13 @@ def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Opti\n         return None\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        # If it's 1, we need to unmask it\n-        return token_type_ids[batch_idx, kv_idx] == 1\n+        # If the difference is less than image size, both are part of the same image block\n+        same_image_block = torch.abs(kv_idx - q_idx) <= tokens_per_image\n+        # If it's 1 for both query and key/value, we are in an image block\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids[batch_idx, kv_idx] == 1)\n+\n+        # This is bidirectional attention whenever we are dealing with image tokens\n+        return is_image_block & same_image_block\n \n     return inner_mask\n \n@@ -945,7 +950,7 @@ def forward(\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n                 mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device)\n+                    token_type_ids.to(cache_position.device), self.config.mm_tokens_per_image\n                 )\n \n             # Create the masks\n@@ -1211,7 +1216,9 @@ def create_masks_for_generate(\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:\n             # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(token_type_ids.to(cache_position.device))\n+            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+                token_type_ids.to(cache_position.device), config.mm_tokens_per_image\n+            )\n \n         return create_masks_for_generate(**mask_kwargs)\n "
        },
        {
            "sha": "d679d30c8b9452e8b1841eaf8efc75b9f394be0c",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5ececb900f562a1cf507efb4310984bb7bc3205/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5ececb900f562a1cf507efb4310984bb7bc3205/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=b5ececb900f562a1cf507efb4310984bb7bc3205",
            "patch": "@@ -722,7 +722,7 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Optional[Callable]:\n+def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_per_image: int) -> Optional[Callable]:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n     not start and end indices.\n@@ -732,8 +732,13 @@ def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Opti\n         return None\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        # If it's 1, we need to unmask it\n-        return token_type_ids[batch_idx, kv_idx] == 1\n+        # If the difference is less than image size, both are part of the same image block\n+        same_image_block = torch.abs(kv_idx - q_idx) <= tokens_per_image\n+        # If it's 1 for both query and key/value, we are in an image block\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids[batch_idx, kv_idx] == 1)\n+\n+        # This is bidirectional attention whenever we are dealing with image tokens\n+        return is_image_block & same_image_block\n \n     return inner_mask\n \n@@ -836,7 +841,7 @@ def forward(\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n                 mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device)\n+                    token_type_ids.to(cache_position.device), self.config.mm_tokens_per_image\n                 )\n \n             # Create the masks\n@@ -1055,7 +1060,9 @@ def create_masks_for_generate(\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:\n             # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(token_type_ids.to(cache_position.device))\n+            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+                token_type_ids.to(cache_position.device), config.mm_tokens_per_image\n+            )\n \n         return create_masks_for_generate(**mask_kwargs)\n "
        }
    ],
    "stats": {
        "total": 34,
        "additions": 24,
        "deletions": 10
    }
}