{
    "author": "unknown",
    "message": "Improved Documentation Of Audio Classification (#35368)\n\n* Improved Documentation Of Audio Classification\n\n* Updated documentation as per review\n\n* Updated audio_classification.md\n\n* Update audio_classification.md",
    "sha": "94fe0b915b459b2d011854661a030257152306f9",
    "files": [
        {
            "sha": "973f95e1e9555dd77e3a5a6b48a5131598dfa2df",
            "filename": "docs/source/en/tasks/audio_classification.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/94fe0b915b459b2d011854661a030257152306f9/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/94fe0b915b459b2d011854661a030257152306f9/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md?ref=94fe0b915b459b2d011854661a030257152306f9",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n \n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n -->\n@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n <Youtube id=\"KWwzcmG98Ds\"/>\n \n-Audio classification - just like with text - assigns a class label output from the input data. The only difference is instead of text inputs, you have raw audio waveforms. Some practical applications of audio classification include identifying speaker intent, language classification, and even animal species by their sounds.\n+Audio classification - just like with text - assigns a class label as output from the input data. The only difference is instead of text inputs, you have raw audio waveforms. Some practical applications of audio classification include identifying speaker intent, language classification, and even animal species by their sounds.\n \n This guide will show you how to:\n \n@@ -57,7 +57,7 @@ Start by loading the MInDS-14 dataset from the ðŸ¤— Datasets library:\n >>> minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n ```\n \n-Split the dataset's `train` split into a smaller train and test set with the [`~datasets.Dataset.train_test_split`] method. This'll give you a chance to experiment and make sure everything works before spending more time on the full dataset.\n+Split the dataset's `train` split into a smaller train and test set with the [`~datasets.Dataset.train_test_split`] method. This will give you a chance to experiment and make sure everything works before spending more time on the full dataset.\n \n ```py\n >>> minds = minds.train_test_split(test_size=0.2)\n@@ -79,13 +79,13 @@ DatasetDict({\n })\n ```\n \n-While the dataset contains a lot of useful information, like `lang_id` and `english_transcription`, you'll focus on the `audio` and `intent_class` in this guide. Remove the other columns with the [`~datasets.Dataset.remove_columns`] method:\n+While the dataset contains a lot of useful information, like `lang_id` and `english_transcription`, you will focus on the `audio` and `intent_class` in this guide. Remove the other columns with the [`~datasets.Dataset.remove_columns`] method:\n \n ```py\n >>> minds = minds.remove_columns([\"path\", \"transcription\", \"english_transcription\", \"lang_id\"])\n ```\n \n-Take a look at an example now:\n+Here's an example:\n \n ```py\n >>> minds[\"train\"][0]\n@@ -155,7 +155,7 @@ Now create a preprocessing function that:\n ...     return inputs\n ```\n \n-To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once. Remove the columns you don't need, and rename `intent_class` to `label` because that's the name the model expects:\n+To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once. Remove unnecessary columns and rename `intent_class` to `label`, as required by the model:\n \n ```py\n >>> encoded_minds = minds.map(preprocess_function, remove_columns=\"audio\", batched=True)\n@@ -260,7 +260,7 @@ For a more in-depth example of how to fine-tune a model for audio classification\n \n Great, now that you've fine-tuned a model, you can use it for inference!\n \n-Load an audio file you'd like to run inference on. Remember to resample the sampling rate of the audio file to match the sampling rate of the model if you need to!\n+Load an audio file for inference. Remember to resample the sampling rate of the audio file to match the model's sampling rate, if necessary.\n \n ```py\n >>> from datasets import load_dataset, Audio"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}