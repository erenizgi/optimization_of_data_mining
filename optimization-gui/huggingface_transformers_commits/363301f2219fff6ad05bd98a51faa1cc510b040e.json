{
    "author": "itazap",
    "message": "support loading model without config.json file (#32356)\n\n* support loading model without config.json file\r\n\r\n* fix condition\r\n\r\n* update tests\r\n\r\n* add test\r\n\r\n* ruff\r\n\r\n* ruff\r\n\r\n* ruff",
    "sha": "363301f2219fff6ad05bd98a51faa1cc510b040e",
    "files": [
        {
            "sha": "2339c4cd6b51d0f1c5d6dbd89a3cdcef745eb6dd",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -565,6 +565,8 @@ def get_config_dict(\n         original_kwargs = copy.deepcopy(kwargs)\n         # Get config dict associated with the base config file\n         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n+        if config_dict is None:\n+            return {}, kwargs\n         if \"_commit_hash\" in config_dict:\n             original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n \n@@ -635,6 +637,8 @@ def _get_config_dict(\n                     subfolder=subfolder,\n                     _commit_hash=commit_hash,\n                 )\n+                if resolved_config_file is None:\n+                    return None, kwargs\n                 commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n             except EnvironmentError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to"
        },
        {
            "sha": "6fe960c78eb10b3141d99b38fc4c3b757f0766e4",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -51,7 +51,7 @@ def __init__(self, feature_extractor, tokenizer):\n     def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         try:\n             return super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-        except OSError:\n+        except (OSError, ValueError):\n             warnings.warn(\n                 f\"Loading a tokenizer inside {cls.__name__} from a config that does not\"\n                 \" include a `tokenizer_class` attribute is deprecated and will be \""
        },
        {
            "sha": "3b2704498c1d7a89601eb3333b5e3d18466af00f",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -2439,6 +2439,13 @@ def _from_pretrained(\n                 \"Unable to load vocabulary from file. \"\n                 \"Please check that the provided vocabulary is accessible and not corrupted.\"\n             )\n+        except RuntimeError as e:\n+            if \"sentencepiece_processor.cc\" in str(e):\n+                logger.info(\n+                    \"Unable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\"\n+                    \"(SentencePiece RuntimeError: Tried to load SPM model with non-SPM vocab file).\",\n+                )\n+                return False\n \n         if added_tokens_decoder != {} and max(list(added_tokens_decoder.keys())[-1], 0) > tokenizer.vocab_size:\n             logger.info("
        },
        {
            "sha": "92be9c0b0509e648036422fa55c8aa29552c62ba",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -370,7 +370,7 @@ def cached_file(\n     if os.path.isdir(path_or_repo_id):\n         resolved_file = os.path.join(os.path.join(path_or_repo_id, subfolder), filename)\n         if not os.path.isfile(resolved_file):\n-            if _raise_exceptions_for_missing_entries:\n+            if _raise_exceptions_for_missing_entries and filename not in [\"config.json\", f\"{subfolder}/config.json\"]:\n                 raise EnvironmentError(\n                     f\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\n                     f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\"\n@@ -454,6 +454,8 @@ def cached_file(\n             return None\n         if revision is None:\n             revision = \"main\"\n+        if filename in [\"config.json\", f\"{subfolder}/config.json\"]:\n+            return None\n         raise EnvironmentError(\n             f\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\n             f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\""
        },
        {
            "sha": "69f9029f909b3e8d650d226b0c42a5ceb253b9d1",
            "filename": "tests/models/auto/test_configuration_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -104,13 +104,6 @@ def test_revision_not_found(self):\n         ):\n             _ = AutoConfig.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER, revision=\"aaaaaa\")\n \n-    def test_configuration_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError,\n-            \"hf-internal-testing/no-config-test-repo does not appear to have a file named config.json.\",\n-        ):\n-            _ = AutoConfig.from_pretrained(\"hf-internal-testing/no-config-test-repo\")\n-\n     def test_from_pretrained_dynamic_config(self):\n         # If remote code is not set, we will time out when asking whether to load the model.\n         with self.assertRaises(ValueError):"
        },
        {
            "sha": "094a511616d0313576a47c7c175ebbba53d1c5c5",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -20,6 +20,7 @@\n import unittest\n \n from datasets import load_dataset\n+from huggingface_hub import hf_hub_download\n \n from transformers import (\n     SPIECE_UNDERLINE,\n@@ -330,6 +331,15 @@ def test_add_prefix_space(self):\n             fast_.decode(EXPECTED_WITH_SPACE, skip_special_tokens=True),\n         )\n \n+    def test_load_tokenizer_with_model_file_only(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            hf_hub_download(repo_id=\"huggyllama/llama-7b\", filename=\"tokenizer.model\", local_dir=tmp_dir)\n+            tokenizer_fast = self.rust_tokenizer_class.from_pretrained(tmp_dir)\n+            self.assertEqual(tokenizer_fast.encode(\"This is a test\"), [1, 910, 338, 263, 1243])\n+\n+            tokenizer_slow = self.tokenizer_class.from_pretrained(tmp_dir)\n+            self.assertEqual(tokenizer_slow.encode(\"This is a test\"), [1, 910, 338, 263, 1243])\n+\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "76394daf9ced4a7b75e4c79bcd8af09a63b15be5",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/363301f2219fff6ad05bd98a51faa1cc510b040e/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=363301f2219fff6ad05bd98a51faa1cc510b040e",
            "patch": "@@ -247,12 +247,10 @@ def test_nested_config_load_from_dict(self):\n         self.assertEqual(config.text_config.__class__.__name__, \"CLIPTextConfig\")\n \n     def test_from_pretrained_subfolder(self):\n-        with self.assertRaises(OSError):\n-            # config is in subfolder, the following should not work without specifying the subfolder\n-            _ = BertConfig.from_pretrained(\"hf-internal-testing/tiny-random-bert-subfolder\")\n+        config = BertConfig.from_pretrained(\"hf-internal-testing/tiny-random-bert-subfolder\")\n+        self.assertIsNotNone(config)\n \n         config = BertConfig.from_pretrained(\"hf-internal-testing/tiny-random-bert-subfolder\", subfolder=\"bert\")\n-\n         self.assertIsNotNone(config)\n \n     def test_cached_files_are_used_when_internet_is_down(self):"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 27,
        "deletions": 13
    }
}