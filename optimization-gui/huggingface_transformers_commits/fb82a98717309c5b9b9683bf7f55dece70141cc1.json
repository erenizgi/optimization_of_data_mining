{
    "author": "yao-matrix",
    "message": "enable large_gpu and torchao cases on XPU (#38355)\n\n* cohere2 done\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* enable torchao cases on XPU\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* rename\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix comments\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>",
    "sha": "fb82a98717309c5b9b9683bf7f55dece70141cc1",
    "files": [
        {
            "sha": "9aa1ab7c68833a879a77cebd4d6628ded541c0d2",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 10,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=fb82a98717309c5b9b9683bf7f55dece70141cc1",
            "patch": "@@ -23,10 +23,11 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, Cohere2Config, is_torch_available, pipeline\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n-    require_torch_large_gpu,\n+    require_torch_large_accelerator,\n     slow,\n     torch_device,\n )\n@@ -130,7 +131,7 @@ def test_generate_continue_from_inputs_embeds(self):\n \n @slow\n @require_read_token\n-@require_torch_large_gpu\n+@require_torch_large_accelerator\n class Cohere2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n \n@@ -155,10 +156,15 @@ def test_model_bf16(self):\n \n     def test_model_fp16(self):\n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n-        EXPECTED_TEXTS = [\n-            \"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\",\n-            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",\n-        ]\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): [\"<BOS_TOKEN>Hello I am doing a project for my school and I need to create a website for a fictional company. I have the\", \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\"],\n+                (\"cuda\", 7): [\"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\", \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        # fmt: on\n \n         model = AutoModelForCausalLM.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n@@ -170,7 +176,7 @@ def test_model_fp16(self):\n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     def test_model_pipeline_bf16(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Cohere2 before this PR\n@@ -223,9 +229,15 @@ def test_export_static_cache(self):\n         )\n \n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"Hello I am doing a project on the effects of social media on mental health. I have a few questions. 1. What is the relationship\",\n-        ]\n+        # fmt: off\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"xpu\", 3): [\"Hello I am doing a project for a friend and I am stuck on a few things. I have a 2004 Ford F-\"],\n+                (\"cuda\", 7): [\"Hello I am doing a project on the effects of social media on mental health. I have a few questions. 1. What is the relationship\",],\n+            }\n+        )\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+        # fmt: on\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"<PAD>\", padding_side=\"right\")\n         # Load model\n@@ -270,6 +282,9 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n+        if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n+            self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n+\n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_COMPLETIONS = [\n             \" the mountains, the lakes, the rivers, the waterfalls, the waterfalls, the waterfalls, the waterfalls\","
        },
        {
            "sha": "5eb2265d9cea1c85820aee7f97b709754ef36ce6",
            "filename": "tests/quantization/autoround/test_auto_round.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py?ref=fb82a98717309c5b9b9683bf7f55dece70141cc1",
            "patch": "@@ -143,9 +143,9 @@ def test_save_pretrained(self):\n             self.assertIn(output_tokens, self.EXPECTED_OUTPUTS)\n \n     @require_torch_multi_accelerator\n-    def test_quantized_model_multi_gpu(self):\n+    def test_quantized_model_multi_accelerator(self):\n         \"\"\"\n-        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        Simple test that checks if the quantized model is working properly with multiple accelerators\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         quantization_config = AutoRoundConfig(backend=\"triton\")"
        },
        {
            "sha": "bf60deef8b63427d27ba3fcac87e48e00de7e42a",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 112,
            "deletions": 38,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb82a98717309c5b9b9683bf7f55dece70141cc1/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=fb82a98717309c5b9b9683bf7f55dece70141cc1",
            "patch": "@@ -21,10 +21,11 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     get_device_properties,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     require_torchao,\n     require_torchao_version_greater_or_equal,\n     torch_device,\n@@ -52,14 +53,22 @@\n \n     if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\"):\n         from torchao.dtypes import Int4CPULayout\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.11.0\"):\n+        from torchao.dtypes import Int4XPULayout\n \n \n def check_torchao_int4_wo_quantized(test_module, qlayer):\n     weight = qlayer.weight\n     test_module.assertEqual(weight.quant_min, 0)\n     test_module.assertEqual(weight.quant_max, 15)\n     test_module.assertTrue(isinstance(weight, AffineQuantizedTensor))\n-    layout = Int4CPULayout if weight.device.type == \"cpu\" else TensorCoreTiledLayout\n+    layout = None\n+    if weight.device.type == \"cpu\":\n+        layout = Int4CPULayout\n+    elif weight.device.type == \"xpu\":\n+        layout = Int4XPULayout\n+    elif weight.device.type == \"cuda\":\n+        layout = TensorCoreTiledLayout\n     test_module.assertTrue(isinstance(weight.tensor_impl._layout, layout))\n \n \n@@ -123,7 +132,6 @@ def test_json_serializable(self):\n class TorchAoTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     device = \"cpu\"\n     quant_scheme_kwargs = (\n@@ -132,6 +140,11 @@ class TorchAoTest(unittest.TestCase):\n         else {\"group_size\": 32}\n     )\n \n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n+\n     def tearDown(self):\n         gc.collect()\n         backend_empty_cache(torch_device)\n@@ -261,11 +274,25 @@ def test_per_module_config_skip(self):\n         self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n \n \n-@require_torch_gpu\n-class TorchAoGPUTest(TorchAoTest):\n+@require_torch_accelerator\n+class TorchAoAcceleratorTest(TorchAoTest):\n     device = torch_device\n     quant_scheme_kwargs = {\"group_size\": 32}\n \n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        # fmt: off\n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+                (\"cuda\", 7): \"What are we having for dinner?\\n- 1. What is the temperature outside\",\n+            }\n+        )\n+        # fmt: on\n+        cls.EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n+\n     def test_int4wo_offload(self):\n         \"\"\"\n         Simple test that checks if the quantized model int4 weight only is working properly with cpu/disk offload\n@@ -312,16 +339,27 @@ def test_int4wo_offload(self):\n \n         input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n \n+        # fmt: off\n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+                (\"cuda\", 7): \"What are we having for dinner?\\n- 2. What is the temperature outside\",\n+            }\n+        )\n+        # fmt: on\n+        EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n+\n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 2. What is the temperature outside\"\n+        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n \n-        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n+        self.assertEqual(generated_text, EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n-    def test_int4wo_quant_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_int4wo_quant_multi_accelerator(self):\n         \"\"\"\n-        Simple test that checks if the quantized model int4 weight only is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n+        Simple test that checks if the quantized model int4 weight only is working properly with multiple accelerators\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 CUDA GPUs\n+        set ZE_AFFINITY_MASK=0,1 if you have more than 2 Intel XPUs\n         \"\"\"\n \n         quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n@@ -373,7 +411,6 @@ def test_autoquant(self):\n class TorchAoSerializationTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     quant_scheme = \"int4_weight_only\"\n     quant_scheme_kwargs = (\n@@ -387,6 +424,7 @@ class TorchAoSerializationTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n \n     def setUp(self):\n         self.quant_config = TorchAoConfig(self.quant_scheme, **self.quant_scheme_kwargs)\n@@ -430,58 +468,91 @@ def test_serialization_expected_output(self):\n \n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n-    @require_torch_gpu\n-    def test_serialization_expected_output_on_cuda(self):\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+\n+    @require_torch_accelerator\n+    def test_serialization_expected_output_on_accelerator(self):\n         \"\"\"\n-        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        Test if we can serialize on device (cpu) and load/infer the model on accelerator\n         \"\"\"\n-        self.check_serialization_expected_output(\"cuda\", self.EXPECTED_OUTPUT)\n+        self.check_serialization_expected_output(torch_device, self.EXPECTED_OUTPUT)\n \n \n class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n-    @require_torch_gpu\n-    def test_serialization_expected_output_on_cuda(self):\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+\n+    @require_torch_accelerator\n+    def test_serialization_expected_output_on_accelerator(self):\n         \"\"\"\n-        Test if we can serialize on device (cpu) and load/infer the model on cuda\n+        Test if we can serialize on device (cpu) and load/infer the model on accelerator\n         \"\"\"\n-        self.check_serialization_expected_output(\"cuda\", self.EXPECTED_OUTPUT)\n+        self.check_serialization_expected_output(torch_device, self.EXPECTED_OUTPUT)\n \n \n-@require_torch_gpu\n-class TorchAoSerializationGPTTest(TorchAoSerializationTest):\n+@require_torch_accelerator\n+class TorchAoSerializationAcceleratorTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n     device = f\"{torch_device}:0\"\n \n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        # fmt: off\n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"xpu\", 3): \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+                (\"cuda\", 7): \"What are we having for dinner?\\n- 1. What is the temperature outside\",\n+            }\n+        )\n+        # fmt: on\n+        cls.EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n+\n \n-@require_torch_gpu\n-class TorchAoSerializationW8A8GPUTest(TorchAoSerializationTest):\n+@require_torch_accelerator\n+class TorchAoSerializationW8A8AcceleratorTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = f\"{torch_device}:0\"\n \n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+\n \n-@require_torch_gpu\n-class TorchAoSerializationW8GPUTest(TorchAoSerializationTest):\n+@require_torch_accelerator\n+class TorchAoSerializationW8AcceleratorTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = f\"{torch_device}:0\"\n \n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+\n \n-@require_torch_gpu\n+@require_torch_accelerator\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n-class TorchAoSerializationFP8GPUTest(TorchAoSerializationTest):\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+class TorchAoSerializationFP8AcceleratorTest(TorchAoSerializationTest):\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if not (get_device_properties()[0] == \"cuda\" and get_device_properties()[1] >= 9):\n+        if get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Float8WeightOnlyConfig\n@@ -491,17 +562,18 @@ def setUpClass(cls):\n \n         super().setUpClass()\n \n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n-@require_torch_gpu\n+\n+@require_torch_accelerator\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n class TorchAoSerializationA8W4Test(TorchAoSerializationTest):\n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if not (get_device_properties()[0] == \"cuda\" and get_device_properties()[1] >= 9):\n+        if get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Int8DynamicActivationInt4WeightConfig\n@@ -511,6 +583,8 @@ def setUpClass(cls):\n \n         super().setUpClass()\n \n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+\n \n if __name__ == \"__main__\":\n     unittest.main()"
        }
    ],
    "stats": {
        "total": 189,
        "additions": 139,
        "deletions": 50
    }
}