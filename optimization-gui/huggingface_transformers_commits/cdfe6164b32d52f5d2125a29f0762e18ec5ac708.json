{
    "author": "Vixel2006",
    "message": "fix(pipelines): QA pipeline returns fewer than top_k results in batch mode (#39193)\n\n* fixing the bug\n\n* Try a simpler approach\n\n* make fixup\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>",
    "sha": "cdfe6164b32d52f5d2125a29f0762e18ec5ac708",
    "files": [
        {
            "sha": "2eee80a907b8528ab662fa67b17278f5db9dec90",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdfe6164b32d52f5d2125a29f0762e18ec5ac708/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdfe6164b32d52f5d2125a29f0762e18ec5ac708/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=cdfe6164b32d52f5d2125a29f0762e18ec5ac708",
            "patch": "@@ -556,8 +556,18 @@ def postprocess(\n                 output[\"attention_mask\"].numpy() if output.get(\"attention_mask\", None) is not None else None\n             )\n \n+            pre_topk = (\n+                top_k * 2 + 10 if align_to_words else top_k\n+            )  # Some candidates may be deleted if we align to words\n             starts, ends, scores, min_null_score = select_starts_ends(\n-                start_, end_, p_mask, attention_mask, min_null_score, top_k, handle_impossible_answer, max_answer_len\n+                start_,\n+                end_,\n+                p_mask,\n+                attention_mask,\n+                min_null_score,\n+                pre_topk,\n+                handle_impossible_answer,\n+                max_answer_len,\n             )\n \n             if not self.tokenizer.is_fast:"
        },
        {
            "sha": "d46dd489c515cd0edfa01769d4399f6237206378",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/cdfe6164b32d52f5d2125a29f0762e18ec5ac708/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cdfe6164b32d52f5d2125a29f0762e18ec5ac708/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=cdfe6164b32d52f5d2125a29f0762e18ec5ac708",
            "patch": "@@ -168,10 +168,11 @@ def test_small_model_pt(self):\n         )\n \n         outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n+            question=\"Where was HuggingFace founded ?\",\n+            context=\"HuggingFace was founded in Paris.\",\n         )\n \n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.01, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n+        self.assertEqual(nested_simplify(outputs), {\"score\": 0.063, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n \n     @require_torch\n     def test_small_model_pt_fp16(self):\n@@ -182,10 +183,11 @@ def test_small_model_pt_fp16(self):\n         )\n \n         outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n+            question=\"Where was HuggingFace founded ?\",\n+            context=\"HuggingFace was founded in Paris.\",\n         )\n \n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.01, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n+        self.assertEqual(nested_simplify(outputs), {\"score\": 0.063, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n \n     @require_torch\n     def test_small_model_pt_bf16(self):\n@@ -196,10 +198,11 @@ def test_small_model_pt_bf16(self):\n         )\n \n         outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n+            question=\"Where was HuggingFace founded ?\",\n+            context=\"HuggingFace was founded in Paris.\",\n         )\n \n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.01, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n+        self.assertEqual(nested_simplify(outputs), {\"score\": 0.063, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n \n     @require_torch\n     def test_small_model_pt_iterator(self):\n@@ -211,7 +214,9 @@ def data():\n                 yield {\"question\": \"Where was HuggingFace founded ?\", \"context\": \"HuggingFace was founded in Paris.\"}\n \n         for outputs in pipe(data()):\n-            self.assertEqual(nested_simplify(outputs), {\"score\": 0.01, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n+            self.assertEqual(\n+                nested_simplify(outputs), {\"score\": 0.063, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"}\n+            )\n \n     @require_torch\n     def test_small_model_pt_softmax_trick(self):\n@@ -242,10 +247,11 @@ def ensure_large_logits_postprocess(\n         question_answerer.postprocess = ensure_large_logits_postprocess\n \n         outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n+            question=\"Where was HuggingFace founded ?\",\n+            context=\"HuggingFace was founded in Paris.\",\n         )\n \n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.028, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n+        self.assertEqual(nested_simplify(outputs), {\"score\": 0.111, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n \n     @slow\n     @require_torch"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 26,
        "deletions": 10
    }
}