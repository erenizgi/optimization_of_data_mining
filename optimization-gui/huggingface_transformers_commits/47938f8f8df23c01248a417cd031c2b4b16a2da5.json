{
    "author": "thisisiron",
    "message": "Add Ovis2 model and processor implementation (#37088)\n\n* Add Ovis2 model and processor implementation\n\n* Apply style fixes\n\n* Add unit tests for Ovis2 image processing and processor\n\n* Refactor image processing functions for clarity and efficiency\n\n* Add Ovis2 ImageProcessorFast\n\n* Refactor Ovis2 code\n\n* Refactor Ovis2 model components and update processor functionality\n\n* Fix repo consistency issues for Ovis2: docstring, config cleanup\n\n* Update Ovis2 model integration tests\n\n* Update Ovis2 configuration and processing classes for improved documentation\n\n* Remove duplicate entry for 'ovis2' in VLM_CLASS_NAMES\n\n* Fix conflict\n\n* Fix import order\n\n* Update image processor class names\n\n* Update Ovis2 model structure\n\n* Refactor Ovis2 configuration\n\n* Fix typos\n\n* Refactor Ovis2 model classes and remove unused code\n\n* Fix typos\n\n* Refactor Ovis2 model initialization\n\n* Fiix typos\n\n* Remove Ovis2 model mapping from MODEL_MAPPING_NAMES in modeling_auto.py\n\n* Add license and update type hints\n\n* Refactor token function and update docstring handling\n\n* Add license\n\n* Add Ovis2 model support and update documentation\n\n* Refactor Ovis2 model structure and enhance multimodal capabilities\n\n* Update Ovis2 weight mapping for consistency and clarity in key patterns\n\n* Remove unused 'grids' parameter from Ovis2 model and Update processing logic to handle image grids more efficiently.\n\n* Refactor Ovis2 model test structure to include Ovis2Model\n\n* Add optional disable_grouping param to Ovis2ImageProcessorFast\n\n* Refactor type hints in Ovis2 modules\n\n* Add licensing information in Ovis2 modules and tests\n\n* Refactor Ovis2 model by removing unused methods\n\n* Refactor Ovis2 model tests by renaming test classes and removing skipped tests\n\n* Refactor Ovis2 model output classes\n\n* Refactor Ovis2 weight conversion and Update model embedding classes\n\n* Refactor Ovis2 model imports and remove unused functions\n\n* Enhance vision configuration extraction in Ovis2 weight conversion\n\n* Refactor Ovis2 model's forward method to remove interpolation option\n\n* Update Ovis2 model documentation\n\n* Refactor Ovis2 model input handling and tokenizer configuration\n\n* Update return type hints in Ovis2 model\n\n* Remove commented-out code\n\n* fix config for tests and remove key mappings\n\n* Update tokenizer configuration to use add_special_tokens method\n\n* skip torchscript\n\n* Fix image placeholder generation in Ovis2Processor\n\n* Refactor Ovis2 model to rename visual_table to visual_embeddings_table\n\n* Enhance Ovis2 model by adding vision_feature_select_strategy parameter\n\n* Refactor Ovis2 model weights conversion and architecture\n\n* Refactor Ovis2 model by removing vision_feature_select_strategy parameter\n\n* Update Ovis2 model examples\n\n* Refactor Ovis2 model\n\n* Update Ovis2 model\n\n* Update Ovis2 model configuration\n\n* Refactor Ovis2 model test setup\n\n* Refactor flash attention support\n\n* Refactor\n\n* Fix typo\n\n* Refactor\n\n* Refactor model classes\n\n* Update expected output in Ovis2\n\n* Refactor docstrings\n\n* Fix\n\n* Fix\n\n* Fix\n\n* Update input in tests\n\n* Fix\n\n* Fix get_decoder method\n\n* Refactor\n\n* Refactor Ovis2\n\n* Fix\n\n* Fix\n\n* Fix test\n\n* Add get_placeholder_mask\n\n* Refactor Ovis2 model tests\n\n* Fix\n\n* Refactor\n\n* Fix\n\n* Fix\n\n* Fix Ovis2 test\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "47938f8f8df23c01248a417cd031c2b4b16a2da5",
    "files": [
        {
            "sha": "ecc3f2553d60b09a2d228717b90e54fffcc1a20d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -1077,6 +1077,8 @@\n         title: OmDet-Turbo\n       - local: model_doc/oneformer\n         title: OneFormer\n+      - local: model_doc/ovis2\n+        title: Ovis2\n       - local: model_doc/owlvit\n         title: OWL-ViT\n       - local: model_doc/owlv2"
        },
        {
            "sha": "d1eda8d1b001e82d4585a9c4569c4a037e6d3308",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,105 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Ovis2\n+\n+## Overview\n+\n+The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://arxiv.org/abs/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group. \n+\n+Ovis2 is the latest advancement in multi-modal large language models (MLLMs), succeeding Ovis1.6. It retains the architectural design of the Ovis series, which focuses on aligning visual and textual embeddings, and introduces major improvements in data curation and training methods.\n+\n+<img src=\"https://cdn-uploads.huggingface.co/production/uploads/637aebed7ce76c3b834cea37/XB-vgzDL6FshrSNGyZvzc.png\"  width=\"600\">\n+\n+<small> Ovis2 architecture.</small>\n+\n+This model was contributed by [thisisiron](https://huggingface.co/thisisiron).\n+\n+## Usage example\n+\n+```python\n+\n+from PIL import Image\n+import requests\n+import torch\n+from torchvision import io\n+from typing import Dict\n+from transformers.image_utils import load_images, load_video\n+from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n+\n+model = AutoModelForVision2Seq.from_pretrained(\n+    \"thisisiron/Ovis2-2B-hf\",\n+    torch_dtype=torch.bfloat16,\n+).eval().to(\"cuda:0\")\n+processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+        ],\n+    },\n+]\n+url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+messages = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+print(messages)\n+\n+inputs = processor(\n+    images=[image],\n+    text=messages,\n+    return_tensors=\"pt\",\n+)\n+inputs = inputs.to(\"cuda:0\")\n+inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n+\n+with torch.inference_mode():\n+    output_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n+    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+    print(output_text)\n+```\n+\n+## Ovis2Config\n+\n+[[autodoc]] Ovis2Config\n+\n+## Ovis2VisionConfig\n+\n+[[autodoc]] Ovis2VisionConfig\n+\n+## Ovis2Model\n+\n+[[autodoc]] Ovis2Model\n+\n+## Ovis2ForConditionalGeneration\n+\n+[[autodoc]] Ovis2ForConditionalGeneration\n+    - forward\n+\n+## Ovis2ImageProcessor\n+\n+[[autodoc]] Ovis2ImageProcessor\n+\n+## Ovis2ImageProcessorFast\n+\n+[[autodoc]] Ovis2ImageProcessorFast\n+\n+## Ovis2Processor\n+\n+[[autodoc]] Ovis2Processor"
        },
        {
            "sha": "f2d7310d39306c4de7ade9f473e31e312be0dd23",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -240,6 +240,7 @@\n     from .oneformer import *\n     from .openai import *\n     from .opt import *\n+    from .ovis2 import *\n     from .owlv2 import *\n     from .owlvit import *\n     from .paligemma import *"
        },
        {
            "sha": "62efc7d9ad943737f4b6a4de4b19892117878beb",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -280,6 +280,7 @@\n         (\"open-llama\", \"OpenLlamaConfig\"),\n         (\"openai-gpt\", \"OpenAIGPTConfig\"),\n         (\"opt\", \"OPTConfig\"),\n+        (\"ovis2\", \"Ovis2Config\"),\n         (\"owlv2\", \"Owlv2Config\"),\n         (\"owlvit\", \"OwlViTConfig\"),\n         (\"paligemma\", \"PaliGemmaConfig\"),\n@@ -707,6 +708,7 @@\n         (\"open-llama\", \"OpenLlama\"),\n         (\"openai-gpt\", \"OpenAI GPT\"),\n         (\"opt\", \"OPT\"),\n+        (\"ovis2\", \"Ovis2\"),\n         (\"owlv2\", \"OWLv2\"),\n         (\"owlvit\", \"OWL-ViT\"),\n         (\"paligemma\", \"PaliGemma\"),"
        },
        {
            "sha": "40cf52b59d05f5d6d620ab6c5e21b9c1c2279d6b",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -139,6 +139,7 @@\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n             (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n+            (\"ovis2\", (\"Ovis2ImageProcessor\", \"Ovis2ImageProcessorFast\")),\n             (\"owlv2\", (\"Owlv2ImageProcessor\", \"Owlv2ImageProcessorFast\")),\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),"
        },
        {
            "sha": "ea093b389290c5c47a77170bbb6c6bb4590844ab",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -279,6 +279,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"open-llama\", \"OpenLlamaModel\"),\n         (\"openai-gpt\", \"OpenAIGPTModel\"),\n         (\"opt\", \"OPTModel\"),\n+        (\"ovis2\", \"Ovis2Model\"),\n         (\"owlv2\", \"Owlv2Model\"),\n         (\"owlvit\", \"OwlViTModel\"),\n         (\"paligemma\", \"PaliGemmaModel\"),\n@@ -948,6 +949,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n         (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n+        (\"ovis2\", \"Ovis2ForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),\n@@ -997,6 +999,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n         (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n+        (\"ovis2\", \"Ovis2ForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"perception_lm\", \"PerceptionLMForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),"
        },
        {
            "sha": "c1609ea0165e667fbf363af89b49fd1219b8f576",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -104,6 +104,7 @@\n         (\"mm-grounding-dino\", \"GroundingDinoProcessor\"),\n         (\"moonshine\", \"Wav2Vec2Processor\"),\n         (\"oneformer\", \"OneFormerProcessor\"),\n+        (\"ovis2\", \"Ovis2Processor\"),\n         (\"owlv2\", \"Owlv2Processor\"),\n         (\"owlvit\", \"OwlViTProcessor\"),\n         (\"paligemma\", \"PaliGemmaProcessor\"),"
        },
        {
            "sha": "902e015b055f816e59bb2a69ccb0357d5b05cdb1",
            "filename": "src/transformers/models/ovis2/__init__.py",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2F__init__.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,32 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_ovis2 import *\n+    from .image_processing_ovis2 import *\n+    from .image_processing_ovis2_fast import *\n+    from .modeling_ovis2 import *\n+    from .processing_ovis2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e48f03f5741648482ef3352ba004829048a00bb5",
            "filename": "src/transformers/models/ovis2/configuration_ovis2.py",
            "status": "added",
            "additions": 179,
            "deletions": 0,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fconfiguration_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fconfiguration_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fconfiguration_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,179 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..qwen2.configuration_qwen2 import Qwen2Config\n+\n+\n+class Ovis2VisionConfig(PretrainedConfig):\n+    r\"\"\"This is the configuration class to store the configuration of a [`Ovis2VisionModel`]. It is used to instantiate a\n+    Ovis2VisionModel model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of Ovis2.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2816):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the RMSNorm layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        qkv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a learnable bias to the query, key, and value sequences at each attention head.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a learnable bias to the MLP layers.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        vocab_size (`int`, *optional*, defaults to 16384):\n+            Vocabulary size of the Vision Transformer.\n+        hidden_stride (`int`, *optional*, defaults to 1):\n+            The stride of the hidden layer in the Vision Transformer.\n+        num_visual_indicator_tokens (`int`, *optional*, defaults to 5):\n+            Number of visual indicator tokens.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated normal initializer for initializing all weight matrices.\n+        tokenize_function (`str`, *optional*, defaults to `\"softmax\"`):\n+            The function used to tokenize the visual indicator tokens.\n+    ```\"\"\"\n+\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 1024,\n+        intermediate_size: int = 2816,\n+        num_hidden_layers: int = 24,\n+        num_attention_heads: int = 8,\n+        num_channels: int = 3,\n+        image_size: int = 224,\n+        patch_size: int = 14,\n+        rms_norm_eps: float = 1e-5,\n+        attention_dropout: float = 0.0,\n+        qkv_bias: bool = False,\n+        mlp_bias: bool = False,\n+        hidden_act=\"silu\",\n+        vocab_size=16384,\n+        hidden_stride=1,\n+        num_visual_indicator_tokens=5,\n+        initializer_range=0.02,\n+        tokenize_function=\"softmax\",\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+\n+        self.attention_dropout = attention_dropout\n+        self.hidden_act = hidden_act\n+        self.qkv_bias = qkv_bias\n+        self.mlp_bias = mlp_bias\n+        self.rms_norm_eps = rms_norm_eps\n+        self.vocab_size = vocab_size\n+        self.hidden_stride = hidden_stride\n+        self.num_visual_indicator_tokens = num_visual_indicator_tokens\n+        self.tokenize_function = tokenize_function\n+        self.initializer_range = initializer_range\n+\n+\n+class Ovis2Config(PretrainedConfig):\n+    r\"\"\"This is the configuration class to store the configuration of a [`Ovis2ForConditionalGeneration`]. It is used to instantiate a\n+    Ovis2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of Ovis2.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    e.g. [thisisiron/Ovis2-1B-hf](https://huggingface.co/thisisiron/Ovis2-1B-hf)\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `Ovis2VisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_id (`int`, *optional*, defaults to 151665):\n+            The image token id to encode the image prompt.\n+        visual_indicator_token_ids (`List[int]`, *optional*, defaults to `[151666, 151667, 151668, 151669, 151670]`):\n+            The visual indicator token ids to encode the image prompt.\n+        vocab_size (`int`, *optional*, defaults to 151643):\n+            Vocabulary size of the text model.\n+        hidden_size (`int`, *optional*, defaults to 1536):\n+            Dimensionality of the encoder layers and the pooler layer.\n+\n+    ```python\n+    >>> from transformers import Ovis2ForConditionalGeneration, Ovis2Config\n+\n+    >>> # Initializing a Ovis2 style configuration\n+    >>> configuration = Ovis2Config()\n+\n+    >>> # Initializing a model from the Ovis2-2B style configuration\n+    >>> model = Ovis2ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ovis2\"\n+    sub_configs = {\"text_config\": Qwen2Config, \"vision_config\": Ovis2VisionConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_id=151665,\n+        visual_indicator_token_ids=[151666, 151667, 151668, 151669, 151670],\n+        vocab_size=151643,\n+        hidden_size=1536,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = Ovis2VisionConfig(**vision_config)\n+        elif isinstance(vision_config, Ovis2VisionConfig):\n+            self.vision_config = vision_config\n+        if vision_config is None:\n+            self.vision_config = Ovis2VisionConfig(num_visual_indicator_tokens=len(visual_indicator_token_ids))\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = Qwen2Config(**text_config)\n+        elif isinstance(text_config, Qwen2Config):\n+            self.text_config = text_config\n+        elif text_config is None:\n+            self.text_config = Qwen2Config()\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.image_token_id = image_token_id\n+        self.visual_indicator_token_ids = visual_indicator_token_ids\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Ovis2VisionConfig\", \"Ovis2Config\"]"
        },
        {
            "sha": "d98bfbb5dc4ea0f62e82b7fdb3d66b84245bd149",
            "filename": "src/transformers/models/ovis2/convert_ovis2_weights_to_hf.py",
            "status": "added",
            "additions": 404,
            "deletions": 0,
            "changes": 404,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,404 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import os\n+import re\n+\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoModelForImageTextToText,\n+    AutoProcessor,\n+    AutoTokenizer,\n+)\n+from transformers.models.auto.configuration_auto import CONFIG_MAPPING_NAMES\n+from transformers.models.ovis2.configuration_ovis2 import Ovis2Config, Ovis2VisionConfig\n+from transformers.models.ovis2.image_processing_ovis2 import Ovis2ImageProcessor\n+from transformers.models.ovis2.modeling_ovis2 import Ovis2ForConditionalGeneration\n+from transformers.models.ovis2.processing_ovis2 import Ovis2Processor\n+from transformers.models.qwen2.configuration_qwen2 import Qwen2Config\n+\n+\n+# Constants\n+CONTEXT_LENGTH = 32768  # multimodal_max_length\n+\n+\n+# fmt: off\n+\n+# Mapping from original model key patterns to HF key patterns\n+ORIGINAL_TO_HF_MAPPING = {\n+    r\"trunk.blocks\\.(\\d+)\\.norm_1\":                 r\"encoder.layers.\\1.rms_norm1\",\n+    r\"trunk.blocks\\.(\\d+)\\.norm_2\":                 r\"encoder.layers.\\1.rms_norm2\",\n+    r\"trunk.blocks\\.(\\d+)\\.attn.proj\":              r\"encoder.layers.\\1.attention.out_proj\",\n+    r\"visual_tokenizer\":                            r\"model.vision_tower\",\n+    r\"backbone\":                                    r\"transformer\",\n+    r\"preprocessor\":                                r\"embeddings\",\n+    r\"patchifier.proj\":                             r\"patch_embedding\",\n+    r\"patchifier.norm\":                             r\"rms_norm\",\n+    r\"trunk.post_trunk_norm\":                       r\"rms_norm\",\n+    r\"trunk.blocks\":                                r\"encoder.layers\",\n+    r\"mlp.fc1\":                                     r\"ffn.gate_proj\",\n+    r\"mlp.fc2\":                                     r\"ffn.down_proj\",\n+    r\"mlp.fc3\":                                     r\"ffn.up_proj\",\n+    r\"head.0\":                                      r\"head_linear\",\n+    r\"head.1\":                                      r\"head_norm\",\n+    r\"vte.weight\":                                  r\"model.visual_embeddings_table.weight\",\n+    r\"llm.model\":                                   r\"model.language_model\",\n+    r\"llm.lm_head\":                                 r\"lm_head\",\n+}\n+# fmt: on\n+\n+# Special tokens for the tokenizer\n+SPECIAL_TOKENS = [\n+    \"<IMG_ATOM>\",\n+    \"<IMG_START>\",\n+    \"<IMG_GRID>\",\n+    \"<IMG_COL>\",\n+    \"<IMG_ROW>\",\n+    \"<IMG_END>\",\n+]\n+\n+# Configuration keys to ignore when converting\n+UNNECESSARY_CONFIG_KEYS = [\n+    \"_name_or_path\",\n+    \"_attn_implementation_autoset\",\n+    \"auto_map\",\n+    \"use_bfloat16\",\n+    \"use_flash_attn\",\n+    \"qk_normalization\",\n+    \"bias\",\n+    \"norm_type\",\n+]\n+\n+# Chat template for the tokenizer\n+CHAT_TEMPLATE = (\n+    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n+    \"{% for message in messages %}\"\n+    \"{{'<|im_start|>' + message['role'] + '\\n'}}\"\n+    \"{% if message['content'] is string %}\"\n+    \"{{ message['content'] }}\"\n+    \"{% else %}\"\n+    \"{% for content in message['content'] %}\"\n+    \"{% if content['type'] == 'image' %}\"\n+    \"{{ '<image>\\n' }}\"\n+    \"{% elif content['type'] == 'text' %}\"\n+    \"{{ content['text'] }}\"\n+    \"{% endif %}\"\n+    \"{% endfor %}\"\n+    \"{% endif %}\"\n+    \"{{'<|im_end|>\\n'}}\"\n+    \"{% endfor %}\"\n+    \"{% if add_generation_prompt %}\"\n+    \"{{'<|im_start|>assistant\\n' }}\"\n+    \"{% endif %}\"\n+)\n+\n+\n+def create_tokenizer(model_name_or_path, save_dir):\n+    \"\"\"\n+    Create and configure a tokenizer for the Ovis2 model.\n+\n+    Args:\n+        model_name_or_path: Path to the source model or tokenizer\n+        save_dir: Directory to save the tokenizer to\n+\n+    Returns:\n+        The configured tokenizer\n+    \"\"\"\n+    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, return_token_type_ids=False)\n+    tokenizer.model_max_length = CONTEXT_LENGTH\n+    tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n+    tokenizer.chat_template = CHAT_TEMPLATE\n+    setattr(tokenizer, \"image_token\", \"<IMG_ATOM>\")  # 151665\n+    setattr(tokenizer, \"image_token_id\", tokenizer.convert_tokens_to_ids(tokenizer.image_token))\n+\n+    return tokenizer\n+\n+\n+def create_image_processor(save_dir):\n+    \"\"\"\n+    Create and save an image processor for the Ovis2 model.\n+\n+    Args:\n+        save_dir: Directory to save the image processor to\n+\n+    Returns:\n+        The configured image processor\n+    \"\"\"\n+    image_processor = Ovis2ImageProcessor(\n+        crop_to_patches=True,\n+        size={\"height\": 448, \"width\": 448},\n+    )\n+    return image_processor\n+\n+\n+def extract_vision_config_from_original(orig_config):\n+    \"\"\"\n+    Extract and format vision configuration from the original model config.\n+\n+    Args:\n+        orig_config: Original model configuration\n+\n+    Returns:\n+        dict: Cleaned vision configuration dictionary\n+    \"\"\"\n+    visual_tokenizer_config = orig_config.visual_tokenizer_config.to_dict()\n+    # backbone_config = visual_tokenizer_config.pop(\"backbone_config\")\n+\n+    # Copy required fields from backbone config\n+    visual_tokenizer_config[\"hidden_size\"] = orig_config.visual_tokenizer_config.backbone_config.hidden_size\n+    visual_tokenizer_config[\"intermediate_size\"] = (\n+        orig_config.visual_tokenizer_config.backbone_config.intermediate_size\n+    )\n+    visual_tokenizer_config[\"num_attention_heads\"] = (\n+        orig_config.visual_tokenizer_config.backbone_config.num_attention_heads\n+    )\n+    visual_tokenizer_config[\"num_hidden_layers\"] = (\n+        orig_config.visual_tokenizer_config.backbone_config.num_hidden_layers\n+    )\n+    visual_tokenizer_config[\"rms_norm_eps\"] = orig_config.visual_tokenizer_config.backbone_config.rms_norm_eps\n+    visual_tokenizer_config[\"image_size\"] = orig_config.visual_tokenizer_config.backbone_config.image_size\n+    visual_tokenizer_config[\"num_channels\"] = orig_config.visual_tokenizer_config.backbone_config.num_channels\n+    visual_tokenizer_config[\"patch_size\"] = orig_config.visual_tokenizer_config.backbone_config.patch_size\n+    visual_tokenizer_config[\"qkv_bias\"] = orig_config.visual_tokenizer_config.backbone_config.qkv_bias\n+\n+    # Remove unnecessary keys\n+    return {k: v for k, v in visual_tokenizer_config.items() if k not in UNNECESSARY_CONFIG_KEYS}\n+\n+\n+def get_ovis2_config(model_name_or_path):\n+    \"\"\"\n+    Create an Ovis2 configuration from the original model.\n+\n+    Args:\n+        model_name_or_path: Path to the original model\n+\n+    Returns:\n+        Ovis2Config: Configuration for the HF implementation\n+    \"\"\"\n+    orig_config = AutoModelForCausalLM.from_pretrained(\n+        model_name_or_path,\n+        trust_remote_code=True,\n+    ).config\n+\n+    # Extract and clean LLM config\n+    llm_config = orig_config.llm_config.to_dict()\n+    llm_config = {k: v for k, v in llm_config.items() if k not in UNNECESSARY_CONFIG_KEYS}\n+\n+    # Extract and clean vision config\n+    visual_tokenizer_config = extract_vision_config_from_original(orig_config)\n+\n+    return Ovis2Config(\n+        text_config=Qwen2Config(**llm_config),\n+        vision_config=Ovis2VisionConfig(**visual_tokenizer_config),\n+        hidden_size=llm_config[\"hidden_size\"],\n+        vocab_size=llm_config[\"vocab_size\"],\n+        initializer_range=llm_config[\"initializer_range\"],\n+    )\n+\n+\n+def load_orig_state_dict(model_name_or_path):\n+    \"\"\"\n+    Load the state dictionary from the original model.\n+\n+    Args:\n+        model_name_or_path: Path to the original model\n+\n+    Returns:\n+        dict: Original model state dictionary\n+    \"\"\"\n+    model = AutoModelForCausalLM.from_pretrained(\n+        model_name_or_path,\n+        torch_dtype=torch.bfloat16,\n+        trust_remote_code=True,\n+    ).eval()\n+\n+    return model.state_dict()\n+\n+\n+def convert_orig2hf(state_dict, dim):\n+    \"\"\"\n+    Convert original state dictionary keys to HF format.\n+\n+    Args:\n+        state_dict: Original state dictionary\n+        dim: Hidden dimension for splitting QKV weights\n+\n+    Returns:\n+        dict: Converted state dictionary for HF model\n+    \"\"\"\n+    new_state_dict = {}\n+\n+    for key, val in state_dict.items():\n+        orig_key = key\n+\n+        # Apply regex pattern replacements\n+        for pattern, replacement in ORIGINAL_TO_HF_MAPPING.items():\n+            key = re.sub(pattern, replacement, key)\n+\n+        # Handle special cases\n+        if \"attn.qkv\" in key:\n+            # Split QKV into separate Q, K, V matrices\n+            new_key_query = key.replace(\"attn.qkv\", \"attention.q_proj\")\n+            new_state_dict[new_key_query] = state_dict[orig_key][:dim]\n+\n+            new_key_key = key.replace(\"attn.qkv\", \"attention.k_proj\")\n+            new_state_dict[new_key_key] = state_dict[orig_key][dim : 2 * dim]\n+\n+            new_key_value = key.replace(\"attn.qkv\", \"attention.v_proj\")\n+            new_state_dict[new_key_value] = state_dict[orig_key][-dim:]\n+\n+        elif \"pos_embed\" in key:\n+            new_key = key.replace(\"pos_embed\", \"position_embedding.weight\")\n+            new_state_dict[new_key] = state_dict[orig_key][0]\n+\n+        else:\n+            new_state_dict[key] = val\n+\n+    return new_state_dict\n+\n+\n+def convert_model(model_name_or_path):\n+    \"\"\"\n+    Convert and save the model in HF format.\n+\n+    Args:\n+        model_name_or_path: Path to the original model\n+        save_dir: Directory to save the converted model\n+\n+    Returns:\n+        The converted model\n+    \"\"\"\n+\n+    config = get_ovis2_config(model_name_or_path)\n+    config.architectures = [\"Ovis2ForConditionalGeneration\"]\n+\n+    # Load and convert weights\n+    orig_state_dict = load_orig_state_dict(model_name_or_path)\n+    new_state_dict = convert_orig2hf(orig_state_dict, config.vision_config.hidden_size)\n+\n+    # Create model and load converted weights\n+    model = Ovis2ForConditionalGeneration(config)\n+    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n+\n+    # Report any issues with weight loading\n+    if missing_keys:\n+        print(f\"Missing keys: {missing_keys}\")\n+    if unexpected_keys:\n+        print(f\"Unexpected keys: {unexpected_keys}\")\n+\n+    return model\n+\n+\n+def main():\n+    \"\"\"Process command line arguments and execute the conversion pipeline.\"\"\"\n+    parser = argparse.ArgumentParser(description=\"Convert Ovis2 model to HF format\")\n+    parser.add_argument(\n+        \"--model_name_or_path\",\n+        default=\"AIDC-AI/Ovis2-2B\",\n+        choices=[\n+            \"AIDC-AI/Ovis2-1B\",\n+            \"AIDC-AI/Ovis2-2B\",\n+            \"AIDC-AI/Ovis2-4B\",\n+            \"AIDC-AI/Ovis2-8B\",\n+            \"AIDC-AI/Ovis2-16B\",\n+            \"AIDC-AI/Ovis2-34B\",\n+        ],\n+        help=\"Location of original Ovis2 model\",\n+    )\n+    parser.add_argument(\"--save_dir\", default=\"Ovis2-2B-hf\", help=\"Location to write HF model and processors\")\n+    parser.add_argument(\"--hub_dir\", default=\"thisisiron/Ovis2-2B-hf\", help=\"Hub repository name if pushing to hub\")\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether to push the converted model to the Hugging Face hub\"\n+    )\n+\n+    args = parser.parse_args()\n+\n+    # Execute conversion pipeline\n+    print(f\"Converting model from {args.model_name_or_path} to {args.save_dir}\")\n+\n+    # If already included in the transformers library, remove to avoid duplication.\n+    if \"aimv2\" in CONFIG_MAPPING_NAMES:\n+        CONFIG_MAPPING_NAMES.pop(\"aimv2\")\n+\n+    tokenizer = create_tokenizer(\n+        model_name_or_path=args.model_name_or_path,\n+        save_dir=args.save_dir,\n+    )\n+\n+    image_processor = create_image_processor(\n+        save_dir=args.save_dir,\n+    )\n+\n+    os.makedirs(args.save_dir, exist_ok=True)\n+\n+    # Convert and save the model\n+    model = convert_model(model_name_or_path=args.model_name_or_path)\n+    model.save_pretrained(args.save_dir)\n+\n+    # Save the processor\n+    processor = Ovis2Processor(tokenizer=tokenizer, image_processor=image_processor, chat_template=CHAT_TEMPLATE)\n+    processor.save_pretrained(args.save_dir)\n+\n+    # Push to hub if requested\n+    if args.push_to_hub:\n+        processor.push_to_hub(args.hub_dir, use_temp_dir=True)\n+        model.push_to_hub(args.hub_dir, use_temp_dir=True)\n+\n+    model = (\n+        AutoModelForImageTextToText.from_pretrained(\n+            args.save_dir,\n+            torch_dtype=torch.bfloat16,\n+        )\n+        .eval()\n+        .to(\"cuda:0\")\n+    )\n+    processor = AutoProcessor.from_pretrained(args.save_dir)\n+\n+    messages = [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"image\"},\n+                {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+            ],\n+        },\n+    ]\n+    url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+    messages = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+    print(messages)\n+\n+    inputs = processor(\n+        images=[image],\n+        text=messages,\n+        return_tensors=\"pt\",\n+    )\n+    inputs = inputs.to(\"cuda:0\")\n+    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n+\n+    with torch.inference_mode():\n+        output_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n+        generated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+        output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        print(output_text)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "39f558f0334cdaeb6a07dfc14165469ab205ce89",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "added",
            "additions": 573,
            "deletions": 0,
            "changes": 573,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,573 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from functools import lru_cache\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Similar to image_processing_mllama.get_all_supported_aspect_ratios\n+@lru_cache(maxsize=10)\n+def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> list[tuple[int, int]]:\n+    \"\"\"\n+    Computes all allowed aspect ratios for a given minimum and maximum number of input tiles.\n+\n+    This function calculates all possible arrangements of tiles that can be formed\n+    within the constraint of the minimum and maximum number of tiles. Each arrangement is\n+    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n+\n+    Args:\n+        min_image_tiles (`int`):\n+            The minimum number of tiles allowed.\n+        max_image_tiles (`int`):\n+            The maximum number of tiles allowed.\n+\n+    Returns:\n+        `List[Tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n+        configuration in terms of number of tiles.\n+\n+    Example:\n+        >>> get_all_supported_aspect_ratios(1, 4)\n+        [(1, 1), (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (2, 2), (4, 1)]\n+\n+    \"\"\"\n+    aspect_ratios = []\n+    for width in range(1, max_image_tiles + 1):\n+        for height in range(1, max_image_tiles + 1):\n+            if width * height <= max_image_tiles and width * height >= min_image_tiles:\n+                aspect_ratios.append((width, height))\n+\n+    aspect_ratios = sorted(aspect_ratios, key=lambda x: x[0] * x[1])\n+\n+    return aspect_ratios\n+\n+\n+@lru_cache(maxsize=100)\n+def get_optimal_tiled_canvas(\n+    original_image_size: tuple[int, int],\n+    target_tile_size: tuple[int, int],\n+    min_image_tiles: int,\n+    max_image_tiles: int,\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Given a minimum and maximum number of tiles, find the canvas with the closest aspect ratio to the\n+    original image aspect ratio.\n+    In case of tie-breaking condition when two canvases have the same aspect ratio difference, we favor the canvas with\n+    more tiles, until the area covered by the tiles is more than twice the target area, in order to avoid unnecessarily\n+    excessive tiling.\n+    \"\"\"\n+    possible_tile_arrangements = get_all_supported_aspect_ratios(min_image_tiles, max_image_tiles)\n+\n+    original_height, original_width = original_image_size\n+    target_tile_height, target_tile_width = target_tile_size\n+    aspect_ratio = original_width / original_height\n+    area = original_width * original_height\n+\n+    # find the grid with the best aspect ratio\n+    best_ratio_diff = float(\"inf\")\n+    best_grid = (1, 1)\n+    for grid in possible_tile_arrangements:\n+        grid_aspect_ratio = grid[0] / grid[1]\n+        ratio_diff = abs(aspect_ratio - grid_aspect_ratio)\n+        if ratio_diff < best_ratio_diff:\n+            best_ratio_diff = ratio_diff\n+            best_grid = grid\n+        elif ratio_diff == best_ratio_diff:\n+            # if the aspect ratio difference is the same, we favor the grid with more patches\n+            # until the area covered by the patches is more than twice the original image area\n+            if area > 0.5 * target_tile_height * target_tile_width * grid[0] * grid[1]:\n+                best_grid = grid\n+\n+    return best_grid\n+\n+\n+def compute_patch_covering_area(left: int, upper: int, right: int, lower: int, side: int) -> float:\n+    w = right - left\n+    h = lower - upper\n+    w, h = max(w, h), min(w, h)\n+    if w > side:\n+        h = h / w * side\n+        w = side\n+    return w * h\n+\n+\n+def split_image_into_grid(h: int, w: int, grid: tuple[int, int]) -> list[tuple[int, int, int, int]]:\n+    row_height = h // grid[0]\n+    col_width = w // grid[1]\n+    return [\n+        (\n+            col * col_width,\n+            row * row_height,\n+            w if col == grid[1] - 1 else (col + 1) * col_width,\n+            h if row == grid[0] - 1 else (row + 1) * row_height,\n+        )\n+        for row in range(grid[0])\n+        for col in range(grid[1])\n+    ]\n+\n+\n+@lru_cache(maxsize=100)\n+def get_min_tile_covering_grid(\n+    image_size: tuple[int, int],\n+    target_patch_size: int,\n+    max_image_tiles: int,\n+    covering_threshold: float = 0.9,\n+) -> tuple[int, int]:\n+    image_height, image_width = image_size\n+    image_area = image_width * image_height\n+\n+    candidate_tile_grids = get_all_supported_aspect_ratios(1, max_image_tiles)\n+    evaluated_grids = []\n+    sufficient_covering_grids = []\n+\n+    for tile_grid in candidate_tile_grids:\n+        tile_regions = split_image_into_grid(image_height, image_width, tile_grid)\n+        tile_covering_ratio = (\n+            sum([compute_patch_covering_area(*region, target_patch_size) for region in tile_regions]) / image_area\n+        )\n+\n+        evaluated_grids.append((tile_grid, tile_covering_ratio))\n+        if tile_covering_ratio > covering_threshold:\n+            sufficient_covering_grids.append((tile_grid, tile_covering_ratio))\n+\n+    if sufficient_covering_grids:\n+        # Prefer fewer tiles and higher covering ratio\n+        return sorted(sufficient_covering_grids, key=lambda x: (x[0][0] * x[0][1], -x[1]))[0][0]\n+    else:\n+        # Fallback: prefer higher covering even if below threshold\n+        return sorted(evaluated_grids, key=lambda x: (-x[1], x[0][0] * x[0][1]))[0][0]\n+\n+\n+class Ovis2ImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a Ovis2 image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        crop_to_patches (`bool`, *optional*, defaults to `False`):\n+            Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+            `preprocess` method.\n+        min_patches (`int`, *optional*, defaults to 1):\n+            The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+        max_patches (`int`, *optional*, defaults to 12):\n+            The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+            Whether to use the covering area grid to determine the number of patches. Only has an effect if\n+            `crop_to_patches` is set to `True`. Can be overridden by the `use_covering_area_grid` parameter in the\n+            `preprocess` method.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        crop_to_patches: bool = False,\n+        min_patches: int = 1,\n+        max_patches: int = 12,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        use_covering_area_grid: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.crop_to_patches = crop_to_patches\n+        self.min_patches = min_patches\n+        self.max_patches = max_patches\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        size = get_size_dict(size)\n+        if \"height\" not in size or \"width\" not in size:\n+            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n+        output_size = (size[\"height\"], size[\"width\"])\n+        return resize(\n+            image,\n+            size=output_size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        crop_to_patches: Optional[bool] = None,\n+        min_patches: Optional[int] = None,\n+        max_patches: Optional[int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        use_covering_area_grid: bool = True,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Controls the size of the image after `resize`. The shortest edge of the image is resized to\n+                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n+                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n+                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            crop_to_patches (`bool`, *optional*, defaults to `self.crop_to_patches`):\n+                Whether to crop the image to patches.\n+            min_patches (`int`, *optional*, defaults to `self.min_patches`):\n+                The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`.\n+            max_patches (`int`, *optional*, defaults to `self.max_patches`):\n+                The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to normalize the image by if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+                Whether to use the covering area grid to determine the number of patches. Only has an effect if\n+                `crop_to_patches` is set to `True`.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        crop_to_patches = crop_to_patches if crop_to_patches is not None else self.crop_to_patches\n+        min_patches = min_patches if min_patches is not None else self.min_patches\n+        max_patches = max_patches if max_patches is not None else self.max_patches\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+        # PIL RGBA images are converted to RGB\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if crop_to_patches and max_patches > 1:\n+            images = [\n+                self.crop_image_to_patches(\n+                    image,\n+                    min_patches=min_patches,\n+                    max_patches=max_patches,\n+                    patch_size=size,\n+                    data_format=input_data_format,\n+                    use_covering_area_grid=use_covering_area_grid,\n+                )\n+                for image in images\n+            ]\n+            grids = [grid for _, grid in images]\n+            images = [image for images_list, _ in images for image in images_list]\n+        else:\n+            grids = [(1, 1)] * len(images)\n+\n+        for i, image in enumerate(images):\n+            if do_resize:\n+                images[i] = self.resize(image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                images[i] = self.rescale(image=images[i], scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                images[i] = self.normalize(\n+                    image=images[i],\n+                    mean=image_mean,\n+                    std=image_std,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            images[i] = to_channel_dimension_format(images[i], data_format, input_channel_dim=input_data_format)\n+\n+        encoded_outputs = BatchFeature(data={\"pixel_values\": images, \"grids\": grids}, tensor_type=return_tensors)\n+\n+        return encoded_outputs\n+\n+    def crop_image_to_patches(\n+        self,\n+        images: np.ndarray,\n+        min_patches: int,\n+        max_patches: int,\n+        use_covering_area_grid: bool = True,\n+        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        data_format: ChannelDimension = None,\n+        covering_threshold: float = 0.9,\n+    ):\n+        \"\"\"\n+        Crop the image to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            images (`np.ndarray`):\n+                The image to be cropped.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+                Whether to use the covering area grid to determine the number of patches.\n+            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+            data_format (`ChannelDimension`, *optional*):\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+            covering_threshold (`float`, *optional*, defaults to `0.9`):\n+                The threshold for the covering area grid. If the covering area is less than this value, the grid is\n+                considered invalid.\n+\n+        Returns:\n+            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        if data_format is None:\n+            data_format = infer_channel_dimension_format(images)\n+        images = to_channel_dimension_format(images, ChannelDimension.FIRST, data_format)\n+        patch_size_height, patch_size_width = patch_size[\"height\"], patch_size[\"width\"]\n+        original_height, original_width = images.shape[-2:]\n+\n+        if use_covering_area_grid:\n+            # Use the original OVIS2 approach: compute the minimal number of tiles that cover at least 90% of the image area\n+            num_columns, num_rows = get_min_tile_covering_grid(\n+                (original_height, original_width),\n+                target_patch_size=patch_size_height,  # square patch size\n+                max_image_tiles=max_patches,\n+                covering_threshold=covering_threshold,\n+            )\n+        else:\n+            # find the closest aspect ratio to the target\n+            num_columns, num_rows = get_optimal_tiled_canvas(\n+                (original_height, original_width),\n+                (patch_size_height, patch_size_width),\n+                min_patches,\n+                max_patches,\n+            )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = self.resize(\n+            images,\n+            {\"height\": target_height, \"width\": target_width},\n+            data_format=ChannelDimension.FIRST,\n+            input_data_format=ChannelDimension.FIRST,\n+        )\n+\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image[..., box[1] : box[3], box[0] : box[2]]\n+            patch_image = to_channel_dimension_format(patch_image, data_format, ChannelDimension.FIRST)\n+            processed_images.append(patch_image)\n+\n+        if len(processed_images) != 1:\n+            thumbnail_img = self.resize(\n+                images, patch_size, data_format=data_format, input_data_format=ChannelDimension.FIRST\n+            )\n+            processed_images.insert(0, thumbnail_img)\n+\n+        return processed_images, (num_rows, num_columns)\n+\n+\n+__all__ = [\"Ovis2ImageProcessor\"]"
        },
        {
            "sha": "e5940421828dc633ae424e0f134ac5dddfbd4153",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,254 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+from .image_processing_ovis2 import get_min_tile_covering_grid, get_optimal_tiled_canvas\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class Ovis2ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    Args:\n+        crop_to_patches (`bool`, *optional*, defaults to `False`):\n+            Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+            `preprocess` method.\n+        min_patches (`int`, *optional*, defaults to 1):\n+            The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+        max_patches (`int`, *optional*, defaults to 12):\n+            The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+        use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+            Whether to use the covering area grid to determine the number of patches. Only has an effect if\n+            `crop_to_patches` is set to `True`. Can be overridden by the `use_covering_area_grid` parameter in the\n+            `preprocess` method.\n+    \"\"\"\n+\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+    use_covering_area_grid: Optional[bool]\n+\n+\n+@auto_docstring\n+class Ovis2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    default_to_square = None\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    crop_to_patches = False\n+    min_patches = 1\n+    max_patches = 12\n+    use_covering_area_grid = True\n+    valid_kwargs = Ovis2ImageProcessorKwargs\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Ovis2ImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def crop_image_to_patches(\n+        self,\n+        images: \"torch.Tensor\",\n+        min_patches: int,\n+        max_patches: int,\n+        use_covering_area_grid: bool = True,\n+        covering_threshold: float = 0.9,\n+        patch_size: Optional[Union[tuple, int, dict]] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+    ):\n+        \"\"\"\n+        Crop the images to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to be cropped.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+                Whether to use the original OVIS2 approach: compute the minimal number of tiles that cover at least 90%\n+                of the image area. If `False`, the closest aspect ratio to the target is used.\n+            covering_threshold (`float`, *optional*, defaults to `0.9`):\n+                The threshold for the covering area. Only has an effect if `use_covering_area_grid` is set to `True`.\n+            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+            interpolation (`InterpolationMode`):\n+                Resampling filter to use if resizing the image.\n+\n+        Returns:\n+            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        num_image = images.shape[0]\n+        patch_size_height, patch_size_width = patch_size.height, patch_size.width\n+        original_height, original_width = images.shape[-2:]\n+\n+        if use_covering_area_grid:\n+            # Use the original OVIS2 approach: compute the minimal number of tiles that cover at least 90% of the image area\n+            num_columns, num_rows = get_min_tile_covering_grid(\n+                (original_height, original_width),\n+                target_patch_size=patch_size_height,  # square patch size\n+                max_image_tiles=max_patches,\n+                covering_threshold=covering_threshold,\n+            )\n+        else:\n+            # find the closest aspect ratio to the target\n+            num_columns, num_rows = get_optimal_tiled_canvas(\n+                (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n+            )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = self.resize(\n+            images, SizeDict(height=target_height, width=target_width), interpolation=interpolation\n+        )\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image[..., box[1] : box[3], box[0] : box[2]]\n+            processed_images.append(patch_image)\n+\n+        if len(processed_images) != 1:\n+            thumbnail_img = self.resize(images, patch_size, interpolation=interpolation)\n+            processed_images.insert(0, thumbnail_img)\n+\n+        processed_images = torch.stack(processed_images, dim=0).transpose(0, 1).contiguous()\n+        grid = [[num_rows, num_columns] for _ in range(num_image)]\n+\n+        return processed_images, grid\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        crop_to_patches: bool,\n+        min_patches: int,\n+        max_patches: int,\n+        use_covering_area_grid: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        if crop_to_patches and max_patches > 1:\n+            grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+            processed_images_grouped = {}\n+            grids = {}\n+            for shape, stacked_images in grouped_images.items():\n+                stacked_images, grid = self.crop_image_to_patches(\n+                    stacked_images,\n+                    min_patches,\n+                    max_patches,\n+                    patch_size=size,\n+                    use_covering_area_grid=use_covering_area_grid,\n+                    interpolation=interpolation,\n+                )\n+                processed_images_grouped[shape] = stacked_images\n+                grids[shape] = grid\n+            images = reorder_images(processed_images_grouped, grouped_images_index)\n+            images = [image for images_list in images for image in images_list]\n+            grids = reorder_images(grids, grouped_images_index)\n+        else:\n+            grids = [[1, 1] for _ in range(len(images))]\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images, \"grids\": grids}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Ovis2ImageProcessorFast\"]"
        },
        {
            "sha": "8ac758cebad37255e869fb01c623cbe8b66b52ec",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "added",
            "additions": 902,
            "deletions": 0,
            "changes": 902,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,902 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/ovis2/modular_ovis2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ovis2.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ..auto import AutoModel\n+from .configuration_ovis2 import Ovis2Config, Ovis2VisionConfig\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Ovis2ModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Ovis2 causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Ovis2CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Ovis2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Ovis2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Ovis2VisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Ovis2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            padding=\"valid\",\n+        )\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2\n+        self.num_positions = self.num_patches\n+        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n+        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n+        self.rms_norm = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n+        embeddings = self.rms_norm(embeddings)\n+\n+        embeddings = embeddings + self.position_embedding(self.position_ids)\n+\n+        return embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Ovis2VisionAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Ovis2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Ovis2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Ovis2VisionEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.attention = Ovis2Attention(config)\n+        self.ffn = Ovis2MLP(config)\n+        self.rms_norm1 = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.rms_norm2 = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        norm_hidden_states = self.rms_norm1(hidden_states)\n+        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+\n+        hidden_states = hidden_states + attn_output\n+        norm_hidden_states = self.rms_norm2(hidden_states)\n+        mlp_output = self.ffn(norm_hidden_states)\n+\n+        hidden_states = hidden_states + mlp_output\n+        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+\n+\n+class Ovis2VisionEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Ovis2VisionEncoderLayer`].\n+\n+    Args:\n+        config: Ovis2VisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Ovis2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+class Ovis2VisionTransformer(nn.Module):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embeddings = Ovis2VisionEmbeddings(config)\n+        self.encoder = Ovis2VisionEncoder(config)\n+        self.rms_norm = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ):\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=last_hidden_state,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Ovis2VisualEmbeddingTable(nn.Embedding):\n+    def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n+        if visual_tokens.dtype in [torch.int8, torch.int16, torch.int32, torch.int64, torch.long]:\n+            return super().forward(visual_tokens)\n+        return torch.matmul(visual_tokens, self.weight)\n+\n+\n+class Ovis2PreTrainedModel(PreTrainedModel):\n+    config: Ovis2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Ovis2VisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+\n+\n+def hard_softmax(logits: torch.Tensor, dim: int):\n+    y_soft = logits.softmax(dim)\n+    # Straight through.\n+    index = y_soft.max(dim, keepdim=True)[1]\n+    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n+    ret = y_hard - y_soft.detach() + y_soft\n+\n+    return ret\n+\n+\n+class Ovis2VisionModel(Ovis2PreTrainedModel):\n+    config: Ovis2VisionConfig\n+\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.transformer = Ovis2VisionTransformer(config)\n+        self.num_visual_indicator_tokens = config.num_visual_indicator_tokens\n+        self.vocab_size = config.vocab_size\n+        self.head_linear = nn.Linear(\n+            config.hidden_size * config.hidden_stride * config.hidden_stride,\n+            self.vocab_size - self.num_visual_indicator_tokens,\n+            bias=False,\n+        )\n+        self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        outputs = self.transformer(pixel_values)\n+        last_hidden_state = outputs.last_hidden_state\n+\n+        if self.config.hidden_stride > 1:\n+            num_images, seq_len, hidden_dim = last_hidden_state.shape\n+            hidden_stride = self.config.hidden_stride\n+\n+            sqrt_l = int(math.sqrt(seq_len))\n+            if sqrt_l * sqrt_l != seq_len:\n+                raise ValueError(\"Token sequence length must be a perfect square\")\n+\n+            pad_size = (hidden_stride - (sqrt_l % hidden_stride)) % hidden_stride\n+            last_hidden_state = nn.functional.pad(last_hidden_state, (0, 0, 0, pad_size, 0, pad_size), \"constant\", 0)\n+            sqrt_l += pad_size\n+\n+            last_hidden_state = last_hidden_state.reshape(\n+                num_images, sqrt_l // hidden_stride, hidden_stride, sqrt_l // hidden_stride, hidden_stride, hidden_dim\n+            )\n+            last_hidden_state = last_hidden_state.permute(0, 1, 3, 2, 4, 5)\n+            last_hidden_state = last_hidden_state.reshape(\n+                num_images, -1, hidden_stride * hidden_stride * hidden_dim\n+            )  # (n, (sqrt_l//hs)^2, hs^2*d)\n+\n+        logits = self.head_linear(last_hidden_state)\n+        logits = self.head_norm(logits)\n+\n+        if self.config.tokenize_function == \"gumbel_argmax\":\n+            prob_token = nn.functional.gumbel_softmax(logits, dim=-1, hard=True)\n+        elif self.config.tokenize_function == \"st_argmax\":\n+            prob_token = hard_softmax(logits, dim=-1)\n+        elif self.config.tokenize_function == \"softmax\":\n+            prob_token = nn.functional.softmax(logits, dim=-1)\n+\n+        return prob_token\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Ovis2 model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n+)\n+class Ovis2Model(Ovis2PreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Ovis2Config):\n+        super().__init__(config)\n+        self.vision_tower = Ovis2VisionModel(config.vision_config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.visual_embeddings_table = Ovis2VisualEmbeddingTable(config.vision_config.vocab_size, config.hidden_size)\n+\n+        self.visual_vocab_size = config.vision_config.vocab_size\n+        self.vocab_size = config.vocab_size\n+        self.visual_indicator_token_ids = config.visual_indicator_token_ids\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`, *optional*):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_features = self.vision_tower(pixel_values)\n+        batch_size, img_seq_len, _ = image_features.shape\n+        padding_tensor = torch.zeros(\n+            (batch_size, img_seq_len, self.vision_tower.num_visual_indicator_tokens),\n+            dtype=image_features.dtype,\n+            device=image_features.device,\n+            requires_grad=False,\n+            layout=image_features.layout,\n+        )\n+        image_features = torch.cat([image_features, padding_tensor], dim=2)\n+        image_features = self.visual_embeddings_table(image_features)\n+\n+        visual_indicator = torch.arange(\n+            self.visual_vocab_size - self.vision_tower.num_visual_indicator_tokens,\n+            self.visual_vocab_size,\n+            dtype=torch.long,\n+        ).to(image_features.device)\n+        visual_indicator_features = self.visual_embeddings_table(visual_indicator)\n+\n+        return image_features, visual_indicator_features\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, Ovis2ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features, visual_indicator_features = self.get_image_features(pixel_values=pixel_values)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = special_image_mask.sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = image_features.reshape(-1, image_features.shape[-1])\n+            n_image_features = image_features.shape[0]\n+            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+            for i, visual_indicator_id in enumerate(self.visual_indicator_token_ids):\n+                if input_ids is None:\n+                    mask = inputs_embeds == self.get_input_embeddings()(\n+                        torch.tensor(visual_indicator_id, dtype=torch.long, device=inputs_embeds.device)\n+                    )\n+                    mask = mask.all(-1)\n+                else:\n+                    mask = (input_ids == visual_indicator_id).to(inputs_embeds.device)\n+\n+                if mask.any():\n+                    inputs_embeds[mask] = (\n+                        visual_indicator_features[i]\n+                        .expand_as(inputs_embeds[mask])\n+                        .to(inputs_embeds.device, inputs_embeds.dtype)\n+                    )\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        return Ovis2ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring\n+class Ovis2ForConditionalGeneration(Ovis2PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: Ovis2Config):\n+        super().__init__(config)\n+        self.model = Ovis2Model(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        return self.model.get_image_features(pixel_values=pixel_values)\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        raise AttributeError(\"Not needed for Ovis2\")\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, Ovis2CausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Ovis2ForConditionalGeneration\n+\n+        >>> model = Ovis2ForConditionalGeneration.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n+        >>> processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n+\n+        >>> prompt = \"<|im_start|>user\\n<image>\\nDescribe the image.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        >>> url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True)[0]\n+        \"user\\n\\nDescribe the image.\\nassistant\\nThe image features a brown dog standing on a wooden floor, looking up with\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Ovis2CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"Ovis2PreTrainedModel\", \"Ovis2Model\", \"Ovis2ForConditionalGeneration\"]"
        },
        {
            "sha": "63d946eebb9095e9ae13ce2a87766e21ad48be6f",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "added",
            "additions": 443,
            "deletions": 0,
            "changes": 443,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,443 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ..aimv2.modeling_aimv2 import Aimv2Attention, Aimv2EncoderLayer\n+from ..auto import AutoModel\n+from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n+from ..llava.modeling_llava import LlavaForConditionalGeneration, LlavaModel\n+from ..llava_next.modeling_llava_next import LlavaNextCausalLMOutputWithPast, LlavaNextModelOutputWithPast\n+from ..siglip.modeling_siglip import SiglipEncoder, SiglipVisionEmbeddings\n+from .configuration_ovis2 import Ovis2Config, Ovis2VisionConfig\n+\n+\n+def hard_softmax(logits: torch.Tensor, dim: int):\n+    y_soft = logits.softmax(dim)\n+    # Straight through.\n+    index = y_soft.max(dim, keepdim=True)[1]\n+    y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n+    ret = y_hard - y_soft.detach() + y_soft\n+\n+    return ret\n+\n+\n+class Ovis2ModelOutputWithPast(LlavaNextModelOutputWithPast):\n+    pass\n+\n+\n+class Ovis2CausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n+    pass\n+\n+\n+class Ovis2RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Ovis2VisionMLP(LlamaMLP):\n+    pass\n+\n+\n+class Ovis2VisionEmbeddings(SiglipVisionEmbeddings):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.rms_norm = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def interpolate_pos_encoding(self):\n+        raise NotImplementedError(\"Not needed for Ovis2\")\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n+        embeddings = self.rms_norm(embeddings)\n+\n+        embeddings = embeddings + self.position_embedding(self.position_ids)\n+\n+        return embeddings\n+\n+\n+class Ovis2VisionAttention(Aimv2Attention):\n+    pass\n+\n+\n+class Ovis2VisionEncoderLayer(Aimv2EncoderLayer):\n+    pass\n+\n+\n+class Ovis2VisionEncoder(SiglipEncoder):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.layers = nn.ModuleList([Ovis2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+\n+class Ovis2VisionTransformer(nn.Module):\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embeddings = Ovis2VisionEmbeddings(config)\n+        self.encoder = Ovis2VisionEncoder(config)\n+        self.rms_norm = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ):\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=last_hidden_state,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Ovis2VisualEmbeddingTable(nn.Embedding):\n+    def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n+        if visual_tokens.dtype in [torch.int8, torch.int16, torch.int32, torch.int64, torch.long]:\n+            return super().forward(visual_tokens)\n+        return torch.matmul(visual_tokens, self.weight)\n+\n+\n+class Ovis2PreTrainedModel(PreTrainedModel):\n+    config: Ovis2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Ovis2VisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+\n+\n+class Ovis2VisionModel(Ovis2PreTrainedModel):\n+    config: Ovis2VisionConfig\n+\n+    def __init__(self, config: Ovis2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.transformer = Ovis2VisionTransformer(config)\n+        self.num_visual_indicator_tokens = config.num_visual_indicator_tokens\n+        self.vocab_size = config.vocab_size\n+        self.head_linear = nn.Linear(\n+            config.hidden_size * config.hidden_stride * config.hidden_stride,\n+            self.vocab_size - self.num_visual_indicator_tokens,\n+            bias=False,\n+        )\n+        self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        outputs = self.transformer(pixel_values)\n+        last_hidden_state = outputs.last_hidden_state\n+\n+        if self.config.hidden_stride > 1:\n+            num_images, seq_len, hidden_dim = last_hidden_state.shape\n+            hidden_stride = self.config.hidden_stride\n+\n+            sqrt_l = int(math.sqrt(seq_len))\n+            if sqrt_l * sqrt_l != seq_len:\n+                raise ValueError(\"Token sequence length must be a perfect square\")\n+\n+            pad_size = (hidden_stride - (sqrt_l % hidden_stride)) % hidden_stride\n+            last_hidden_state = nn.functional.pad(last_hidden_state, (0, 0, 0, pad_size, 0, pad_size), \"constant\", 0)\n+            sqrt_l += pad_size\n+\n+            last_hidden_state = last_hidden_state.reshape(\n+                num_images, sqrt_l // hidden_stride, hidden_stride, sqrt_l // hidden_stride, hidden_stride, hidden_dim\n+            )\n+            last_hidden_state = last_hidden_state.permute(0, 1, 3, 2, 4, 5)\n+            last_hidden_state = last_hidden_state.reshape(\n+                num_images, -1, hidden_stride * hidden_stride * hidden_dim\n+            )  # (n, (sqrt_l//hs)^2, hs^2*d)\n+\n+        logits = self.head_linear(last_hidden_state)\n+        logits = self.head_norm(logits)\n+\n+        if self.config.tokenize_function == \"gumbel_argmax\":\n+            prob_token = nn.functional.gumbel_softmax(logits, dim=-1, hard=True)\n+        elif self.config.tokenize_function == \"st_argmax\":\n+            prob_token = hard_softmax(logits, dim=-1)\n+        elif self.config.tokenize_function == \"softmax\":\n+            prob_token = nn.functional.softmax(logits, dim=-1)\n+\n+        return prob_token\n+\n+\n+class Ovis2Model(LlavaModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Ovis2Config):\n+        super().__init__(config)\n+        self.vision_tower = Ovis2VisionModel(config.vision_config)\n+        self.visual_embeddings_table = Ovis2VisualEmbeddingTable(config.vision_config.vocab_size, config.hidden_size)\n+\n+        self.visual_vocab_size = config.vision_config.vocab_size\n+        self.vocab_size = config.vocab_size\n+        self.visual_indicator_token_ids = config.visual_indicator_token_ids\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        del self.multi_modal_projector\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ) -> torch.FloatTensor:\n+        image_features = self.vision_tower(pixel_values)\n+        batch_size, img_seq_len, _ = image_features.shape\n+        padding_tensor = torch.zeros(\n+            (batch_size, img_seq_len, self.vision_tower.num_visual_indicator_tokens),\n+            dtype=image_features.dtype,\n+            device=image_features.device,\n+            requires_grad=False,\n+            layout=image_features.layout,\n+        )\n+        image_features = torch.cat([image_features, padding_tensor], dim=2)\n+        image_features = self.visual_embeddings_table(image_features)\n+\n+        visual_indicator = torch.arange(\n+            self.visual_vocab_size - self.vision_tower.num_visual_indicator_tokens,\n+            self.visual_vocab_size,\n+            dtype=torch.long,\n+        ).to(image_features.device)\n+        visual_indicator_features = self.visual_embeddings_table(visual_indicator)\n+\n+        return image_features, visual_indicator_features\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, Ovis2ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features, visual_indicator_features = self.get_image_features(pixel_values=pixel_values)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = special_image_mask.sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = image_features.reshape(-1, image_features.shape[-1])\n+            n_image_features = image_features.shape[0]\n+            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+            for i, visual_indicator_id in enumerate(self.visual_indicator_token_ids):\n+                if input_ids is None:\n+                    mask = inputs_embeds == self.get_input_embeddings()(\n+                        torch.tensor(visual_indicator_id, dtype=torch.long, device=inputs_embeds.device)\n+                    )\n+                    mask = mask.all(-1)\n+                else:\n+                    mask = (input_ids == visual_indicator_id).to(inputs_embeds.device)\n+\n+                if mask.any():\n+                    inputs_embeds[mask] = (\n+                        visual_indicator_features[i]\n+                        .expand_as(inputs_embeds[mask])\n+                        .to(inputs_embeds.device, inputs_embeds.dtype)\n+                    )\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        return Ovis2ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring\n+class Ovis2ForConditionalGeneration(LlavaForConditionalGeneration, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Ovis2Config):\n+        super().__init__(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+    @property\n+    def multi_modal_projector(self):\n+        raise AttributeError(\"Not needed for Ovis2\")\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        return self.model.get_image_features(pixel_values=pixel_values)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, Ovis2CausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Ovis2ForConditionalGeneration\n+\n+        >>> model = Ovis2ForConditionalGeneration.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n+        >>> processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n+\n+        >>> prompt = \"<|im_start|>user\\n<image>\\nDescribe the image.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        >>> url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True)[0]\n+        \"user\\n\\nDescribe the image.\\nassistant\\nThe image features a brown dog standing on a wooden floor, looking up with\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Ovis2CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+\n+__all__ = [\"Ovis2PreTrainedModel\", \"Ovis2Model\", \"Ovis2ForConditionalGeneration\"]"
        },
        {
            "sha": "dcc9b25d7b34d18cd7856ec97804ba2eab5c1a51",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "added",
            "additions": 181,
            "deletions": 0,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,181 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Ovis2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"image_kwargs\": {},\n+    }\n+\n+\n+class Ovis2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Ovis2 processor which wraps Ovis2 image processor and a Qwen2 tokenizer into a single processor.\n+\n+    [`Ovis2Processor`] offers all the functionalities of [`Ovis2VideoProcessor`], [`Ovis2ImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n+    [`~Ovis2Processor.__call__`] and [`~Ovis2Processor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`Ovis2ImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        image_seq_length (`int`, *optional*, defaults to 256):\n+            The number of image tokens to be used for each image in the input.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        image_token=\"<image>\",\n+        image_seq_length=256,\n+        **kwargs,\n+    ):\n+        self.image_seq_length = image_seq_length\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if getattr(tokenizer, \"image_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.image_token)\n+        )\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        **kwargs: Unpack[Ovis2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        Ovis2ImageProcessor's [`~Ovis2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **image_sizes** -- Size of each image that will be used to unpad an image. Returned when `images` is not `None`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            Ovis2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        image_inputs = {}\n+\n+        if images is not None:\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            image_grids = image_inputs.pop(\"grids\").tolist()\n+            text = self._expand_image_tokens(text, image_grids)\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def _expand_image_tokens(\n+        self,\n+        text: list[TextInput],\n+        grids: list[list[int]],\n+    ):\n+        processed_text = []\n+        grid_index = 0\n+        for sample in text:\n+            while \"<image>\" in sample:\n+                grid = grids[grid_index]\n+                row, col = grid[0], grid[1]\n+                placeholder = f\"<IMG_START>{'<IMG_ATOM>' * self.image_seq_length}<IMG_GRID>\"\n+                if row * col > 1:\n+                    for r in range(row):\n+                        for c in range(col):\n+                            placeholder += f\"{'<IMG_ATOM>' * self.image_seq_length}\"\n+                            if c < col - 1:\n+                                placeholder += \"<IMG_COL>\"\n+                        if r < row - 1:\n+                            placeholder += \"<IMG_ROW>\"\n+                placeholder += \"<IMG_END>\"\n+\n+                sample = sample.replace(\"<image>\", placeholder, 1)\n+                grid_index += 1\n+            processed_text.append(sample)\n+        return processed_text\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+\n+__all__ = [\"Ovis2Processor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ovis2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2F__init__.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5"
        },
        {
            "sha": "f0bf1370ab359fecfec91a24560345fd69a1abef",
            "filename": "tests/models/ovis2/test_image_processing_ovis2.py",
            "status": "added",
            "additions": 177,
            "deletions": 0,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_image_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_image_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2Ftest_image_processing_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,177 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.image_utils import SizeDict\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from transformers import Ovis2ImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import Ovis2ImageProcessorFast\n+\n+\n+class Ovis2ImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        do_pad=False,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_pad = do_pad\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"do_pad\": self.do_pad,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class Ovis2ProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = Ovis2ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Ovis2ImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Ovis2ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+\n+    def test_slow_fast_equivalence_crop_to_patches(self):\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)[0]\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        # torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+    def test_slow_fast_equivalence_batched_crop_to_patches(self):\n+        # Prepare image inputs so that we have two groups of images with equal resolution with a group of images with\n+        # different resolutions in between\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        dummy_images += self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        dummy_images += self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        # torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+    def test_crop_to_patches(self):\n+        # test slow image processor\n+        image_processor = self.image_processor_list[0](**self.image_processor_dict)\n+        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)[0]\n+        processed_images, grid = image_processor.crop_image_to_patches(\n+            image,\n+            min_patches=1,\n+            max_patches=6,\n+            patch_size={\"height\": 20, \"width\": 20},\n+        )\n+        self.assertEqual(len(processed_images), 5)\n+        self.assertEqual(processed_images[0].shape[:2], (20, 20))\n+        self.assertEqual(len(grid), 2)  # (row, col)\n+\n+        # test fast image processor (process batch)\n+        image_processor = self.image_processor_list[1](**self.image_processor_dict)\n+        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)[0]\n+        processed_images, grid = image_processor.crop_image_to_patches(\n+            image.unsqueeze(0),\n+            min_patches=1,\n+            max_patches=6,\n+            patch_size=SizeDict(height=20, width=20),\n+        )\n+        self.assertEqual(len(processed_images[0]), 5)\n+        self.assertEqual(processed_images.shape[-2:], (20, 20))\n+        self.assertEqual(len(grid[0]), 2)"
        },
        {
            "sha": "55f12114a12c4a25eb9813d4c78d328b500ea5f9",
            "filename": "tests/models/ovis2/test_modeling_ovis2.py",
            "status": "added",
            "additions": 384,
            "deletions": 0,
            "changes": 384,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,384 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    Ovis2Config,\n+    Ovis2ForConditionalGeneration,\n+    Ovis2Model,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class Ovis2VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        seq_length=7,\n+        text_config={\n+            \"model_type\": \"qwen2\",\n+            \"seq_length\": 7,\n+            \"is_training\": True,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 64,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 4,\n+            \"intermediate_size\": 54,\n+            \"hidden_act\": \"gelu\",\n+            \"max_position_embeddings\": 580,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"pad_token_id\": 0,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"image_size\": 32,\n+            \"patch_size\": 8,\n+            \"num_channels\": 3,\n+            \"hidden_size\": 64,\n+            \"vocab_size\": 99,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 54,\n+            \"attention_dropout\": 0.0,\n+            \"hidden_act\": \"silu\",\n+            \"qkv_bias\": False,\n+            \"hidden_stride\": 2,\n+            \"tokenize_function\": \"softmax\",\n+        },\n+        image_token_id=1,\n+        visual_indicator_token_ids=[],\n+        vocab_size=99,\n+        hidden_size=64,\n+        ignore_id=-100,\n+    ):\n+        self.parent = parent\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+        self.visual_indicator_token_ids = visual_indicator_token_ids\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.image_seq_length = (\n+            vision_config[\"image_size\"] // (vision_config[\"patch_size\"] * vision_config[\"hidden_stride\"])\n+        ) ** 2\n+        self.seq_length = seq_length + self.image_seq_length\n+        self.is_training = is_training\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.ignore_id = ignore_id\n+\n+        self.batch_size = 3\n+        self.num_channels = 3\n+\n+    def get_config(self):\n+        return Ovis2Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_id=self.image_token_id,\n+            visual_indicator_token_ids=self.visual_indicator_token_ids,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.vision_config[\"num_channels\"],\n+                self.vision_config[\"image_size\"],\n+                self.vision_config[\"image_size\"],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+\n+        vocab_range = self.vocab_size - 2\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], vocab_range) + 2\n+        input_ids[:, : self.image_seq_length] = config.image_token_id\n+\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+\n+        labels = torch.zeros((self.batch_size, self.seq_length), dtype=torch.long, device=torch_device)\n+        labels[:, : self.image_seq_length] = self.ignore_id\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"labels\": labels,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Ovis2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `Ovis2ForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            Ovis2Model,\n+            Ovis2ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = {\"image-text-to-text\": Ovis2ForConditionalGeneration} if is_torch_available() else {}\n+    _is_composite = True\n+    test_pruning = False\n+    test_torchscript = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = Ovis2VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Ovis2Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+\n+@require_torch\n+@slow\n+class Ovis2IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\",\n+        )\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        self.image = Image.open(requests.get(url, stream=True).raw)\n+        self.prompt_image = \"\"\n+        self.messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What do you see in this image?\"},\n+                ],\n+            }\n+        ]\n+        self.text = self.processor.apply_chat_template(self.messages, add_generation_prompt=True, tokenize=False)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_small_model_integration_test(self):\n+        model = Ovis2ForConditionalGeneration.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\", torch_dtype=\"bfloat16\", device_map=torch_device\n+        )\n+\n+        inputs = self.processor(images=self.image, text=self.text, return_tensors=\"pt\").to(\n+            torch_device, torch.bfloat16\n+        )\n+\n+        self.assertTrue(inputs.input_ids.shape[1] == 1314)  # should expand num-image-tokens times\n+        self.assertTrue(inputs.pixel_values.shape == torch.Size([5, 3, 448, 448]))\n+\n+        inputs = inputs.to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=64)\n+        EXPECTED_DECODED_TEXT = 'system\\nYou are a helpful assistant.\\nuser\\n\\nWhat do you see in this image?\\nassistant\\nI see two cats lying on a pink blanket. There are also two remote controls on the blanket.'  # fmt: skip\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch(self):\n+        model = Ovis2ForConditionalGeneration.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\", torch_dtype=\"bfloat16\", device_map=torch_device\n+        )\n+\n+        inputs = self.processor(\n+            text=[self.text],\n+            images=self.image,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.bfloat16)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = ['system\\nYou are a helpful assistant.\\nuser\\n\\nWhat do you see in this image?\\nassistant\\nI see two cats lying on a pink blanket. There are also two remote controls on the blanket.']  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_multi_image(self):\n+        # related to (#29835)\n+        model = Ovis2ForConditionalGeneration.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\",\n+            torch_dtype=\"bfloat16\",\n+            device_map=torch_device,\n+        )\n+\n+        url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        prompt = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What do you see in these images?\"},\n+                ],\n+            }\n+        ]\n+        text = self.processor.apply_chat_template(prompt, add_generation_prompt=True, tokenize=False)\n+        inputs = self.processor(text=text, images=[self.image, image], return_tensors=\"pt\").to(\n+            torch_device, torch.bfloat16\n+        )\n+\n+        output = model.generate(**inputs, max_new_tokens=40)\n+        EXPECTED_DECODED_TEXT = 'system\\nYou are a helpful assistant.\\nuser\\n\\n\\nWhat do you see in these images?\\nassistant\\nIn the first image, I see two cats lying on a pink blanket with remote controls nearby. The second image shows a dog standing on a wooden floor near a kitchen cabinet.'  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch_different_resolutions(self):\n+        model = Ovis2ForConditionalGeneration.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\", torch_dtype=\"bfloat16\", device_map=torch_device\n+        )\n+\n+        lowres_url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000537955.jpg\"\n+        lowres_img = Image.open(requests.get(lowres_url, stream=True).raw).resize((320, 240))\n+\n+        inputs = self.processor(\n+            text=[self.text, self.text],\n+            images=[lowres_img, self.image],\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.bfloat16)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'system\\nYou are a helpful assistant.\\nuser\\n\\nWhat do you see in this image?\\nassistant\\nAnswer: I see a brown dog standing on a wooden floor in what appears to be a kitchen.',\n+            'system\\nYou are a helpful assistant.\\nuser\\n\\nWhat do you see in this image?\\nassistant\\nI see two cats lying on a pink blanket. There are also two remote controls on the blanket.'\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch_matches_single(self):\n+        model = Ovis2ForConditionalGeneration.from_pretrained(\n+            \"thisisiron/Ovis2-2B-hf\",\n+            torch_dtype=\"bfloat16\",\n+            device_map=torch_device,\n+        )\n+\n+        lowres_url = \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\"\n+        lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n+\n+        inputs_batched = self.processor(\n+            text=[self.text, self.text],\n+            images=[self.image, lowres_img],\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.bfloat16)\n+\n+        inputs_single = self.processor(text=self.text, images=self.image, return_tensors=\"pt\", padding=True).to(\n+            torch_device, torch.bfloat16\n+        )\n+\n+        output_batched = model.generate(**inputs_batched, max_new_tokens=50)\n+        output_single = model.generate(**inputs_single, max_new_tokens=50)\n+\n+        self.assertEqual(\n+            self.processor.decode(output_batched[0], skip_special_tokens=True),\n+            self.processor.decode(output_single[0], skip_special_tokens=True),\n+        )"
        },
        {
            "sha": "b431b186089a397b0cd87f6c92bef252fab17fe1",
            "filename": "tests/models/ovis2/test_processor_ovis2.py",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_processor_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/tests%2Fmodels%2Fovis2%2Ftest_processor_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2Ftest_processor_ovis2.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -0,0 +1,118 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers.testing_utils import require_av, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import (\n+        AutoProcessor,\n+        Ovis2ImageProcessor,\n+        Ovis2Processor,\n+        Qwen2TokenizerFast,\n+    )\n+\n+\n+@require_vision\n+class Ovis2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Ovis2Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = Ovis2ImageProcessor()\n+        tokenizer = Qwen2TokenizerFast.from_pretrained(\"thisisiron/Ovis2-1B-hf\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = Ovis2Processor(image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def prepare_processor_dict(self):\n+        return {\n+            \"chat_template\": \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n'}}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<image>\\n' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{'<|im_end|>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}\",\n+        }  # fmt: skip\n+\n+    def test_processor_to_json_string(self):\n+        processor = self.get_processor()\n+        obj = json.loads(processor.to_json_string())\n+        for key, value in self.prepare_processor_dict().items():\n+            # chat_tempalate are tested as a separate test because they are saved in separate files\n+            if key != \"chat_template\":\n+                self.assertEqual(obj[key], value)\n+                self.assertEqual(getattr(processor, key, None), value)\n+\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded)\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-1B-hf\")\n+        expected_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @require_av\n+    def test_chat_template_dict(self):\n+        processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-1B-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = [[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 27, 1805, 397, 3838, 374, 6839, 304, 419, 2168, 30, 151645, 198, 151644, 77091, 198]]  # fmt: skip\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])"
        },
        {
            "sha": "a6c1b77fe721b33663d78e22eee9598285ab8929",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/47938f8f8df23c01248a417cd031c2b4b16a2da5/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47938f8f8df23c01248a417cd031c2b4b16a2da5/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=47938f8f8df23c01248a417cd031c2b4b16a2da5",
            "patch": "@@ -94,6 +94,7 @@\n     \"Glm4vVisionModel\",\n     \"Glm4vMoeVisionModel\",\n     \"EvollaSaProtPreTrainedModel\",\n+    \"Ovis2VisionModel\",\n ]\n \n # Update this list for models that are not tested with a comment explaining the reason it should not be."
        }
    ],
    "stats": {
        "total": 3763,
        "additions": 3763,
        "deletions": 0
    }
}