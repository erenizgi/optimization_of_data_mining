{
    "author": "Isotr0py",
    "message": "Add GGUF support to Gemma3 Text backbone (#37424)\n\n* add gemma3 gguf support\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix typo and add gguf limit\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix a typo\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* add vision conversion test\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix typos\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n---------\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "6daec12d0b2f922c8ad27b1cb0d51446d00d8016",
    "files": [
        {
            "sha": "f1a25f2744cc7d3d73d7e95d39c1c7a99f08a731",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=6daec12d0b2f922c8ad27b1cb0d51446d00d8016",
            "patch": "@@ -204,6 +204,23 @@\n         \"attention.head_count\": \"num_attention_heads\",\n         \"attention.head_count_kv\": \"num_key_value_heads\",\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"attention.sliding_window\": \"sliding_window\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n+    \"gemma3\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        # NOTE: Gemma3 has key_length==value_length==head_dim\n+        # See: https://github.com/ggml-org/llama.cpp/blob/fe5b78c89670b2f37ecb216306bed3e677b49d9f/convert_hf_to_gguf.py#L3495-L3496\n+        \"attention.key_length\": \"head_dim\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"attention.sliding_window\": \"sliding_window\",\n         \"vocab_size\": \"vocab_size\",\n     },\n }\n@@ -669,6 +686,7 @@ def converted(self) -> Tokenizer:\n     \"mamba\": GGUFGPTConverter,\n     \"nemotron\": GGUFGPTConverter,\n     \"gemma2\": GGUFGemmaConverter,\n+    \"gemma3_text\": GGUFGemmaConverter,\n }\n \n "
        },
        {
            "sha": "c1aa57d790a8dfba2519fcfa6dadff71b6505c94",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=6daec12d0b2f922c8ad27b1cb0d51446d00d8016",
            "patch": "@@ -253,6 +253,7 @@ def process(self, weights, name, **kwargs):\n     \"mamba\": MambaTensorProcessor,\n     \"nemotron\": NemotronTensorProcessor,\n     \"gemma2\": Gemma2TensorProcessor,\n+    \"gemma3\": Gemma2TensorProcessor,\n }\n \n \n@@ -292,6 +293,8 @@ def get_gguf_hf_weights_map(\n         model_type = \"command-r\"\n     elif model_type == \"qwen2_moe\":\n         model_type = \"qwen2moe\"\n+    elif model_type == \"gemma3_text\":\n+        model_type = \"gemma3\"\n     arch = None\n     for key, value in MODEL_ARCH_NAMES.items():\n         if value == model_type:\n@@ -438,6 +441,10 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n         if gguf_key in reader_keys:\n             logger.info(f\"Some keys were not parsed and added into account {gguf_key} | {value}\")\n \n+    # Gemma3 GGUF checkpoint only contains weights of text backbone\n+    if parsed_parameters[\"config\"][\"model_type\"] == \"gemma3\":\n+        parsed_parameters[\"config\"][\"model_type\"] = \"gemma3_text\"\n+\n     # retrieve config vocab_size from tokenizer\n     # Please refer to https://github.com/huggingface/transformers/issues/32526 for more details\n     if \"vocab_size\" not in parsed_parameters[\"config\"]:"
        },
        {
            "sha": "d1f53b986cc4d2f2df1033ceb19cf9a1ee183291",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 71,
            "deletions": 0,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6daec12d0b2f922c8ad27b1cb0d51446d00d8016/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=6daec12d0b2f922c8ad27b1cb0d51446d00d8016",
            "patch": "@@ -296,6 +296,10 @@ class GgufModelTests(unittest.TestCase):\n     nemotron_model_id = \"bartowski/Nemotron-Mini-4B-Instruct-GGUF\"\n     original_gemma2_model_id = \"google/gemma-2-2b-it\"\n     gemma2_model_id = \"bartowski/gemma-2-2b-it-GGUF\"\n+    original_gemma3_text_model_id = \"google/gemma-3-1b-it\"\n+    original_gemma3_vision_model_id = \"google/gemma-3-4b-it\"\n+    gemma3_text_model_id = \"unsloth/gemma-3-1b-it-GGUF\"\n+    gemma3_vision_model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n \n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n@@ -325,6 +329,9 @@ class GgufModelTests(unittest.TestCase):\n     q3_k_gemma2_model_id = \"gemma-2-2b-it-Q3_K_L.gguf\"\n     q8_0_gemma2_model_id = \"gemma-2-2b-it-Q8_0.gguf\"\n     fp32_gemma2_model_id = \"gemma-2-2b-it-f32.gguf\"\n+    q2_k_gemma3_text_model_id = \"gemma-3-1b-it-Q2_K.gguf\"\n+    bf16_gemma3_text_model_id = \"gemma-3-1b-it-BF16.gguf\"\n+    bf16_gemma3_vision_model_id = \"gemma-3-4b-it-BF16.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -881,3 +888,67 @@ def test_gemma2_weights_conversion_fp32(self):\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n             else:\n                 raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n+    def test_gemma3_text_q2_k(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma3_text_model_id,\n+            gguf_file=self.q2_k_gemma3_text_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gemma3_text_model_id, gguf_file=self.q2_k_gemma3_text_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello,\\n\\nI'm looking for a small,\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    @require_read_token\n+    @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n+    def test_gemma3_text_weights_conversion_bf16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_gemma3_text_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma3_text_model_id,\n+            gguf_file=self.bf16_gemma3_text_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    # Test text backbone conversion for gemma3 vision models\n+    @require_read_token\n+    @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n+    def test_gemma3_vision_weights_conversion_bf16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_gemma3_vision_model_id,\n+            torch_dtype=torch.float16,\n+        ).language_model\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma3_vision_model_id,\n+            gguf_file=self.bf16_gemma3_vision_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 96,
        "deletions": 0
    }
}