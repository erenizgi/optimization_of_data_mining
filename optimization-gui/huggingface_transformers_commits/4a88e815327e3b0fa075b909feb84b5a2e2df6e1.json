{
    "author": "agamjots05",
    "message": "Add Fast Image Processor for ImageGPT (#39592)\n\n* initial commit\n\n* initial setup\n\n* Overiding imageGPT specific functions\n\n* imported is_torch_available and utilized it for importing torch in imageGPT fast\n\n* Created init and ImageGPTFastImageProcessorKwargs\n\n* added return_tensors, data_format, and input_data_format to ImageGPTFastImageProcessorKwargs\n\n* set up arguments and process and _preprocess definitions\n\n* Added arguments to _preprocess\n\n* Added additional optional arguments\n\n* Copied logic over from base imageGPT processor\n\n* Implemented 2nd draft of fast imageGPT preprocess using batch processing\n\n* Implemented 3rd draft of imageGPT fast _preprocessor. Pulled logic from BaseImageProcessorFast\n\n* modified imageGPT test file to properly run fast processor tests\n\n* converts images to torch.float32 from torch.unit8\n\n* fixed a typo with self.image_processor_list in the imagegpt test file\n\n* updated more instances of image_processing = self.image_processing_class in the test file to test fast processor\n\n* standardized normalization to not use image mean or std\n\n* Merged changes from solution2 branch\n\n* Merged changes from solution2 test file\n\n* fixed testing through baseImageGPT processor file\n\n* Fixed check_code_quality test. Removed unncessary list comprehension.\n\n* reorganized imports in image_processing_imagegpt_fast\n\n* formatted image_processing_imagegpt_fast.py\n\n* Added arg documentation\n\n* Added FastImageProcessorKwargs class + Docs for new kwargs\n\n* Reformatted previous\n\n* Added F to normalization\n\n* fixed ruff linting and cleaned up fast processor file\n\n* implemented requested changes\n\n* fixed ruff checks\n\n* fixed formatting issues\n\n* fix(ruff after merging main)\n\n* simplify logic and reuse standard equivalenec tests\n\n---------\n\nCo-authored-by: Ethan Ayaay <ayaayethan@gmail.com>\nCo-authored-by: chris <christine05789@gmail.com>\nCo-authored-by: Ethan Ayaay <98191976+ayaayethan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
    "files": [
        {
            "sha": "d995a92ec912d12d825e5f68a9332c9abc015916",
            "filename": "docs/source/en/model_doc/imagegpt.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fimagegpt.md?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -104,6 +104,11 @@ If you're interested in submitting a resource to be included here, please feel f\n [[autodoc]] ImageGPTImageProcessor\n     - preprocess\n \n+## ImageGPTImageProcessorFast\n+\n+[[autodoc]] ImageGPTImageProcessorFast\n+    - preprocess\n+\n ## ImageGPTModel\n \n [[autodoc]] ImageGPTModel"
        },
        {
            "sha": "499bfb5b2bdf74965102587e17d021d69f5afbd7",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -111,7 +111,7 @@\n             (\"idefics2\", (\"Idefics2ImageProcessor\", \"Idefics2ImageProcessorFast\")),\n             (\"idefics3\", (\"Idefics3ImageProcessor\", \"Idefics3ImageProcessorFast\")),\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"imagegpt\", (\"ImageGPTImageProcessor\", None)),\n+            (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),"
        },
        {
            "sha": "098ffb6296f547e6dd9f1f990d21e28bc5cb0f7b",
            "filename": "src/transformers/models/imagegpt/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2F__init__.py?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_imagegpt import *\n     from .feature_extraction_imagegpt import *\n     from .image_processing_imagegpt import *\n+    from .image_processing_imagegpt_fast import *\n     from .modeling_imagegpt import *\n else:\n     import sys"
        },
        {
            "sha": "1f202662751572ceaa427a32ac398f9173365eb6",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -26,7 +26,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_flat_list_of_images,\n+    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -238,7 +238,7 @@ def preprocess(\n         clusters = clusters if clusters is not None else self.clusters\n         clusters = np.array(clusters)\n \n-        images = make_flat_list_of_images(images)\n+        images = make_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -247,7 +247,7 @@ def preprocess(\n             )\n \n         # Here, normalize() is using a constant factor to divide pixel values.\n-        # hence, the method does not need image_mean and image_std.\n+        # hence, the method does not need iamge_mean and image_std.\n         validate_preprocess_arguments(\n             do_resize=do_resize,\n             size=size,\n@@ -291,14 +291,24 @@ def preprocess(\n \n             # We need to convert back to a list of images to keep consistent behaviour across processors.\n             images = list(images)\n+            data = {\"input_ids\": images}\n         else:\n-            images = [\n-                to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-                for image in images\n-            ]\n-\n-        data = {\"input_ids\": images}\n+            images = [to_channel_dimension_format(image, data_format, input_data_format) for image in images]\n+            data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n+    def to_dict(self):\n+        output = super().to_dict()\n+        # Ensure clusters are JSON/equality friendly\n+        if output.get(\"clusters\") is not None and isinstance(output[\"clusters\"], np.ndarray):\n+            output[\"clusters\"] = output[\"clusters\"].tolist()\n+        # Need to set missing keys from slow processor to match the expected behavior in save/load tests compared to fast processor\n+        missing_keys = [\"image_mean\", \"image_std\", \"rescale_factor\", \"do_rescale\"]\n+        for key in missing_keys:\n+            if key in output:\n+                output[key] = None\n+\n+        return output\n+\n \n __all__ = [\"ImageGPTImageProcessor\"]"
        },
        {
            "sha": "736666fd28a0fdc8a719bc8f564654b8dbf17b21",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt_fast.py",
            "status": "added",
            "additions": 209,
            "deletions": 0,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -0,0 +1,209 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for ImageGPT.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import PILImageResampling\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+def squared_euclidean_distance_torch(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute squared Euclidean distances between all pixels and clusters.\n+\n+    Args:\n+        a: (N, 3) tensor of pixel RGB values\n+        b: (M, 3) tensor of cluster RGB values\n+\n+    Returns:\n+        (N, M) tensor of squared distances\n+    \"\"\"\n+    b = b.t()  # (3, M)\n+    a2 = torch.sum(a**2, dim=1)  # (N,)\n+    b2 = torch.sum(b**2, dim=0)  # (M,)\n+    ab = torch.matmul(a, b)  # (N, M)\n+    d = a2[:, None] - 2 * ab + b2[None, :]  # Squared Euclidean Distance: a^2 - 2ab + b^2\n+    return d  # (N, M) tensor of squared distances\n+\n+\n+def color_quantize_torch(x: torch.Tensor, clusters: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Assign each pixel to its nearest color cluster.\n+\n+    Args:\n+        x: (H*W, 3) tensor of flattened pixel RGB values\n+        clusters: (n_clusters, 3) tensor of cluster RGB values\n+\n+    Returns:\n+        (H*W,) tensor of cluster indices\n+    \"\"\"\n+    d = squared_euclidean_distance_torch(x, clusters)\n+    return torch.argmin(d, dim=1)\n+\n+\n+class ImageGPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):\n+        The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n+        in `preprocess`.\n+    do_color_quantize (`bool`, *optional*, defaults to `True`):\n+        Controls whether to apply color quantization to convert continuous pixel values to discrete cluster indices.\n+        When True, each pixel is assigned to its nearest color cluster, enabling ImageGPT's discrete token modeling.\n+    \"\"\"\n+\n+    clusters: Optional[Union[np.ndarray, list[list[int]], torch.Tensor]]\n+    do_color_quantize: Optional[bool]\n+\n+\n+@auto_docstring\n+class ImageGPTImageProcessorFast(BaseImageProcessorFast):\n+    model_input_names = [\"input_ids\"]\n+    resample = PILImageResampling.BILINEAR\n+    do_color_quantize = True\n+    clusters = None\n+    image_mean = [0.5, 0.5, 0.5]\n+    image_std = [0.5, 0.5, 0.5]\n+    do_rescale = True\n+    do_normalize = True\n+    valid_kwargs = ImageGPTFastImageProcessorKwargs\n+\n+    def __init__(\n+        self,\n+        clusters: Optional[Union[list, np.ndarray, torch.Tensor]] = None,  # keep as arg for backwards compatibility\n+        **kwargs: Unpack[ImageGPTFastImageProcessorKwargs],\n+    ):\n+        r\"\"\"\n+        clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):\n+            The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n+            in `preprocess`.\n+        \"\"\"\n+        clusters = torch.as_tensor(clusters, dtype=torch.float32) if clusters is not None else None\n+        super().__init__(clusters=clusters, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: dict[str, int],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: dict[str, int],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_color_quantize: Optional[bool] = None,\n+        clusters: Optional[Union[list, np.ndarray, torch.Tensor]] = None,\n+        disable_grouping: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ):\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        pixel_values = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # If color quantization is requested, perform it; otherwise return pixel values\n+        if do_color_quantize:\n+            # Prepare clusters\n+            if clusters is None:\n+                raise ValueError(\"Clusters must be provided for color quantization.\")\n+            # Convert to torch tensor if needed (clusters might be passed as list/numpy)\n+            clusters_torch = (\n+                torch.as_tensor(clusters, dtype=torch.float32) if not isinstance(clusters, torch.Tensor) else clusters\n+            ).to(pixel_values[0].device, dtype=pixel_values[0].dtype)\n+\n+            # Group images by shape for batch processing\n+            # We need to check if the pixel values are a tensor or a list of tensors\n+            grouped_images, grouped_images_index = group_images_by_shape(\n+                pixel_values, disable_grouping=disable_grouping\n+            )\n+            # Process each group\n+            input_ids_grouped = {}\n+\n+            for shape, stacked_images in grouped_images.items():\n+                input_ids = color_quantize_torch(\n+                    stacked_images.permute(0, 2, 3, 1).reshape(-1, 3), clusters_torch\n+                )  # (B*H*W, C)\n+                input_ids_grouped[shape] = input_ids.reshape(stacked_images.shape[0], -1).reshape(\n+                    stacked_images.shape[0], -1\n+                )  # (B, H, W)\n+\n+            input_ids = reorder_images(input_ids_grouped, grouped_images_index)\n+\n+            return BatchFeature(\n+                data={\"input_ids\": torch.stack(input_ids, dim=0) if return_tensors else input_ids},\n+                tensor_type=return_tensors,\n+            )\n+\n+        pixel_values = torch.stack(pixel_values, dim=0) if return_tensors else pixel_values\n+        return BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n+\n+    def to_dict(self):\n+        # Convert torch tensors to lists for JSON serialization\n+        output = super().to_dict()\n+        if output.get(\"clusters\") is not None and isinstance(output[\"clusters\"], torch.Tensor):\n+            output[\"clusters\"] = output[\"clusters\"].tolist()\n+\n+        return output\n+\n+\n+__all__ = [\"ImageGPTImageProcessorFast\"]"
        },
        {
            "sha": "8c04d9585022e1ca93f1b84a724f7c0561aab202",
            "filename": "tests/models/imagegpt/test_image_processing_imagegpt.py",
            "status": "modified",
            "additions": 172,
            "deletions": 83,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a88e815327e3b0fa075b909feb84b5a2e2df6e1/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py?ref=4a88e815327e3b0fa075b909feb84b5a2e2df6e1",
            "patch": "@@ -19,11 +19,21 @@\n import unittest\n \n import numpy as np\n+import pytest\n+import requests\n from datasets import load_dataset\n+from packaging import version\n \n from transformers import AutoImageProcessor\n-from transformers.testing_utils import check_json_file_has_correct_format, require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    check_json_file_has_correct_format,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -36,6 +46,9 @@\n \n     from transformers import ImageGPTImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ImageGPTImageProcessorFast\n+\n \n class ImageGPTImageProcessingTester:\n     def __init__(\n@@ -94,6 +107,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class ImageGPTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ImageGPTImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ImageGPTImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -104,50 +118,54 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"clusters\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"clusters\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_image_processor_to_json_string(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        obj = json.loads(image_processor.to_json_string())\n-        for key, value in self.image_processor_dict.items():\n-            if key == \"clusters\":\n-                self.assertTrue(np.array_equal(value, obj[key]))\n-            else:\n-                self.assertEqual(obj[key], value)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            obj = json.loads(image_processor.to_json_string())\n+            for key, value in self.image_processor_dict.items():\n+                if key == \"clusters\":\n+                    self.assertTrue(np.array_equal(value, obj[key]))\n+                else:\n+                    self.assertEqual(obj[key], value)\n \n     def test_image_processor_to_json_file(self):\n-        image_processor_first = self.image_processing_class(**self.image_processor_dict)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_first = image_processing_class(**self.image_processor_dict)\n \n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            json_file_path = os.path.join(tmpdirname, \"image_processor.json\")\n-            image_processor_first.to_json_file(json_file_path)\n-            image_processor_second = self.image_processing_class.from_json_file(json_file_path).to_dict()\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                json_file_path = os.path.join(tmpdirname, \"image_processor.json\")\n+                image_processor_first.to_json_file(json_file_path)\n+                image_processor_second = image_processing_class.from_json_file(json_file_path).to_dict()\n \n-        image_processor_first = image_processor_first.to_dict()\n-        for key, value in image_processor_first.items():\n-            if key == \"clusters\":\n-                self.assertTrue(np.array_equal(value, image_processor_second[key]))\n-            else:\n-                self.assertEqual(value, value)\n+            image_processor_first = image_processor_first.to_dict()\n+            for key, value in image_processor_first.items():\n+                if key == \"clusters\":\n+                    self.assertTrue(np.array_equal(value, image_processor_second[key]))\n+                else:\n+                    self.assertEqual(image_processor_first[key], value)\n \n     def test_image_processor_from_and_save_pretrained(self):\n         for image_processing_class in self.image_processor_list:\n-            image_processor_first = self.image_processing_class(**self.image_processor_dict)\n+            image_processor_first = image_processing_class(**self.image_processor_dict)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 image_processor_first.save_pretrained(tmpdirname)\n-                image_processor_second = self.image_processing_class.from_pretrained(tmpdirname).to_dict()\n+                image_processor_second = image_processing_class.from_pretrained(tmpdirname).to_dict()\n \n             image_processor_first = image_processor_first.to_dict()\n             for key, value in image_processor_first.items():\n@@ -181,68 +199,139 @@ def test_init_without_params(self):\n \n     # Override the test from ImageProcessingTestMixin as ImageGPT model takes input_ids as input\n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(encoded_images)\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(encoded_images)\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n \n     # Override the test from ImageProcessingTestMixin as ImageGPT model takes input_ids as input\n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(encoded_images)\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(encoded_images)\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n \n     @unittest.skip(reason=\"ImageGPT assumes clusters for 3 channels\")\n     def test_call_numpy_4_channels(self):\n         pass\n \n     # Override the test from ImageProcessingTestMixin as ImageGPT model takes input_ids as input\n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n-        self.assertEqual(\n-            tuple(encoded_images.shape),\n-            (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").input_ids\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").input_ids\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )\n+\n+    # For quantization-based processors, use absolute tolerance only to avoid infinity issues\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.input_ids.float(), encoding_fast.input_ids.float(), atol=1.0, rtol=0\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.input_ids.float(), encoding_fast.input_ids.float(), atol=1.0, rtol=0\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.input_ids.float(), output_compiled.input_ids.float(), atol=1.0, rtol=0\n         )\n \n "
        }
    ],
    "stats": {
        "total": 500,
        "additions": 407,
        "deletions": 93
    }
}