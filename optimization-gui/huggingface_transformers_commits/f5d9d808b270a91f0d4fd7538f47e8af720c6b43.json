{
    "author": "Cyrilvallez",
    "message": "Hardcode the factor in caching allocator (#42996)\n\n* small stuff\n\n* fix",
    "sha": "f5d9d808b270a91f0d4fd7538f47e8af720c6b43",
    "files": [
        {
            "sha": "48063f2720ad5fea85f548470e591b415aa288b2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d9d808b270a91f0d4fd7538f47e8af720c6b43/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d9d808b270a91f0d4fd7538f47e8af720c6b43/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f5d9d808b270a91f0d4fd7538f47e8af720c6b43",
            "patch": "@@ -4590,8 +4590,6 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n     - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n     However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n-    factor = 2\n-\n     # Remove disk, cpu and meta devices, and cast to proper torch.device\n     accelerator_device_map = {\n         param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)\n@@ -4620,7 +4618,8 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n                 index\n             ) - torch_accelerator_module.memory_allocated(index)\n             byte_count = int(max(0, byte_count - unused_memory))\n-        _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n+        # We divide by 2 here as we allocate in fp16\n+        _ = torch.empty(byte_count // 2, dtype=torch.float16, device=device, requires_grad=False)\n \n \n class AttentionInterface(GeneralInterface):"
        },
        {
            "sha": "a055bf5c62639bdc3469a8f1b9f84ee259995a82",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5d9d808b270a91f0d4fd7538f47e8af720c6b43/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5d9d808b270a91f0d4fd7538f47e8af720c6b43/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f5d9d808b270a91f0d4fd7538f47e8af720c6b43",
            "patch": "@@ -1214,18 +1214,14 @@ def test_all_tensors_are_parameter_or_buffer(self):\n             # Now, run all the inits\n             model.init_weights()\n \n-            # Prepare inputs to correct device\n+            # Prepare inputs\n             inputs = self._prepare_for_class(inputs_dict, model_class)\n-            final_inputs = {}\n-            for k, v in inputs.items():\n-                if isinstance(v, torch.Tensor):\n-                    final_inputs[k] = v.to(device=\"cpu\")\n-                else:\n-                    final_inputs[k] = v\n+            # Inputs may be on cuda -> move to cpu, we don't care about accelerator for this test\n+            inputs = {k: v.to(\"cpu\") if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n \n             # Try running a forward, to see if a tensor stayed on meta somewhere\n             try:\n-                _ = model(**final_inputs)\n+                _ = model(**inputs)\n             except (RuntimeError, NotImplementedError) as e:\n                 # Re-raise a more friendly exception (unfortunately, we cannot know which tensor it was...)\n                 if \"Cannot copy out of meta tensor; no data!\" in str("
        }
    ],
    "stats": {
        "total": 17,
        "additions": 6,
        "deletions": 11
    }
}