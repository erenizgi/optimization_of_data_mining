{
    "author": "SunMarc",
    "message": "deprecate `overwrite_output_dir` (#41323)\n\n* dep\n\n* style\n\n* rm\n\n* wut\n\n* style",
    "sha": "776eea8612458f60bbe684f38ea34f85fe890486",
    "files": [
        {
            "sha": "77de2998ad0bff6d09cbf2458548fcaa113d6bd8",
            "filename": "ISSUES.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/ISSUES.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/ISSUES.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/ISSUES.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -153,7 +153,7 @@ You are not required to read the following guidelines before opening an issue. H\n     cd examples/seq2seq\n     torchrun --nproc_per_node=2 ./finetune_trainer.py \\\n     --model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro \\\n-    --output_dir output_dir --overwrite_output_dir \\\n+    --output_dir output_dir \\\n     --do_train --n_train 500 --num_train_epochs 1 \\\n     --per_device_train_batch_size 1  --freeze_embeds \\\n     --src_lang en_XX --tgt_lang ro_RO --task translation \\"
        },
        {
            "sha": "052e6e1d8440dd91587ebfe33b43811276168da7",
            "filename": "docs/source/ar/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Far%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Far%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -93,7 +93,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -117,7 +116,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -140,7 +138,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -197,7 +194,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -225,7 +221,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -239,25 +234,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n Ø®ÙŠØ§Ø± Ø¢Ø®Ø± Ù…ÙÙŠØ¯ Ù„ØªÙ…ÙƒÙŠÙ†Ù‡ Ù‡Ùˆ Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø³Ø§Ø¨Ù‚Ø©. Ø³ÙŠØ¶Ù…Ù† Ø°Ù„Ùƒ Ø£Ù†Ùƒ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù…Ù† Ø­ÙŠØ« ØªÙˆÙ‚ÙØª Ø¯ÙˆÙ† Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ØªÙ… Ù…Ù‚Ø§Ø·Ø¹Ø© ØªØ¯Ø±ÙŠØ¨Ùƒ. Ù‡Ù†Ø§Ùƒ Ø·Ø±ÙŠÙ‚ØªØ§Ù† Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´.\n \n-ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„Ù…Ø¹Ù„Ù…Ø© `output_dir previous_output_dir` Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø£Ø­Ø¯Ø« Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ø®Ø²Ù†Ø© ÙÙŠ `output_dir`. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¥Ø²Ø§Ù„Ø© `overwrite_output_dir`:\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ø¹Ù„Ù…Ø© `resume_from_checkpoint path_to_specific_checkpoint` Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù…Ø¬Ù„Ø¯ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ø­Ø¯Ø¯Ø©.\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -269,7 +245,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -301,6 +276,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "be9e44cfa620ce2c87ff5057059e9052119b50dc",
            "filename": "docs/source/ar/trainer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -611,7 +611,6 @@ accelerate launch \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ù† Ù…Ù„Ù `config_file.yaml` Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±:\n@@ -634,7 +633,6 @@ accelerate launch --num_processes=2 \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠ [Launching your Accelerate scripts](https://huggingface.co/docs/accelerate/basic_tutorials/launch) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ `accelerate_launch` ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©."
        },
        {
            "sha": "833d886c7e810ecca98839e0e9fbd8d22f2b8071",
            "filename": "docs/source/de/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fde%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fde%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -98,7 +98,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -122,7 +121,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -144,7 +142,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -201,7 +198,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -229,7 +225,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -243,25 +238,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n Eine weitere hilfreiche Option, die Sie aktivieren kÃ¶nnen, ist die Wiederaufnahme des Trainings von einem frÃ¼heren Kontrollpunkt aus. Auf diese Weise kÃ¶nnen Sie im Falle einer Unterbrechung Ihres Trainings dort weitermachen, wo Sie aufgehÃ¶rt haben, ohne von vorne beginnen zu mÃ¼ssen. Es gibt zwei Methoden, um das Training von einem Kontrollpunkt aus wieder aufzunehmen.\n \n-Die erste Methode verwendet das Argument `output_dir previous_output_dir`, um das Training ab dem letzten in `output_dir` gespeicherten Kontrollpunkt wieder aufzunehmen. In diesem Fall sollten Sie `overwrite_output_dir` entfernen:\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-Die zweite Methode verwendet das Argument `Resume_from_checkpoint path_to_specific_checkpoint`, um das Training ab einem bestimmten Checkpoint-Ordner wieder aufzunehmen.\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -273,7 +249,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -305,6 +280,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "7971854011ee04afa5192a6b707328a24d501789",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -593,7 +593,7 @@ To deploy DeepSpeed on multiple GPUs, add `--num_gpus`. You don't need to add `-\n deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero3.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -616,7 +616,7 @@ To deploy DeepSpeed on a single GPU, add `--num_gpus`. You don't need to add `--\n deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero2.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro"
        },
        {
            "sha": "74473b5228e3d04df55ea341a10db0a4fe7bc085",
            "filename": "docs/source/en/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -61,9 +61,8 @@ The example below fine-tunes [T5-small](https://huggingface.co/google-t5/t5-smal\n \n The example script downloads and preprocesses a dataset, and then fine-tunes it with [`Trainer`] with a supported model architecture.\n \n-Resuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again. There are two ways to resume training from a checkpoint.\n+Resuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again: \n \n-* `--output dir previous_output_dir` resumes training from the latest checkpoint stored in `output_dir`. Remove the `--overwrite_output_dir` parameter if you're using this method.\n * `--resume_from_checkpoint path_to_specific_checkpoint` resumes training from a specific checkpoint folder.\n \n Share your model on the [Hub](https://huggingface.co/) with the `--push_to_hub` parameter. It creates a repository and uploads the model to the folder name specified in `--output_dir`. You could also use the `--push_to_hub_model_id` parameter to specify the repository name.\n@@ -85,9 +84,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --per_device_eval_batch_size=4 \\\n     --push_to_hub \\\n     --push_to_hub_model_id finetuned-t5-cnn_dailymail \\\n-    # remove if using `output_dir previous_output_dir`\n-    # --overwrite_output_dir \\\n-    --output_dir previous_output_dir \\\n     # --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate \\\n ```\n@@ -168,7 +164,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate \\"
        },
        {
            "sha": "98b23a3e7b9493fa89a2648e380ad6c720690782",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -361,8 +361,7 @@ accelerate launch \\\n     --per_device_train_batch_size 16 \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n-    --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n+    --output_dir /tmp/$TASK_NAME/\n ```\n \n > [!TIP]"
        },
        {
            "sha": "6db78af2ce5ebd4702f687bf48e473e5c5a3eed1",
            "filename": "docs/source/es/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fes%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fes%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -98,7 +98,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -122,7 +121,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -144,7 +142,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -201,7 +198,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -229,7 +225,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -243,25 +238,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n Otra opciÃ³n Ãºtil para habilitar es reanudar el entrenamiento desde un punto de control anterior. Esto asegurarÃ¡ que puedas continuar donde lo dejaste sin comenzar de nuevo si tu entrenamiento se interrumpe. Hay dos mÃ©todos para reanudar el entrenamiento desde un punto de control.\n \n-El primer mÃ©todo utiliza el argumento `output_dir previous_output_dir` para reanudar el entrenamiento desde el Ãºltimo punto de control almacenado en `output_dir`. En este caso, debes eliminar `overwrite_output_dir`:\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-El segundo mÃ©todo utiliza el argumento `resume_from_checkpoint path_to_specific_checkpoint` para reanudar el entrenamiento desde una carpeta de punto de control especÃ­fica.\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -273,7 +249,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -305,6 +280,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "335ec54b39bd3406b91383d0e7896129193a66b8",
            "filename": "docs/source/es/trainer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -381,7 +381,6 @@ accelerate launch \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n TambiÃ©n puedes especificar los parÃ¡metros del archivo config_file.yaml directamente en la lÃ­nea de comandos:\n@@ -404,7 +403,6 @@ accelerate launch --num_processes=2 \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n Consulta el tutorial [Lanzamiento de tus scripts con Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch) para obtener mÃ¡s informaciÃ³n sobre `accelerate_launch` y las configuraciones personalizadas."
        },
        {
            "sha": "43e3ed0241503fc0b037f10642b3e5df151d6d27",
            "filename": "docs/source/fr/run_scripts_fr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Frun_scripts_fr.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -100,7 +100,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -124,7 +123,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -147,7 +145,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -204,7 +201,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -231,7 +227,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -243,26 +238,7 @@ examples/pytorch/summarization/run_summarization.py -h\n \n ## Reprendre l'entraÃ®nement Ã  partir d'un point de contrÃ´le\n \n-Une autre option utile est de reprendre l'entraÃ®nement Ã  partir d'un point de contrÃ´le prÃ©cÃ©dent. Cela vous permettra de reprendre lÃ  oÃ¹ vous vous Ã©tiez arrÃªtÃ© sans recommencer si votre entraÃ®nement est interrompu. Il existe deux mÃ©thodes pour reprendre l'entraÃ®nement Ã  partir d'un point de contrÃ´le.\n-\n-La premiÃ¨re mÃ©thode utilise l'argument `output_dir previous_output_dir` pour reprendre l'entraÃ®nement Ã  partir du dernier point de contrÃ´le stockÃ© dans `output_dir`. Dans ce cas, vous devez supprimer l'argument `overwrite_output_dir`.\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-La seconde mÃ©thode utilise l'argument `resume_from_checkpoint path_to_specific_checkpoint` pour reprendre l'entraÃ®nement Ã  partir d'un dossier de point de contrÃ´le spÃ©cifique.\n+Une autre option utile est de reprendre l'entraÃ®nement Ã  partir d'un point de contrÃ´le prÃ©cÃ©dent. Cela vous permettra de reprendre lÃ  oÃ¹ vous vous Ã©tiez arrÃªtÃ© sans recommencer si votre entraÃ®nement est interrompu: \n \n ```bash\n python examples/pytorch/summarization/run_summarization.py\n@@ -275,7 +251,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -308,6 +283,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "e0986b83977108cfa3c1d9621412e2218f964041",
            "filename": "docs/source/it/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fit%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fit%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -98,7 +98,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -122,7 +121,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -144,7 +142,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -201,7 +198,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -229,7 +225,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -243,25 +238,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n Un'altra utile opzione Ã¨ riavviare un addestramento da un checkpoint precedente. Questo garantirÃ  che tu possa riprendere da dove hai interrotto senza ricominciare se l'addestramento viene interrotto. Ci sono due metodi per riavviare l'addestramento da un checkpoint: \n \n-Il primo metodo usa l'argomento `output_dir previous_output_dir` per riavviare l'addestramento dall'ultima versione del checkpoint contenuto in `output_dir`. In questo caso, dovresti rimuovere `overwrite_output_dir`:\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-Il secondo metodo usa l'argomento `resume_from_checkpoint path_to_specific_checkpoint` per riavviare un addestramento da una specifica cartella di checkpoint.\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -273,7 +249,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -305,6 +280,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "a8801f0379ea93caf884b49954889fb6372f6c21",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -188,7 +188,7 @@ deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.js\n deepspeed examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero3.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -211,7 +211,7 @@ DeepSpeed é–¢é€£ã®å¼•æ•°ãŒ 2 ã¤ã‚ã‚Šã¾ã™ãŒã€ç°¡å˜ã«ã™ã‚‹ãŸã‚ã§ã‚\n deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero2.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -1789,7 +1789,7 @@ deepspeed examples/pytorch/translation/run_translation.py \\\n --model_name_or_path google-t5/t5-small --output_dir output_dir \\\n --do_eval --max_eval_samples 50 --warmup_steps 50  \\\n --max_source_length 128 --val_max_target_length 128 \\\n---overwrite_output_dir --per_device_eval_batch_size 4 \\\n+--per_device_eval_batch_size 4 \\\n --predict_with_generate --dataset_config \"ro-en\" --fp16 \\\n --source_lang en --target_lang ro --dataset_name wmt16 \\\n --source_prefix \"translate English to Romanian: \""
        },
        {
            "sha": "e5d55ff77b4cacaca9b70cd9e38148145480e0f3",
            "filename": "docs/source/ja/main_classes/trainer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -534,7 +534,6 @@ python examples/pytorch/text-classification/run_glue.py \\\n   --learning_rate 2e-5 \\\n   --num_train_epochs 3 \\\n   --output_dir /tmp/$TASK_NAME/ \\\n-  --overwrite_output_dir\n ```\n \n **æ³¨æ„ã™ã¹ãã„ãã¤ã‹ã®æ³¨æ„äº‹é …**\n@@ -669,7 +668,6 @@ accelerate launch \\\n --learning_rate 5e-5 \\\n --num_train_epochs 3 \\\n --output_dir /tmp/$TASK_NAME/ \\\n---overwrite_output_dir\n ```\n \n 4. `accelerate launch`ã™ã‚‹ãŸã‚ã® cmd å¼•æ•°ã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä¸Šã®ä¾‹ã¯æ¬¡ã®ã‚ˆã†ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚\n@@ -694,7 +692,6 @@ accelerate launch --num_processes=2 \\\n --learning_rate 5e-5 \\\n --num_train_epochs 3 \\\n --output_dir /tmp/$TASK_NAME/ \\\n---overwrite_output_dir\n ```\n \n è©³ç´°ã«ã¤ã„ã¦ã¯ã€ğŸ¤— Accelerate CLI ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„: [ğŸ¤— Accelerate ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èµ·å‹•](https://huggingface.co/docs/accelerate/basic_tutorials/launch)ã€‚"
        },
        {
            "sha": "bf0ed8627024acf10b495bc4e1f4339387753397",
            "filename": "docs/source/ja/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fja%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -104,7 +104,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -131,7 +130,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -153,7 +151,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -212,7 +209,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -240,7 +236,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -254,26 +249,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n ä»¥å‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã™ã‚‹ãŸã‚ã®å½¹ç«‹ã¤ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‚ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚ŒãŸå ´åˆã§ã‚‚ã€æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™ã“ã¨ãªãã€ä¸­æ–­ã—ãŸã¨ã“ã‚ã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã™ã‚‹ãŸã‚ã®2ã¤ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚\n \n-æœ€åˆã®æ–¹æ³•ã¯ã€`output_dir previous_output_dir` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ã€`output_dir` ã«ä¿å­˜ã•ã‚ŒãŸæœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã™ã‚‹æ–¹æ³•ã§ã™ã€‚ã“ã®å ´åˆã€`overwrite_output_dir` ã‚’å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-2ç•ªç›®ã®æ–¹æ³•ã§ã¯ã€`resume_from_checkpoint path_to_specific_checkpoint` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã—ã¾ã™ã€‚\n-\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -285,7 +260,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -319,7 +293,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n "
        },
        {
            "sha": "823865168be463a67d67ccfca92b7ccc653384c2",
            "filename": "docs/source/ko/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fdeepspeed.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -590,7 +590,7 @@ bf16ì€ ì„¤ì • íŒŒì¼ì—ì„œ ì„¤ì •í•˜ê±°ë‚˜ ë‹¤ìŒ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ëª…ë ¹\n deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero3.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -605,7 +605,7 @@ deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \\\n deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero2.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro"
        },
        {
            "sha": "7927f3748f07fd9357a9efed1acd9708b04bf861",
            "filename": "docs/source/ko/perf_train_special.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Fperf_train_special.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Fperf_train_special.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_train_special.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -54,8 +54,7 @@ python examples/pytorch/text-classification/run_glue.py \\\n   --per_device_train_batch_size 32 \\\n   --learning_rate 2e-5 \\\n   --num_train_epochs 3 \\\n-  --output_dir /tmp/$TASK_NAME/ \\\n-  --overwrite_output_dir\n+  --output_dir /tmp/$TASK_NAME/\n ```\n \n `gloco`ì™€ `nccl`ê³¼ ê°™ì€ [ë¶„ì‚° í•™ìŠµ ë°±ì—”ë“œ](https://pytorch.org/docs/stable/distributed.html#backends)ëŠ” `mps` ì¥ì¹˜ì—ì„œ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, MPS ë°±ì—”ë“œì—ì„œëŠ” ë‹¨ì¼ GPUë¡œë§Œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        },
        {
            "sha": "2db8fe16c600a6722c0a75fe2b94a71bf9e67da7",
            "filename": "docs/source/ko/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -106,7 +106,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -131,7 +130,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -156,7 +154,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -216,7 +213,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -245,7 +241,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -262,8 +257,6 @@ examples/pytorch/summarization/run_summarization.py -h\n ì´ë ‡ê²Œ í•˜ë©´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ë”ë¼ë„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•˜ì§€ ì•Šê³  ì¤‘ë‹¨í•œ ë¶€ë¶„ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n \n-ì²« ë²ˆì§¸ëŠ” `output_dir previous_output_dir` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `output_dir`ì— ì €ì¥ëœ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¶€í„° í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n-ì´ ê²½ìš° `overwrite_output_dir`ì„ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤:\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -275,24 +268,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-ë‘ ë²ˆì§¸ëŠ” `resume_from_checkpoint path_to_specific_checkpoint` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ í´ë”ì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -325,6 +300,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "65c3fbef982f2b970f5b8592ef5f0dfc90f7719c",
            "filename": "docs/source/ko/trainer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fko%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -505,7 +505,6 @@ accelerate launch \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n `config_file.yaml` íŒŒì¼ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§ì ‘ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n@@ -528,7 +527,6 @@ accelerate launch --num_processes=2 \\\n     --learning_rate 5e-5 \\\n     --num_train_epochs 3 \\\n     --output_dir /tmp/$TASK_NAME/ \\\n-    --overwrite_output_dir\n ```\n \n `accelerate_launch`ì™€ ì‚¬ìš©ì ì •ì˜ êµ¬ì„±ì— ëŒ€í•´ ë” ì•Œì•„ë³´ë ¤ë©´ [Accelerate ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰](https://huggingface.co/docs/accelerate/basic_tutorials/launch) íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”.\n\\ No newline at end of file"
        },
        {
            "sha": "72060f98571d90738efe06f990612145c0f7ce6c",
            "filename": "docs/source/pt/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -99,7 +99,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -123,7 +122,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -145,7 +143,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -203,7 +200,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -231,7 +227,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -245,25 +240,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n Outra opÃ§Ã£o Ãºtil para habilitar Ã© retomar o treinamento de um checkpoint anterior. Isso garantirÃ¡ que vocÃª possa continuar de onde parou sem recomeÃ§ar se o seu treinamento for interrompido. Existem dois mÃ©todos para retomar o treinamento a partir de um checkpoint.\n \n-O primeiro mÃ©todo usa o argumento `output_dir previous_output_dir` para retomar o treinamento do Ãºltimo checkpoint armazenado em `output_dir`. Neste caso, vocÃª deve remover `overwrite_output_dir`:\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-O segundo mÃ©todo usa o argumento `resume_from_checkpoint path_to_specific_checkpoint` para retomar o treinamento de uma pasta de checkpoint especÃ­fica.\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -275,7 +251,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -307,6 +282,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "2fe0d554a7f6612050266faba340cf8332ed465e",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -179,7 +179,7 @@ deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.js\n deepspeed examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero3.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -202,7 +202,7 @@ deepspeed examples/pytorch/translation/run_translation.py \\\n deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n --deepspeed tests/deepspeed/ds_config_zero2.json \\\n --model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n---output_dir output_dir --overwrite_output_dir --fp16 \\\n+--output_dir output_dir --fp16 \\\n --do_train --max_train_samples 500 --num_train_epochs 1 \\\n --dataset_name wmt16 --dataset_config \"ro-en\" \\\n --source_lang en --target_lang ro\n@@ -1659,7 +1659,7 @@ deepspeed examples/pytorch/translation/run_translation.py \\\n --model_name_or_path google-t5/t5-small --output_dir output_dir \\\n --do_eval --max_eval_samples 50 --warmup_steps 50  \\\n --max_source_length 128 --val_max_target_length 128 \\\n---overwrite_output_dir --per_device_eval_batch_size 4 \\\n+--per_device_eval_batch_size 4 \\\n --predict_with_generate --dataset_config \"ro-en\" --fp16 \\\n --source_lang en --target_lang ro --dataset_name wmt16 \\\n --source_prefix \"translate English to Romanian: \""
        },
        {
            "sha": "5d587fd6d57505a0175e9adc9904a61270244840",
            "filename": "docs/source/zh/main_classes/trainer.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -471,7 +471,6 @@ python examples/pytorch/text-classification/run_glue.py \\\n   --learning_rate 2e-5 \\\n   --num_train_epochs 3 \\\n   --output_dir /tmp/$TASK_NAME/ \\\n-  --overwrite_output_dir\n ```\n \n **éœ€è¦æ³¨æ„çš„ä¸€äº›æ³¨æ„äº‹é¡¹**\n@@ -606,7 +605,6 @@ accelerate launch \\\n --learning_rate 5e-5 \\\n --num_train_epochs 3 \\\n --output_dir /tmp/$TASK_NAME/ \\\n---overwrite_output_dir\n ```\n \n 4. ä½ ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨`accelerate launch`çš„cmdå‚æ•°ã€‚ä¸Šé¢çš„ç¤ºä¾‹å°†æ˜ å°„åˆ°ï¼š\n@@ -631,7 +629,6 @@ accelerate launch --num_processes=2 \\\n --learning_rate 5e-5 \\\n --num_train_epochs 3 \\\n --output_dir /tmp/$TASK_NAME/ \\\n---overwrite_output_dir\n ```\n \n æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… ğŸ¤— Accelerate CLI æŒ‡å—ï¼š[å¯åŠ¨æ‚¨çš„ ğŸ¤— Accelerate è„šæœ¬](https://huggingface.co/docs/accelerate/basic_tutorials/launch)ã€‚"
        },
        {
            "sha": "8e7929ecb1cda43fec592e4ba587e5fad4e6ac83",
            "filename": "docs/source/zh/perf_train_special.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fperf_train_special.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Fperf_train_special.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fperf_train_special.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -50,7 +50,6 @@ python examples/pytorch/text-classification/run_glue.py \\\n   --learning_rate 2e-5 \\\n   --num_train_epochs 3 \\\n   --output_dir /tmp/$TASK_NAME/ \\\n-  --overwrite_output_dir\n ```\n \n ç”¨äº[åˆ†å¸ƒå¼è®¾ç½®](https://pytorch.org/docs/stable/distributed.html#backends)çš„åç«¯(å¦‚`gloo`å’Œ`nccl`)ä¸æ”¯æŒ`mps`è®¾å¤‡ï¼Œè¿™ä¹Ÿæ„å‘³ç€ä½¿ç”¨ MPS åç«¯æ—¶åªèƒ½åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚"
        },
        {
            "sha": "60e78fcb9e9b231dddbea461d03cc83f6123bce3",
            "filename": "docs/source/zh/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Frun_scripts.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -99,7 +99,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -124,7 +123,6 @@ torchrun \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -145,7 +143,6 @@ python xla_spawn.py --num_cores 8 \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -202,7 +199,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --summary_column summary_column_name \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate\n@@ -231,7 +227,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -245,26 +240,6 @@ examples/pytorch/summarization/run_summarization.py -h\n \n å¦ä¸€ä¸ªæœ‰ç”¨çš„é€‰é¡¹æ˜¯ä»ä¹‹å‰çš„checkpointæ¢å¤è®­ç»ƒã€‚è¿™å°†ç¡®ä¿åœ¨è®­ç»ƒä¸­æ–­æ—¶ï¼Œæ‚¨å¯ä»¥ä»ä¹‹å‰åœæ­¢çš„åœ°æ–¹ç»§ç»­è¿›è¡Œï¼Œè€Œæ— éœ€é‡æ–°å¼€å§‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä»checkpointæ¢å¤è®­ç»ƒã€‚\n \n-ç¬¬ä¸€ç§æ–¹æ³•ä½¿ç”¨`output_dir previous_output_dir`å‚æ•°ä»å­˜å‚¨åœ¨`output_dir`ä¸­çš„æœ€æ–°çš„checkpointæ¢å¤è®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥åˆ é™¤`overwrite_output_dir`ï¼š\n-\n-```bash\n-python examples/pytorch/summarization/run_summarization.py\n-    --model_name_or_path google-t5/t5-small \\\n-    --do_train \\\n-    --do_eval \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --source_prefix \"summarize: \" \\\n-    --output_dir /tmp/tst-summarization \\\n-    --per_device_train_batch_size=4 \\\n-    --per_device_eval_batch_size=4 \\\n-    --output_dir previous_output_dir \\\n-    --predict_with_generate\n-```\n-\n-ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨`resume_from_checkpoint path_to_specific_checkpoint`å‚æ•°ä»ç‰¹å®šçš„checkpointæ–‡ä»¶å¤¹æ¢å¤è®­ç»ƒã€‚\n-\n-\n ```bash\n python examples/pytorch/summarization/run_summarization.py\n     --model_name_or_path google-t5/t5-small \\\n@@ -276,7 +251,6 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --resume_from_checkpoint path_to_specific_checkpoint \\\n     --predict_with_generate\n ```\n@@ -309,6 +283,5 @@ python examples/pytorch/summarization/run_summarization.py\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```"
        },
        {
            "sha": "92947e2092cf25755c11f306ea391f5dfaaec032",
            "filename": "examples/legacy/multiple_choice/run_multiple_choice.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -95,17 +95,6 @@ def main():\n     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n     model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use\"\n-            \" --overwrite_output_dir to overcome.\"\n-        )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\","
        },
        {
            "sha": "126fa197ee27e9d39d942dafd801619d7355ca46",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -641,9 +641,6 @@ def main():\n         help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n     )\n     parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n-    )\n     parser.add_argument(\n         \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n     )\n@@ -677,18 +674,6 @@ def main():\n             \"stride or increase the maximum length to ensure the features are correctly built.\"\n         )\n \n-    if (\n-        os.path.exists(args.output_dir)\n-        and os.listdir(args.output_dir)\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n-                args.output_dir\n-            )\n-        )\n-\n     # Setup distant debugging if needed\n     if args.server_ip and args.server_port:\n         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script"
        },
        {
            "sha": "5288e3019b9a672482447ec6d2b4f759c399d213",
            "filename": "examples/legacy/question-answering/run_squad_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -76,17 +76,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use\"\n-            \" --overwrite_output_dir to overcome.\"\n-        )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\","
        },
        {
            "sha": "64c92fa205e0720ccdeb15f9fb9b465728610c0a",
            "filename": "examples/legacy/run_language_modeling.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Frun_language_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Frun_language_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_language_modeling.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -207,16 +207,6 @@ def main():\n             \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n             \"or remove the --do_eval argument.\"\n         )\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use\"\n-            \" --overwrite_output_dir to overcome.\"\n-        )\n \n     # Setup logging\n     logging.basicConfig("
        },
        {
            "sha": "8c80cf8a347acb3c48dfa982aab76156bf66a45d",
            "filename": "examples/legacy/run_swag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_swag.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -557,9 +557,6 @@ def main():\n         help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n     )\n     parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n-    )\n     parser.add_argument(\n         \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n     )\n@@ -584,18 +581,6 @@ def main():\n     parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n     args = parser.parse_args()\n \n-    if (\n-        os.path.exists(args.output_dir)\n-        and os.listdir(args.output_dir)\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n-                args.output_dir\n-            )\n-        )\n-\n     # Setup distant debugging if needed\n     if args.server_ip and args.server_port:\n         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script"
        },
        {
            "sha": "54ca2c898c82656e24bd9b7f4444aa8980ca4006",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -39,7 +39,6 @@\n     Seq2SeqDataset,\n     assert_all_frozen,\n     build_compute_metrics_fn,\n-    check_output_dir,\n     freeze_embeds,\n     freeze_params,\n     lmap,\n@@ -168,8 +167,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    check_output_dir(training_args)\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\","
        },
        {
            "sha": "d0e1075bb0790cbe66c8139dcc53be61746512c2",
            "filename": "examples/legacy/seq2seq/train_distil_marian_enro.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -20,7 +20,7 @@ export MAX_LEN=128\n python finetune_trainer.py \\\n     --tokenizer_name $m --model_name_or_path $m \\\n     --data_dir $ENRO_DIR \\\n-    --output_dir marian_en_ro_6_3 --overwrite_output_dir \\\n+    --output_dir marian_en_ro_6_3 \\\n     --learning_rate=3e-4 \\\n     --warmup_steps 500 --sortish_sampler \\\n     --fp16 \\"
        },
        {
            "sha": "fcf4ea13698f047702075c515c018ae3656d0e39",
            "filename": "examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -22,7 +22,7 @@ python xla_spawn.py --num_cores $TPU_NUM_CORES \\\n     finetune_trainer.py \\\n     --tokenizer_name $m --model_name_or_path $m \\\n     --data_dir $ENRO_DIR \\\n-    --output_dir marian_en_ro_6_3 --overwrite_output_dir \\\n+    --output_dir marian_en_ro_6_3 \\\n     --learning_rate=3e-4 \\\n     --warmup_steps 500 \\\n     --per_device_train_batch_size=$BS --per_device_eval_batch_size=$BS \\"
        },
        {
            "sha": "a490019588ce89bf4c92c15615b86819bffeb487",
            "filename": "examples/legacy/seq2seq/train_distilbart_cnn.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -21,7 +21,7 @@ export MAX_TGT_LEN=142\n python finetune_trainer.py \\\n     --model_name_or_path $m --tokenizer_name $tok \\ \n     --data_dir cnn_dm \\\n-    --output_dir distilbart-cnn-12-6 --overwrite_output_dir \\\n+    --output_dir distilbart-cnn-12-6 \\\n     --learning_rate=3e-5 \\\n     --warmup_steps 500 --sortish_sampler \\\n     --fp16 \\"
        },
        {
            "sha": "fb31790a2c192cc09a8c6f5a26417a467cd05885",
            "filename": "examples/legacy/seq2seq/train_mbart_cc25_enro.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -15,7 +15,7 @@\n python finetune_trainer.py \\\n     --model_name_or_path=facebook/mbart-large-cc25 \\\n     --data_dir $ENRO_DIR \\\n-    --output_dir mbart_cc25_enro --overwrite_output_dir \\\n+    --output_dir mbart_cc25_enro \\\n     --learning_rate=3e-5 \\\n     --warmup_steps 500 \\ \n     --fp16 \\"
        },
        {
            "sha": "43d66128360df8b01a714a621b5ba6096ae5e95a",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -639,27 +639,3 @@ def chunks(lst, n):\n     \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n     for i in range(0, len(lst), n):\n         yield lst[i : i + n]\n-\n-\n-def check_output_dir(args, expected_items=0):\n-    \"\"\"\n-    Checks whether to bail out if output_dir already exists and has more than expected_items in it\n-\n-    `args`: needs to have the following attributes of `args`:\n-      - output_dir\n-      - do_train\n-      - overwrite_output_dir\n-\n-    `expected_items`: normally 0 (default) - i.e. empty dir, but in some cases a few files are expected (e.g. recovery from OOM)\n-    \"\"\"\n-    if (\n-        os.path.exists(args.output_dir)\n-        and len(os.listdir(args.output_dir)) > expected_items\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({args.output_dir}) already exists and \"\n-            f\"has {len(os.listdir(args.output_dir))} items in it (expected {expected_items} items). \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )"
        },
        {
            "sha": "1e6e5e4023105e230a1a99e08a3576c4e70d63b5",
            "filename": "examples/legacy/token-classification/run_ner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -111,17 +111,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use\"\n-            \" --overwrite_output_dir to overcome.\"\n-        )\n-\n     module = import_module(\"tasks\")\n     try:\n         token_classification_task_clazz = getattr(module, model_args.task_type)"
        },
        {
            "sha": "f803f71525e3d1c057c042a0ab350de67e007173",
            "filename": "examples/pytorch/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -72,8 +72,7 @@ token-classification/run_ner.py -h\n \n You can resume training from a previous checkpoint like this:\n \n-1. Pass `--output_dir previous_output_dir` without `--overwrite_output_dir` to resume training from the latest checkpoint in `output_dir` (what you would use if the training was interrupted, for instance).\n-2. Pass `--resume_from_checkpoint path_to_a_specific_checkpoint` to resume training from that checkpoint folder.\n+1. Pass `--resume_from_checkpoint path_to_a_specific_checkpoint` to resume training from that checkpoint folder.\n \n Should you want to turn an example into a notebook where you'd no longer have access to the command\n line, ğŸ¤— Trainer supports resuming from a checkpoint via `trainer.train(resume_from_checkpoint)`."
        },
        {
            "sha": "2910b358762383e11ed11f04d3bc4b16673d5211",
            "filename": "examples/pytorch/audio-classification/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Faudio-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Faudio-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -34,7 +34,6 @@ python run_audio_classification.py \\\n     --dataset_name superb \\\n     --dataset_config_name ks \\\n     --output_dir wav2vec2-base-ft-keyword-spotting \\\n-    --overwrite_output_dir \\\n     --remove_unused_columns False \\\n     --do_train \\\n     --do_eval \\\n@@ -76,7 +75,6 @@ python run_audio_classification.py \\\n     --audio_column_name audio \\\n     --label_column_name language \\\n     --output_dir wav2vec2-base-lang-id \\\n-    --overwrite_output_dir \\\n     --remove_unused_columns False \\\n     --do_train \\\n     --do_eval \\"
        },
        {
            "sha": "3abd6e158d39bf7531df00e4fc409cc828a50682",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -47,7 +47,6 @@\n     TrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -245,21 +244,6 @@ def main():\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to train from scratch.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Initialize our dataset and prepare it for the audio classification task.\n     raw_datasets = DatasetDict()\n     raw_datasets[\"train\"] = load_dataset(\n@@ -408,8 +392,6 @@ def compute_metrics(eval_pred):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "9211c0cc59f9fe5f5a54d96b6d35193ecb6850af",
            "filename": "examples/pytorch/contrastive-image-text/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fcontrastive-image-text%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fcontrastive-image-text%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -97,6 +97,5 @@ python run_clip.py \\\n     --per_device_train_batch_size=\"64\" \\\n     --per_device_eval_batch_size=\"64\" \\\n     --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1 \\\n-    --overwrite_output_dir \\\n     --push_to_hub\n ```"
        },
        {
            "sha": "eac0977a212ba10ae89d3631749e52cf949c3244",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -55,7 +55,6 @@\n     TrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -271,21 +270,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # 3. Detecting last checkpoint and eventually continue from last checkpoint\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # 4. Load dataset\n     # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n     # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n@@ -497,8 +481,6 @@ def filter_corrupt_images(examples):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         tokenizer.save_pretrained(training_args.output_dir)"
        },
        {
            "sha": "04107ffdfb3e356374e910d61e2da39a4c5c6293",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -58,7 +58,6 @@\n     TrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -225,21 +224,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -418,8 +402,6 @@ def val_transforms(example_batch):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "9e1eae48f83496da2791a9dbf8f33e269f60bf24",
            "filename": "examples/pytorch/image-pretraining/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -44,7 +44,6 @@ Alternatively, one can decide to further pre-train an already pre-trained (or fi\n !python run_mim.py \\\n     --model_type vit \\\n     --output_dir ./outputs/ \\\n-    --overwrite_output_dir \\\n     --remove_unused_columns False \\\n     --label_names bool_masked_pos \\\n     --do_train \\\n@@ -95,7 +94,6 @@ Next, we can run the script by providing the path to this custom configuration (\n     --config_name_or_path path_to_config \\\n     --model_type swin \\\n     --output_dir ./outputs/ \\\n-    --overwrite_output_dir \\\n     --remove_unused_columns False \\\n     --label_names bool_masked_pos \\\n     --do_train \\"
        },
        {
            "sha": "ea350a855feb5b7394b1466ee3f8a99cfdefe913",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -41,7 +41,6 @@\n     ViTMAEConfig,\n     ViTMAEForPreTraining,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -217,21 +216,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Initialize our dataset.\n     ds = load_dataset(\n         data_args.dataset_name,\n@@ -377,8 +361,6 @@ def preprocess_images(examples):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "c0b86d009ba806c28a6d085d4f701db32f134ca8",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -44,7 +44,6 @@\n     Trainer,\n     TrainingArguments,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -281,21 +280,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Initialize our dataset.\n     ds = load_dataset(\n         data_args.dataset_name,\n@@ -456,8 +440,6 @@ def preprocess_images(examples):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "8e6342a316074d6faf3142aeedc2337c33ef239c",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -49,7 +49,6 @@\n )\n from transformers.image_processing_utils import BatchFeature\n from transformers.trainer import EvalPrediction\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -334,18 +333,6 @@ def find_last_checkpoint(training_args: TrainingArguments) -> Optional[str]:\n     checkpoint = None\n     if training_args.resume_from_checkpoint is not None:\n         checkpoint = training_args.resume_from_checkpoint\n-    elif os.path.isdir(training_args.output_dir) and not training_args.overwrite_output_dir:\n-        checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n \n     return checkpoint\n "
        },
        {
            "sha": "986f873870dceb8ff62f952bb4e8e602fc10adda",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -63,7 +63,6 @@\n     set_seed,\n )\n from transformers.testing_utils import CaptureLogger\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -317,21 +316,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -665,8 +649,6 @@ def compute_metrics(eval_preds):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "14457275811d615e84d096dbad9222655002651b",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -66,7 +66,6 @@\n )\n from transformers.integrations import is_deepspeed_zero3_enabled\n from transformers.testing_utils import CaptureLogger\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -344,21 +343,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -806,8 +790,6 @@ def compute_metrics(eval_preds):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "80ad49bef983919f306a6a723dc925a4b9706a31",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -62,7 +62,6 @@\n     is_torch_xla_available,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -290,21 +289,6 @@ def main():\n     # Set the verbosity to info of the Transformers logger (on main process only):\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -631,8 +615,6 @@ def compute_metrics(eval_preds):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n         metrics = train_result.metrics"
        },
        {
            "sha": "f81fd37d5e40b4d0a7940edcd741934bb8fabd93",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -55,7 +55,6 @@\n     XLNetLMHeadModel,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -269,21 +268,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -528,8 +512,6 @@ def group_texts(examples):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n         metrics = train_result.metrics"
        },
        {
            "sha": "d1f785d51ca12b95203473b882136f60456b0dfc",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -52,7 +52,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n \n \n@@ -213,21 +212,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -401,8 +385,6 @@ def compute_metrics(eval_predictions):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n         metrics = train_result.metrics"
        },
        {
            "sha": "e2018bf97226d6c891b617086e8bcb21676b65cc",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -51,7 +51,6 @@\n from transformers.image_processing_utils import BatchFeature\n from transformers.image_transforms import center_to_corners_format\n from transformers.trainer import EvalPrediction\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -373,23 +372,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    checkpoint = None\n-    if training_args.resume_from_checkpoint is not None:\n-        checkpoint = training_args.resume_from_checkpoint\n-    elif os.path.isdir(training_args.output_dir) and not training_args.overwrite_output_dir:\n-        checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # ------------------------------------------------------------------------------------------------\n     # Load dataset, prepare splits\n     # ------------------------------------------------------------------------------------------------\n@@ -510,7 +492,7 @@ def main():\n \n     # Training\n     if training_args.do_train:\n-        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n+        train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)\n         trainer.save_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "d0bcb1c8478f20d12db22bd0feb3e6539ad8a52a",
            "filename": "examples/pytorch/old_test_xla_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fold_test_xla_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fold_test_xla_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fold_test_xla_examples.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -55,7 +55,6 @@ def test_run_glue(self):\n             ./examples/pytorch/text-classification/run_glue.py\n             --model_name_or_path distilbert/distilbert-base-uncased\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n             --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n             --do_train"
        },
        {
            "sha": "ab5bf3267b742af96ca1acd45fb0c5e439a4d254",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -43,7 +43,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -262,21 +261,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -646,8 +630,6 @@ def compute_metrics(p: EvalPrediction):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "882f4fa9b272141c970ba425b0d62d864150d5a5",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -41,7 +41,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -260,21 +259,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -672,8 +656,6 @@ def compute_metrics(p: EvalPrediction):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "10827108b93a460c81bc92ec3b6afa294021a485",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -39,7 +39,7 @@\n     Seq2SeqTrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import EvalLoopOutput, EvalPrediction, get_last_checkpoint\n+from transformers.trainer_utils import EvalLoopOutput, EvalPrediction\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -307,21 +307,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -669,8 +654,6 @@ def post_processing_function(\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "6fd83df7aa8f97d021dbb131e764ec80f4da6975",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -52,7 +52,6 @@\n     TrainingArguments,\n     default_data_collator,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -221,21 +220,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Load dataset\n     # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n     # download the dataset.\n@@ -419,8 +403,6 @@ def preprocess_batch(example_batch, transforms: A.Compose):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()\n         trainer.log_metrics(\"train\", train_result.metrics)"
        },
        {
            "sha": "24595492064539abac6be503c3bc938ecda169fb",
            "filename": "examples/pytorch/speech-recognition/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -70,7 +70,6 @@ python run_speech_recognition_ctc.py \\\n \t--model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\n \t--dataset_config_name=\"tr\" \\\n \t--output_dir=\"./wav2vec2-common_voice-tr-demo\" \\\n-\t--overwrite_output_dir \\\n \t--num_train_epochs=\"15\" \\\n \t--per_device_train_batch_size=\"16\" \\\n \t--gradient_accumulation_steps=\"2\" \\\n@@ -106,7 +105,6 @@ torchrun \\\n \t--model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\n \t--dataset_config_name=\"tr\" \\\n \t--output_dir=\"./wav2vec2-common_voice-tr-demo-dist\" \\\n-\t--overwrite_output_dir \\\n \t--num_train_epochs=\"15\" \\\n \t--per_device_train_batch_size=\"4\" \\\n \t--learning_rate=\"3e-4\" \\\n@@ -156,7 +154,6 @@ However, the `--shuffle_buffer_size` argument controls how many examples we can\n \t--train_split_name=\"train+validation\" \\\n \t--eval_split_name=\"test\" \\\n \t--output_dir=\"wav2vec2-xls-r-common_voice-tr-ft\" \\\n-\t--overwrite_output_dir \\\n \t--max_steps=\"5000\" \\\n \t--per_device_train_batch_size=\"8\" \\\n \t--gradient_accumulation_steps=\"2\" \\\n@@ -390,7 +387,6 @@ python run_speech_recognition_seq2seq.py \\\n \t--freeze_feature_encoder=\"False\" \\\n \t--gradient_checkpointing \\\n \t--fp16 \\\n-\t--overwrite_output_dir \\\n \t--do_train \\\n \t--do_eval \\\n \t--predict_with_generate \\\n@@ -431,7 +427,6 @@ torchrun \\\n \t--freeze_feature_encoder=\"False\" \\\n \t--gradient_checkpointing \\\n \t--fp16 \\\n-\t--overwrite_output_dir \\\n \t--do_train \\\n \t--do_eval \\\n \t--predict_with_generate \\\n@@ -539,7 +534,6 @@ python run_speech_recognition_seq2seq.py \\\n \t--output_dir=\"./\" \\\n \t--preprocessing_num_workers=\"16\" \\\n \t--length_column_name=\"input_length\" \\\n-\t--overwrite_output_dir \\\n \t--num_train_epochs=\"5\" \\\n \t--per_device_train_batch_size=\"8\" \\\n \t--per_device_eval_batch_size=\"8\" \\\n@@ -581,7 +575,6 @@ torchrun \\\n \t--output_dir=\"./\" \\\n \t--preprocessing_num_workers=\"16\" \\\n \t--length_column_name=\"input_length\" \\\n-\t--overwrite_output_dir \\\n \t--num_train_epochs=\"5\" \\\n \t--per_device_train_batch_size=\"8\" \\\n \t--per_device_eval_batch_size=\"8\" \\"
        },
        {
            "sha": "f94105cd1d10cc2a982af07f2b31f49beb093ca4",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -55,7 +55,7 @@\n     Wav2Vec2Processor,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint, is_main_process\n+from transformers.trainer_utils import is_main_process\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -429,21 +429,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n@@ -558,7 +543,7 @@ def remove_special_characters(batch):\n         vocab_file = os.path.join(tokenizer_name_or_path, \"vocab.json\")\n \n         with training_args.main_process_first():\n-            if training_args.overwrite_output_dir and os.path.isfile(vocab_file):\n+            if os.path.isfile(vocab_file):\n                 try:\n                     os.remove(vocab_file)\n                 except OSError:\n@@ -781,9 +766,7 @@ def compute_metrics(pred):\n     # Training\n     if training_args.do_train:\n         # use last checkpoint if exist\n-        if last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n-        elif os.path.isdir(model_args.model_name_or_path):\n+        if os.path.isdir(model_args.model_name_or_path):\n             checkpoint = model_args.model_name_or_path\n         else:\n             checkpoint = None"
        },
        {
            "sha": "adfe91a31d5f6c4eba5a26fefc34ed26d560ca51",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -58,7 +58,7 @@\n     set_seed,\n )\n from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n-from transformers.trainer_utils import get_last_checkpoint, is_main_process\n+from transformers.trainer_utils import is_main_process\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -406,21 +406,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n@@ -557,7 +542,7 @@ def remove_special_characters(batch):\n         vocab_file = os.path.join(tokenizer_name_or_path, \"vocab.json\")\n \n         with training_args.main_process_first():\n-            if training_args.overwrite_output_dir and os.path.isfile(vocab_file):\n+            if os.path.isfile(vocab_file):\n                 try:\n                     os.remove(vocab_file)\n                 except OSError:\n@@ -773,9 +758,7 @@ def compute_metrics(pred):\n     # Training\n     if training_args.do_train:\n         # use last checkpoint if exist\n-        if last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n-        elif os.path.isdir(model_args.model_name_or_path):\n+        if os.path.isdir(model_args.model_name_or_path):\n             checkpoint = model_args.model_name_or_path\n         else:\n             checkpoint = None"
        },
        {
            "sha": "7e124d4326f364e8a3551a5d9923eda570ffc292",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -54,7 +54,7 @@\n     Seq2SeqTrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint, is_main_process\n+from transformers.trainer_utils import is_main_process\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -340,21 +340,6 @@ def main():\n         transformers.utils.logging.set_verbosity_info()\n     logger.info(\"Training/evaluation parameters %s\", training_args)\n \n-    # 3. Detecting last checkpoint and eventually continue from last checkpoint\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -603,8 +588,6 @@ def compute_metrics(pred):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the feature extractor too for easy upload\n "
        },
        {
            "sha": "e47f09120514d21f2229fef36cb40e0cffc7eb80",
            "filename": "examples/pytorch/summarization/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsummarization%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsummarization%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -50,7 +50,6 @@ python run_summarization.py \\\n     --output_dir /tmp/tst-summarization \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -72,7 +71,6 @@ python run_summarization.py \\\n     --validation_file path_to_csv_or_jsonlines_file \\\n     --source_prefix \"summarize: \" \\\n     --output_dir /tmp/tst-summarization \\\n-    --overwrite_output_dir \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n     --predict_with_generate"
        },
        {
            "sha": "c27f5dc8221c4ce6743777decfa6fd38740680a2",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -61,7 +61,6 @@\n     Seq2SeqTrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version, is_offline_mode\n from transformers.utils.versions import require_version\n \n@@ -374,21 +373,6 @@ def main():\n             \"`--source_prefix 'summarize: ' `\"\n         )\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -698,8 +682,6 @@ def compute_metrics(eval_preds):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "8bfadac779d3828a54a672cf1356d22d727df1b0",
            "filename": "examples/pytorch/test_pytorch_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_pytorch_examples.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -104,7 +104,6 @@ def test_run_glue(self):\n             run_glue.py\n             --model_name_or_path distilbert/distilbert-base-uncased\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n             --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n             --do_train\n@@ -140,7 +139,6 @@ def test_run_clm(self):\n             --per_device_eval_batch_size 5\n             --num_train_epochs 2\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             \"\"\".split()\n \n         if backend_device_count(torch_device) > 1:\n@@ -188,7 +186,6 @@ def test_run_mlm(self):\n             --train_file ./tests/fixtures/sample_text.txt\n             --validation_file ./tests/fixtures/sample_text.txt\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --do_train\n             --do_eval\n             --prediction_loss_only\n@@ -214,7 +211,6 @@ def test_run_ner(self):\n             --train_file tests/fixtures/tests_samples/conll/sample.json\n             --validation_file tests/fixtures/tests_samples/conll/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --do_train\n             --do_eval\n             --warmup_steps=2\n@@ -243,7 +239,6 @@ def test_run_squad(self):\n             --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n             --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --max_steps=10\n             --warmup_steps=2\n             --do_train\n@@ -271,7 +266,6 @@ def test_run_squad_seq2seq(self):\n             --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n             --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --max_steps=10\n             --warmup_steps=2\n             --do_train\n@@ -296,7 +290,6 @@ def test_run_swag(self):\n             --train_file tests/fixtures/tests_samples/swag/sample.json\n             --validation_file tests/fixtures/tests_samples/swag/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --max_steps=20\n             --warmup_steps=2\n             --do_train\n@@ -334,7 +327,6 @@ def test_run_summarization(self):\n             --train_file tests/fixtures/tests_samples/xsum/sample.json\n             --validation_file tests/fixtures/tests_samples/xsum/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --max_steps=50\n             --warmup_steps=8\n             --do_train\n@@ -364,7 +356,6 @@ def test_run_translation(self):\n             --train_file tests/fixtures/tests_samples/wmt16/sample.json\n             --validation_file tests/fixtures/tests_samples/wmt16/sample.json\n             --output_dir {tmp_dir}\n-            --overwrite_output_dir\n             --max_steps=50\n             --warmup_steps=8\n             --do_train\n@@ -396,7 +387,6 @@ def test_run_image_classification(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --dataloader_num_workers 16\n             --metric_for_best_model accuracy\n             --max_steps 10\n@@ -429,7 +419,6 @@ def test_run_speech_recognition_ctc(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --preprocessing_num_workers 16\n             --max_steps 10\n             --seed 42\n@@ -459,7 +448,6 @@ def test_run_speech_recognition_ctc_adapter(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --preprocessing_num_workers 16\n             --max_steps 10\n             --target_language tur\n@@ -491,7 +479,6 @@ def test_run_speech_recognition_seq2seq(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 4\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --preprocessing_num_workers 16\n             --max_steps 10\n             --seed 42\n@@ -523,7 +510,6 @@ def test_run_audio_classification(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --num_train_epochs 10\n             --max_steps 50\n             --seed 42\n@@ -572,7 +558,6 @@ def test_run_vit_mae_pretraining(self):\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --dataloader_num_workers 16\n             --metric_for_best_model accuracy\n             --max_steps 10\n@@ -597,7 +582,6 @@ def test_run_semantic_segmentation(self):\n             --do_train\n             --do_eval\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --max_steps 10\n             --learning_rate=2e-4\n             --per_device_train_batch_size=2\n@@ -624,7 +608,6 @@ def test_run_object_detection(self):\n             --do_train\n             --do_eval\n             --remove_unused_columns False\n-            --overwrite_output_dir True\n             --eval_do_concat_batches False\n             --max_steps 10\n             --learning_rate=1e-6"
        },
        {
            "sha": "78a9d009491131e3398b4042ac687661cb9acf28",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -55,7 +55,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -321,21 +320,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -693,8 +677,6 @@ def compute_metrics(p: EvalPrediction):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         metrics = train_result.metrics\n         max_train_samples = ("
        },
        {
            "sha": "0272eef968f424bd91f002ed6dc25995d0ef670b",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -57,7 +57,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -266,21 +265,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -566,8 +550,6 @@ def compute_metrics(p: EvalPrediction):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         metrics = train_result.metrics\n         max_train_samples = ("
        },
        {
            "sha": "b64ac29552f1278594eb1a38abaa251a1fe232da",
            "filename": "examples/pytorch/text-classification/run_xnli.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -56,7 +56,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -224,21 +223,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -412,8 +396,6 @@ def compute_metrics(p: EvalPrediction):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         metrics = train_result.metrics\n         max_train_samples = ("
        },
        {
            "sha": "f21ea058d0779b3563f205737acc4148847297fa",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -54,7 +54,6 @@\n     TrainingArguments,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -263,21 +262,6 @@ def main():\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -587,8 +571,6 @@ def compute_metrics(p):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         metrics = train_result.metrics\n         trainer.save_model()  # Saves the tokenizer too for easy upload"
        },
        {
            "sha": "2aab14e2e0567a66e8a5577693f3198897ed54e8",
            "filename": "examples/pytorch/translation/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftranslation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftranslation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2FREADME.md?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -53,7 +53,6 @@ python run_translation.py \\\n     --output_dir /tmp/tst-translation \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -74,7 +73,6 @@ python run_translation.py \\\n     --output_dir /tmp/tst-translation \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -96,7 +94,6 @@ python run_translation.py \\\n     --output_dir /tmp/tst-translation \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n  ```\n \n@@ -118,7 +115,6 @@ python run_translation.py \\\n     --output_dir /tmp/tst-translation \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n ```\n \n@@ -144,7 +140,6 @@ python run_translation.py \\\n     --output_dir /tmp/tst-translation \\\n     --per_device_train_batch_size=4 \\\n     --per_device_eval_batch_size=4 \\\n-    --overwrite_output_dir \\\n     --predict_with_generate\n  ```\n "
        },
        {
            "sha": "b068cd3ce2de7fa895c659929b40f63d6bf6fbb6",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -60,7 +60,6 @@\n     default_data_collator,\n     set_seed,\n )\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -322,21 +321,6 @@ def main():\n             \"`--source_prefix 'translate English to German: ' `\"\n         )\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Set seed before initializing model.\n     set_seed(training_args.seed)\n \n@@ -617,8 +601,6 @@ def compute_metrics(eval_preds):\n         checkpoint = None\n         if training_args.resume_from_checkpoint is not None:\n             checkpoint = training_args.resume_from_checkpoint\n-        elif last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n         train_result = trainer.train(resume_from_checkpoint=checkpoint)\n         trainer.save_model()  # Saves the tokenizer too for easy upload\n "
        },
        {
            "sha": "06a064b86f5de51d270e3ffef68a3593314afcef",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -1644,7 +1644,7 @@ def _log_model_checkpoint(self, source_directory: str, checkpoint: str):\n \n     def on_init_end(self, args, state, control, **kwargs):\n         self._volatile_checkpoints_dir = None\n-        if self._log_checkpoints and (args.overwrite_output_dir or args.save_total_limit is not None):\n+        if self._log_checkpoints and args.save_total_limit is not None:\n             self._volatile_checkpoints_dir = tempfile.TemporaryDirectory().name\n \n         if self._log_checkpoints == \"best\" and not args.load_best_model_at_end:"
        },
        {
            "sha": "573956a5bca0d2a7e8c12e78e70743f52c5b60a9",
            "filename": "src/transformers/models/cwm/configuration_cwm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -21,11 +21,11 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class CwmConfig(PretrainedConfig):\n+class CwmConfig(PreTrainedConfig):\n     \"\"\"\n     Configuration for Code World Model (CWM).\n     This is an inherited Llama3-compatible configuration with layer-interleaved\n@@ -136,13 +136,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,  # [\"full_attention\"|\"sliding_attention\"] per layer\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         if rope_scaling is None:\n             rope_scaling = {\n                 \"factor\": 16.0,\n@@ -189,6 +182,14 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n         self.sliding_window = int(sliding_window) if sliding_window else None\n         self.layer_types = list(layer_types)\n "
        },
        {
            "sha": "5ab732e7d650a70a10763784ace3008993b5e6cf",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -19,7 +19,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Union\n+from collections.abc import Callable\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -36,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cwm import CwmConfig\n \n@@ -131,7 +131,6 @@ def __init__(self, config: CwmConfig, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -225,7 +224,6 @@ def __init__(self, config: CwmConfig, layer_idx: int):\n         self.post_attention_layernorm = CwmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -339,7 +337,7 @@ def __init__(self, config: CwmConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs\n+    @check_model_inputs()\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "83944c5c8419b78fa501ad9e2dc989b1c0145c55",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -208,9 +208,6 @@ class TrainingArguments:\n     Parameters:\n         output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n             The output directory where the model predictions and checkpoints will be written.\n-        overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n-            If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n-            points to a checkpoint directory.\n         do_train (`bool`, *optional*, defaults to `False`):\n             Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n             by your training/evaluation scripts instead. See the [example\n@@ -787,15 +784,6 @@ class TrainingArguments:\n             \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'trainer_output' if not provided.\"\n         },\n     )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n \n     do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n     do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})"
        },
        {
            "sha": "89ee64a5d8843eb8309836a3cd986b77bf317781",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -219,21 +219,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n@@ -452,16 +437,12 @@ def tokenize_function(examples):\n     # Training\n     if training_args.do_train:\n {%- if cookiecutter.can_train_from_scratch == \"False\" %}\n-        if last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n-        elif os.path.isdir(model_args.model_name_or_path):\n+        if os.path.isdir(model_args.model_name_or_path):\n             checkpoint = model_args.model_name_or_path\n         else:\n             checkpoint = None\n {%- elif cookiecutter.can_train_from_scratch == \"True\" %}\n-        if last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n-        elif model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path):\n+        if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path):\n             checkpoint = model_args.model_name_or_path\n         else:\n             checkpoint = None"
        },
        {
            "sha": "bf3aba7e1a4dd0e07a1ec1351fe32e858c5ce825",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -1303,7 +1303,6 @@ def run_trainer(\n             --train_file {data_dir}/train.json\n             --validation_file {data_dir}/val.json\n             --output_dir {output_dir}\n-            --overwrite_output_dir\n             --max_source_length {max_len}\n             --max_target_length {max_len}\n             --val_max_target_length {max_len}\n@@ -1373,7 +1372,6 @@ def test_clm(self, stage, dtype):\n             --train_file {data_dir}/sample_text.txt\n             --validation_file {data_dir}/sample_text.txt\n             --output_dir {output_dir}\n-            --overwrite_output_dir\n             --do_train\n             --do_eval\n             --max_train_samples 16\n@@ -1410,7 +1408,6 @@ def test_clm_from_config_zero3_fp16(self):\n             --train_file {data_dir}/sample_text.txt\n             --validation_file {data_dir}/sample_text.txt\n             --output_dir {output_dir}\n-            --overwrite_output_dir\n             --do_train\n             --max_train_samples 4\n             --per_device_train_batch_size 2"
        },
        {
            "sha": "a7201ef7f9a03c139597e589c01a34a1e3aab497",
            "filename": "tests/deepspeed/test_model_zoo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_model_zoo.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -161,7 +161,6 @@ def make_task_cmds():\n         --num_train_epochs 1\n         --fp16\n         --report_to none\n-        --overwrite_output_dir\n         \"\"\".split()\n \n     # try to cover as many models as possible once (it's enough to run on one task per model)"
        },
        {
            "sha": "80e71691bbbdcd020467bbfd1bea8d93ad7a2577",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -267,7 +267,6 @@ def run_trainer(\n             --validation_file {data_dir}/val.json\n             --test_file {data_dir}/test.json\n             --output_dir {output_dir}\n-            --overwrite_output_dir\n             --max_train_samples 8\n             --max_source_length {max_len}\n             --max_target_length {max_len}"
        },
        {
            "sha": "16f010947b57be00ccc976e4ebba8ee63106822e",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -447,7 +447,6 @@ def get_base_args(self, output_dir, num_epochs, logging_steps):\n             --model_name_or_path google-bert/bert-base-cased\n             --task_name mrpc\n             --output_dir {output_dir}\n-            --overwrite_output_dir\n             --do_train\n             --max_seq_length 128\n             --per_device_train_batch_size 16"
        },
        {
            "sha": "879cfff1c5ea15716f7addc4924585edaf0da0e7",
            "filename": "tests/sagemaker/conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fsagemaker%2Fconftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fsagemaker%2Fconftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fconftest.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -21,7 +21,6 @@ class SageMakerTestEnvironment:\n         \"do_eval\": True,\n         \"do_predict\": True,\n         \"output_dir\": \"/opt/ml/model\",\n-        \"overwrite_output_dir\": True,\n         \"max_steps\": 500,\n         \"save_steps\": 5500,\n     }"
        },
        {
            "sha": "bb7a263a0ebefe8fdd966c7076ba0b57383f890a",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -38,7 +38,6 @@\n     set_seed,\n )\n from transformers.trainer import Trainer\n-from transformers.trainer_utils import get_last_checkpoint\n from transformers.training_args import TrainingArguments\n from transformers.utils import check_min_version\n \n@@ -198,21 +197,6 @@ def main():\n     else:\n         model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n \n-    # Detecting last checkpoint.\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n     # Setup logging\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n@@ -459,9 +443,7 @@ def compute_metrics(p: EvalPrediction):\n     # Training\n     if training_args.do_train:\n         checkpoint = None\n-        if last_checkpoint is not None:\n-            checkpoint = last_checkpoint\n-        elif os.path.isdir(model_args.model_name_or_path):\n+        if os.path.isdir(model_args.model_name_or_path):\n             # Check the config from that potential checkpoint has the right number of labels before using it as a\n             # checkpoint.\n             if AutoConfig.from_pretrained(model_args.model_name_or_path).num_labels == num_labels:"
        },
        {
            "sha": "ebb2d1699be1c93a5073cb9e209121b82a30298b",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/776eea8612458f60bbe684f38ea34f85fe890486/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=776eea8612458f60bbe684f38ea34f85fe890486",
            "patch": "@@ -4455,7 +4455,6 @@ def test_end_to_end_example(self):\n                 \"1\",\n                 \"--output_dir\",\n                 tmpdir,\n-                \"--overwrite_output_dir\",\n                 \"--do_train\",\n                 \"--max_train_samples\",\n                 \"64\","
        }
    ],
    "stats": {
        "total": 985,
        "additions": 46,
        "deletions": 939
    }
}