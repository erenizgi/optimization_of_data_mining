{
    "author": "MekkCyber",
    "message": "Handling an exception related to HQQ quantization in modeling (#36702)\n\n* adding exception\n\n* style\n\n* add types",
    "sha": "4a60bae8e236140a4e711f773c975707b8acd032",
    "files": [
        {
            "sha": "723dc53b34a19637fd0bb6978c27ef9229d86cda",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a60bae8e236140a4e711f773c975707b8acd032/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a60bae8e236140a4e711f773c975707b8acd032/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4a60bae8e236140a4e711f773c975707b8acd032",
            "patch": "@@ -709,9 +709,19 @@ def _find_identical(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor]\n \n \n def _infer_parameter_dtype(\n-    model: \"PreTrainedModel\", param_name: str, empty_param, keep_in_fp32_modules=None\n+    model: \"PreTrainedModel\",\n+    param_name: str,\n+    empty_param: torch.Tensor,\n+    keep_in_fp32_modules: Optional[List[str]] = None,\n+    hf_quantizer: Optional[HfQuantizer] = None,\n ) -> Union[bool, Optional[torch.dtype]]:\n-    old_param = model.get_parameter_or_buffer(param_name)\n+    try:\n+        old_param = model.get_parameter_or_buffer(param_name)\n+    except Exception as e:\n+        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ:\n+            return True, None\n+        else:\n+            raise e\n     is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n     # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n     # in int/uint/bool and not cast them.\n@@ -781,6 +791,7 @@ def _load_state_dict_into_meta_model(\n             param_name,\n             empty_param,\n             keep_in_fp32_modules,\n+            hf_quantizer,\n         )\n \n         if device_mesh is not None:  # In this case, the param is already on the correct device!"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 13,
        "deletions": 2
    }
}