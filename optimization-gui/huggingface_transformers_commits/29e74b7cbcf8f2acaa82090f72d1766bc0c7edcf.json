{
    "author": "jp1924",
    "message": "Add: num_additional_image_tokens to models (#35052)\n\n* Add: num_additional_image_tokens to models\r\n\r\n* docs: update docstring for num_additional_image_tokens in configuration files\r\n\r\n* Add num_additional_image_tokens to LlavaNextVideo model and update feature selection logic\r\n\r\n* revert\r\n\r\n* Fix: adjust num_image_tokens calculation in LlavaProcessor\r\n\r\n* Remove num_additional_image_tokens initialization from configuration files\r\n\r\n* Fix test error\r\n\r\n* revert\r\n\r\n* Fix: adjust num_image_tokens calculation in LlavaNextVideoProcessor\r\n\r\n* fix conflict\r\n\r\n* Fix: adjust num_image_tokens calculation in VideoLlavaProcessor\r\n\r\n* make style\r\n\r\n---------\r\n\r\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
    "files": [
        {
            "sha": "09c63e39b3df8e6e2079bb7283939963e553d2d4",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -240,6 +240,7 @@ def __init__(self, config: LlavaConfig):\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n         self.post_init()\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "630ccdce143439ef00a3c29db922f0f33fe597f9",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -157,7 +157,9 @@ def __call__(\n             # Replace the image token with the expanded image token sequence\n             pixel_values = image_inputs[\"pixel_values\"]\n             height, width = get_image_size(to_numpy_array(pixel_values[0]))\n-            num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+            num_image_tokens = (height // self.patch_size) * (\n+                width // self.patch_size\n+            ) + self.num_additional_image_tokens\n             if self.vision_feature_select_strategy == \"default\":\n                 num_image_tokens -= 1\n "
        },
        {
            "sha": "d8034ca9fa56f29bec9ab14dc3a815cda9504b41",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -155,6 +155,9 @@ def __call__(\n             for sample in text:\n                 while self.image_token in sample:\n                     image_size = next(image_sizes)\n+                    if not isinstance(image_size, (list, tuple)):\n+                        # cast to list to avoid numerical precision errors when calculating unpadding\n+                        image_size = image_size.tolist()\n                     orig_height, orig_width = image_size\n                     num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                     if self.vision_feature_select_strategy == \"default\":"
        },
        {
            "sha": "e9307ee37c9e3aed58f058813aaceec145e2d969",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -180,6 +180,9 @@ def __call__(\n             for sample in text:\n                 while self.image_token in sample:\n                     image_size = next(image_sizes)\n+                    if not isinstance(image_size, (list, tuple)):\n+                        # cast to list to avoid numerical precision errors when calculating unpadding\n+                        image_size = image_size.tolist()\n                     orig_height, orig_width = image_size\n                     num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                     if self.vision_feature_select_strategy == \"default\":\n@@ -193,6 +196,8 @@ def __call__(\n             one_video = to_numpy_array(videos_inputs.get(\"pixel_values_videos\")[0])\n             height, width = get_image_size(one_video[0])\n             num_frames = one_video.shape[0]  # frame dim is always after batch dim\n+\n+            # no `self.num_additional_image_tokens` added because video always has a default feature selection strategy\n             num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n             num_video_tokens = num_image_tokens // 4 * num_frames  # divide by 4 needed for avg pooling layer\n             prompt_strings = []"
        },
        {
            "sha": "9d11f42685183405f1fb147a3cd813fd91297f5a",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -179,7 +179,7 @@ def __call__(\n             ) + self.num_additional_image_tokens\n             num_video_tokens = num_image_tokens * num_frames\n             if self.vision_feature_select_strategy == \"default\":\n-                num_image_tokens -= self.num_additional_image_tokens\n+                num_image_tokens -= 1\n \n             prompt_strings = []\n             for sample in text:"
        },
        {
            "sha": "084b92c9771c4e071e3244095c3a80f0d8f49b29",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=29e74b7cbcf8f2acaa82090f72d1766bc0c7edcf",
            "patch": "@@ -243,6 +243,7 @@ def __init__(self, config: VipLlavaConfig):\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n         self.post_init()\n \n     def get_input_embeddings(self):"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 14,
        "deletions": 2
    }
}