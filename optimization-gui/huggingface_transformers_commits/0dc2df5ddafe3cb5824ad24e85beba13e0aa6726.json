{
    "author": "ahadnagy",
    "message": "CI workflow for performed test regressions (#39198)\n\n* WIP script to compare test runs for models\n\n* Update line normalitzation logic\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "0dc2df5ddafe3cb5824ad24e85beba13e0aa6726",
    "files": [
        {
            "sha": "9724c5506bd5337e0c2dfae64d0b24e7b9967768",
            "filename": "utils/compare_test_runs.py",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fcompare_test_runs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fcompare_test_runs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcompare_test_runs.py?ref=0dc2df5ddafe3cb5824ad24e85beba13e0aa6726",
            "patch": "@@ -0,0 +1,91 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import re\n+\n+\n+def normalize_test_line(line):\n+    line = line.strip()\n+\n+    # Normalize SKIPPED/XFAIL/etc with path:line and reason\n+    match = re.match(r\"^(SKIPPED|XFAIL|XPASS|EXPECTEDFAIL)\\s+\\[?\\d*\\]?\\s*(\\S+:\\d+)\", line)\n+    if match:\n+        status, location = match.groups()\n+        return f\"{status} {location}\"\n+\n+    # Normalize ERROR/FAILED lines with optional message\n+    if line.startswith(\"ERROR\") or line.startswith(\"FAILED\"):\n+        return re.split(r\"\\s+-\\s+\", line)[0].strip()\n+\n+    return line\n+\n+\n+def parse_summary_file(file_path):\n+    test_set = set()\n+    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+        in_summary = False\n+        for line in f:\n+            if line.strip().startswith(\"===\"):\n+                in_summary = not in_summary\n+                continue\n+            if in_summary:\n+                stripped = line.strip()\n+                if stripped:\n+                    normalized = normalize_test_line(stripped)\n+                    test_set.add(normalized)\n+    return test_set\n+\n+\n+def compare_job_sets(job_set1, job_set2):\n+    all_job_names = sorted(set(job_set1) | set(job_set2))\n+    report_lines = []\n+\n+    for job_name in all_job_names:\n+        file1 = job_set1.get(job_name)\n+        file2 = job_set2.get(job_name)\n+\n+        tests1 = parse_summary_file(file1) if file1 else set()\n+        tests2 = parse_summary_file(file2) if file2 else set()\n+\n+        added = tests2 - tests1\n+        removed = tests1 - tests2\n+\n+        if added or removed:\n+            report_lines.append(f\"=== Diff for job: {job_name} ===\")\n+            if removed:\n+                report_lines.append(\"--- Absent in current run:\")\n+                for test in sorted(removed):\n+                    report_lines.append(f\"    - {test}\")\n+            if added:\n+                report_lines.append(\"+++ Appeared in current run:\")\n+                for test in sorted(added):\n+                    report_lines.append(f\"    + {test}\")\n+            report_lines.append(\"\")  # blank line\n+\n+    return \"\\n\".join(report_lines) if report_lines else \"No differences found.\"\n+\n+\n+# Example usage:\n+# job_set_1 = {\n+#     \"albert\": \"prev/multi-gpu_run_models_gpu_models/albert_test_reports/summary_short.txt\",\n+#     \"bloom\": \"prev/multi-gpu_run_models_gpu_models/bloom_test_reports/summary_short.txt\",\n+# }\n+\n+# job_set_2 = {\n+#     \"albert\": \"curr/multi-gpu_run_models_gpu_models/albert_test_reports/summary_short.txt\",\n+#     \"bloom\": \"curr/multi-gpu_run_models_gpu_models/bloom_test_reports/summary_short.txt\",\n+# }\n+\n+# report = compare_job_sets(job_set_1, job_set_2)\n+# print(report)"
        },
        {
            "sha": "97d3696465cf19fec0dbc154e9f124c3e6425d66",
            "filename": "utils/get_previous_daily_ci.py",
            "status": "modified",
            "additions": 35,
            "deletions": 11,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fget_previous_daily_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fget_previous_daily_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_previous_daily_ci.py?ref=0dc2df5ddafe3cb5824ad24e85beba13e0aa6726",
            "patch": "@@ -91,45 +91,69 @@ def get_last_daily_ci_run_commit(token, workflow_run_id=None, workflow_id=None,\n \n \n def get_last_daily_ci_artifacts(\n-    artifact_names, output_dir, token, workflow_run_id=None, workflow_id=None, commit_sha=None\n+    output_dir,\n+    token,\n+    workflow_run_id=None,\n+    workflow_id=None,\n+    commit_sha=None,\n+    artifact_names=None,\n ):\n     \"\"\"Get the artifacts of last completed workflow run id of the scheduled (daily) CI.\"\"\"\n     workflow_run_id = get_last_daily_ci_workflow_run_id(\n         token, workflow_run_id=workflow_run_id, workflow_id=workflow_id, commit_sha=commit_sha\n     )\n     if workflow_run_id is not None:\n         artifacts_links = get_artifacts_links(worflow_run_id=workflow_run_id, token=token)\n+\n+        if artifact_names is None:\n+            artifact_names = artifacts_links.keys()\n+\n+        downloaded_artifact_names = []\n         for artifact_name in artifact_names:\n             if artifact_name in artifacts_links:\n                 artifact_url = artifacts_links[artifact_name]\n                 download_artifact(\n                     artifact_name=artifact_name, artifact_url=artifact_url, output_dir=output_dir, token=token\n                 )\n+                downloaded_artifact_names.append(artifact_name)\n+\n+        return downloaded_artifact_names\n \n \n def get_last_daily_ci_reports(\n-    artifact_names, output_dir, token, workflow_run_id=None, workflow_id=None, commit_sha=None\n+    output_dir,\n+    token,\n+    workflow_run_id=None,\n+    workflow_id=None,\n+    commit_sha=None,\n+    artifact_names=None,\n ):\n     \"\"\"Get the artifacts' content of the last completed workflow run id of the scheduled (daily) CI.\"\"\"\n-    get_last_daily_ci_artifacts(\n-        artifact_names,\n+    downloaded_artifact_names = get_last_daily_ci_artifacts(\n         output_dir,\n         token,\n         workflow_run_id=workflow_run_id,\n         workflow_id=workflow_id,\n         commit_sha=commit_sha,\n+        artifact_names=artifact_names,\n     )\n \n     results = {}\n-    for artifact_name in artifact_names:\n+    for artifact_name in downloaded_artifact_names:\n         artifact_zip_path = os.path.join(output_dir, f\"{artifact_name}.zip\")\n         if os.path.isfile(artifact_zip_path):\n-            results[artifact_name] = {}\n+            target_dir = os.path.join(output_dir, artifact_name)\n             with zipfile.ZipFile(artifact_zip_path) as z:\n-                for filename in z.namelist():\n-                    if not os.path.isdir(filename):\n-                        # read the file\n-                        with z.open(filename) as f:\n-                            results[artifact_name][filename] = f.read().decode(\"UTF-8\")\n+                z.extractall(target_dir)\n+\n+            results[artifact_name] = {}\n+            filename = os.listdir(target_dir)\n+            for filename in filename:\n+                file_path = os.path.join(target_dir, filename)\n+                if not os.path.isdir(file_path):\n+                    # read the file\n+                    with open(file_path) as fp:\n+                        content = fp.read()\n+                        results[artifact_name][filename] = content\n \n     return results"
        },
        {
            "sha": "04beca72c34d2ef1bb8a799fd15dc4f80778bd14",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 57,
            "deletions": 2,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dc2df5ddafe3cb5824ad24e85beba13e0aa6726/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=0dc2df5ddafe3cb5824ad24e85beba13e0aa6726",
            "patch": "@@ -24,6 +24,7 @@\n from typing import Any, Optional, Union\n \n import requests\n+from compare_test_runs import compare_job_sets\n from get_ci_error_statistics import get_jobs\n from get_previous_daily_ci import get_last_daily_ci_reports, get_last_daily_ci_run, get_last_daily_ci_workflow_run_id\n from huggingface_hub import HfApi\n@@ -672,6 +673,21 @@ def payload(self) -> str:\n                     }\n                     blocks.append(block)\n \n+        if diff_file_url is not None:\n+            block = {\n+                \"type\": \"section\",\n+                \"text\": {\n+                    \"type\": \"mrkdwn\",\n+                    \"text\": f\"*Test results diff*\\n\\n(compared to previous run: <https://github.com/huggingface/transformers/actions/runs/{prev_workflow_run_id}|{prev_workflow_run_id}>)\",\n+                },\n+                \"accessory\": {\n+                    \"type\": \"button\",\n+                    \"text\": {\"type\": \"plain_text\", \"text\": \"Check test result diff file\"},\n+                    \"url\": diff_file_url,\n+                },\n+            }\n+            blocks.append(block)\n+\n         if len(new_failure_blocks) > 0:\n             blocks.extend(new_failure_blocks)\n \n@@ -1460,13 +1476,14 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     prev_ci_artifacts = (None, None)\n     other_ci_artifacts = []\n \n+    output_dir = os.path.join(os.getcwd(), \"previous_reports\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n     for idx, target_workflow_run_id in enumerate([prev_workflow_run_id] + other_workflow_run_ids):\n         if target_workflow_run_id is None or target_workflow_run_id == \"\":\n             continue\n         else:\n             artifact_names = [f\"ci_results_{job_name}\"]\n-            output_dir = os.path.join(os.getcwd(), \"previous_reports\")\n-            os.makedirs(output_dir, exist_ok=True)\n             ci_artifacts = get_last_daily_ci_reports(\n                 artifact_names=artifact_names,\n                 output_dir=output_dir,\n@@ -1478,6 +1495,44 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n             else:\n                 other_ci_artifacts.append((target_workflow_run_id, ci_artifacts))\n \n+    # Only for AMD at this moment.\n+    # TODO: put this into a method\n+    if is_amd_daily_ci_workflow:\n+        diff_file_url = None\n+        if not (prev_workflow_run_id is None or prev_workflow_run_id == \"\"):\n+            ci_artifacts = get_last_daily_ci_reports(\n+                artifact_names=None,\n+                output_dir=output_dir,\n+                token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"],\n+                workflow_run_id=prev_workflow_run_id,\n+            )\n+\n+            current_artifacts = sorted([d for d in os.listdir() if os.path.isdir(d) and d.endswith(\"_test_reports\")])\n+            prev_artifacts = sorted([d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and d.endswith(\"_test_reports\")])  # fmt: skip\n+\n+            current_artifacts_set = {}\n+            for d in current_artifacts:\n+                current_artifacts_set[d] = os.path.join(d, \"summary_short.txt\")\n+\n+            prev_artifacts_set = {}\n+            for d in prev_artifacts:\n+                prev_artifacts_set[d] = os.path.join(output_dir, d, \"summary_short.txt\")\n+\n+            report = compare_job_sets(prev_artifacts_set, current_artifacts_set)\n+\n+            with open(f\"ci_results_{job_name}/test_results_diff.json\", \"w\") as fp:\n+                fp.write(report)\n+\n+            # upload\n+            commit_info = api.upload_file(\n+                path_or_fileobj=f\"ci_results_{job_name}/test_results_diff.json\",\n+                path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/test_results_diff.json\",\n+                repo_id=report_repo_id,\n+                repo_type=\"dataset\",\n+                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+            )\n+            diff_file_url = f\"https://huggingface.co/datasets/{report_repo_id}/resolve/{commit_info.oid}/{report_repo_folder}/ci_results_{job_name}/test_results_diff.json\"\n+\n     ci_name_in_report = \"\"\n     if job_name in job_to_test_map:\n         ci_name_in_report = job_to_test_map[job_name]"
        }
    ],
    "stats": {
        "total": 196,
        "additions": 183,
        "deletions": 13
    }
}