{
    "author": "zucchini-nlp",
    "message": "[vlm] adjust max length for special tokens (#37342)\n\n* update\n\n* apply suggestion\n\n* fix tests for main branch\n\n* remove unused logger\n\n* add special tokens in tests\n\n* nit\n\n* fix more tests\n\n* fix test\n\n* pg also",
    "sha": "32eca7197a8d2618417a0d665db38d0af3695a2c",
    "files": [
        {
            "sha": "b087c21536697fca404ae15368b84aefec2c6caf",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -946,6 +946,8 @@ def __init__(\n             size_conversion = {490: 128, 980: 256}\n         self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n \n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n         if tokenizer is not None and tokenizer.pad_token is None:\n             tokenizer.pad_token = tokenizer.unk_token\n \n@@ -986,10 +988,12 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n         if images is not None:\n             image_inputs = self.image_processor(\n                 images,\n@@ -1007,12 +1011,11 @@ def __call__(\n             image_inputs = {}\n             prompt_strings = text\n \n-        text_inputs = self.tokenizer(\n-            prompt_strings,\n-            **output_kwargs[\"text_kwargs\"],\n-        )\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "7d3076247550521b0900b07991ba0d68ad24747a",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -72,6 +72,8 @@ def __init__(\n             size_conversion = {490: 128, 980: 256}\n         self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n \n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n         if tokenizer is not None and tokenizer.pad_token is None:\n             tokenizer.pad_token = tokenizer.unk_token\n \n@@ -112,10 +114,12 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n         if images is not None:\n             image_inputs = self.image_processor(\n                 images,\n@@ -133,12 +137,11 @@ def __call__(\n             image_inputs = {}\n             prompt_strings = text\n \n-        text_inputs = self.tokenizer(\n-            prompt_strings,\n-            **output_kwargs[\"text_kwargs\"],\n-        )\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "3b9afecda50fd1f34d681922eff43949bf44a519",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -121,6 +121,7 @@ def __init__(\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n         self.image_token = image_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.patch_size = patch_size * downsample_factor\n         self.img_size = img_size\n \n@@ -224,9 +225,11 @@ def __call__(\n \n             text = processed_text\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "f0c592180e9f76f9c6ef3efcc69929206a81621d",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -68,6 +68,7 @@ class ChameleonProcessor(ProcessorMixin):\n     def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):\n         self.image_seq_length = image_seq_length\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.image_start_token = (\n             tokenizer.boi_token if hasattr(tokenizer, \"boi_token\") else \"<racm3:break>\"\n         )  # fixed tokens for start and end, so can hardcode\n@@ -140,12 +141,14 @@ def __call__(\n                 sample += self.tokenizer.sep_token  # special Chameleon treatment to add sep for chat mode\n             prompt_strings.append(sample)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(prompt_strings, data, modalities=[\"image\"])\n \n         if images is not None:\n             data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n-        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"])\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "a94dc08cd97da2cd0669d263bd423446479d69cf",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -75,6 +75,7 @@ def __init__(\n         **kwargs,\n     ):\n         self.image_token = tokenizer.image_token  # image_token as placeholder to be replaced by vq-vae tokens\n+        self.image_token_id = tokenizer.image_token_id\n         self.image_start_token = tokenizer.boi_token  # \"<|image start|>\" fixed tokens for start and end of image\n         self.image_end_token = tokenizer.eoi_token  # \"<|image end|>\"\n         self.fake_token_around_image = tokenizer.image_wrapper_token  # \"<|image token|>\"  every image starts with it\n@@ -177,10 +178,13 @@ def __call__(\n             image_features[\"image_sizes\"] = [[height, width]] * len(text)\n \n         # else just generate from text-only input, and we do no special treatment for text\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         data = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, data, modalities=[\"image\"])\n+\n         data.update(**image_features)\n \n-        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None))\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def calculate_generate_size(self, ratio, image_area, spatial_factor):\n         width, height = map(int, ratio.split(\":\"))"
        },
        {
            "sha": "f887e11d5c180a9b7111f582f50f655558cd6ab0",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -65,7 +65,7 @@ def __init__(\n         self.image_seq_length = image_seq_length\n         self.image_token_id = tokenizer.image_token_id\n         self.boi_token = tokenizer.boi_token\n-        self.image_token = tokenizer.boi_token\n+        self.image_token = tokenizer.image_token\n         image_tokens_expanded = \"\".join([tokenizer.image_token] * image_seq_length)\n         self.full_image_sequence = f\"\\n\\n{tokenizer.boi_token}{image_tokens_expanded}{tokenizer.eoi_token}\\n\\n\"\n \n@@ -138,6 +138,7 @@ def __call__(\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"], return_tensors=\"np\")\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n         # Add token type ids manually, as tokenizer can't do arbitrary position token types\n         array_ids = text_inputs[\"input_ids\"]"
        },
        {
            "sha": "5e40d14dee85697bc72fb175bb1cc18217c90644",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -107,6 +107,8 @@ def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **k\n         self.img_start_token = \"<img>\"\n         self.img_end_token = \"</img>\"\n         self.img_pad_token = \"<imgpad>\"\n+        self.image_token = \"<imgpad>\"  # keep the above for BC, but we need to call it `image_token`\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.system_query = \"system\\nYou should follow the instructions carefully and explain your answers in detail.\"\n \n     def _make_list_of_inputs(self, images, text, box, color, multi_page):\n@@ -250,8 +252,11 @@ def __call__(\n                 )\n                 text.append(prompt)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "ab144f3f9de487bf63d5517d3f1dd35bf35240e5",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -98,13 +98,15 @@ def __init__(\n             raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         if not hasattr(tokenizer, \"image_token\"):\n-            self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True)\n-            self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+            self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n+            self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content\n             tokens_to_add = {\"additional_special_tokens\": [self.fake_image_token, self.image_token]}\n             tokenizer.add_special_tokens(tokens_to_add)\n+            self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         else:\n             self.fake_image_token = tokenizer.image_boundary_token\n             self.image_token = tokenizer.image_token\n+            self.image_token_id = tokenizer.image_token_id\n \n         self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True)\n         tokenizer.add_special_tokens({\"additional_special_tokens\": [self.end_of_utterance_token]})\n@@ -190,9 +192,10 @@ def __call__(\n         )\n         image_seq_len = output_kwargs[\"images_kwargs\"].pop(\"image_seq_len\", None)\n         image_seq_len = image_seq_len if image_seq_len is not None else self.image_seq_len\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n \n         n_images_in_text = []\n-        inputs = BatchFeature()\n+        inputs = {}\n \n         if text is not None:\n             if isinstance(text, str):\n@@ -201,13 +204,14 @@ def __call__(\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n             # Replace the image token with fake tokens around the expanded image token sequence of length `image_seq_len`\n-            fake_image_token = self.fake_image_token.content\n-            image_token = self.image_token.content\n+            fake_image_token = self.fake_image_token\n+            image_token = self.image_token\n             image_str = f\"{fake_image_token}{image_token * image_seq_len}{fake_image_token}\"\n \n             if self.image_processor.do_image_splitting:\n                 # A single image token is split into 4 patches + 1 original image\n                 image_str = image_str * 5\n+                image_seq_len *= 5\n \n             prompt_strings = []\n             for sample in text:\n@@ -218,6 +222,7 @@ def __call__(\n                 prompt_strings.append(sample)\n \n             text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+            self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n             inputs.update(text_inputs)\n \n         if images is not None:\n@@ -259,7 +264,7 @@ def __call__(\n             image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             inputs.update(image_inputs)\n \n-        return inputs\n+        return BatchFeature(inputs, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "1fcce0a453ae84226cf6d78cf4b690e62556ce27",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -141,9 +141,9 @@ def __init__(\n         if tokenizer is None:\n             raise ValueError(\"You need to specify a `tokenizer`.\")\n \n-        self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True)\n-        self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n-        self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True)\n+        self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n+        self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content\n+        self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True).content\n         self.global_image_tag = \"<global-img>\"  # https://github.com/huggingface/transformers/pull/32473/files/8063e5e17362571b693f1db95167f5443a3be1b2#r1734825341\n         self.image_seq_len = image_seq_len\n \n@@ -159,6 +159,7 @@ def __init__(\n             ]\n         }\n         tokenizer.add_special_tokens(tokens_to_add)\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n \n@@ -240,17 +241,18 @@ def __call__(\n         )\n \n         image_seq_len = image_seq_len if image_seq_len is not None else self.image_seq_len\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n \n         n_images_in_text = []\n         n_images_in_images = []\n-        inputs = BatchFeature()\n+        inputs = {}\n \n         if text is not None:\n             if isinstance(text, str):\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n-            n_images_in_text = [sample.count(self.image_token.content) for sample in text]\n+            n_images_in_text = [sample.count(self.image_token) for sample in text]\n \n         if images is not None:\n             if is_image_or_image_url(images):\n@@ -259,8 +261,8 @@ def __call__(\n                 if text is not None:\n                     if sum(n_images_in_text) != len(images):\n                         raise ValueError(\n-                            f\"The total number of {self.image_token.content} tokens in the prompts should be the same as the number of images passed.\"\n-                            f\" Found {sum(n_images_in_text)} {self.image_token.content} tokens and {len(images)} images.\"\n+                            f\"The total number of {self.image_token} tokens in the prompts should be the same as the number of images passed.\"\n+                            f\" Found {sum(n_images_in_text)} {self.image_token} tokens and {len(images)} images.\"\n                         )\n                     # Reorganize the images to match the prompts\n                     cumsum_images_in_text = [0] + list(accumulate(n_images_in_text))\n@@ -295,8 +297,8 @@ def __call__(\n                 image_rows = inputs.pop(\"rows\", [[0] * len(text)])\n                 image_cols = inputs.pop(\"cols\", [[0] * len(text)])\n \n-                fake_image_token = self.fake_image_token.content\n-                image_token = self.image_token.content\n+                fake_image_token = self.fake_image_token\n+                image_token = self.image_token\n                 global_img_token = self.global_image_tag\n \n                 prompt_strings = []\n@@ -324,18 +326,19 @@ def __call__(\n                         sample += image_prompt_string + split_sample[i + 1]\n                     prompt_strings.append(sample)\n \n-                text_inputs = self.tokenizer(text=prompt_strings, **output_kwargs[\"text_kwargs\"])\n+                text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+                self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n                 inputs.update(text_inputs)\n \n         elif text is not None:\n             if any(n_images_in_text):\n                 raise ValueError(\n-                    f\"Found {sum(n_images_in_text)} {self.image_token.content} tokens in the text but no images were passed.\"\n+                    f\"Found {sum(n_images_in_text)} {self.image_token} tokens in the text but no images were passed.\"\n                 )\n             text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n             inputs.update(text_inputs)\n \n-        return inputs\n+        return BatchFeature(inputs, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "7ca562571cbf304ea7c069789ac190e92b25eadf",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -122,6 +122,7 @@ def __init__(\n \n         self.fake_image_token = fake_image_token\n         self.image_token = image_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.start_of_img_token = start_of_image_token\n         self.end_of_img_token = end_of_image_token\n         self.img_patch_token = patch_token\n@@ -148,6 +149,7 @@ def _prompt_split_image(self, aspect_ratio, num_patches_per_chunk):\n                         img_string += \"<|tile_x_separator|>\"\n \n                 img_string += \"<|tile_y_separator|>\"\n+\n         img_string += \"<|image|>\"\n         img_string += \"<|patch|>\" * num_patches_per_chunk\n         img_string += \"<|image_end|>\"\n@@ -247,9 +249,11 @@ def __call__(\n \n             text = processed_text\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "72a61bff7194247c202074c6485ff2a84819fe56",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -173,8 +173,10 @@ def __call__(\n                 sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n                 prompt_strings.append(sample)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "78175adc212ef7b6ced18b6d71b5a7df90550bdf",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -171,9 +171,11 @@ def __call__(\n                 prompt_strings.append(sample)\n             prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:\n         image_grid_pinpoints = self.image_processor.image_grid_pinpoints"
        },
        {
            "sha": "c7ff0a1d7a0281c1068b6e5f4290b234615aed9d",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -224,8 +224,11 @@ def __call__(\n                 prompt_strings.append(sample)\n             text = prompt_strings\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n     # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_number_of_features\n     def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:"
        },
        {
            "sha": "51d8dcf9c09032df13aa1ecf60eacdafe4f1c6f5",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -171,7 +171,7 @@ def __call__(\n                 to_numpy_array(image_inputs[\"pixel_values\"][0][0]),\n                 channel_dim=output_kwargs[\"images_kwargs\"].get(\"data_format\"),\n             )\n-            text = self._expand_image_tokens(text, image_sizes, height, width, self.image_token)\n+            text, num_image_tokens = self._expand_image_tokens(text, image_sizes, height, width, self.image_token)\n \n         if videos is not None:\n             video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n@@ -188,8 +188,11 @@ def __call__(\n             num_video_tokens = (num_frames * pooled_height_width * pooled_height_width) + 1  # +1 for newline token\n             text = [sample.replace(self.video_token, self.video_token * num_video_tokens) for sample in text]\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        return BatchFeature(data={**text_inputs, **image_inputs, **video_inputs})\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **video_inputs}, tensor_type=return_tensors)\n \n     def _expand_image_tokens(\n         self,\n@@ -201,6 +204,7 @@ def _expand_image_tokens(\n         num_frames: int = 1,\n     ):\n         prompt_strings = []\n+        max_num_vision_tokens = 0\n         for sample in text:\n             while special_token in sample:\n                 image_size_list = next(image_sizes)\n@@ -210,12 +214,13 @@ def _expand_image_tokens(\n                     original_size = original_size.tolist()\n                 orig_height, orig_width = original_size\n                 num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                max_num_vision_tokens = max(max_num_vision_tokens, num_image_tokens)\n                 if self.vision_feature_select_strategy == \"default\":\n                     num_image_tokens -= 1\n                 sample = sample.replace(special_token, \"<placeholder>\" * num_image_tokens * num_frames, 1)\n             prompt_strings.append(sample)\n         text = [sample.replace(\"<placeholder>\", special_token) for sample in prompt_strings]\n-        return text\n+        return text, max_num_vision_tokens\n \n     def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:\n         image_grid_pinpoints = self.image_processor.image_grid_pinpoints"
        },
        {
            "sha": "ad2ff2ddb83baf7f83d8e6be70dbb37c6d1cc416",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -274,6 +274,7 @@ def __call__(\n         )\n \n         text_kwargs = output_kwargs[\"text_kwargs\"]\n+        text_kwargs[\"return_tensors\"] = None\n         images_kwargs = output_kwargs[\"images_kwargs\"]\n         common_kwargs = output_kwargs[\"common_kwargs\"]\n \n@@ -287,6 +288,8 @@ def __call__(\n             text = [build_string_from_input(text_item, self.bos_token, self.image_token) for text_item in text]\n             _ = text_kwargs.pop(\"padding_side\", None)  # hack until padding-side is an accepted kwarg by tokenizers\n             encoding = self.tokenizer(text, **text_kwargs)\n+            self._check_special_mm_tokens(text, encoding, modalities=[\"image\"])\n+            n_images_in_ids = [token_ids.count(self.image_token_id) for token_ids in encoding[\"input_ids\"]]\n             data.update(encoding)\n \n         n_images_in_images = [0]\n@@ -301,13 +304,18 @@ def __call__(\n                 raise ValueError(\n                     \"If a batch of text is provided, there should be either no images or at least one image per sample\"\n                 )\n-            if sum(n_images_in_text) > 0 and n_images_in_images != n_images_in_text:\n+            if sum(n_images_in_text) > 0 and (\n+                n_images_in_images != n_images_in_text or n_images_in_ids != n_images_in_images\n+            ):\n                 if images is None:\n                     raise ValueError(\"No image were provided, but there are image tokens in the prompt\")\n                 else:\n                     add_message = \"\"\n-                    if sum(n_images_in_images) == sum(n_images_in_text):\n+                    if sum(n_images_in_images) == sum(n_images_in_text) and n_images_in_images != n_images_in_text:\n                         add_message = \"Make sure to pass your images as a nested list, where each sub-list holds images per batch\"\n+                    elif n_images_in_ids != n_images_in_images:\n+                        add_message = \"If you activated truncation with `max_length`, increase the `max_length` so image tokens aren't cropped.\"\n+\n                     raise ValueError(\n                         f\"The number of image tokens in each text ({n_images_in_text}) should be the same as the \"\n                         f\"number of provided images per batch ({n_images_in_images}). {add_message}\""
        },
        {
            "sha": "5048f0c3eef860509af06eb4184013c9faa20e7f",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -298,23 +298,21 @@ def __call__(\n             suffix = [sfx + self.tokenizer.eos_token for sfx in suffix]\n         pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n-        # max_length has to account for the image tokens\n-        if output_kwargs[\"text_kwargs\"].get(\"max_length\", None) is not None:\n-            output_kwargs[\"text_kwargs\"][\"max_length\"] += self.image_seq_length\n-\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         inputs = self.tokenizer(\n             input_strings,\n             text_pair=suffix,\n             return_token_type_ids=return_token_type_ids,\n             **output_kwargs[\"text_kwargs\"],\n         )\n+        self._check_special_mm_tokens(input_strings, inputs, modalities=[\"image\"])\n \n         return_data = {**inputs, \"pixel_values\": pixel_values}\n \n         if return_token_type_ids:\n             labels = inputs[\"input_ids\"].masked_fill(inputs[\"token_type_ids\"] == 0, -100)\n             return_data.update({\"labels\": labels})\n-        return BatchFeature(data=return_data)\n+        return BatchFeature(data=return_data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "a0d5a75a59cb6d90122bb508c88e05f30c936a08",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -71,6 +71,10 @@ def __init__(\n         tokenizer,\n         **kwargs,\n     ):\n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n+        self.audio_token = tokenizer.audio_token\n+        self.audio_token_id = tokenizer.audio_token_id\n         super().__init__(image_processor, audio_processor, tokenizer, **kwargs)\n \n     def __call__(\n@@ -113,7 +117,6 @@ def __call__(\n         output_kwargs = self._merge_kwargs(Phi4MultimodalProcessorKwargs, self.tokenizer.init_kwargs, **kwargs)\n         image_kwargs = output_kwargs[\"images_kwargs\"]\n         audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-        text_kwargs = output_kwargs[\"text_kwargs\"]\n \n         image_inputs = self.image_processor(images, **image_kwargs) if images is not None else {}\n         audio_inputs = self.audio_processor(audio, **audio_kwargs) if audio is not None else {}\n@@ -154,7 +157,9 @@ def __call__(\n             re.sub(re.escape(audio_token), lambda _: audio_token * next(audio_count_iter), t) for t in processed_text\n         ]\n \n-        text_inputs = self.tokenizer(processed_text, **text_kwargs)\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        text_inputs = self.tokenizer(processed_text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(processed_text, text_inputs, modalities=[\"image\"])\n \n         # prepare batch feature\n         data = {\n@@ -163,7 +168,7 @@ def __call__(\n             **audio_inputs,\n         }\n \n-        return BatchFeature(data=data)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "4531c56b5aaf5d7ac28a2049e00219adf6971bcb",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -103,6 +103,7 @@ def __init__(\n         self.patch_size = patch_size\n         self.spatial_merge_size = spatial_merge_size\n         self.image_token = image_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.image_break_token = image_break_token\n         self.image_end_token = image_end_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n@@ -211,10 +212,10 @@ def __call__(\n                     sample = sample.replace(\"<placeholder>\", replace_str, 1)\n                 prompt_strings.append(sample)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n-        return BatchFeature(\n-            data={**text_inputs, **image_inputs}, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n-        )\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "b47db208627414f797b57bda586965a8086c96b4",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -941,16 +941,14 @@ def __call__(\n         if not isinstance(text, list):\n             text = [text]\n \n+        text = text.copy()  # below lines change text in-place\n         if image_grid_thw is not None:\n             merge_length = self.image_processor.merge_size**2\n             index = 0\n             for i in range(len(text)):\n                 while self.image_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.image_token,\n-                        \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length),\n-                        1,\n-                    )\n+                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n \n@@ -959,17 +957,16 @@ def __call__(\n             index = 0\n             for i in range(len(text)):\n                 while self.video_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.video_token,\n-                        \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length),\n-                        1,\n-                    )\n+                    num_video_tokens = video_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.video_token, \"<|placeholder|>\" * num_video_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n \n __all__ = ["
        },
        {
            "sha": "d0994323833c869eb34da85ffd4604a3768864a3",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 18,
            "deletions": 11,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -77,6 +77,16 @@ class Qwen2_5_VLProcessor(ProcessorMixin):\n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if getattr(tokenizer, \"image_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.image_token)\n+        )\n+        self.video_token_id = (\n+            tokenizer.video_token_id\n+            if getattr(tokenizer, \"video_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.video_token)\n+        )\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -157,16 +167,14 @@ def __call__(\n         if not isinstance(text, list):\n             text = [text]\n \n+        text = text.copy()  # below lines change text in-place\n         if image_grid_thw is not None:\n             merge_length = self.image_processor.merge_size**2\n             index = 0\n             for i in range(len(text)):\n                 while self.image_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.image_token,\n-                        \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length),\n-                        1,\n-                    )\n+                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n \n@@ -175,17 +183,16 @@ def __call__(\n             index = 0\n             for i in range(len(text)):\n                 while self.video_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.video_token,\n-                        \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length),\n-                        1,\n-                    )\n+                    num_video_tokens = video_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.video_token, \"<|placeholder|>\" * num_video_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n-        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "0be2428f2ef5614db25d54155d76998a9517dc5d",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -76,6 +76,7 @@ def __init__(\n         if chat_template is None:\n             chat_template = self.default_chat_template\n         self.audio_token = tokenizer.audio_token if hasattr(tokenizer, \"audio_token\") else audio_token\n+        self.audio_token_id = tokenizer.convert_tokens_to_ids(self.audio_token)\n         self.audio_bos_token = tokenizer.audio_bos_token if hasattr(tokenizer, \"audio_bos_token\") else audio_bos_token\n         self.audio_eos_token = tokenizer.audio_eos_token if hasattr(tokenizer, \"audio_eos_token\") else audio_eos_token\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n@@ -179,12 +180,14 @@ def __call__(\n                 expanded_text.append(sample)\n             text = expanded_text\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, inputs, modalities=[\"audio\"])\n \n         if audio is not None:\n             inputs.update(audio_inputs)\n \n-        return BatchFeature(data={**inputs})\n+        return BatchFeature(data={**inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "241d63b029fa028589c37bb317e93578fc839b04",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 8,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -72,6 +72,16 @@ class Qwen2VLProcessor(ProcessorMixin):\n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if getattr(tokenizer, \"image_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.image_token)\n+        )\n+        self.video_token_id = (\n+            tokenizer.video_token_id\n+            if getattr(tokenizer, \"video_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.video_token)\n+        )\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -139,14 +149,15 @@ def __call__(\n         if not isinstance(text, list):\n             text = [text]\n \n+        text = text.copy()  # below lines change text in-place\n+\n         if image_grid_thw is not None:\n             merge_length = self.image_processor.merge_size**2\n             index = 0\n             for i in range(len(text)):\n                 while self.image_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.image_token, \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n-                    )\n+                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n \n@@ -155,15 +166,15 @@ def __call__(\n             index = 0\n             for i in range(len(text)):\n                 while self.video_token in text[i]:\n-                    text[i] = text[i].replace(\n-                        self.video_token, \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length), 1\n-                    )\n+                    num_video_tokens = video_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.video_token, \"<|placeholder|>\" * num_video_tokens, 1)\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-\n-        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "3a2357a4cc1d052a4b235a8547b41058abc84eb9",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -149,6 +149,7 @@ def __init__(\n     ):\n         self.fake_image_token = getattr(tokenizer, \"fake_image_token\", \"<fake_token_around_image>\")\n         self.image_token = getattr(tokenizer, \"image_token\", \"<image>\")\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n         self.end_of_utterance_token = getattr(tokenizer, \"end_of_utterance_token\", \"<end_of_utterance>\")\n         self.global_image_token = getattr(tokenizer, \"global_image_token\", \"<global-img>\")\n         self.image_seq_len = image_seq_len\n@@ -290,7 +291,7 @@ def __call__(\n             if n_images_in_text > 0 and (images is None and videos is None):\n                 raise ValueError(f\"We detected {n_images_in_text} tokens in the text but no images/videos were passed\")\n \n-        inputs = BatchFeature()\n+        inputs = {}\n         # Images and videos are mutually exclusive, so process one which is present\n         if images is not None:\n             images = make_nested_list_of_images(images)\n@@ -313,11 +314,14 @@ def __call__(\n             )\n             inputs.update(vision_inputs)\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+\n         if text is not None:\n-            text_inputs = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n+            text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+            self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n             inputs.update(text_inputs)\n \n-        return inputs\n+        return BatchFeature(inputs, tensor_type=return_tensors)\n \n     def _process_messages_for_chat_template(\n         self,"
        },
        {
            "sha": "a49bb1b22521a93bba989390ea058a3a11e31afb",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -87,6 +87,8 @@ def __init__(\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n+        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        self.video_token_id = tokenizer.convert_tokens_to_ids(self.video_token)\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -195,14 +197,16 @@ def __call__(\n \n         text_inputs = self.tokenizer(\n             prompt_strings,\n-            return_tensors=return_tensors,\n+            return_tensors=None,\n             padding=padding,\n             truncation=truncation,\n             max_length=max_length,\n         )\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\", \"video\"])\n+\n         data.update(text_inputs)\n \n-        return BatchFeature(data=data)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "dc6bd4bf1428dfb0bdbaa2f78e27e1c3283c0f7c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -31,6 +31,7 @@\n \n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n+from .feature_extraction_utils import BatchFeature\n from .image_utils import (\n     ChannelDimension,\n     ImageInput,\n@@ -1615,6 +1616,23 @@ def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens\n         \"\"\"\n         return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs)\n \n+    def _check_special_mm_tokens(self, text: list[str], text_inputs: \"BatchFeature\", modalities: list[str]):\n+        \"\"\"\n+        Checks that number of special tokens in text and processed text is same. The count can be different\n+        if tokenized text was truncated, leading to issues in model code.\n+        \"\"\"\n+        for modality in modalities:\n+            token_str = getattr(self, f\"{modality}_token\")\n+            token_id = getattr(self, f\"{modality}_token_id\")\n+            ids_count = [list(ids).count(token_id) for ids in text_inputs[\"input_ids\"]]\n+            text_count = [sample.count(token_str) for sample in text]\n+\n+            if ids_count != text_count:\n+                raise ValueError(\n+                    f\"Mismatch in `{modality}` token count between text and `input_ids`. Got ids={ids_count} and text={text_count}. \"\n+                    \"Likely due to `truncation='max_length'`. Please disable truncation or increase `max_length`.\"\n+                )\n+\n \n def _validate_images_text_input_order(images, text):\n     \"\"\""
        },
        {
            "sha": "9df833661a0c5470a047c4621de8b439aa5b73d4",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -271,3 +271,29 @@ def test_image_chat_template_accepts_processing_kwargs(self):\n             return_tensors=\"np\",\n         )\n         self.assertListEqual(list(out_dict[self.images_input_name].shape), [1, 3, 980, 980])\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=3,\n+            )"
        },
        {
            "sha": "d11321c9a8701652b80c9e332f7957adbfaba81d",
            "filename": "tests/models/chameleon/test_processor_chameleon.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -40,10 +40,37 @@ def setUpClass(cls):\n         tokenizer = LlamaTokenizer(vocab_file=SAMPLE_VOCAB)\n         tokenizer.pad_token_id = 0\n         tokenizer.sep_token_id = 1\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n         processor = cls.processor_class(image_processor=image_processor, tokenizer=tokenizer, image_seq_length=2)\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image_token = processor.image_token\n \n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=20,\n+            )\n+\n     @staticmethod\n     def prepare_processor_dict():\n         return {\"image_seq_length\": 2}  # fmt: skip"
        },
        {
            "sha": "968a852d64edfb31bf8e519c519a68ede1f3a639",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -124,3 +124,28 @@ def test_pan_and_scan(self):\n         # base image + 4 crops\n         self.assertEqual(len(inputs[self.images_input_name]), 5)\n         self.assertEqual(len(inputs[self.text_input_name][0]), 67)\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=5,\n+            )"
        },
        {
            "sha": "a39d14d4f17ee6d9613662873c220060e7787685",
            "filename": "tests/models/idefics2/test_processor_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -66,8 +66,8 @@ def setUpClass(cls):\n             )\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n-        cls.image_token = processor.image_token.content\n-        cls.fake_image_token = processor.fake_image_token.content\n+        cls.image_token = processor.image_token\n+        cls.fake_image_token = processor.fake_image_token\n \n         cls.bos_token_id = processor.tokenizer.convert_tokens_to_ids(cls.bos_token)\n         cls.image_token_id = processor.tokenizer.convert_tokens_to_ids(cls.image_token)"
        },
        {
            "sha": "99b931a12c280c4f7cbf474a4aeb988c008804dd",
            "filename": "tests/models/idefics3/test_processor_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -60,8 +60,8 @@ def setUpClass(cls):\n             )\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n-        cls.image_token = processor.image_token.content\n-        cls.fake_image_token = processor.fake_image_token.content\n+        cls.image_token = processor.image_token\n+        cls.fake_image_token = processor.fake_image_token\n         cls.global_img_token = processor.global_image_tag\n \n         cls.bos_token_id = processor.tokenizer.convert_tokens_to_ids(cls.bos_token)"
        },
        {
            "sha": "51ed955b845f1df0f7165d7e45272463c18fdd41",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -40,6 +40,7 @@ def setUpClass(cls):\n \n         image_processor = CLIPImageProcessor(do_center_crop=False)\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = LlavaProcessor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n@@ -79,3 +80,29 @@ def test_can_load_various_tokenizers(self):\n             processor = LlavaProcessor.from_pretrained(checkpoint)\n             tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n             self.assertEqual(processor.tokenizer.__class__, tokenizer.__class__)\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=5,\n+            )"
        },
        {
            "sha": "a565212b49e7698613be6c61418063d2cf429a13",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -40,6 +40,7 @@ def setUpClass(cls):\n \n         image_processor = LlavaNextImageProcessor()\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = LlavaNextProcessor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)"
        },
        {
            "sha": "d985e79aea9de02b9a30831451d38341cb6ecac2",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -41,6 +41,7 @@ def setUpClass(cls):\n         image_processor = LlavaNextImageProcessor()\n         video_processor = LlavaNextVideoImageProcessor()\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\", \"<video>\"]})\n         processor_kwargs = cls.prepare_processor_dict()\n \n         processor = LlavaNextVideoProcessor("
        },
        {
            "sha": "7e25eed4105fbfc96fab06d566d4cf2c93f4e18b",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -45,6 +45,7 @@ def setUpClass(cls):\n         image_processor = LlavaOnevisionImageProcessor()\n         video_processor = LlavaOnevisionVideoProcessor()\n         tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\", \"<video>\"]})\n         processor_kwargs = cls.prepare_processor_dict()\n \n         processor = LlavaOnevisionProcessor("
        },
        {
            "sha": "bb8d4ce8e246ea2d7997f8e4c38b0bfcb4b54bf9",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -290,3 +290,29 @@ def test_processor_returns_full_length_batches(self):\n         inputs_image = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 5)\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=3,\n+            )"
        },
        {
            "sha": "864c948d10df5438592ef790099e6ddf55328e72",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -360,3 +360,29 @@ def test_unstructured_kwargs_batched(self):\n             len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n             and len(inputs[self.text_input_name][1]) < 76\n         )\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        image_input = [[image_input[0]], [image_input[1]]]\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=3,\n+            )"
        },
        {
            "sha": "8ccae4588750277e0226898acea82c37a90bf580",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -39,6 +39,7 @@ def setUpClass(cls):\n         image_processor = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n         image_processor.image_seq_length = 0  # TODO: raushan fix me in #37342\n         tokenizer = GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n         processor = PaliGemmaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image_token = processor.image_token\n@@ -59,7 +60,7 @@ def test_image_seq_length(self):\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n         )\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 112 + 14)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n \n     def test_text_with_image_tokens(self):\n         image_processor = self.get_component(\"image_processor\")"
        },
        {
            "sha": "8e4efb0877e4cbe975149bf914283dad39e9b03c",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -397,3 +397,29 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         self.assertEqual(inputs[self.images_input_name].shape[0], 100)\n         inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=20,\n+            )"
        },
        {
            "sha": "00644e399cf090450f604c888b8b56d7284ef0e3",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 31,
            "deletions": 4,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32eca7197a8d2618417a0d665db38d0af3695a2c/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=32eca7197a8d2618417a0d665db38d0af3695a2c",
            "patch": "@@ -435,7 +435,8 @@ def test_unstructured_kwargs_batched(self):\n         image_processor = self.get_component(\"image_processor\")\n         tokenizer = self.get_component(\"tokenizer\")\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n@@ -445,14 +446,14 @@ def test_unstructured_kwargs_batched(self):\n             text=input_str,\n             images=image_input,\n             return_tensors=\"pt\",\n-            padding=\"longest\",\n+            padding=\"max_length\",\n             max_length=76,\n             truncation=True,\n-            max_image_size={\"longest_edge\": 30},\n+            max_image_size={\"longest_edge\": 300},\n         )\n \n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 3)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 30)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 300)\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n \n     @require_torch\n@@ -529,3 +530,29 @@ def test_missing_images_error(self):\n         with self.assertRaises(ValueError) as context:\n             processor(text=texts, images=None)\n         self.assertTrue(\"tokens in the text but no images/videos were passed\" in str(context.exception))\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        image_input = [[image_input[0]], [image_input[1]]]\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=20,\n+            )"
        }
    ],
    "stats": {
        "total": 512,
        "additions": 414,
        "deletions": 98
    }
}