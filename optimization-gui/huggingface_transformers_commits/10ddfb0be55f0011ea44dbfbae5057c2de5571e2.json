{
    "author": "cyyever",
    "message": "Add more missing arguments (#40354)\n\nAdd missing arguments\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
    "files": [
        {
            "sha": "f0aac49dd1c7b79c1b3e3a92af73af92f73b05e5",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -708,7 +708,7 @@ class BambaRMSNorm(LlamaRMSNorm):\n \n class BambaDecoderLayer(JambaAttentionDecoderLayer):\n     def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\"):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n \n         del self.self_attn\n "
        },
        {
            "sha": "56a72b1022036670e37e327937a19f77b5b43849",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -271,7 +271,7 @@ class Cohere2LayerNorm(CohereLayerNorm):\n     pass\n \n \n-class Cohere2Attention(CohereAttention, nn.Module):\n+class Cohere2Attention(CohereAttention):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):"
        },
        {
            "sha": "52ac7fef7b0d9a6b414bbf3c72173ac93520a29b",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -898,7 +898,7 @@ def __init__(self, config: DFineConfig):\n \n class DFineForObjectDetection(RTDetrForObjectDetection, DFinePreTrainedModel):\n     def __init__(self, config: DFineConfig):\n-        DFinePreTrainedModel.__init__(config)\n+        DFinePreTrainedModel.__init__(self, config)\n \n         # D-FINE encoder-decoder model\n         self.eval_idx = config.eval_idx if config.eval_idx >= 0 else config.decoder_layers + config.eval_idx"
        },
        {
            "sha": "91cb04730e4aec01edff0f35701eb12c3ee5af3d",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -112,7 +112,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Data2VecAudioFeatureEncoder(Wav2Vec2FeatureEncoder, nn.Module):\n+class Data2VecAudioFeatureEncoder(Wav2Vec2FeatureEncoder):\n     def __init__(self, config):\n         nn.Module.__init__(self)\n         self.conv_layers = nn.ModuleList(\n@@ -183,7 +183,7 @@ def load_adapter(self):\n \n class Data2VecAudioModel(Data2VecAudioPreTrainedModel, Wav2Vec2Model):\n     def __init__(self, config: Data2VecAudioConfig):\n-        Data2VecAudioPreTrainedModel.__init__(config)\n+        Data2VecAudioPreTrainedModel.__init__(self, config)\n         self.config = config\n         self.feature_extractor = Data2VecAudioFeatureEncoder(config)\n         self.feature_projection = Data2VecAudioFeatureProjection(config)\n@@ -215,7 +215,7 @@ def forward(self, **super_kwargs):\n \n class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel, Wav2Vec2ForCTC):\n     def __init__(self, config):\n-        Data2VecAudioPreTrainedModel.__init__(config)\n+        Data2VecAudioPreTrainedModel.__init__(self, config)\n \n         self.data2vec_audio = Data2VecAudioModel(config)\n         self.dropout = nn.Dropout(config.final_dropout)"
        },
        {
            "sha": "3ee8d15a5873c5b1bc5c76b463127730ca424766",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -107,7 +107,7 @@ class DiaRotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n-class DiaSelfAttention(LlamaAttention, nn.Module):\n+class DiaSelfAttention(LlamaAttention):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx: int, is_causal: bool = False):"
        },
        {
            "sha": "345265a14080c7e8978c37b34124e208536b1da2",
            "filename": "src/transformers/models/dots1/modular_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -62,7 +62,7 @@ class Dots1TopkRouter(DeepseekV3TopkRouter):\n \n class Dots1DecoderLayer(DeepseekV3DecoderLayer):\n     def __init__(self, config: Dots1Config, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         self.attention_type = config.layer_types[layer_idx]\n \n "
        },
        {
            "sha": "7cec0232ca6842e72b453f53560ff5da017f794d",
            "filename": "src/transformers/models/ernie4_5/modular_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -84,7 +84,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n class Ernie4_5MLP(LlamaMLP):\n     def __init__(self, config: Ernie4_5Config):\n-        super().__init__()\n+        super().__init__(config)\n \n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)"
        },
        {
            "sha": "e92e65d29bfb2e7a0ad640e71d983c1c467d3a54",
            "filename": "src/transformers/models/esm/openfold_utils/residue_constants.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -541,7 +541,6 @@ def make_bond_key(atom1_name: str, atom2_name: str) -> str:\n \n # A compact atom encoding with 14 columns\n # pylint: disable=line-too-long\n-# pylint: disable=bad-whitespace\n restype_name_to_atom14_names: dict[str, list[str]] = {\n     \"ALA\": [\"N\", \"CA\", \"C\", \"O\", \"CB\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n     \"ARG\": [\"N\", \"CA\", \"C\", \"O\", \"CB\", \"CG\", \"CD\", \"NE\", \"CZ\", \"NH1\", \"NH2\", \"\", \"\", \"\"],\n@@ -566,7 +565,6 @@ def make_bond_key(atom1_name: str, atom2_name: str) -> str:\n     \"UNK\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n }\n # pylint: enable=line-too-long\n-# pylint: enable=bad-whitespace\n \n \n # This is the standard residue order when coding AA type as a number."
        },
        {
            "sha": "7b4252477d74a01b1597cb913712ff98101a727e",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -65,7 +65,7 @@\n \n class EvollaSaProtEmbeddings(EsmEmbeddings):\n     def __init__(self, config):\n-        super().__init__()\n+        super().__init__(config)\n         # remove the position_ids in EsmEmbeddings\n         self.position_ids = None\n \n@@ -127,7 +127,7 @@ def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch\n         )\n \n \n-class EvollaSaProtSelfAttention(EsmSelfAttention, nn.Module):\n+class EvollaSaProtSelfAttention(EsmSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cross_attention=False):\n         nn.Module.__init__(self)\n         self.config = config"
        },
        {
            "sha": "fc90ac62248680e9831012b3d9865a0bd1573408",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -1007,7 +1007,7 @@ def forward(\n \n \n class FalconH1MLP(nn.Module):\n-    def __init__(self, config: FalconH1Config = None):\n+    def __init__(self, config: FalconH1Config):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "1ff8288f9c4f54da573f63af1a15c4beea470567",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -252,7 +252,7 @@ def forward(\n \n class FalconH1RMSNormGated(MambaRMSNormGated):\n     def __init__(self, hidden_size, eps=1e-6, n_groups=1, norm_before_gate=True):\n-        super().__init__()\n+        super().__init__(hidden_size=hidden_size, eps=eps)\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n         self.n_groups = n_groups\n@@ -812,8 +812,8 @@ def forward(\n \n \n class FalconH1MLP(LlamaMLP):\n-    def __init__(self, config: FalconH1Config = None):\n-        super().__init__()\n+    def __init__(self, config: FalconH1Config):\n+        super().__init__(config)\n         self.gate_multiplier, self.down_multiplier = config.mlp_multipliers\n \n     def forward(self, x):"
        },
        {
            "sha": "090a147d31e2260488c7b0d3f73c7526b2348494",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -517,7 +517,7 @@ class FalconMambaCausalLMOutput(MambaCausalLMOutput):\n \n class FalconMambaModel(MambaModel, FalconMambaPreTrainedModel):\n     def __init__(self, config):\n-        FalconMambaPreTrainedModel.__init__(config)\n+        FalconMambaPreTrainedModel.__init__(self, config)\n \n         self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n         self.layers = nn.ModuleList("
        },
        {
            "sha": "417e296071de7e415cc1cc8dd551471855289824",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -1065,7 +1065,7 @@ def forward(self, seq_embeds: torch.Tensor) -> torch.Tensor:\n \n class Florence2VisionMLP(Llama4VisionMLP):\n     def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n-        super().__init__()\n+        super().__init__(config)\n         self.fc1 = nn.Linear(config.embed_dim[stage_idx], int(config.embed_dim[stage_idx] * config.mlp_ratio))\n         self.activation_fn = ACT2FN[config.activation_function]\n         self.fc2 = nn.Linear(int(config.embed_dim[stage_idx] * config.mlp_ratio), config.embed_dim[stage_idx])"
        },
        {
            "sha": "ea37bd31ef12f95f4703e82febf0711f04ad85a7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -360,7 +360,7 @@ def extra_repr(self):\n \n class GemmaMLP(LlamaMLP):\n     def __init__(self, config):\n-        super().__init__()\n+        super().__init__(config)\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)"
        },
        {
            "sha": "7f101ff1ec0a3e95d2bf57a261bbeac2ab1bdbe7",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -209,7 +209,7 @@ class Gemma2RMSNorm(GemmaRMSNorm):\n \n class Gemma2MLP(GemmaMLP):\n     def __init__(self, config):\n-        super().__init__()\n+        super().__init__(config)\n         self.act_fn = ACT2FN[config.hidden_activation]\n \n "
        },
        {
            "sha": "6e06671ea0bbd983714191e5cb97b01ac41a7212",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -383,7 +383,7 @@ def __init__(self, config: Gemma3TextConfig):\n \n class Gemma3RMSNorm(Gemma2RMSNorm):\n     def __init__(self, dim: int, eps: float = 1e-6):\n-        super().__init__()\n+        super().__init__(dim=dim, eps=eps)\n \n \n class Gemma3RotaryEmbedding(Gemma2RotaryEmbedding):\n@@ -396,7 +396,7 @@ class Gemma3Attention(Gemma2Attention):\n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n \n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         self.sliding_window = config.sliding_window if self.is_sliding else None\n \n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)"
        },
        {
            "sha": "9df2d6a05bf1999f06c04baaaa1dc9a41e06a1a6",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -1739,7 +1739,7 @@ def apply_rotary_pos_emb(\n \n class Gemma3nTextAttention(Gemma3Attention):\n     def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         del self.attn_logit_softcapping\n         del self.scaling\n         self.v_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps, with_scale=False)\n@@ -2234,7 +2234,7 @@ class Gemma3nModel(PaliGemmaModel):\n     _checkpoint_conversion_mapping = {}\n \n     def __init__(self, config: Gemma3nConfig):\n-        super().__init__()\n+        super().__init__(config)\n         del self.multi_modal_projector  # Replaced by Gemma3nVisionEmbedder\n         self.vocab_size_per_layer_input = config.text_config.vocab_size_per_layer_input\n         self.audio_tower = AutoModel.from_config(config.audio_config)"
        },
        {
            "sha": "bc07483c7f22f6dfea4615f520839fd384be4a91",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -255,7 +255,7 @@ def __init__(\n         )\n \n \n-class Glm4MoeAttention(CohereAttention, nn.Module):\n+class Glm4MoeAttention(CohereAttention):\n     def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n         nn.Module.__init__(self)\n         self.config = config\n@@ -287,7 +287,7 @@ class Glm4MoeMLP(DeepseekV3MLP):\n     pass\n \n \n-class Glm4MoeTopkRouter(DeepseekV3TopkRouter, nn.Module):\n+class Glm4MoeTopkRouter(DeepseekV3TopkRouter):\n     def __init__(self, config: Glm4MoeConfig):\n         nn.Module.__init__(self)\n         self.config = config"
        },
        {
            "sha": "e872995aed329718c7666e959ea4b6420ab8ee99",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -507,15 +507,15 @@ def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torc\n \n class Glm4vVisionAttention(Qwen2_5_VLVisionAttention):\n     def __init__(self, config: Glm4vVisionConfig) -> None:\n-        super().__init__()\n+        super().__init__(config)\n         self.attention_dropout = config.attention_dropout\n         self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.attention_bias)\n         self.proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n \n \n class Glm4vVisionBlock(Qwen2_5_VLVisionBlock):\n     def __init__(self, config) -> None:\n-        super().__init__()\n+        super().__init__(config)\n         self.norm1 = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.norm2 = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attn = Glm4vVisionAttention(config)"
        },
        {
            "sha": "e51e4b12c79878179fd7eaacf7a8734153e51d8d",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -240,7 +240,7 @@ class GotOcr2VisionAttention(SamVisionAttention):\n \n class GotOcr2VisionLayer(SamVisionLayer):\n     def __init__(self, config, window_size):\n-        super().__init__()\n+        super().__init__(config, window_size)\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.attn = GotOcr2VisionAttention(config, window_size)\n         self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)"
        },
        {
            "sha": "fe53f7820abb1e36030bbab4e73e2f78983cda0e",
            "filename": "src/transformers/models/helium/modular_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -104,7 +104,7 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n \n class HeliumDecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n \n         self.mlp = HeliumMLP(config)\n         self.input_layernorm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)"
        },
        {
            "sha": "c79ccc6a616dd343d5aa11b3aec4ae78320c8830",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -114,7 +114,7 @@ def forward(\n \n class HunYuanDenseV1DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         self.layer_idx = layer_idx\n \n "
        },
        {
            "sha": "645c54ae73afd1b43db807535c0ac36957eac7ff",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -187,7 +187,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n class HunYuanMoEV1DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         self.hidden_size = config.hidden_size\n         self.self_attn = HunYuanMoEV1Attention(config=config, layer_idx=layer_idx)\n         self.mlp = HunYuanMoEV1Moe(config, layer_idx=layer_idx)"
        },
        {
            "sha": "1e0757d6cf0cd6060493870261e11f1e185cbe48",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -79,7 +79,7 @@ class InternVLVisionRMSNorm(LlamaRMSNorm):\n \n class InternVLVisionAttention(JanusVisionAttention):\n     def __init__(self, config: InternVLVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         del self.num_key_value_groups\n \n         # Needed for flash attention"
        },
        {
            "sha": "2b5a4c09e023f4e58e81560a7d7a39031222c1b8",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -536,7 +536,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n class JanusVisionEncoderLayer(SiglipEncoderLayer):\n     def __init__(self, config: JanusVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         self.embed_dim = config.hidden_size\n         self.self_attn = JanusVisionAttention(config)"
        },
        {
            "sha": "290d60b91e66ccb1bb9aaf853b521cc56f20e79a",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -44,7 +44,7 @@ def __init__(self, config):\n \n class MistralAttention(LlamaAttention):\n     def __init__(self, config: MistralConfig, layer_idx: int):\n-        super().__init__()\n+        super().__init__(config, layer_idx)\n         self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)"
        },
        {
            "sha": "a05045a68cb57f7a5fab69e7ccd31bfae518d2bc",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -337,7 +337,7 @@ class MMGroundingDinoDecoder(GroundingDinoDecoder):\n \n class MMGroundingDinoModel(GroundingDinoModel, MMGroundingDinoPreTrainedModel):\n     def __init__(self, config: MMGroundingDinoConfig):\n-        MMGroundingDinoPreTrainedModel.__init__(config)\n+        MMGroundingDinoPreTrainedModel.__init__(self, config)\n \n         # Create backbone + positional encoding\n         backbone = MMGroundingDinoConvEncoder(config)\n@@ -400,7 +400,7 @@ class MMGroundingDinoForObjectDetection(GroundingDinoForObjectDetection, MMGroun\n     ]\n \n     def __init__(self, config: MMGroundingDinoConfig):\n-        MMGroundingDinoPreTrainedModel.__init__(config)\n+        MMGroundingDinoPreTrainedModel.__init__(self, config)\n \n         self.model = MMGroundingDinoModel(config)\n "
        },
        {
            "sha": "2eeff19fa85231ba70d09e2ec24464e0ef2268f6",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -60,7 +60,7 @@ class Ovis2VisionMLP(LlamaMLP):\n \n class Ovis2VisionEmbeddings(SiglipVisionEmbeddings):\n     def __init__(self, config: Ovis2VisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.rms_norm = Ovis2RMSNorm(config.hidden_size, config.rms_norm_eps)\n \n     def interpolate_pos_encoding(self):\n@@ -87,7 +87,7 @@ class Ovis2VisionEncoderLayer(Aimv2EncoderLayer):\n \n class Ovis2VisionEncoder(SiglipEncoder):\n     def __init__(self, config: Ovis2VisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.layers = nn.ModuleList([Ovis2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n \n "
        },
        {
            "sha": "bfab86560abe35325c53804e4fa12c44226d0617",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -528,7 +528,7 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n \n class Phi4MultimodalVisionEncoder(SiglipEncoder):\n     def __init__(self, config: Phi4MultimodalVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.layers = nn.ModuleList(\n             [Phi4MultimodalVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n         )\n@@ -582,7 +582,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-class Phi4MultimodalVisionEmbeddings(SiglipVisionEmbeddings, nn.Module):\n+class Phi4MultimodalVisionEmbeddings(SiglipVisionEmbeddings):\n     def __init__(self, config: Phi4MultimodalVisionConfig):\n         nn.Module.__init__(self)\n         self.config = config\n@@ -1455,7 +1455,7 @@ def _init_weights(self, module):\n             module.sub_img_feature_extensor.data.zero_()\n \n \n-class Phi4MultimodalModel(Phi3Model, nn.Module):\n+class Phi4MultimodalModel(Phi3Model):\n     def __init__(self, config: Phi4MultimodalConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1570,7 +1570,7 @@ def forward(\n         )\n \n \n-class Phi4MultimodalForCausalLM(Phi3ForCausalLM, nn.Module):\n+class Phi4MultimodalForCausalLM(Phi3ForCausalLM):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "6425f6df9129a42cf995b8a1586c2334078ed9e7",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -2061,7 +2061,7 @@ def __init__(self, config: Qwen2_5OmniThinkerConfig, device=None):\n \n # It's same as `Qwen2_5_VLAttention`, but talker model's hidden_size isn't divisible by num_heads.\n # Removes the value error as a workaround.\n-class Qwen2_5OmniAttention(Qwen2_5_VLAttention, nn.Module):\n+class Qwen2_5OmniAttention(Qwen2_5_VLAttention):\n     def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n         nn.Module.__init__(self)\n         self.config = config"
        },
        {
            "sha": "af28015d146265529d24a56db61181526ad98f75",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -610,7 +610,7 @@ class RTDetrV2MLPPredictionHead(RTDetrMLPPredictionHead):\n \n class RTDetrV2ForObjectDetection(RTDetrForObjectDetection, RTDetrV2PreTrainedModel):\n     def __init__(self, config: RTDetrV2Config):\n-        RTDetrV2PreTrainedModel.__init__(config)\n+        RTDetrV2PreTrainedModel.__init__(self, config)\n         # RTDETR encoder-decoder model\n         self.model = RTDetrV2Model(config)\n "
        },
        {
            "sha": "2956187763c8388914573e1d480bcbfa849e1eb7",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -232,7 +232,7 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n \n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     # Update: add `spatial_shapes` and `attention_mask`"
        },
        {
            "sha": "900079b7bb9b469f562bab3be8ea3605f03bd0f4",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -215,7 +215,7 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n \n class UniSpeechModel(UniSpeechPreTrainedModel, Wav2Vec2Model):\n     def __init__(self, config: UniSpeechConfig):\n-        UniSpeechPreTrainedModel.__init__(config)\n+        UniSpeechPreTrainedModel.__init__(self, config)\n         self.config = config\n         self.feature_extractor = UniSpeechFeatureEncoder(config)\n         self.feature_projection = UniSpeechFeatureProjection(config)"
        },
        {
            "sha": "3e1d999392150890adc54c4cf345a2e9a3624c8e",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -101,7 +101,7 @@ class UniSpeechSatEncoderStableLayerNorm(Wav2Vec2EncoderStableLayerNorm):\n \n class UniSpeechSatGumbelVectorQuantizer(Wav2Vec2GumbelVectorQuantizer):\n     def __init__(self, config):\n-        super().__init__()\n+        super().__init__(config)\n         self.weight_proj = nn.Linear(config.hidden_size, self.num_groups * self.num_vars)\n \n     @staticmethod\n@@ -227,7 +227,7 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n \n class UniSpeechSatModel(UniSpeechSatPreTrainedModel, Wav2Vec2Model):\n     def __init__(self, config: UniSpeechSatConfig):\n-        UniSpeechSatPreTrainedModel.__init__(config)\n+        UniSpeechSatPreTrainedModel.__init__(self, config)\n         self.config = config\n         self.feature_extractor = UniSpeechSatFeatureEncoder(config)\n         self.feature_projection = UniSpeechSatFeatureProjection(config)"
        },
        {
            "sha": "b9b60a6bd3ad839d9e01fdacdf52adbcd0b7edb4",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -64,7 +64,7 @@ def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Ten\n     return mask\n \n \n-class Wav2Vec2BertRotaryPositionalEmbedding(Wav2Vec2ConformerRotaryPositionalEmbedding, nn.Module):\n+class Wav2Vec2BertRotaryPositionalEmbedding(Wav2Vec2ConformerRotaryPositionalEmbedding):\n     def __init__(self, config):\n         nn.Module.__init__(self)\n         dim = config.hidden_size // config.num_attention_heads\n@@ -96,7 +96,7 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-class Wav2Vec2BertFeedForward(Wav2Vec2FeedForward, nn.Module):\n+class Wav2Vec2BertFeedForward(Wav2Vec2FeedForward):\n     def __init__(self, config, act_fn=None, hidden_size=None):\n         nn.Module.__init__(self)\n         act_fn = act_fn if act_fn is not None else config.hidden_act\n@@ -671,7 +671,7 @@ def _get_feature_vector_attention_mask(\n \n class Wav2Vec2BertModel(Wav2Vec2Model, Wav2Vec2BertPreTrainedModel):\n     def __init__(self, config: Wav2Vec2BertConfig):\n-        Wav2Vec2BertPreTrainedModel.__init__(config)\n+        Wav2Vec2BertPreTrainedModel.__init__(self, config)\n         self.config = config\n         self.feature_projection = Wav2Vec2BertFeatureProjection(config)\n "
        },
        {
            "sha": "2c009c004453314f074f91d8ed89cf694c82726c",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -647,7 +647,7 @@ def _get_feature_vector_attention_mask(\n \n class Wav2Vec2ConformerModel(Wav2Vec2ConformerPreTrainedModel, Wav2Vec2Model):\n     def __init__(self, config: Wav2Vec2ConformerConfig):\n-        Wav2Vec2ConformerPreTrainedModel.__init__(config)\n+        Wav2Vec2ConformerPreTrainedModel.__init__(self, config)\n         self.config = config\n         self.feature_extractor = Wav2Vec2ConformerFeatureEncoder(config)\n         self.feature_projection = Wav2Vec2ConformerFeatureProjection(config)"
        },
        {
            "sha": "14f189d2f1ccfe2b83a0dbeab4b34d05af5623a8",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10ddfb0be55f0011ea44dbfbae5057c2de5571e2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=10ddfb0be55f0011ea44dbfbae5057c2de5571e2",
            "patch": "@@ -1434,7 +1434,7 @@ def forward(\n             offset = 0\n             with torch.no_grad():\n                 if cache_params is None:\n-                    cache_params = xLSTMCache(config=self.config, batch_size=hidden_states.shape[0])\n+                    cache_params = xLSTMCache(config=self.config, max_batch_size=hidden_states.shape[0])\n                 final_state = torch.zeros_like(hidden_states)\n                 while offset < hidden_states.shape[1]:\n                     hidden_states_chunk = hidden_states["
        }
    ],
    "stats": {
        "total": 108,
        "additions": 53,
        "deletions": 55
    }
}