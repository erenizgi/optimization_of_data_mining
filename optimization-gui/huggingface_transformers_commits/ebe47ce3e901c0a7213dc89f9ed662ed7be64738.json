{
    "author": "rahul-tuli",
    "message": "Fix: Unexpected Keys, Improve `run_compressed`, Rename Test Folder (#37077)",
    "sha": "ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
    "files": [
        {
            "sha": "218c8dc6e9ef21ce51b75c0362db5991971dd326",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "patch": "@@ -1352,6 +1352,7 @@ def _find_missing_and_unexpected_keys(\n \n     if hf_quantizer is not None:\n         missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n+        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n \n     # Model-specific exceptions for missing and unexpected keys (e.g. if the modeling change over time, or any other reason...)\n     if cls._keys_to_ignore_on_load_missing is not None:"
        },
        {
            "sha": "ee1d0df380e12cea2653e1e714fc3055d9b39c34",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 10,
            "deletions": 30,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "patch": "@@ -46,6 +46,10 @@ def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n                 \"`pip install compressed-tensors`\"\n             )\n \n+        # Call post_init here to ensure proper config setup when `run_compressed`\n+        # is provided directly via CompressedTensorsConfig, and to avoid duplicate logging.\n+\n+        quantization_config.post_init()\n         from compressed_tensors.compressors import ModelCompressor\n \n         self.compressor = ModelCompressor.from_compression_config(quantization_config)\n@@ -117,16 +121,16 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n         ct_quantization_config = self.compressor.quantization_config\n \n         if self.run_compressed:\n-            if not self.is_quantization_compressed:\n-                raise ValueError(\"`run_compressed` is only supported for quantized_compressed models\")\n             apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n-        elif self.is_quantized and not self.is_quantization_compressed:\n+        elif not self.quantization_config.is_quantization_compressed:\n             apply_quantization_config(model, ct_quantization_config)\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"Decompress loaded model if necessary - need for qat\"\"\"\n \n-        if (self.is_quantization_compressed and not self.run_compressed) or self.is_sparsification_compressed:\n+        if (\n+            self.quantization_config.is_quantization_compressed and not self.run_compressed\n+        ) or self.quantization_config.is_sparsification_compressed:\n             config = kwargs.get(\"config\", None)\n             cache_path = config._name_or_path\n \n@@ -136,44 +140,20 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n                 config_file_path = cached_file(cache_path, \"config.json\")\n                 cache_path = os.path.sep.join(config_file_path.split(os.path.sep)[:-1])\n \n-            if self.is_quantization_compressed and not self.run_compressed:\n+            if self.quantization_config.is_quantization_compressed and not self.run_compressed:\n                 from compressed_tensors.quantization import QuantizationStatus\n \n                 self.compressor.quantization_config.quantization_status = QuantizationStatus.FROZEN\n             self.compressor.decompress(model_path=cache_path, model=model)\n \n-    @property\n-    def is_quantized(self):\n-        return self.quantization_config.quantization_config is not None and bool(\n-            self.quantization_config.quantization_config.config_groups\n-        )\n-\n-    @property\n-    def is_quantization_compressed(self):\n-        from compressed_tensors.quantization import QuantizationStatus\n-\n-        return (\n-            self.quantization_config.quantization_config is not None\n-            and self.quantization_config.quantization_config.quantization_status == QuantizationStatus.COMPRESSED\n-        )\n-\n-    @property\n-    def is_sparsification_compressed(self):\n-        from compressed_tensors.config.base import CompressionFormat\n-\n-        return (\n-            self.quantization_config.sparsity_config is not None\n-            and self.quantization_config.sparsity_config.format != CompressionFormat.dense.value\n-        )\n-\n     @property\n     def is_trainable(self):\n         return True\n \n     def is_qat_trainable(self) -> bool:\n         \"\"\"Loaded Models can carry out quantization aware training\"\"\"\n         # models need to be decompressed carry out qat\n-        return not self.run_compressed or not self.is_quantization_compressed\n+        return not self.run_compressed or not self.quantization_config.is_quantization_compressed\n \n     def is_serializable(self, safe_serialization=None) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\""
        },
        {
            "sha": "b0f119c58b67b919112884cb2e9253c1c295ebcc",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "patch": "@@ -1263,7 +1263,7 @@ def __init__(\n \n         # parse from dict to load nested QuantizationScheme objects\n         if config_groups or kv_cache_scheme:\n-            self.quantization_config = QuantizationConfig.parse_obj(\n+            self.quantization_config = QuantizationConfig.model_validate(\n                 {\n                     \"config_groups\": config_groups,\n                     \"quant_method\": quant_method,\n@@ -1282,7 +1282,19 @@ def __init__(\n                 sparsity_config.get(\"format\"), **sparsity_config\n             )\n \n-        super().__init__(quant_method=QuantizationMethod.COMPRESSED_TENSORS)\n+        self.quant_method = QuantizationMethod.COMPRESSED_TENSORS\n+\n+    def post_init(self):\n+        if self.run_compressed:\n+            if self.is_sparsification_compressed:\n+                logger.warn(\n+                    \"`run_compressed` is only supported for quantized_compressed models\"\n+                    \" and not for sparsified models. Setting `run_compressed=False`\"\n+                )\n+                self.run_compressed = False\n+            elif not self.is_quantization_compressed:\n+                logger.warn(\"`run_compressed` is only supported for compressed models. Setting `run_compressed=False`\")\n+                self.run_compressed = False\n \n     @classmethod\n     def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n@@ -1356,6 +1368,28 @@ def to_diff_dict(self) -> Dict[str, Any]:\n     def get_loading_attributes(self):\n         return {\"run_compressed\": self.run_compressed}\n \n+    @property\n+    def is_quantized(self):\n+        return bool(self.quantization_config) and bool(self.quantization_config.config_groups)\n+\n+    @property\n+    def is_quantization_compressed(self):\n+        from compressed_tensors.quantization import QuantizationStatus\n+\n+        return self.is_quantized and self.quantization_config.quantization_status == QuantizationStatus.COMPRESSED\n+\n+    @property\n+    def is_sparsification_compressed(self):\n+        from compressed_tensors.config import (\n+            CompressionFormat,\n+            SparsityCompressionConfig,\n+        )\n+\n+        return (\n+            isinstance(self.sparsity_config, SparsityCompressionConfig)\n+            and self.sparsity_config.format != CompressionFormat.dense.value\n+        )\n+\n \n @dataclass\n class FbgemmFp8Config(QuantizationConfigMixin):"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/compressed_tensors_integration/__init__.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2F__init__.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "previous_filename": "tests/quantization/compressed_tensors/__init__.py"
        },
        {
            "sha": "074c943431a988d103edca8d2e92832f2d5737f9",
            "filename": "tests/quantization/compressed_tensors_integration/test_compressed_models.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "patch": "@@ -185,6 +185,7 @@ def test_default_run_compressed__True(self):\n     def test_default_run_compressed__False(self):\n         from compressed_tensors.linear.compressed_linear import CompressedLinear\n         from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n         from transformers.utils.quantization_config import CompressedTensorsConfig\n \n         quantization_config = CompressedTensorsConfig(run_compressed=False)",
            "previous_filename": "tests/quantization/compressed_tensors/test_compressed_models.py"
        },
        {
            "sha": "47e7849806043d6f4dfb3eef3f0ed2dbd0732ccf",
            "filename": "tests/quantization/compressed_tensors_integration/test_compressed_tensors.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebe47ce3e901c0a7213dc89f9ed662ed7be64738/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py?ref=ebe47ce3e901c0a7213dc89f9ed662ed7be64738",
            "previous_filename": "tests/quantization/compressed_tensors/test_compressed_tensors.py"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 48,
        "deletions": 32
    }
}