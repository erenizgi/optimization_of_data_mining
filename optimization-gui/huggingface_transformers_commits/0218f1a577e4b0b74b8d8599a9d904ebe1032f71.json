{
    "author": "nandan2003",
    "message": "Fix: Pass local_files_only from pipeline() to model loading (#42318)\n\n* Fix: Propagate local_files_only to hub_kwargs and PEFT adapter loading\n\n* Fix conflicts and code style\n\n* Apply formatting fixes\n\n* Apply style fixes\n\n* Fix style: Apply minimal changes for local_files_only\n\n* Style: Revert formatting and finalize local_files_only fix in __init__.py\n\n* Apply style fixes\n\n* Trigger tests\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Matt <rocketknight1@gmail.com>",
    "sha": "0218f1a577e4b0b74b8d8599a9d904ebe1032f71",
    "files": [
        {
            "sha": "45ae4bc9c02fed27ba59525711eb94340ec3dd1f",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0218f1a577e4b0b74b8d8599a9d904ebe1032f71/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0218f1a577e4b0b74b8d8599a9d904ebe1032f71/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=0218f1a577e4b0b74b8d8599a9d904ebe1032f71",
            "patch": "@@ -84,6 +84,7 @@ def load_adapter(\n         low_cpu_mem_usage: bool = False,\n         is_trainable: bool = False,\n         hotswap: bool | Literal[\"auto\"] = \"auto\",\n+        local_files_only: bool = False,\n         adapter_kwargs: dict[str, Any] | None = None,\n     ) -> None:\n         \"\"\"\n@@ -243,6 +244,7 @@ def load_adapter(\n             adapter_config_file = find_adapter_config_file(\n                 peft_model_id,\n                 token=token,\n+                local_files_only=local_files_only,\n                 **adapter_kwargs,\n             )\n \n@@ -255,6 +257,7 @@ def load_adapter(\n             peft_config = PeftConfig.from_pretrained(\n                 peft_model_id,\n                 token=token,\n+                local_files_only=local_files_only,\n                 **adapter_kwargs,\n             )\n             peft_config.inference_mode = not is_trainable\n@@ -268,6 +271,8 @@ def load_adapter(\n             self._hf_peft_config_loaded = True\n \n         if peft_model_id is not None:\n+            if \"local_files_only\" not in adapter_kwargs:\n+                adapter_kwargs[\"local_files_only\"] = local_files_only\n             adapter_state_dict = load_peft_weights(peft_model_id, token=token, device=device, **adapter_kwargs)\n \n         # We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility"
        },
        {
            "sha": "34d6222d71651d872141101d808be6f727638455",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0218f1a577e4b0b74b8d8599a9d904ebe1032f71/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0218f1a577e4b0b74b8d8599a9d904ebe1032f71/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=0218f1a577e4b0b74b8d8599a9d904ebe1032f71",
            "patch": "@@ -700,12 +700,14 @@ def pipeline(\n \n     code_revision = kwargs.pop(\"code_revision\", None)\n     commit_hash = kwargs.pop(\"_commit_hash\", None)\n+    local_files_only = kwargs.get(\"local_files_only\", False)\n \n     hub_kwargs = {\n         \"revision\": revision,\n         \"token\": token,\n         \"trust_remote_code\": trust_remote_code,\n         \"_commit_hash\": commit_hash,\n+        \"local_files_only\": local_files_only,\n     }\n \n     if task is None and model is None:"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 7,
        "deletions": 0
    }
}