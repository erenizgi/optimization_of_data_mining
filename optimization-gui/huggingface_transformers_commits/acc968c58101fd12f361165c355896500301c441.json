{
    "author": "kashif",
    "message": "[CP] Add attention_mask to the buffer when the mask is causal  (#40619)\n\nFix attention mask validation for context parallelism\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "acc968c58101fd12f361165c355896500301c441",
    "files": [
        {
            "sha": "e1948205264abd1150834923ac358ebf6d8911d4",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/acc968c58101fd12f361165c355896500301c441/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acc968c58101fd12f361165c355896500301c441/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=acc968c58101fd12f361165c355896500301c441",
            "patch": "@@ -3912,20 +3912,31 @@ def _prepare_context_parallel_inputs(self, model, inputs: dict[str, Union[torch.\n             if \"shift_labels\" in inputs:\n                 buffers.append(inputs[\"shift_labels\"])\n                 buffer_seq_dims.append(1)\n-            if \"attention_mask\" in inputs and not getattr(self, \"_attn_mask_causal_checked\", False):\n-                # Context parallel currently doesn't support other masks than causal\n-                # Accelerate applies hooks to replace mask with is_causal arg in SDPA\n-                # Check if the mask is really causal and if not throw an error\n-                # TODO: check this only once or always, with speed being the cost\n-                attention_mask = inputs[\"attention_mask\"]\n-                if not self._is_attention_mask_causal(attention_mask):\n-                    raise ValueError(\n-                        \"Context parallelism only supports causal attention masks. \"\n-                        \"The provided attention_mask is not causal. \"\n-                        \"Please ensure your data uses causal masking (lower triangular) \"\n-                        \"or remove the attention_mask to use the model's default causal masking.\"\n-                    )\n-                self._attn_mask_causal_checked = True\n+            # Add attention_mask to buffers for context parallel splitting (only if causal)\n+            if \"attention_mask\" in inputs:\n+                # Only validate causal mask once for performance\n+                if not getattr(self, \"_attn_mask_causal_checked\", False):\n+                    # Context parallel currently doesn't support other masks than causal\n+                    # Accelerate applies hooks to replace mask with is_causal arg in SDPA\n+                    # Check if the mask is really causal and if not throw an error\n+                    attention_mask = inputs[\"attention_mask\"]\n+                    if not self._is_attention_mask_causal(attention_mask):\n+                        raise ValueError(\n+                            \"Context parallelism only supports causal attention masks. \"\n+                            \"The provided attention_mask is not causal. \"\n+                            \"Please ensure your data uses causal masking (lower triangular) \"\n+                            \"or remove the attention_mask to use the model's default causal masking.\"\n+                        )\n+                    self._attn_mask_causal_checked = True\n+                if self._attn_mask_causal_checked:\n+                    # Add to buffers only after validation (or if validation already passed)\n+                    attention_mask = inputs[\"attention_mask\"]\n+                    if attention_mask.dim() == 2:\n+                        buffers.append(attention_mask)\n+                        buffer_seq_dims.append(1)\n+                    else:\n+                        # Other dimensionality; keep as-is without sharding to avoid incorrect splits\n+                        pass\n             # Include position_ids in context parallelism splitting\n             if \"position_ids\" in inputs and inputs[\"position_ids\"] is not None:\n                 buffers.append(inputs[\"position_ids\"])"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 25,
        "deletions": 14
    }
}