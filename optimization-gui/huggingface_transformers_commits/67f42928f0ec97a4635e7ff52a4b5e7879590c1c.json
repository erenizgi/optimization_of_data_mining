{
    "author": "DWarez",
    "message": "Remove residual quantization attribute from dequantized models (#39373)\n\n* fix: removing quantization trace attribute from dequantized model\n\nFixes #39295\n\n* add: test `to(dtype=torch.float16)` after dequantization",
    "sha": "67f42928f0ec97a4635e7ff52a4b5e7879590c1c",
    "files": [
        {
            "sha": "0a4ddf680461c9cbab889c869ae01a9454a8b250",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/67f42928f0ec97a4635e7ff52a4b5e7879590c1c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67f42928f0ec97a4635e7ff52a4b5e7879590c1c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=67f42928f0ec97a4635e7ff52a4b5e7879590c1c",
            "patch": "@@ -248,6 +248,7 @@ def dequantize(self, model):\n         del model.hf_quantizer\n         del model.config.quantization_config\n         del model.config._pre_quantization_dtype\n+        del model.quantization_method\n         model.is_quantized = False\n \n         return model"
        },
        {
            "sha": "3c0977f21c6dbf177f31a005e961e84984627704",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/67f42928f0ec97a4635e7ff52a4b5e7879590c1c/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67f42928f0ec97a4635e7ff52a4b5e7879590c1c/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=67f42928f0ec97a4635e7ff52a4b5e7879590c1c",
            "patch": "@@ -271,6 +271,33 @@ def test_generate_quality_dequantize(self):\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n+    def test_clear_quantization_trace(self):\n+        r\"\"\"\n+        Test that dequantizing the model won't leave any attribute relative to quantization in the model's configuration\n+        \"\"\"\n+        bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+        model_4bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=bnb_config, device_map=\"auto\"\n+        )\n+        model_4bit.dequantize()\n+\n+        self.assertFalse(hasattr(model_4bit, \"hf_quantizer\"))\n+        self.assertFalse(hasattr(model_4bit.config, \"quantization_config\"))\n+        self.assertFalse(hasattr(model_4bit.config, \"_pre_quantization_dtype\"))\n+        self.assertFalse(hasattr(model_4bit, \"quantization_method\"))\n+        self.assertFalse(model_4bit.is_quantized)\n+\n+    def test_to_device_dequantized(self):\n+        r\"\"\"\n+        Test that dequantizing the model won't prevent converting it to a different dtype\n+        \"\"\"\n+        bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+        model_4bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=bnb_config, device_map=\"auto\"\n+        )\n+        model_4bit.dequantize()\n+        model_4bit.to(dtype=torch.float16)\n+\n     def test_device_assignment(self):\n         if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n             self.skipTest(reason=\"This test requires bitsandbytes >= 0.43.2\")"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 28,
        "deletions": 0
    }
}