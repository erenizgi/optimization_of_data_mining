{
    "author": "Manalelaidouni",
    "message": "Add X-Codec model (#38248)\n\n* add working x-codec\n\n* nit\n\n* fix styling + copies\n\n* fix docstring\n\n* fix docstring and config attribute\n\n* Update args + config\n\n* update convertion script\n\n* update docs + cleanup\n\n* Ruff fix\n\n* fix doctrings",
    "sha": "3f4c85fef06549d8e5f11b1931f178419e1913cd",
    "files": [
        {
            "sha": "83751acb99f351d13ca270c89c7509b667e9f95a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -693,6 +693,8 @@\n         title: UL2\n       - local: model_doc/umt5\n         title: UMT5\n+      - local: model_doc/xcodec\n+        title: X-CODEC\n       - local: model_doc/xmod\n         title: X-MOD\n       - local: model_doc/xglm"
        },
        {
            "sha": "959c6b71207aad5de74a82eaf84bec0f2826d0c7",
            "filename": "docs/source/en/model_doc/xcodec.md",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxcodec.md?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,93 @@\n+<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# X-Codec\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+The X-Codec model was proposed in [Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model](https://arxiv.org/abs/2408.17175) by Zhen Ye,Â Peiwen Sun,Â Jiahe Lei,Â Hongzhan Lin,Â Xu Tan,Â Zheqi Dai,Â Qiuqiang Kong,Â Jianyi Chen,Â Jiahao Pan,Â Qifeng Liu,Â Yike Guo,Â Wei Xue\n+\n+The X-Codec model is a neural audio codec that integrates semantic information from self-supervised models (e.g., HuBERT) alongside traditional acoustic information. This enables :\n+\n+- **Music continuation** : Better modeling of musical semantics yields more coherent continuations.\n+- **Text-to-Sound Synthesis** : X-Codec captures semantic alignment between text prompts and generated audio.\n+- **Semantic aware audio tokenization**: X-Codec is used as an audio tokenizer in the YuE lyrics to song generation model.\n+\n+The abstract of the paper states the following:\n+\n+*Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation.* \n+\n+Demos can be found in this [post](https://x-codec-audio.github.io/).\n+\n+\n+This model was contributed byÂ [Manal El Aidouni](https://huggingface.co/Manel). The original code can be foundÂ [here](https://github.com/zhenye234/xcodec) and original checkpoint [here](https://huggingface.co/ZhenYe234/xcodec/blob/main/xcodec_speech_hubert_librispeech.pth).\n+\n+\n+\n+## Usage example \n+\n+Here is a quick example of how to encode and decode an audio using this model:\n+\n+```python \n+from datasets import load_dataset, Audio\n+from transformers import XcodecModel, AutoFeatureExtractor\n+dummy_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+\n+# load model and feature extractor\n+model = XcodecModel.from_pretrained(\"Manel/X-Codec\")\n+feature_extractor = AutoFeatureExtractor.from_pretrained(\"Manel/X-Codec\")\n+# load audio sample\n+dummy_dataset = dummy_dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n+audio_sample = dummy_dataset[-1][\"audio\"][\"array\"]\n+inputs = feature_extractor(raw_audio=audio_sample, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n+\n+encoder_outputs = model.encode(inputs[\"input_values\"])\n+decoder_outputs = model.decode(encoder_outputs.audio_codes)\n+audio_values = decoder_outputs.audio_values\n+\n+# or the equivalent with a forward pass\n+audio_values = model(inputs[\"input_values\"]).audio_values\n+\n+```\n+To listen to the original and reconstructed audio, run the snippet below and then open the generated `original.wav` and `reconstruction.wav` files in your music player to compare.\n+\n+```python\n+import soundfile as sf\n+\n+original = audio_sample\n+reconstruction = audio_values[0].cpu().detach().numpy()\n+sampling_rate = feature_extractor.sampling_rate\n+\n+sf.write(\"original.wav\", original, sampling_rate)\n+sf.write(\"reconstruction.wav\", reconstruction.T, sampling_rate)\n+```\n+\n+\n+## XcodecConfig\n+\n+[[autodoc]] XcodecConfig\n+\n+\n+## XcodecModel\n+\n+[[autodoc]] XcodecModel\n+    - decode\n+    - encode\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "66f9da16b71d66a09a6db7c915e185de7b75b3ff",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -357,6 +357,7 @@\n     from .wavlm import *\n     from .whisper import *\n     from .x_clip import *\n+    from .xcodec import *\n     from .xglm import *\n     from .xlm import *\n     from .xlm_roberta import *"
        },
        {
            "sha": "92d1ea0555d84feaa4fadd852c1604b9d33d94b3",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -416,6 +416,7 @@\n         (\"wavlm\", \"WavLMConfig\"),\n         (\"whisper\", \"WhisperConfig\"),\n         (\"xclip\", \"XCLIPConfig\"),\n+        (\"xcodec\", \"XcodecConfig\"),\n         (\"xglm\", \"XGLMConfig\"),\n         (\"xlm\", \"XLMConfig\"),\n         (\"xlm-prophetnet\", \"XLMProphetNetConfig\"),\n@@ -848,6 +849,7 @@\n         (\"wavlm\", \"WavLM\"),\n         (\"whisper\", \"Whisper\"),\n         (\"xclip\", \"X-CLIP\"),\n+        (\"xcodec\", \"X-CODEC\"),\n         (\"xglm\", \"XGLM\"),\n         (\"xlm\", \"XLM\"),\n         (\"xlm-prophetnet\", \"XLM-ProphetNet\"),"
        },
        {
            "sha": "f4666c7c56f9bab2bc9d69739733e9df3b92b9ce",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -115,6 +115,7 @@\n         (\"wavlm\", \"Wav2Vec2FeatureExtractor\"),\n         (\"whisper\", \"WhisperFeatureExtractor\"),\n         (\"xclip\", \"CLIPFeatureExtractor\"),\n+        (\"xcodec\", \"EncodecFeatureExtractor\"),\n         (\"yolos\", \"YolosFeatureExtractor\"),\n     ]\n )"
        },
        {
            "sha": "3d6d4eaea78777109f8d236702de550987a30e1b",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -396,6 +396,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"wavlm\", \"WavLMModel\"),\n         (\"whisper\", \"WhisperModel\"),\n         (\"xclip\", \"XCLIPModel\"),\n+        (\"xcodec\", \"XcodecModel\"),\n         (\"xglm\", \"XGLMModel\"),\n         (\"xlm\", \"XLMModel\"),\n         (\"xlm-prophetnet\", \"XLMProphetNetModel\"),"
        },
        {
            "sha": "45e7620f54d7a29b10507afb7c4beca0cdb9b43a",
            "filename": "src/transformers/models/xcodec/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2F__init__.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_xcodec import *\n+    from .modeling_xcodec import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "45bd1077af2e19344662e9d8bae628a64bc5672e",
            "filename": "src/transformers/models/xcodec/configuration_xcodec.py",
            "status": "added",
            "additions": 186,
            "deletions": 0,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,186 @@\n+# coding=utf-8\n+# Copyright 2024 Descript and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Xcodec model configuration\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from transformers import DacConfig, HubertConfig\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class XcodecConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of an [`XcodecModel`]. It is used to instantiate a\n+    Xcodec model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the\n+    [Manel/X-Codec](https://huggingface.co/Manel/X-Codec) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        target_bandwidths (`List[float]`, *optional*, defaults to `[0.5, 1, 1.5, 2, 4]`):\n+            The range of different bandwidths (in kbps) the model can encode audio with.\n+        audio_channels (`int`, *optional*, defaults to 1):\n+            Number of channels in the audio data. Either 1 for mono or 2 for stereo.\n+        sample_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate at which the audio waveform should be digitalized, in hertz (Hz).\n+        input_channels (`int`, *optional*, defaults to 768):\n+            Number of channels of the input to the first convolution in the semantic encoder.\n+        encoder_channels (`int`, *optional*, defaults to 768):\n+            Number of hidden channels in each semantic encoder block.\n+        kernel_size (`int`, *optional*, defaults to 3):\n+            Kernel size for the initial semantic convolution.\n+        channel_ratios (`List[float]`, *optional*, defaults to `[1, 1]`):\n+            Expansion factors for the number of output channels in each semantic block.\n+        strides (`List[int]`, *optional*, defaults to `[1, 1]`):\n+            Strides for each semantic encoder block.\n+        block_dilations (`List[int]`, *optional*, defaults to `[1, 1]`):\n+            Dilation factors for the residual units in semantic blocks.\n+        unit_kernel_size (`int`, *optional*, defaults to 3):\n+            Kernel size inside each ResidualUnit in semantic blocks.\n+        decoder_channels (`int`, *optional*, defaults to 768):\n+            Number of hidden channels in each semantic decoder block.\n+        output_channels (`int`, *optional*, defaults to 768):\n+            Number of output channels in the semantic decoder.\n+        codebook_size (`int`, *optional*, defaults to 1024):\n+            Number of entries in each residual quantizerâ€™s codebook.\n+        num_quantizers (`int`, *optional*, defaults to 8):\n+            Number of sequential quantizers (codebooks) in the RVQ stack.\n+        codebook_dim (`int`, *optional*, defaults to 1024):\n+            Dimensionality of each codebook vector.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation of the truncated normal initializer for all weight matrices.\n+        hidden_dim (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the joint acoustic+semantic FC layer.\n+        intermediate_dim (`int`, *optional*, defaults to 768):\n+            Dimensionality of the next FC layer in the decoder path.\n+        output_dim (`int`, *optional*, defaults to 256):\n+            Dimensionality of the final FC layer before feeding into the acoustic decoder.\n+        acoustic_model_config (`Union[Dict, DacConfig]`, *optional*):\n+            An instance of the configuration for the acoustic (DAC) model.\n+        semantic_model_config (`Union[Dict, HubertConfig]`, *optional*):\n+            An instance of the configuration object for the semantic (HuBERT) model.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import XcodecModel, XcodecConfig\n+\n+    >>> # Initializing a \" \" style configuration\n+    >>> configuration = XcodecConfig()\n+\n+    >>> # Initializing a model (with random weights) from the \" \" style configuration\n+    >>> model = XcodecModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"xcodec\"\n+\n+    sub_configs = {\n+        \"acoustic_model_config\": DacConfig,\n+        \"semantic_model_config\": HubertConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        target_bandwidths: Optional[list[float]] = None,\n+        audio_channels: int = 1,\n+        sample_rate: int = 16000,\n+        input_channels: int = 768,\n+        encoder_channels: int = 768,\n+        kernel_size: int = 3,\n+        channel_ratios: list[float] = [1, 1],\n+        strides: list[int] = [1, 1],\n+        block_dilations: list[int] = [1, 1],\n+        unit_kernel_size: int = 3,\n+        decoder_channels: int = 768,\n+        output_channels: int = 768,\n+        codebook_size: int = 1024,\n+        num_quantizers: int = 8,\n+        codebook_dim: int = 1024,\n+        initializer_range: float = 0.02,\n+        hidden_dim: int = 1024,\n+        intermediate_dim: int = 768,\n+        output_dim: int = 256,\n+        acoustic_model_config: Union[dict, DacConfig] = None,\n+        semantic_model_config: Union[dict, HubertConfig] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if acoustic_model_config is None:\n+            self.acoustic_model_config = DacConfig(\n+                encoder_hidden_size=64,\n+                downsampling_ratios=[8, 5, 4, 2],\n+                decoder_hidden_size=1024,\n+                upsampling_ratios=[8, 5, 4, 2],\n+                hidden_size=256,\n+            )\n+        elif isinstance(acoustic_model_config, dict):\n+            self.acoustic_model_config = DacConfig(**acoustic_model_config)\n+        elif isinstance(acoustic_model_config, DacConfig):\n+            self.acoustic_model_config = acoustic_model_config\n+\n+        if semantic_model_config is None:\n+            self.semantic_model_config = HubertConfig()\n+        elif isinstance(semantic_model_config, dict):\n+            self.semantic_model_config = HubertConfig(**semantic_model_config)\n+        elif isinstance(semantic_model_config, HubertConfig):\n+            self.semantic_model_config = semantic_model_config\n+\n+        if target_bandwidths is None:\n+            target_bandwidths = [0.5, 1, 1.5, 2, 4]\n+\n+        self.target_bandwidths = target_bandwidths\n+        self.audio_channels = audio_channels\n+        self.sample_rate = sample_rate\n+        self.input_channels = input_channels\n+        self.encoder_channels = encoder_channels\n+        self.kernel_size = kernel_size\n+        self.channel_ratios = channel_ratios\n+        self.strides = strides\n+        self.block_dilations = block_dilations\n+        self.unit_kernel_size = unit_kernel_size\n+        self.decoder_channels = decoder_channels\n+        self.output_channels = output_channels\n+        self.codebook_size = codebook_size\n+        self.num_quantizers = num_quantizers\n+        self.codebook_dim = codebook_dim\n+        self.initializer_range = initializer_range\n+        self.hidden_dim = hidden_dim\n+        self.intermediate_dim = intermediate_dim\n+        self.output_dim = output_dim\n+\n+    @property\n+    def frame_rate(self) -> int:\n+        return math.ceil(self.sample_rate / np.prod(self.acoustic_model_config.upsampling_ratios))\n+\n+    @property\n+    def hop_length(self) -> int:\n+        return int(np.prod(self.acoustic_model_config.downsampling_ratios))\n+\n+\n+__all__ = [\"XcodecConfig\"]"
        },
        {
            "sha": "30783234fc2b87584d8ac95b8fa9f7b4ec9dd212",
            "filename": "src/transformers/models/xcodec/convert_xcodec_weights_to_hf.py",
            "status": "added",
            "additions": 241,
            "deletions": 0,
            "changes": 241,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,241 @@\n+# coding=utf-8\n+# Copyright 2024 Descript and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import io\n+import re\n+\n+import torch\n+\n+from transformers import (\n+    EncodecFeatureExtractor,\n+    XcodecConfig,\n+    XcodecModel,\n+    logging,\n+)\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+torch.serialization.add_safe_globals([io.BytesIO])\n+\n+MAPPING_ACOUSTIC_ENCODER = {\n+    r\"^block\\.0\": [\"conv1\"],\n+    r\"^block\\.(\\d+)\\.block\\.(\\d+)\\.block\\.0\": [\"block\", \"res_unit\", \"snake1\"],\n+    r\"^block\\.(\\d+)\\.block\\.(\\d+)\\.block\\.1\": [\"block\", \"res_unit\", \"conv1\"],\n+    r\"^block\\.(\\d+)\\.block\\.(\\d+)\\.block\\.2\": [\"block\", \"res_unit\", \"snake2\"],\n+    r\"^block\\.(\\d+)\\.block\\.(\\d+)\\.block\\.3\": [\"block\", \"res_unit\", \"conv2\"],\n+    r\"^block\\.(\\d+)\\.block\\.3\": [\"block\", \"snake1\"],\n+    r\"^block\\.(\\d+)\\.block\\.4\": [\"block\", \"conv1\"],\n+    r\"^block\\.5\": [\"snake1\"],\n+    r\"^block\\.6\": [\"conv2\"],\n+}\n+\n+MAPPING_ACOUSTIC_DECODER = {\n+    r\"^model\\.0\": [\"conv1\"],\n+    r\"^model\\.(\\d+)\\.block\\.0\": [\"block\", \"snake1\"],\n+    r\"^model\\.(\\d+)\\.block\\.1\": [\"block\", \"conv_t1\"],\n+    r\"^model\\.(\\d+)\\.block\\.(\\d+)\\.block\\.0\": [\"block\", \"res_unit\", \"snake1\"],\n+    r\"^model\\.(\\d+)\\.block\\.(\\d+)\\.block\\.1\": [\"block\", \"res_unit\", \"conv1\"],\n+    r\"^model\\.(\\d+)\\.block\\.(\\d+)\\.block\\.2\": [\"block\", \"res_unit\", \"snake2\"],\n+    r\"^model\\.(\\d+)\\.block\\.(\\d+)\\.block\\.3\": [\"block\", \"res_unit\", \"conv2\"],\n+    r\"^model\\.5\": [\"snake1\"],\n+    r\"^model\\.6\": [\"conv2\"],\n+}\n+\n+MAPPING_SEMANTIC_ENCODER = {\n+    \"conv.conv.\": \"conv.\",\n+    \"conv1.conv.\": \"conv1.\",\n+    \"conv2.conv.\": \"conv2.\",\n+}\n+\n+MAPPING_SEMANTIC_DECODER = {\n+    \"conv1.conv.\": \"conv1.\",\n+    \"conv2.conv.\": \"conv2.\",\n+    \"conv.conv.\": \"conv.\",\n+}\n+\n+MAPPING_QUANTIZER = {\n+    \"quantizer.vq.layers\": \"quantizer.quantizers\",\n+    \"._codebook.\": \".codebook.\",\n+}\n+\n+\n+def safe_load(path: str) -> dict[str, torch.Tensor]:\n+    \"\"\"\n+    Load only the tensor objects from a checkpoint, skipping any BytesIO\n+    \"\"\"\n+    shard = torch.load(path, map_location=\"cpu\", weights_only=True)\n+    return {k: v for k, v in shard.items() if not isinstance(v, io.BytesIO)}\n+\n+\n+def _rewrite_weight_norm(key: str) -> str:\n+    if key.endswith(\"weight_g\"):\n+        return key[: -len(\"weight_g\")] + \"parametrizations.weight.original0\"\n+    if key.endswith(\"weight_v\"):\n+        return key[: -len(\"weight_v\")] + \"parametrizations.weight.original1\"\n+    return key\n+\n+\n+def convert_old_keys_to_new_keys(original_state_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n+    converted_checkpoint: dict[str, torch.Tensor] = {}\n+\n+    for old_key, value in original_state_dict.items():\n+        if old_key.startswith(\"encoder.\"):\n+            layer_key = old_key[len(\"encoder.\") :]\n+            for pattern, path_parts in MAPPING_ACOUSTIC_ENCODER.items():\n+                pattern_match = re.match(pattern, layer_key)\n+                if pattern_match is None:\n+                    continue\n+\n+                digit_strings = [g for g in pattern_match.groups() if g is not None]\n+                digit_indices = [int(ds) for ds in digit_strings]\n+                remainder = layer_key[pattern_match.end() :]\n+\n+                if len(path_parts) == 1:\n+                    mapped_subkey = f\"{path_parts[0]}{remainder}\"\n+                elif len(path_parts) == 2:\n+                    encoder_layer = digit_indices[0] - 1\n+                    mapped_subkey = f\"{path_parts[0]}.{encoder_layer}.{path_parts[1]}{remainder}\"\n+                else:\n+                    encoder_layer, unit_idx = digit_indices\n+                    mapped_subkey = (\n+                        f\"{path_parts[0]}.{encoder_layer - 1}.{path_parts[1]}{unit_idx + 1}.{path_parts[2]}{remainder}\"\n+                    )\n+\n+                new_key = f\"acoustic_encoder.{_rewrite_weight_norm(mapped_subkey)}\"\n+                converted_checkpoint[new_key] = value\n+                break\n+\n+        elif old_key.startswith(\"decoder_2.\"):\n+            layer_key = old_key[len(\"decoder_2.\") :]\n+\n+            for pattern, path_parts in MAPPING_ACOUSTIC_DECODER.items():\n+                pattern_match = re.match(pattern, layer_key)\n+                if pattern_match is None:\n+                    continue\n+                digit_strings = [g for g in pattern_match.groups() if g is not None]\n+                digit_indices = [int(ds) for ds in digit_strings]\n+                remainder = layer_key[pattern_match.end() :]\n+\n+                if len(path_parts) == 1:\n+                    mapped_subkey = f\"{path_parts[0]}{remainder}\"\n+                elif len(path_parts) == 2:\n+                    decoder_layer = digit_indices[0] - 1\n+                    mapped_subkey = f\"{path_parts[0]}.{decoder_layer}.{path_parts[1]}{remainder}\"\n+                else:\n+                    decoder_layer, unit_idx = digit_indices\n+                    mapped_subkey = (\n+                        f\"{path_parts[0]}.{decoder_layer - 1}.{path_parts[1]}{unit_idx - 1}.{path_parts[2]}{remainder}\"\n+                    )\n+                new_key = f\"acoustic_decoder.{_rewrite_weight_norm(mapped_subkey)}\"\n+                converted_checkpoint[new_key] = value\n+                break\n+\n+        elif old_key.startswith(\"encoder_semantic.\"):\n+            semantic_key = old_key[len(\"encoder_semantic.\") :]\n+            for old, new in MAPPING_SEMANTIC_ENCODER.items():\n+                semantic_key = semantic_key.replace(old, new)\n+            converted_checkpoint[f\"encoder_semantic.{semantic_key}\"] = value\n+\n+        elif old_key.startswith(\"decoder_semantic.\"):\n+            semantic_key = old_key[len(\"decoder_semantic.\") :]\n+            for old, new in MAPPING_SEMANTIC_DECODER.items():\n+                semantic_key = semantic_key.replace(old, new)\n+            converted_checkpoint[f\"decoder_semantic.{semantic_key}\"] = value\n+\n+        elif old_key.startswith(\"semantic_model.\"):\n+            converted_checkpoint[old_key] = value\n+\n+        elif old_key.startswith(\"fc_prior.\"):\n+            converted_checkpoint[f\"fc.{old_key[len('fc_prior.') :]}\"] = value\n+\n+        elif old_key.startswith(\"fc_post1.\"):\n+            converted_checkpoint[f\"fc1.{old_key[len('fc_post1.') :]}\"] = value\n+\n+        elif old_key.startswith(\"fc_post2.\"):\n+            converted_checkpoint[f\"fc2.{old_key[len('fc_post2.') :]}\"] = value\n+\n+        elif old_key.startswith(\"quantizer.vq.layers\"):\n+            new_key = old_key\n+            for old_sub, new_sub in MAPPING_QUANTIZER.items():\n+                new_key = new_key.replace(old_sub, new_sub)\n+            converted_checkpoint[new_key] = value\n+\n+    return converted_checkpoint\n+\n+\n+@torch.no_grad()\n+def convert_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_path=None, push_to_hub=None):\n+    if config_path is not None:\n+        config = XcodecConfig.from_pretrained(config_path)\n+    else:\n+        config = XcodecConfig()\n+\n+    with torch.device(\"meta\"):\n+        model = XcodecModel(config)\n+\n+    logger.info(\"Loading original checkpoint ...\")\n+\n+    state_dict = safe_load(checkpoint_path)\n+\n+    # the original checkpoint has weight norm applied\n+    model.apply_weight_norm()\n+\n+    logger.info(\"Converting model ...\")\n+\n+    new_state_dict = convert_old_keys_to_new_keys(state_dict)\n+\n+    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=True, assign=True)  # strict=False)\n+\n+    if len(unexpected_keys) != 0:\n+        raise ValueError(f\"Unexpected keys: {unexpected_keys}\")\n+\n+    if len(missing_keys) != 0:\n+        raise ValueError(f\"missing keys found: {missing_keys}\")\n+\n+    model.remove_weight_norm()\n+\n+    model.save_pretrained(pytorch_dump_folder_path)\n+\n+    feature_extractor = EncodecFeatureExtractor(feature_size=config.audio_channels, sampling_rate=config.sample_rate)\n+\n+    feature_extractor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        print(\"Pushing to the hub...\")\n+        feature_extractor.push_to_hub(push_to_hub)\n+        model.push_to_hub(push_to_hub)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--checkpoint_path\", required=True, default=None, type=str, help=\"Path to original checkpoint\")\n+    parser.add_argument(\"--config_path\", default=None, type=str, help=\"Path to hf config.json of model to convert\")\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ðŸ¤— hub.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_checkpoint(\n+        args.checkpoint_path,\n+        args.pytorch_dump_folder_path,\n+        args.config_path,\n+        args.push_to_hub,\n+    )"
        },
        {
            "sha": "a62d0d2952bf0818b6f0eaeca76a684e8e991a19",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "added",
            "additions": 578,
            "deletions": 0,
            "changes": 578,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,578 @@\n+# coding=utf-8\n+# Copyright 2024 Descript and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Transformers Xcodec model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring\n+from ..auto import AutoModel\n+from .configuration_xcodec import XcodecConfig\n+\n+\n+@dataclass\n+class XcodecOutput(ModelOutput):\n+    \"\"\"\n+    Args:\n+        audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n+            Discrete code indices computed using `model.encode`.\n+        audio_values (`torch.FloatTensor` of shape `(batch_size, channels, num_samples)`, *optional*)\n+            Decoded audio values obtained using the decoder part of Xcodec.\n+    \"\"\"\n+\n+    audio_codes: Optional[torch.LongTensor] = None\n+    audio_values: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+class XcodecEncoderOutput(ModelOutput):\n+    \"\"\"\n+    Args:\n+        audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n+            Discrete code indices computed using `model.encode`.\n+    \"\"\"\n+\n+    audio_codes: Optional[torch.LongTensor] = None\n+\n+\n+@dataclass\n+class XcodecDecoderOutput(ModelOutput):\n+    \"\"\"\n+    Args:\n+        audio_values (`torch.FloatTensor`  of shape `(batch_size, channels, num_samples)`, *optional*):\n+            Decoded audio values obtained using the decoder part of Xcodec.\n+    \"\"\"\n+\n+    audio_values: Optional[torch.FloatTensor] = None\n+\n+\n+class ResidualUnit(nn.Module):\n+    \"\"\"Residual block for SemanticEncoder and SemanticDecoder used in Xcodec.\"\"\"\n+\n+    def __init__(self, config: XcodecConfig, in_channels: int, out_channels: int, dilation: int):\n+        super().__init__()\n+        self.activation = nn.ELU()\n+        padding = ((config.unit_kernel_size - 1) // 2) * dilation\n+        self.conv1 = nn.Conv1d(\n+            in_channels,\n+            out_channels,\n+            config.unit_kernel_size,\n+            stride=1,\n+            padding=padding,\n+            dilation=dilation,\n+            groups=1,\n+            bias=False,\n+        )\n+        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=1, bias=False)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        output_tensor = self.activation(hidden_state)\n+        output_tensor = self.conv1(output_tensor)\n+        output_tensor = self.activation(output_tensor)\n+        output_tensor = self.conv2(output_tensor)\n+        return hidden_state + output_tensor\n+\n+\n+class SemanticEncoderBlock(nn.Module):\n+    def __init__(self, config: XcodecConfig, in_channels: int, out_channels: int, stride: int):\n+        super().__init__()\n+        self.res_units = nn.ModuleList(\n+            [ResidualUnit(config, in_channels, in_channels, dilation) for dilation in config.block_dilations]\n+        )\n+\n+        # special case: stride=1, do not use kernel=2\n+        kernel = 3 if stride == 1 else (2 * stride)\n+        padding = (kernel - 1) // 2\n+        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=padding, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        for unit in self.res_units:\n+            hidden_state = unit(hidden_state)\n+        hidden_state = self.conv(hidden_state)\n+        return hidden_state\n+\n+\n+class SemanticEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        if len(config.strides) != len(config.channel_ratios):\n+            raise ValueError(\"Number of strides must match the number of channel_ratios.\")\n+\n+        self.conv = nn.Conv1d(\n+            config.input_channels, config.encoder_channels, config.kernel_size, 1, config.kernel_size // 2, bias=False\n+        )\n+\n+        in_channels = config.encoder_channels\n+        conv_blocks = []\n+        for i, stride in enumerate(config.strides):\n+            out_channels = int(config.encoder_channels * config.channel_ratios[i])\n+            conv_blocks += [SemanticEncoderBlock(config, in_channels, out_channels, stride)]\n+            in_channels = out_channels\n+\n+        self.conv_blocks = nn.ModuleList(conv_blocks)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.conv(hidden_state)\n+        for block in self.conv_blocks:\n+            hidden_state = block(hidden_state)\n+        return hidden_state\n+\n+\n+class SemanticDecoderBlock(nn.Module):\n+    def __init__(self, config: XcodecConfig, in_channels: int, out_channels: int, stride: int):\n+        super().__init__()\n+        if stride == 1:\n+            self.conv = nn.Conv1d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=3,\n+                stride=1,\n+                padding=1,\n+                bias=True,\n+            )\n+        else:\n+            kernel_size = 2 * stride\n+            padding = (stride + 1) // 2\n+            output_padding = 1 if stride % 2 == 1 else 0\n+            self.conv = nn.ConvTranspose1d(\n+                in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=False\n+            )\n+\n+        self.res_units = nn.ModuleList(\n+            [ResidualUnit(config, out_channels, out_channels, dilation) for dilation in config.block_dilations]\n+        )\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.conv(hidden_state)\n+        for unit in self.res_units:\n+            hidden_state = unit(hidden_state)\n+        return hidden_state\n+\n+\n+class SemanticDecoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv1 = nn.Conv1d(\n+            in_channels=config.decoder_channels,\n+            out_channels=int(config.decoder_channels * config.channel_ratios[0]),\n+            kernel_size=config.kernel_size,\n+            stride=1,\n+            padding=config.kernel_size // 2,\n+            bias=False,\n+        )\n+        conv_blocks = []\n+        for i, stride in enumerate(config.strides):\n+            in_channels = int(config.decoder_channels * config.channel_ratios[i])\n+\n+            if i < (len(config.channel_ratios) - 1):\n+                out_channels = int(config.decoder_channels * config.channel_ratios[i + 1])\n+            else:\n+                out_channels = config.decoder_channels\n+\n+            conv_blocks += [SemanticDecoderBlock(config, in_channels, out_channels, stride)]\n+\n+        self.conv_blocks = nn.ModuleList(conv_blocks)\n+        self.conv2 = nn.Conv1d(\n+            config.decoder_channels,\n+            config.output_channels,\n+            config.kernel_size,\n+            stride=1,\n+            padding=config.kernel_size // 2,\n+            bias=False,\n+        )\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.conv1(hidden_state)\n+        for block in self.conv_blocks:\n+            hidden_state = block(hidden_state)\n+        hidden_state = self.conv2(hidden_state)\n+        return hidden_state\n+\n+\n+class XcodecEuclideanCodebook(nn.Module):\n+    \"\"\"Codebook with Euclidean distance.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        embed = torch.zeros(config.codebook_size, config.codebook_dim)\n+        self.codebook_size = config.codebook_size\n+        self.register_buffer(\"inited\", torch.Tensor([True]))\n+        self.register_buffer(\"cluster_size\", torch.zeros(config.codebook_size))\n+        self.register_buffer(\"embed\", embed)\n+        self.register_buffer(\"embed_avg\", embed.clone())\n+\n+    # Copied from transformers.models.encodec.modeling_encodec.EncodecEuclideanCodebook.quantize\n+    def quantize(self, hidden_states):\n+        embed = self.embed.t()\n+        scaled_states = hidden_states.pow(2).sum(1, keepdim=True)\n+        dist = -(scaled_states - 2 * hidden_states @ embed + embed.pow(2).sum(0, keepdim=True))\n+        embed_ind = dist.max(dim=-1).indices\n+        return embed_ind\n+\n+    def encode(self, hidden_states):\n+        shape = hidden_states.shape\n+        hidden_states = hidden_states.reshape((-1, shape[-1]))\n+        embed_ind = self.quantize(hidden_states)\n+        embed_ind = embed_ind.view(*shape[:-1])\n+        return embed_ind\n+\n+    def decode(self, embed_ind):\n+        quantized = F.embedding(embed_ind, self.embed)\n+        return quantized\n+\n+\n+class XcodecVectorQuantization(nn.Module):\n+    \"\"\"\n+    Vector quantization implementation. Currently supports only euclidean distance.\n+    \"\"\"\n+\n+    def __init__(self, config: XcodecConfig):\n+        super().__init__()\n+        self.codebook = XcodecEuclideanCodebook(config)\n+\n+    # Copied from transformers.models.encodec.modeling_encodec.EncodecVectorQuantization.encode\n+    def encode(self, hidden_states):\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+        embed_in = self.codebook.encode(hidden_states)\n+        return embed_in\n+\n+    # Copied from transformers.models.encodec.modeling_encodec.EncodecVectorQuantization.decode\n+    def decode(self, embed_ind):\n+        quantize = self.codebook.decode(embed_ind)\n+        quantize = quantize.permute(0, 2, 1)\n+        return quantize\n+\n+\n+class XcodecResidualVectorQuantization(nn.Module):\n+    \"\"\"\n+    Residual vector quantization implementation. Follows Algorithm 1 in https://arxiv.org/pdf/2107.03312.pdf\n+    \"\"\"\n+\n+    def __init__(self, config: XcodecConfig):\n+        super().__init__()\n+        self.quantizers = nn.ModuleList([XcodecVectorQuantization(config) for _ in range(config.num_quantizers)])\n+        self.frame_rate = config.frame_rate\n+        self.codebook_size = config.codebook_size\n+        self.num_quantizers = config.num_quantizers\n+\n+    def get_bandwidth_per_quantizer(self):\n+        \"\"\"Return bandwidth per quantizer.\"\"\"\n+        return math.log2(self.codebook_size) * self.frame_rate / 1000\n+\n+    def get_num_quantizers_for_bandwidth(self, bandwidth=None) -> int:\n+        \"\"\"Return num_quantizers based on specified target bandwidth.\"\"\"\n+        bw_per_q = self.get_bandwidth_per_quantizer()\n+        num_quantizers = self.num_quantizers\n+        if bandwidth is not None and bandwidth > 0.0:\n+            num_quantizers = int(max(1, math.floor(bandwidth / bw_per_q)))\n+        return num_quantizers\n+\n+    def encode(self, embeddings: torch.Tensor, bandwidth=None) -> torch.Tensor:\n+        \"\"\"\n+        Encode the input tensor into discrete indices using RVQ, with the number of quantizers selected based on the given bandwidth.\n+        Each quantizer /codebook residually quantizes the input and returns the nearest indices in terms of Euclidian distance.\n+        \"\"\"\n+        num_quantizers = self.get_num_quantizers_for_bandwidth(bandwidth)\n+        residual = embeddings\n+        all_indices = []\n+        for quantizer in self.quantizers[:num_quantizers]:\n+            indices = quantizer.encode(residual)\n+            quantized = quantizer.decode(indices)\n+            residual = residual - quantized\n+            all_indices.append(indices)\n+        out_indices = torch.stack(all_indices)\n+        return out_indices\n+\n+    def decode(self, codes: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Decode the given codes to their quantized representation.\"\"\"\n+        quantized_out = torch.tensor(0.0, device=codes.device)\n+        for i, indices in enumerate(codes):\n+            quantizer = self.quantizers[i]\n+            quantized = quantizer.decode(indices)\n+            quantized_out = quantized_out + quantized\n+        return quantized_out\n+\n+\n+@auto_docstring\n+class XcodecPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = XcodecConfig\n+    base_model_prefix = \"xcodec\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def apply_weight_norm(self):\n+        \"\"\"Apply weight norm in the acoustic encoder and decoder because the original checkpoint has weight norm applied.\"\"\"\n+        weight_norm = torch.nn.utils.weight_norm\n+        if hasattr(torch.nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = torch.nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.acoustic_encoder.conv1)\n+        weight_norm(self.acoustic_encoder.conv2)\n+\n+        for block in self.acoustic_encoder.block:\n+            weight_norm(block.conv1)\n+            for res_unit in (block.res_unit1, block.res_unit2, block.res_unit3):\n+                weight_norm(res_unit.conv1)\n+                weight_norm(res_unit.conv2)\n+\n+        weight_norm(self.acoustic_decoder.conv1, name=\"weight\")\n+        weight_norm(self.acoustic_decoder.conv2, name=\"weight\")\n+\n+        for block in self.acoustic_decoder.block:\n+            weight_norm(block.conv_t1, name=\"weight\")\n+            for res_unit in (block.res_unit1, block.res_unit2, block.res_unit3):\n+                weight_norm(res_unit.conv1, name=\"weight\")\n+                weight_norm(res_unit.conv2, name=\"weight\")\n+\n+    def remove_weight_norm(self):\n+        \"\"\"Remove the weight norm from the acoustic encoder and decoder.\"\"\"\n+        for module in (self.acoustic_encoder, self.acoustic_decoder):\n+            for m in module.modules():\n+                try:\n+                    torch.nn.utils.remove_weight_norm(m, name=\"weight\")\n+                except (ValueError, AttributeError):\n+                    pass\n+                if hasattr(m, \"parametrizations\") and \"weight\" in m.parametrizations:\n+                    torch.nn.utils.parametrize.remove_parametrizations(m, \"weight\", leave_parametrized=True)\n+\n+\n+@auto_docstring(custom_intro=\"\"\"The Xcodec neural audio codec model.\"\"\")\n+class XcodecModel(XcodecPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.config = config\n+        self.pad = config.hop_length // 2\n+        dac = AutoModel.from_config(config.acoustic_model_config)\n+        self.acoustic_encoder = dac.encoder\n+        self.acoustic_decoder = dac.decoder\n+        self._adjust_dac_decoder(self.acoustic_decoder)\n+        self.encoder_semantic = SemanticEncoder(config)\n+        self.decoder_semantic = SemanticDecoder(config)\n+        self.semantic_model = AutoModel.from_config(config.semantic_model_config)\n+        self.fc = nn.Linear(config.hidden_dim, config.hidden_dim)\n+        self.fc1 = nn.Linear(config.hidden_dim, config.intermediate_dim)\n+        self.fc2 = nn.Linear(config.hidden_dim, config.output_dim)\n+        self.quantizer = XcodecResidualVectorQuantization(config)\n+\n+    @staticmethod\n+    def _adjust_dac_decoder(decoder: nn.Module):\n+        r\"\"\"\n+        DAC implemented in Xcodec is slightly different from the HF version.\n+        DAC in Xcodec adjusts the output padding in every ConvTranspose1d in the decoder and removes\n+        the final `nn.Tanh` activation function.\n+        \"\"\"\n+        for module in decoder.modules():\n+            if isinstance(module, nn.ConvTranspose1d):\n+                stride = module.stride[0] if isinstance(module.stride, tuple) else module.stride\n+                module.output_padding = (stride % 2,)\n+        if hasattr(decoder, \"tanh\") and isinstance(decoder.tanh, nn.Tanh):\n+            decoder.tanh = nn.Identity()\n+\n+    def _extract_semantic_features(self, input_values: torch.FloatTensor) -> torch.FloatTensor:\n+        input_values = input_values[:, 0, :]\n+        input_values = F.pad(input_values, (self.pad, self.pad))\n+        with torch.no_grad():\n+            outputs = self.semantic_model(input_values, output_hidden_states=True)\n+            hidden_states = outputs.hidden_states\n+\n+        stacked = torch.stack(hidden_states, dim=1)\n+        return stacked.mean(dim=1)\n+\n+    @auto_docstring\n+    def encode(\n+        self,\n+        input_values: torch.Tensor,\n+        bandwidth: Optional[float] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[torch.Tensor, XcodecEncoderOutput]:\n+        \"\"\"\n+        Encodes the input audio waveform into discrete audio codes.\n+\n+        Args:\n+            input_values (`torch.FloatTensor` of shape `(batch_size, channels, num_samples)`):\n+                Float values of the input audio waveform.\n+            bandwidth (`float`, *optional*):\n+                The target bandwidth in (kbps) supports only values in `config.target_bandwidths`.\n+                Defaults to the highest available bandwidth `4.0` kbps.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`].\n+\n+        Returns:\n+            `torch.LongTensor` of shape `(batch_size, num_quantizers, codes_length)` containing the discrete encoded audio codes.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.return_dict\n+\n+        if input_values.ndim != 3:\n+            raise ValueError(\n+                f\"Expected input shape (batch_size, channels, num_samples), but got shape {input_values.shape}\"\n+            )\n+\n+        _, channels, self._input_length = input_values.shape\n+\n+        if channels not in (1, 2):\n+            raise ValueError(f\"Number of audio channels must be 1 or 2, but got {channels}\")\n+\n+        if bandwidth is None:\n+            bandwidth = self.config.target_bandwidths[-1]\n+        elif bandwidth not in self.config.target_bandwidths:\n+            raise ValueError(\n+                f\"This model doesn't support the bandwidth {bandwidth}. Select one of {self.config.target_bandwidths}.\"\n+            )\n+\n+        e_semantic_input = self._extract_semantic_features(input_values).detach()\n+        e_semantic = self.encoder_semantic(e_semantic_input.transpose(1, 2))\n+        e_acoustic = self.acoustic_encoder(input_values)\n+\n+        if e_acoustic.shape[2] != e_semantic.shape[2]:\n+            # make sure they line up if frames don't match\n+            e_acoustic = self.acoustic_encoder(F.pad(input_values[:, 0, :], (self.pad, self.pad)).unsqueeze(1))\n+\n+        embeddings = torch.cat([e_acoustic, e_semantic], dim=1)\n+        embeddings = self.fc(embeddings.transpose(1, 2)).transpose(1, 2)\n+        audio_codes = self.quantizer.encode(embeddings, bandwidth)\n+        audio_codes = audio_codes.transpose(0, 1)\n+\n+        if not return_dict:\n+            return audio_codes\n+\n+        return XcodecEncoderOutput(audio_codes)\n+\n+    @auto_docstring\n+    def decode(\n+        self, audio_codes: torch.Tensor, return_dict: Optional[bool] = None, **kwargs\n+    ) -> Union[torch.Tensor, XcodecDecoderOutput]:\n+        \"\"\"\n+        Decode the given discrete codes into an output audio waveform.\n+\n+        The produced audio waveform is longer than the audio input, so it's automatically trimmed to match the original input.\n+\n+        Args:\n+            audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`):\n+                Discrete code indices computed using `model.encode`.\n+\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`]\n+\n+        Returns:\n+            Decoded audio values of shape `(batch_size, channels, num_samples)` obtained using the decoder part of Xcodec.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.return_dict\n+\n+        audio_codes = audio_codes.transpose(0, 1)\n+        quantized = self.quantizer.decode(audio_codes)\n+        quantized_acoustic = self.fc2(quantized.transpose(1, 2)).transpose(1, 2)\n+        audio_values = self.acoustic_decoder(quantized_acoustic)\n+\n+        if getattr(self, \"_input_length\", None) is not None:\n+            output_length = audio_values.shape[-1]\n+            if self._input_length != output_length:\n+                extra = output_length - self._input_length\n+                start = extra // 2\n+                audio_values = audio_values[..., start : start + self._input_length]\n+\n+        if not return_dict:\n+            return audio_values\n+\n+        return XcodecDecoderOutput(audio_values)\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_values: torch.Tensor,\n+        audio_codes: Optional[torch.Tensor] = None,\n+        bandwidth: Optional[float] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[tuple[torch.Tensor, torch.Tensor], XcodecOutput]:\n+        r\"\"\"\n+        Encodes and quantizes the input audio into discrete codes, then decodes those codes back into an audio waveform.\n+        Args:\n+            input_values (`torch.FloatTensor` of shape `(batch_size, channels, num_samples)`):\n+                The raw float values of the input audio waveform.\n+            audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`:\n+                Discrete code indices computed using `model.encode`.\n+            bandwidth (`float`, *optional*):\n+                Target bandwidth in kbps. Must be one of `config.target_bandwidths`.\n+                Defaults to the highest available bandwidth.\n+            return_dict (`bool`, *optional*):\n+                Whether to return a [`XcodecOutput`] instead of a plain tuple.\n+\n+        Returns:\n+            `XcodecOutput` or tuple `(audio_codes, audio_values)`:\n+            - `audio_codes` of shape `(batch_size, num_quantizers, codes_length)`: the quantized discrete codes.\n+            - `audio_values` of shape `(batch_size, channels, num_samples)`: the reconstructed audio waveform given the codes.\n+\n+        Example:\n+\n+        ```python\n+        >>> from datasets import load_dataset\n+        >>> from transformers import AutoFeatureExtractor, XcodecModel\n+\n+        >>> dataset = load_dataset(\"hf-internal-testing/ashraq-esc50-1-dog-example\")\n+        >>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n+\n+        >>> model_id = \"Manel/X-Codec\"\n+        >>> model = XcodecModel.from_pretrained(model_id)\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n+\n+        >>> inputs = feature_extractor(raw_audio=audio_sample, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> audio_codes = outputs.audio_codes\n+        >>> audio_values = outputs.audio_values\n+        ```\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.return_dict\n+\n+        if audio_codes is None:\n+            audio_codes = self.encode(input_values, bandwidth, return_dict=False)\n+\n+        audio_values = self.decode(audio_codes, return_dict=return_dict)[0]\n+\n+        if not return_dict:\n+            return (audio_codes, audio_values)\n+\n+        return XcodecOutput(audio_codes=audio_codes, audio_values=audio_values)\n+\n+\n+__all__ = [\"XcodecModel\", \"XcodecPreTrainedModel\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/xcodec/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/tests%2Fmodels%2Fxcodec%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/tests%2Fmodels%2Fxcodec%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2F__init__.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd"
        },
        {
            "sha": "0d2a2aade59394ab7a82a2aca14bd17b2f1d02a8",
            "filename": "tests/models/xcodec/test_modeling_xcodec.py",
            "status": "added",
            "additions": 469,
            "deletions": 0,
            "changes": 469,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -0,0 +1,469 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Xcodec model.\"\"\"\n+\n+import inspect\n+import math\n+import os\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+from datasets import Audio, load_dataset\n+from pytest import mark\n+\n+from tests.test_configuration_common import ConfigTester\n+from tests.test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from transformers import AutoFeatureExtractor, XcodecConfig\n+from transformers.testing_utils import (\n+    is_flaky,\n+    is_torch_available,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import XcodecModel\n+\n+\n+@require_torch\n+class XcodecModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=4,\n+        num_channels=1,\n+        sample_rate=16000,\n+        codebook_size=1024,\n+        num_quantizers=8,\n+        num_samples=400,\n+        is_training=False,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.sample_rate = sample_rate\n+        self.codebook_size = codebook_size\n+        self.num_quantizers = num_quantizers\n+        self.is_training = is_training\n+        self.num_samples = num_samples\n+\n+    def prepare_config_and_inputs(self):\n+        input_values = floats_tensor([self.batch_size, self.num_channels, self.num_samples], scale=1.0)\n+        config = self.get_config()\n+        inputs_dict = {\"input_values\": input_values}\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, inputs_dict = self.prepare_config_and_inputs()\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_model_class(self, model_class):\n+        config, inputs_dict = self.prepare_config_and_inputs()\n+        codes_length = math.ceil(self.num_samples / config.hop_length)\n+        inputs_dict[\"audio_codes\"] = ids_tensor(\n+            [self.batch_size, self.num_quantizers, codes_length], config.codebook_size\n+        )\n+\n+        return config, inputs_dict\n+\n+    def get_config(self):\n+        return XcodecConfig(\n+            sample_rate=self.sample_rate,\n+            audio_channels=self.num_channels,\n+            codebook_size=self.codebook_size,\n+            num_quantizers=self.num_quantizers,\n+        )\n+\n+    def create_and_check_model_forward(self, config, inputs_dict):\n+        model = XcodecModel(config=config).to(torch_device).eval()\n+        input_values = inputs_dict[\"input_values\"]\n+        result = model(input_values)\n+        self.parent.assertEqual(result.audio_values.shape, (self.batch_size, self.num_channels, self.num_samples))\n+\n+\n+@require_torch\n+class XcodecModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (XcodecModel,) if is_torch_available() else ()\n+    is_encoder_decoder = True\n+    test_pruning = False\n+    test_headmasking = False\n+    test_resize_embeddings = False\n+    test_torchscript = False\n+    test_can_init_all_missing_weights = False\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        # model does not support returning hidden states\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+        if \"output_attentions\" in inputs_dict:\n+            inputs_dict.pop(\"output_attentions\")\n+        if \"output_hidden_states\" in inputs_dict:\n+            inputs_dict.pop(\"output_hidden_states\")\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = XcodecModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=XcodecConfig, hidden_size=37, common_properties=[], has_text_modality=False\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_forward(*config_and_inputs)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"input_values\", \"audio_codes\", \"bandwidth\", \"return_dict\"]\n+            self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+\n+    def test_gradient_checkpointing_backward_compatibility(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class.supports_gradient_checkpointing:\n+                continue\n+\n+            config.text_encoder.gradient_checkpointing = True\n+            config.audio_encoder.gradient_checkpointing = True\n+            config.decoder.gradient_checkpointing = True\n+            model = model_class(config)\n+            self.assertTrue(model.is_gradient_checkpointing)\n+\n+    @unittest.skip(\"XcodecModel cannot be tested with meta device\")\n+    def test_can_load_with_meta_device_context_manager(self):\n+        pass\n+\n+    @unittest.skip(reason=\"We cannot configure to output a smaller model.\")\n+    def test_model_is_small(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have `inputs_embeds` logics\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have `inputs_embeds` logics\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have the usual `attention` logic\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have the usual `attention` logic\")\n+    def test_torchscript_output_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have the usual `hidden_states` logic\")\n+    def test_torchscript_output_hidden_state(self):\n+        pass\n+\n+    # Copied from transformers.tests.encodec.test_modeling_encodec.XcodecModelTest._create_and_check_torchscript\n+    def _create_and_check_torchscript(self, config, inputs_dict):\n+        if not self.test_torchscript:\n+            self.skipTest(reason=\"test_torchscript is set to False\")\n+\n+        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n+        configs_no_init.torchscript = True\n+        configs_no_init.return_dict = False\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            model.to(torch_device)\n+            model.eval()\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            main_input_name = model_class.main_input_name\n+\n+            try:\n+                main_input = inputs[main_input_name]\n+                model(main_input)\n+                traced_model = torch.jit.trace(model, main_input)\n+            except RuntimeError:\n+                self.fail(\"Couldn't trace module.\")\n+\n+            with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n+\n+                try:\n+                    torch.jit.save(traced_model, pt_file_name)\n+                except Exception:\n+                    self.fail(\"Couldn't save module.\")\n+\n+                try:\n+                    loaded_model = torch.jit.load(pt_file_name)\n+                except Exception:\n+                    self.fail(\"Couldn't load module.\")\n+\n+            model.to(torch_device)\n+            model.eval()\n+\n+            loaded_model.to(torch_device)\n+            loaded_model.eval()\n+\n+            model_state_dict = model.state_dict()\n+            loaded_model_state_dict = loaded_model.state_dict()\n+\n+            non_persistent_buffers = {}\n+            for key in loaded_model_state_dict.keys():\n+                if key not in model_state_dict.keys():\n+                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n+\n+            loaded_model_state_dict = {\n+                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n+            }\n+\n+            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n+\n+            model_buffers = list(model.buffers())\n+            for non_persistent_buffer in non_persistent_buffers.values():\n+                found_buffer = False\n+                for i, model_buffer in enumerate(model_buffers):\n+                    if torch.equal(non_persistent_buffer, model_buffer):\n+                        found_buffer = True\n+                        break\n+\n+                self.assertTrue(found_buffer)\n+                model_buffers.pop(i)\n+\n+            model_buffers = list(model.buffers())\n+            for non_persistent_buffer in non_persistent_buffers.values():\n+                found_buffer = False\n+                for i, model_buffer in enumerate(model_buffers):\n+                    if torch.equal(non_persistent_buffer, model_buffer):\n+                        found_buffer = True\n+                        break\n+\n+                self.assertTrue(found_buffer)\n+                model_buffers.pop(i)\n+\n+            models_equal = True\n+            for layer_name, p1 in model_state_dict.items():\n+                if layer_name in loaded_model_state_dict:\n+                    p2 = loaded_model_state_dict[layer_name]\n+                    if p1.data.ne(p2.data).sum() > 0:\n+                        models_equal = False\n+\n+            self.assertTrue(models_equal)\n+\n+            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n+            # (Even with this call, there are still memory leak by ~0.04MB)\n+            self.clear_torch_jit_class_registry()\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have the usual `attention` logic\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have the usual `hidden_states` logic\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    # Copied from transformers.tests.encodec.test_modeling_encodecEncodecModelTest.test_determinism\n+    def test_determinism(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def check_determinism(first, second):\n+            # outputs are not tensors but list (since each sequence don't have the same frame_length)\n+            out_1 = first.cpu().numpy()\n+            out_2 = second.cpu().numpy()\n+            out_1 = out_1[~np.isnan(out_1)]\n+            out_2 = out_2[~np.isnan(out_2)]\n+            max_diff = np.amax(np.abs(out_1 - out_2))\n+            self.assertLessEqual(max_diff, 1e-5)\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                first = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n+                second = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n+\n+            if isinstance(first, tuple) and isinstance(second, tuple):\n+                for tensor1, tensor2 in zip(first, second):\n+                    check_determinism(tensor1, tensor2)\n+            else:\n+                check_determinism(first, second)\n+\n+    # Copied from transformers.tests.encodec.test_modeling_encodecEncodecModelTest.test_model_outputs_equivalence\n+    def test_model_outputs_equivalence(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def set_nan_tensor_to_zero(t):\n+            t[t != t] = 0\n+            return t\n+\n+        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n+            with torch.no_grad():\n+                tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)\n+                dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs)\n+\n+                self.assertTrue(isinstance(tuple_output, tuple))\n+                self.assertTrue(isinstance(dict_output, dict))\n+\n+                for tuple_value, dict_value in zip(tuple_output, dict_output.values()):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            set_nan_tensor_to_zero(tuple_value), set_nan_tensor_to_zero(dict_value), atol=1e-5\n+                        ),\n+                        msg=(\n+                            \"Tuple and dict output are not equal. Difference:\"\n+                            f\" {torch.max(torch.abs(tuple_value - dict_value))}. Tuple has `nan`:\"\n+                            f\" {torch.isnan(tuple_value).any()} and `inf`: {torch.isinf(tuple_value)}. Dict has\"\n+                            f\" `nan`: {torch.isnan(dict_value).any()} and `inf`: {torch.isinf(dict_value)}.\"\n+                        ),\n+                    )\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                # skipping the parametrizations original0 tensor\n+                if name == \"semantic_model.encoder.pos_conv_embed.conv.parametrizations.weight.original0\":\n+                    continue\n+\n+                uniform_init_parms = [\"conv\"]\n+\n+                if param.requires_grad:\n+                    if any(x in name for x in uniform_init_parms):\n+                        self.assertTrue(\n+                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n+                            msg=f\"Parameter {name} of {model_class.__name__} seems not properly initialized\",\n+                        )\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_2_inference_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model.main_input_name][:1]\n+                if dummy_input.dtype in [torch.float32, torch.float16]:\n+                    dummy_input = dummy_input.to(torch.bfloat16)\n+\n+                outputs = model(dummy_input)\n+                outputs_fa = model_fa(dummy_input)\n+\n+                logits = outputs[1]\n+                logits_fa = outputs_fa[1]\n+\n+                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+\n+    @unittest.skip(reason=\"The XcodecModel does not support right padding\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(reason=\"The XcodecModel does not have support dynamic compile yet\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+\n+# Copied from transformers.tests.encodec.test_modeling_encodec.normalize\n+def normalize(arr):\n+    norm = np.linalg.norm(arr)\n+    normalized_arr = arr / norm\n+    return normalized_arr\n+\n+\n+# Copied from transformers.tests.encodec.test_modeling_encodec.compute_rmse\n+def compute_rmse(arr1, arr2):\n+    arr1_normalized = normalize(arr1)\n+    arr2_normalized = normalize(arr2)\n+    return np.sqrt(((arr1_normalized - arr2_normalized) ** 2).mean())\n+\n+\n+# @slow\n+@require_torch\n+class XcodecIntegrationTest(unittest.TestCase):\n+    def test_integration(self):\n+        expected_rmse = {\n+            \"0.5\": 0.0065491,\n+            \"4.0\": 0.0070978,\n+        }\n+        expected_codesums = {\n+            \"0.5\": [117262],\n+            \"4.0\": [926416],\n+        }\n+\n+        librispeech = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        model_id = \"Manel/X-Codec\"\n+        model = XcodecModel.from_pretrained(model_id).to(torch_device).eval()\n+        feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n+\n+        librispeech = librispeech.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n+        audio = librispeech[-1][\"audio\"][\"array\"]\n+\n+        inputs = feature_extractor(\n+            raw_audio=audio, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        for bandwidth, exp_rmse in expected_rmse.items():\n+            bandwidth = float(bandwidth)\n+            with torch.no_grad():\n+                audio_codes = model.encode(inputs[\"input_values\"], bandwidth=bandwidth, return_dict=False)\n+                codesum = int(audio_codes.sum().item())\n+\n+                expected_codesum = expected_codesums[str(bandwidth)][0]\n+                self.assertEqual(codesum, expected_codesum)\n+\n+                input_values_dec = model.decode(audio_codes, return_dict=False)\n+                input_values_enc_dec = model(inputs[\"input_values\"], bandwidth=bandwidth)[1]\n+\n+            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+\n+            self.assertTrue(inputs[\"input_values\"].shape == input_values_enc_dec.shape)\n+\n+            arr = inputs[\"input_values\"][0].cpu().numpy()\n+            arr_enc_dec = input_values_enc_dec[0].cpu().numpy()\n+            rmse = compute_rmse(arr, arr_enc_dec)\n+            self.assertTrue(np.abs(rmse - exp_rmse) < 1e-5)"
        },
        {
            "sha": "34bf0511a320ddee2da0fbd6a63ac6455f3f9148",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f4c85fef06549d8e5f11b1931f178419e1913cd/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f4c85fef06549d8e5f11b1931f178419e1913cd/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=3f4c85fef06549d8e5f11b1931f178419e1913cd",
            "patch": "@@ -69,6 +69,8 @@\n     \"Phi3Config\": [\"embd_pdrop\"],\n     # used to compute the property `self.chunk_length`\n     \"EncodecConfig\": [\"overlap\"],\n+    # used to compute `frame_rate`\n+    \"XcodecConfig\": [\"sample_rate\", \"audio_channels\"],\n     # used to compute the property `self.layers_block_type`\n     \"RecurrentGemmaConfig\": [\"block_types\"],\n     # used as in the config to define `intermediate_size`"
        }
    ],
    "stats": {
        "total": 1603,
        "additions": 1603,
        "deletions": 0
    }
}